<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-05-19の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Knowledge-and-Data-Driven Amplitude Spectrum Prediction for Hierarchical
  Neural Vocoders -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.SD/paper_0.html">
      Knowledge-and-Data-Driven Amplitude Spectrum Prediction for Hierarchical
  Neural Vocoders
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このモジュールは、STFTとソースフィルター理論の組み合わせに基づいて設計されています。この場合、ソース部分とフィルター部分は、それぞれ入力F0とmel-cepstraに基づいて設計されます。実験結果では、KDD-ASPを使用するHiNetボコーダーが音声合成（TTS）タスクで従来のASPおよびWaveRNNボコーダーを使用するよりも高品質の合成音声を実現できます。以前の作業では、振幅を予測して音声波形を回復するHiNetと呼ばれるニューラルボコーダーを提案しました。入力音響特徴から階層的に位相スペクトル。 
[ABSTRACT]合成データは、データベースのデータベースのデータ改良に基づいています。hinetデータベースのデータドライバーは、最終的なデータを取得するために使用できます。データドライバーは、データドリブンレイヤーを使用するhinetに基づいています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-16">
        <br>2020-04-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Differentiable Perceptual Audio Metric Learned from Just Noticeable
  Differences -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.SD/paper_1.html">
      A Differentiable Perceptual Audio Metric Learned from Just Noticeable
  Differences
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      被験者は簡単で客観的な質問に答えるように求められます。2つの録音は同一かどうかです。これらのペアは、ノイズ、リバーブ、圧縮アーティファクトなど、さまざまな摂動の下でアルゴリズムによって生成されます。摂動空間は、対象のJust-Noticeable Difference（JND）レベルを効率的に識別することを目的として調査されます。したがって、既存の損失（たとえば、深い特徴の損失）を私たちのメトリックで単純に置き換えると、ノイズ除去ネットワークで大幅な改善が得られます。主観的ペアワイズ比較により測定。 
[要旨]ディープニューラルネットワークをクラウドソーシングされた人間の判断の新しいデータセットにフィッティングすることでメトリックを構築します。対象は、ノイズ、リバーブ、圧縮アーティファクトなど、さまざまな摂動の下でアルゴリズムによって生成されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-13">
        <br>2020-01-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Neural Spatio-Temporal Beamformer for Target Speech Separation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.SD/paper_2.html">
      Neural Spatio-Temporal Beamformer for Target Speech Separation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      純粋なニューラルネットワーク（NN）ベースの音声分離および強化方法は、良好な客観的スコアを達成できますが、必然的に自動音声認識（ASR）に有害な非線形音声歪みを引き起こします。この論文では、マルチタップMVDRビームフォーマーを提案します。音声分離と拡張のための複素数値マスクを使用します。さらに改善されたのは、実数値マスクを複素数値マスクで置き換えることと、複素マスクNNの共同トレーニングです。 
[要約] nn-予測マスクを備えた最小変動歪みなし応答（mvdr）ビームフォーマーは、音声歪みを大幅に削減できます。新しいシステムは、状態に基づいています-nn-マスクベースのmvdr
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-08">
        <br>2020-05-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Towards Robust Neural Vocoding for Speech Generation: A Survey -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.SD/paper_3.html">
      Towards Robust Neural Vocoding for Speech Generation: A Survey
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      普遍的なボコーダーを実現するには、言語よりも話者の多様性の方がはるかに重要であることがわかりました。すべてのボコーダーの自然さをもたらす主観的MOSの結果は、今後の研究で提示されます。 WaveNet、WaveRNN、FFTNet、Parallel WaveGANを5つの異なるデータセットに交互に含めます。 
[ABSTRACT]実際のデータでトレーニングされたボコーダーは、多くの場合、目に見えないシナリオの音声品質を低下させます。研究によると、ボコーダーは主観的研究のために低下することがよくあります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br>2019-12-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: The NTNU System at the Interspeech 2020 Non-Native Children's Speech ASR
  Challenge -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.SD/paper_4.html">
      The NTNU System at the Interspeech 2020 Non-Native Children's Speech ASR
  Challenge
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちのASRシステムのすべてのバリアントは、RNNベースの言語モデルを使用して、主催者がリリースしたテキストデータセットのみでトレーニングされた最初のパスの認識仮説を再評価しました。最適な構成のシステムは2位になり、その結果、単語エラー率（WER）は17.59％ですが、上位の2番目のシステムと公式のベースラインシステムのエラー率はそれぞれ15.67％、18.71％、35.09％です。このASR共有タスクは、非ネイティブと子供たちが話す特性の共存多様性に。 
[要約]最適な構成のシステムが2位になり、その結果、単語エラー率（wer）は17になりました。59％
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: An Effective End-to-End Modeling Approach for Mispronunciation Detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.SD/paper_5.html">
      An Effective End-to-End Modeling Approach for Mispronunciation Detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      一方、提案されたフレームワークとの連携を強化するために、2つのMD決定方法を採用しています。1）認識信頼度に基づく意思決定、または2）音声認識結果に基づく単純な方法です。2番目に、結果のE2EモデルをMDタスクに合わせて調整するためのテキストプロンプト情報。一連のマンダリンMD実験は、このアプローチが既存のハイブリッドDNN-HMMシステムの処理パイプラインを簡素化するだけでなく、体系的で大幅なパフォーマンスの向上をもたらすことを示しています。 
[要約] asrでe2eモデリングフレームワークが広く採用されているにもかかわらず、コンピュータ支援の発音学習で使用するためのe2eフレームワークの調査はまだ不十分です。代わりに、テキストプロンプト情報を使用して入力増強を実行し、結果のモデルを作成しますmdタスクに合わせた調整
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: SenseBERT: Driving Some Sense into BERT -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_0.html">
      SenseBERT: Driving Some Sense into BERT
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      SemEval Word Sense Disambiguationを実験し、Word in Contextタスクで最先端の結果を達成することで示すように、SenseBERTは字句の理解を大幅に改善します。人間の注釈の..この論文は、単語感覚レベルで直接弱い監督を採用する方法を提案する。 
[ABSTRACT] sensebertという名前の私たちのモデルは、弱い単語とその単語ネットのスーパーセンスを予測するように事前トレーニングされています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-15">
        <br>2019-08-15
      </time>
    </span>
  </h3>
</article>
<!-- paper0: How Does NLP Benefit Legal System: A Summary of Legal Artificial
  Intelligence -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_1.html">
      How Does NLP Benefit Legal System: A Summary of Legal Artificial
  Intelligence
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの作業の実装は、https：//github.com/thunlp/CLAIMから見つけることができます。法務専門家とNLP研究者の観点からタスクを説明し、LegalAIのいくつかの代表的なアプリケーションを示します。法務専門家は、ルールベースおよびシンボルベースの方法からのタスクを解決するために、NLPの研究者はデータ駆動型および埋め込み方法にさらに集中します。 
[ABSTRACT] legalaiは、迷惑な事務処理から法務専門家を解放するための法制度に有益です。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-25">
        <br>2020-04-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Syntax-guided Controlled Generation of Paraphrases -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_2.html">
      Syntax-guided Controlled Generation of Paraphrases
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最先端のベースラインに対するSGCPの有効性を実証するために、複数の実際の英語のデータセットに対して広範な自動評価と人間による評価を行います。この制限を本書で取り上げ、Syntax Guided Controlled Paraphraser（SGCP）、構文言い換え生成のためのエンドツーエンドのフレームワーク。SGCPは、関連性を損なうことなく構文に準拠した文を生成できることがわかりました。 
[ABSTRACT]マンゴーは、単純な制約に魅了されている人々の数の例です。最近の作品では、複雑な構文の組み込み-制御された言い換え生成のタスクにおける制約としてのガイダンスの調査が始まっています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Cross-lingual Transfer of Twitter Sentiment Models Using a Common Vector
  Space -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_3.html">
      Cross-lingual Transfer of Twitter Sentiment Models Using a Common Vector
  Space
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      LASERライブラリに実装されている多くの言語の共同数値空間を使用した2つの転送メカニズムに焦点を合わせます。トレーニング済みモデルの転送と、他の言語からのインスタンスによるトレーニングセットの拡張です。実験により、類似した言語間のモデルの転送がデータセットを拡張しても予測性能は向上しませんでしたが、賢明です。単語の埋め込みは、単語間の意味関係がベクトル空間の距離と方向としてエンコードされるように、数値空間の単語を表します。 
[ABSTRACT]クロス-13の言語間で予測を転送する言語の単語の埋め込み。埋め込みは、Twitter拡張用の機械学習予測モデルの転送に使用されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-15">
        <br>2020-05-15
      </time>
    </span>
  </h3>
</article>
<!-- paper0: DoQA -- Accessing Domain-Specific FAQs via Conversational QA -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_4.html">
      DoQA -- Accessing Domain-Specific FAQs via Conversational QA
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      優れた結果は、より困難なIRシナリオにも引き継がれます。2,437ダイアログと10,917 QAペアのデータセットであるDoQAを提示します。既存の強力なシステムの結果は、ウィキペディアQAデータセットと単一のFAQドメインでの微調整により、ドメイン内のトレーニングデータがなくても、FAQ用の高品質の会話型QAシステムを構築できます。 
[要旨] doqaを提示します。データセットは2、437ダイアログ、10、917 qaペアです。その結果、まだ改善の余地があることがわかります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-04">
        <br>2020-05-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Generating Hierarchical Explanations on Text Classification via Feature
  Interaction Detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_5.html">
      Generating Hierarchical Explanations on Text Classification via Feature
  Interaction Detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このような説明は、階層のさまざまなレベルで単語やフレーズがどのように組み合わされるかを視覚化します。これは、ユーザーがブラックボックスモデルの意思決定を理解するのに役立ちます。説明を解釈し、それをモデル予測に接続することは人間にとって難しい課題です。この作業では、機能の相互作用を検出して階層的な説明を作成します。 
[ABSTRACT]既存のメソッドは重要な機能を提供しますが、それらの間の相互作用を無視します。これらには説明として入力テキストから選択された単語またはフレーズが含まれます。提案されたメソッドは3つのニューラルテキスト分類子で評価されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-04">
        <br>2020-04-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Neural CRF Model for Sentence Alignment in Text Simplification -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_6.html">
      Neural CRF Model for Sentence Alignment in Text Simplification
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験は、提案されたアプローチが、F1で5以上の点で、単一言語の文の整列タスクに関するこれまでのすべての作業よりも優れていることを示しています。自動および人間による評価。CRFアライナを適用して、Newsela-AutoとWiki-Autoの2つの新しいテキスト簡略化データセットを構築します。これらは、既存のデータセットに比べてはるかに大きく、品質が優れています。 
[要旨]ウィキペディアとニュースラから作成された、手動で注釈が付けられた2つの文-拡大データセット
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-05">
        <br>2020-05-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Mitigating Gender Bias in Machine Learning Data Sets -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_7.html">
      Mitigating Gender Bias in Machine Learning Data Sets
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このホワイトペーパーでは、機械学習のトレーニングデータで性別バイアスを特定するためのフレームワークを提案します。この研究では、性別理論と社会言語学を利用して、テキストトレーニングデータと関連するニューラルワード埋め込みモデルのバイアスのレベルを体系的に示し、バイアスを削除するための経路を強調していますこのような問題に対処する試みには、学習された関連付けのテスト、公平性の概念の機械学習への統合、トレーニングデータのより厳密な分析の実行が含まれます。テキストデータでアルゴリズムをトレーニングする場合のバイアスの軽減は特に困難です。ジェンダーのイデオロギーが言語に埋め込まれている複雑な方法を考えると。 
[ABSTRACT]ジェンダーバイアスは、ジェンダーバイアスのコンテキストで特定されています。これは、ジェンダーイデオロギーが言語に埋め込まれている複雑な方法が原因です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-14">
        <br>2020-05-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Unified MRC Framework for Named Entity Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_8.html">
      A Unified MRC Framework for Named Entity Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      フラットNERで最も広く使用されているシーケンスラベリングモデルは特定のトークンに1つのラベルしか割り当てることができないため、モデルは通常2つのタスク用に別々に開発されます。これは、トークンが複数割り当てられるネストされたNERには適していません。さらに、クエリは有益な事前知識をエンコードするので、この戦略はエンティティ抽出のプロセスを容易にし、ネストされたNERだけでなくフラットNERのパフォーマンスを向上させます。現在のSOTAよりも大幅にパフォーマンスを向上させることができます。ネストされたNERデータセットのモデル（例：+ 1.28、+ 2.55、+ 5.44、+ 6.37、それぞれACE04、ACE05、GENIA、KBP17）、フラットNERデータセットのSOTA結果（+ 0.24、+ 1.95、+ 0.21）英語CoNLL 2003、英語OntoNotes 5.0、中国語MSRA、中国語OntoNotes 4.0ではそれぞれ+1.49。 
[ABSTRACT]フラットnerで最も広く使用されているバックボーンは、特定のトークンに単一のラベルを割り当てることしかできません。nerのタスクをシーケンスのラベル付け問題として扱うのではなく、それを機械読解（mrc）として定式化することを提案します） 仕事
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-25">
        <br>2019-10-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_9.html">
      Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの方法は、画像内の顕著なオブジェクトを正確に検出できるという観察に動機付けられており、ペアのテキストでしばしば言及されます。私たちは、650万のテキストと画像のペアの公開コーパスでOscarモデルを事前トレーニングし、 -下流のタスクに合わせて調整し、6つの定評のあるビジョン言語の理解と生成タスクで新しい最先端の技術を作成します。画像とテキストのペアのクロスモーダル表現を学習する大規模な事前トレーニング方法が視覚言語のタスクで人気があります。 
[ABSTRACT]新しい学習方法oscarは、アンカーポイントとして画像で検出されたオブジェクトタグを使用します。この方法は、oscarモデルを事前トレーニングすることで簡単に事前トレーニングできます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-13">
        <br>2020-04-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Efficiently Reusing Old Models Across Languages via Transfer Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_10.html">
      Efficiently Reusing Old Models Across Languages via Transfer Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これは、知識を転送して目的の「子」モデルをトレーニングする前に「親」モデルをトレーニングする追加コストが転移学習で特に当てはまります。このアプローチでは、調査対象の言語ペアごとに個別の親モデルは必要ありません。 NMT転送学習で。ランダムな初期化からトレーニングする場合よりも、翻訳品質が向上し、収束時間が短くなります。 
[要約]このホワイトペーパーでは、さまざまな言語ペアに対して既にトレーニング済みのモデルを再利用する簡単な方法を提案します。この方法の適用性を示す必要があります。さまざまな研究者によってトレーニングされた変換モデルをリサイクルします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-24">
        <br>2019-09-24
      </time>
    </span>
  </h3>
</article>
<!-- paper0: CrisisBERT: a Robust Transformer for Crisis Classification and
  Contextual Crisis Embedding -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_11.html">
      CrisisBERT: a Robust Transformer for Crisis Classification and
  Contextual Crisis Embedding
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ただし、どの作品も、トランスフォーマーやドキュメントレベルのコンテキスト埋め込みなど、最新の注意に基づくディープニューラルネットワークモデルを使用して、危機の埋め込みと分類を実行していません。以前の作品では、従来の機械学習とニューラルネットワークを使用してさまざまな危機の埋め込みと分類を提案しましたモデル..私たちの知る限り、私たちの仕事は最初に、トランスフォーマーベースの危機分類と、文献に埋め込まれたドキュメントレベルの文脈的危機の使用を提案することです。 
[ABSTRACT]以前の作品では、機械学習とニューラルネットワークモデルを使用してさまざまな危機の埋め込みが提案されていました。これらのイベントは通常、最近のcovid-19パンデミックなどの民間人には表示されません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-11">
        <br>2020-05-11
      </time>
    </span>
  </h3>
</article>
<!-- paper0: TripPy: A Triple Copy Strategy for Value Independent Neural Dialog State
  Tracking -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_12.html">
      TripPy: A Triple Copy Strategy for Value Independent Neural Dialog State
  Tracking
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      代わりに、すべての値がオンザフライでダイアログコンテキストから抽出されます。スロットは、3つのコピーメカニズムのいずれかによって埋められます。（1）スパン予測は、ユーザー入力から直接値を抽出できます。 （2）値は、システムの通知操作を追跡するシステム通知メモリからコピーできます。 （3）ドメイン内およびドメイン間の相互参照を解決するために、ダイアログ状態に既に含まれている別のスロットから値をコピーできます。このホワイトペーパーでは、さまざまなコピーメカニズムを使用してスロットを埋めるDSTへの新しいアプローチを紹介します。値を持つ。 
[要旨]私たちのモデルは候補値のリストを提示する必要はありません。スロットは、ユーザー入力から直接値を抽出するコピーメカニズムによって埋められます。値は、システムを追跡する別のスロットからコピーされる場合があります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-06">
        <br>2020-05-06
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Arabic Offensive Language on Twitter: Analysis and Experiments -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_13.html">
      Arabic Offensive Language on Twitter: Analysis and Experiments
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちは、下品な言葉やヘイトスピーチの特別なタグを付けた、これまでで最大のアラビア語のデータセットを作成します。最後に、大規模な一連の実験を行い、サポートベクターマシンの手法を使用してデータセットに強力な結果（F1 = 79.7）を作成します。方法を紹介します。トピック、方言、またはターゲットによってバイアスされていない不快なデータセットを構築するため。 
[ABSTRACT]私たちは、下品さや悪意のある表現のための特別なタグを使用して、これまでで最大のアラビア語方言を生成します。データセットで強力な結果を生成するために、大量の実験を実施します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-05">
        <br>2020-04-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Analyzing Temporal Relationships between Trending Terms on Twitter and
  Urban Dictionary Activity -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_14.html">
      Analyzing Temporal Relationships between Trending Terms on Twitter and
  Urban Dictionary Activity
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      一連の相互相関計算を通じて、アーバンディクショナリアクティビティがTwitterで発生するより大きな会話を密接に反映しているケースを特定します。次に、Twitterでのディスカッションとの関連性が高い用語のタイプを分析し、アーバンディクショナリアクティビティを見つけますTwitterと正の相関があるのは、ミーム、人気のある有名人、オフラインイベントに関連する用語が中心です。最後に、用語がTwitterでトレンドになっている期間と、対応するアーバンディクショナリのアクティビティとの関係を調べ、新しい現在Twitterでトレンドになっている用語の定義は、Urban Dictionaryに追加される可能性が高くなります。 
[ABSTRACT] Twitterと正の相関がある都市の辞書活動は、ミーム、人気のある有名人、オフラインイベントに関連する用語が中心です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-15">
        <br>2020-05-15
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Data Manipulation: Towards Effective Instance Learning for Neural
  Dialogue Generation via Learning to Augment and Reweight -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_15.html">
      Data Manipulation: Towards Effective Instance Learning for Neural
  Dialogue Generation via Learning to Augment and Reweight
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      現在の最先端の神経対話モデルは、データ駆動型のパラダイムに従って人間の会話から学習します。提案されたデータ操作フレームワークは完全にデータ駆動型で学習可能です。したがって、効果的な対話学習には、信頼性だけでなく学習サンプルだけでなく、ノイズの多いサンプルも少なくなります。 
[要約]提案されたデータ操作フレームワークは完全にデータ駆動型で学習可能です。信頼できるサンプルに向けてデータ分布を事前に再形成するように設計されています。これにより、これらのデータ駆動型神経対話モデルの学習が妨げられます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-06">
        <br>2020-04-06
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Variational Inference for Learning Representations of Natural Language
  Edits -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_16.html">
      Variational Inference for Learning Representations of Natural Language
  Edits
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ドキュメント編集は、情報の生成の広範なコンポーネントになりました。バージョン管理システムにより、編集を効率的に保存および適用できます。さらに、これまでの直接的な人間の入力に大きく依存していた編集表現の標準化された自動評価を容易にするために、自然言語処理のコンテキストで編集表現の品質を測定するように特別に設計された一連のダウンストリームタスクPEERを提案します。これを考慮して、変分推論を使用してベクトル表現の連続潜在空間を学習する新しいアプローチを提案しますドキュメントの編集プロセスに関する基本的な意味情報を取得します。 
[要約]編集の分散連続表現を学習するタスクが最近提案されました。これには、同じ機能を明示的にモデル化する潜在変数が含まれています。一連のダウンストリーム編集、peer、コンテキストの編集表現の品質を測定するように特別に設計されています自然言語処理
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-20">
        <br>2020-04-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: The NTNU System at the Interspeech 2020 Non-Native Children's Speech ASR
  Challenge -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_17.html">
      The NTNU System at the Interspeech 2020 Non-Native Children's Speech ASR
  Challenge
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ASRシステムのすべてのバリアントは、RNNベースの言語モデルを使用して、主催者がリリースしたテキストデータセットのみでトレーニングされた最初のパスの認識仮説を再評価しました。最適な構成のシステムが2位になり、その結果、単語エラー率（WER）は17.59％ですが、上位の2番目のシステムと公式のベースラインシステムのエラー率は、それぞれ15.67％、18.71％、35.09％です。このリソース不足の問題を回避するには、は、CNN-TDNNFベースの音響モデルの上にASRシステムを構築し、その一方で、発話レベルと単語レベルの速度摂動とスペクトログラム拡張の両方を含むさまざまなデータ拡張戦略の相乗効果を、シンプルでありながら効果的なデータクレンジングアプローチとともに利用しました。 
[要約]最適な構成のシステムが2位になり、その結果、単語エラー率（wer）は17になりました。59％
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: An Effective End-to-End Modeling Approach for Mispronunciation Detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_18.html">
      An Effective End-to-End Modeling Approach for Mispronunciation Detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      それに応じて、最初にMDタスクへのハイブリッドCTCAttentionアプローチの新しい使い方を紹介し、CTCと注意力ベースのモデルの両方の長所を活用しながら、電話レベルの強制整列の必要性を回避します。次に、入力を実行します。結果として得られるE2EモデルをMDタスクに合わせて調整するためのテキストプロンプト情報による拡張。さらに、テキストプロンプトによる入力拡張は、E2EベースのMDアプローチに対して非常に有望であるようです。 
[要約] asrでe2eモデリングフレームワークが広く採用されているにもかかわらず、コンピュータ支援の発音学習で使用するためのe2eフレームワークの調査はまだ不十分です。代わりに、テキストプロンプト情報を使用して入力増強を実行し、結果のモデルを作成しますmdタスクに合わせた調整
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: An Evaluation of Recent Neural Sequence Tagging Models in Turkish Named
  Entity Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_19.html">
      An Evaluation of Recent Neural Sequence Tagging Models in Turkish Named
  Entity Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの研究は、形態学的に豊富な言語の処理に対する転移学習の影響を定量化する文献に貢献しています。NERは、関係抽出や質問応答などのダウンストリーム言語処理アプリケーションだけでなく、リアルタイムなどの大規模なビッグデータ操作でも重要ですオンラインデジタルメディアコンテンツの分析..私たちの結果は、長距離のコンテキストをモデル化できるトランスフォーマーベースのネットワークが、文字、サブワード、およびワードレベルでの異なる入力機能が利用されるBiLSTMネットワークの制限を克服することを示しています。 
[ABSTRACT] nerは、質問応答などの言語処理アプリケーションでは重要ですが、大規模なビッグデータ操作でも重要です。最近のニューラルアーキテクチャは、同じ設定でトルコ語のnerタグ付けを行うために提案されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-14">
        <br>2020-05-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Distilling neural networks into skipgram-level decision lists -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/cs.CL/paper_20.html">
      Distilling neural networks into skipgram-level decision lists
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらの制限を克服するために、スキップグラム上の決定リスト（ルールとも呼ばれる）によってRNNを説明するパイプラインを提案します。リカレントニューラルネットワークの説明に関するこれまでのいくつかの研究では、ネットワークの最も重要な入力セグメントをそのネットワークとして見つけるアプローチに焦点を当てています説明..説明を評価するために、私たちは合成敗血症識別データセットを作成し、追加の臨床および感情分析データセットに私たちの技術を適用します。 
[ABSTRACT]私たちは合成敗血症-識別データセットを作成します。説明の評価のために、より重要なセンチメントグラムデータセットに私たちの手法を使用します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-14">
        <br>2020-05-14
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Augmenting Generative Adversarial Networks for Speech Emotion
  Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_0.html">
      Augmenting Generative Adversarial Networks for Speech Emotion
  Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの結果は、提案されたフレームワークが圧縮された感情表現を効果的に学習できるだけでなく、コーパス内およびコーパス間の評価のパフォーマンスを改善するのに役立つ合成サンプルを生成できることを示しています。生成敵対的ネットワーク（GAN）は、感情属性の学習に潜在能力を示し、新しいデータサンプルの生成..提案されたフレームワークの有効性を示すために、（i）合成特徴ベクトル、（ii）合成特徴によるトレーニングデータの拡張、（iii）圧縮表現での特徴のエンコードに関するSERの結果を示します。 
[要約]提案されたシステムは、音声感情認識を向上させるために使用できます。ただし、新しいシステムで開発できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: An Overview on Audio, Signal, Speech, & Language Processing for COVID-19 -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_1.html">
      An Overview on Audio, Signal, Speech, & Language Processing for COVID-19
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、ソリューションの使いやすさ、課題、および重要なテクノロジーの達成に関する観察結果についても説明します。また、短期間に有能なソリューションを生み出すためにこの方向で使用された詳細な手法をまとめています。音声および音声分析のインスピレーションを補完または提供する可能性がある非音声モダリティからの寄与の概要。 
[ABSTRACT]現在の状況と闘い、将来のための安全で安心できる社会を構築するために、スピーチイニシアチブを備えた多くのAIが採用されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Many-to-Many Voice Transformer Network -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_2.html">
      Many-to-Many Voice Transformer Network
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このモデルに加えて、「アイデンティティマッピングロス」と呼ばれるトレーニングロスを組み込むことを提案します。これにより、入力フィーチャシーケンスがターゲットドメインにすでに属している場合でも変更されないようにします。これにより、モデルは利用可能なトレーニングを完全に利用できます。異なるドメイン間で共有できる共通の潜在的な特徴をキャプチャすることによって複数のドメインから収集されたデータ。話者ID変換実験を実施し、モデルがベースライン方法よりも高い音質と話者類似性を得たことを示しました。 
[ABSTRACT]これにより、モデルは複数のドメインから収集された利用可能なトレーニングデータを十分に活用できるようになります。モデルトレーニングの損失は、テスト時のモデルのパフォーマンスを向上させるのに非常に効果的であることがわかっています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Quasi-Periodic Parallel WaveGAN Vocoder: A Non-autoregressive
  Pitch-dependent Dilated Convolution Model for Parametric Speech Generation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_3.html">
      Quasi-Periodic Parallel WaveGAN Vocoder: A Non-autoregressive
  Pitch-dependent Dilated Convolution Model for Parametric Speech Generation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ただし、固定された一般的なネットワーク構造のため、PWGボコーダーが、スケーリングされたピッチなどの見えないピッチ機能を含む音響機能に条件付けられると、PWG生成音声のピッチ精度が低下します。提案されているQPPWGは、ピッチ依存の拡張畳み込みネットワークを採用しています。 （PDCNN）モジュールは、動的に変更されたネットワークアーキテクチャを介してピッチ情報をPWGに導入し、バニラPWGのピッチ制御性と音声モデリング機能を改善します。PWGは、このような音響機能に基づいて音声を生成するボコーダーと見なされます。スペクトルおよび韻律の特徴として、PWGで生成された音声には高い忠実度が含まれています。 
[要約] pwgはコンパクトな非自己回帰（非ar）音声生成モデルであり、その生成速度はリアルタイムよりはるかに高速です。ppiは新しいモデルであり、バニラpwgの70％のみを制御します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Attention-based Transducer for Online Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_4.html">
      Attention-based Transducer for Online Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最初に、ジョイントネットワークにチャンク単位の注意を導入します。2番目に、エンコーダに自己注意が導入されます。本番サーバーのシングルCPUコア上。 
[ABSTRACT]トレーニングでは、1.7倍以上の高速化を達成しました。1万時間のデータで、最終的なシステムは、最高のkaldi tdnn-fレシピでトレーニングしたものよりも5.5％少ないwer削減を達成しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Mask CTC: Non-Autoregressive End-to-End ASR with CTC and Mask Predict -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_5.html">
      Mask CTC: Non-Autoregressive End-to-End ASR with CTC and Mask Predict
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この作業では、マスク予測とCTCの共同トレーニングを備えたトランスフォーマーエンコーダー/デコーダーを使用して、マスクCTCモデルをトレーニングします。すべてのコードは公開されます。推論中、ターゲットシーケンスは貪欲なCTC出力と低い-confidenceトークンは、CTC確率に基づいてマスクされます。 
[ABSTRACT]モデルは、出力トークン間の条件付き依存関係に基づいています。これらのマスクされた低-信頼性トークンは、高信頼性トークンに基づいて条件付けが予測されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Defending Your Voice: Adversarial Attack on Voice Conversion -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_6.html">
      Defending Your Voice: Adversarial Attack on Voice Conversion
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      発話の話者の特性を別の話者の特性に変換する音声変換において、発話の言語的内容を変更することなく、近年大幅な改善が達成されました。予備実験は、現在最先端の2つの0ショット音声変換モデル。音声を保護するスピーカーの発話に人間の知覚できないノイズを導入します。 
[要約]変換技術の向上により、プライバシーと認証に関する懸念も生じました。これは、音声変換に対して敵対的な攻撃を試みた最初の既知の試みです
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Knowledge-and-Data-Driven Amplitude Spectrum Prediction for Hierarchical
  Neural Vocoders -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_7.html">
      Knowledge-and-Data-Driven Amplitude Spectrum Prediction for Hierarchical
  Neural Vocoders
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このモジュールは、STFTとソースフィルター理論の組み合わせに基づいて設計されています。この場合、ソース部分とフィルター部分は、それぞれ入力F0とmel-cepstraに基づいて設計されます。実験結果では、KDD-ASPを使用するHiNetボコーダーが音声合成（TTS）タスクで従来のASPおよびWaveRNNボコーダーを使用するよりも高品質の合成音声を実現できます。次に、回復されたALASは、複数のトレーニング可能なたたみ込みで構成されるデータ駆動型LAS精製モジュールによって処理されます。最終的なLASを取得するためのレイヤー。 
[ABSTRACT]合成データは、データベースのデータベースのデータ改良に基づいています。hinetデータベースのデータドライバーは、最終的なデータを取得するために使用できます。データドライバーは、データドリブンレイヤーを使用するhinetに基づいています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-16">
        <br>2020-04-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Design Choices for X-vector Based Speaker Anonymization -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_8.html">
      Design Choices for X-vector Based Speaker Anonymization
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      達成された匿名化の強度を評価するために、匿名化スキームの知識に応じて、登録に元の音声または匿名化された音声を使用する可能性のあるxベクトルベースの話者検証システムを使用する攻撃者を考慮します。実験は、LibriSpeechから派生したデータセットを使用して実行され、プライバシーとユーティリティの観点から、設計の選択の最適な組み合わせ。攻撃者によって達成された等価エラー率（EER）と匿名化されたデータのデコードWordエラー率（WER）は、プライバシーとユーティリティの測定値として報告されます。 
[ABSTRACT]これは最初のvoiceprivacyチャレンジです。最初のvoiceprivcyチャレンジのベースラインとして柔軟な疑似化手法を提示します。匿名化の知識に応じて、登録に元の音声または匿名化された音声を使用する可能性がある話者検証システムを使用する攻撃者を検討しますスキーム
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Metric Learning for Keyword Spotting -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_9.html">
      Metric Learning for Keyword Spotting
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実際には、キーワードスポッティングは、事前定義されたターゲットキーワードがさまざまな未知の音から検出される検出問題です。これは、見えない未知の非ターゲットサウンドをターゲットキーワードから明確に区別する必要があるという点で、メトリック学習問題と多くの類似点を共有します。ただし、重要な違いは、ターゲットキーワードが既知であり、事前定義されていることです。 
[ABSTRACT]ほとんどの既存の作品は、クローズドセット分類メトリック問題としてキーワードスポッティングに対処しています。キーワードスポッティングは、事前定義されたターゲットキーワードが検出されたときに発生します。ただし、主な違いは、ターゲットスポッティングが既知で事前定義されていることです。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Approaches to Improving Recognition of Underrepresented Named Entities
  in Hybrid ASR Systems -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_10.html">
      Approaches to Improving Recognition of Underrepresented Named Entities
  in Hybrid ASR Systems
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これにより、不十分に表現されたNE認識のパフォーマンスが大幅に向上します。前述のアプローチを組み合わせると、NE認識が相対的に最大42％向上します。次に、まれな言葉を扱う。 
[ABSTRACT]さまざまな設定で調査し、過小評価されたnesを処理する効果を実証します。最終的には、神経lmsによってリスコアされた単語ラティスにnesを含む発話の可能性スコアを高め、さらにパフォーマンスを向上させます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Surfboard: Audio Feature Extraction for Modern Machine Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_11.html">
      Surfboard: Audio Feature Extraction for Modern Machine Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      パッケージは、Pythonとそのコマンドラインインターフェイスの両方からプログラムでアクセスできるため、機械学習ワークフロー内に簡単に統合できます。同様のフレームワークを確認し、その機能の臨床的動機を含むSurfboardのアーキテクチャについて説明します。mPowerデータセットの使用、パーキンソン病の分類タスクへのSurfboardの適用を説明し、既存の研究における一般的な落とし穴を強調します。 
[ABSTRACT]サーフボードは、最新の機械学習フレームワークとの併用を容易にするように設計されています。最先端のオーディオ分析パッケージに基づいて構築され、大規模なワークロードを処理するためのマルチプロセッシングサポートを提供します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Differentiable Perceptual Audio Metric Learned from Just Noticeable
  Differences -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_12.html">
      A Differentiable Perceptual Audio Metric Learned from Just Noticeable
  Differences
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      したがって、主観的なペアワイズ比較によって測定されるように、既存の損失（たとえば、深い特徴の損失）を測定基準で単純に置き換えると、ノイズ除去ネットワークの大幅な改善が得られます。被験者は、直接的な客観的な質問に答えるよう求められます。 ？。ディープネットワークであるため、メトリックは微分可能であり、他のタスクの損失関数として適しています。 
[要旨]ディープニューラルネットワークをクラウドソーシングされた人間の判断の新しいデータセットにフィッティングすることでメトリックを構築します。対象は、ノイズ、リバーブ、圧縮アーティファクトなど、さまざまな摂動の下でアルゴリズムによって生成されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-13">
        <br>2020-01-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Deep Architecture Enhancing Robustness to Noise, Adversarial Attacks,
  and Cross-corpus Setting for Speech Emotion Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_13.html">
      Deep Architecture Enhancing Robustness to Noise, Adversarial Attacks,
  and Cross-corpus Setting for Speech Emotion Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この論文では、DenseNet、LSTM、Highway Networkを融合して、ノイズに対してロバストな強力な識別機能を学習する、より深いニューラルネットワークアーキテクチャを提案します。また、ロバストネスをさらに改善するために、ネットワークアーキテクチャによるデータ拡張を提案します。包括的な評価このアーキテクチャは、（1）ノイズ、（2）敵対的攻撃、および（3）コーパス間の設定に対するデータの増大と組み合わされています。 
[ABSTRACT]正確なserの堅牢なモデルの設計は困難であり、実用的なアプリケーションでの使用が制限されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: End-to-End Lip Synchronisation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_14.html">
      End-to-End Lip Synchronisation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      2つのモダリティ間の類似度マトリックスは、最初に特徴から計算されます。次に、オフセットの推論は、マトリックスが画像と同等であると見なされるパターン認識問題であると見なすことができます。提案されたアプローチが前の作業よりも優れていることを示します。 LRS2およびLRS3データセットのマージンが大幅に増加しました。これらのメソッドは満足のいくパフォーマンスを示していますが、ネットワークはタスクで直接トレーニングされていません。 
[要約]システムは、オーディオストリームと後続のビデオストリームの間のオフセットを予測するために使用できます。また、エンドツーエンドのシステムを開発するために開発することもできます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: MoBoAligner: a Neural Alignment Model for Non-autoregressive TTS with
  Monotonic Boundary Search -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_15.html">
      MoBoAligner: a Neural Alignment Model for Non-autoregressive TTS with
  Monotonic Boundary Search
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      テキストトークンとスペクトルフレーム間の対応を制御し、合成オーディオのリズムと速度を決定するため、継続時間ベースの整列は重要な役割を果たします。継続時間ベースの整列を改善し、非自己回帰音声合成の品質を向上させるには、この論文では、MoboAlignerという名前の新しい神経アライメントモデルを提案します。TransformerTTSによって抽出された期間と比較して、MoboAlignerはMOSの非自己回帰TTSモデルに改善をもたらします（FastSpeechの3.44と比較して3.74）。 
[ABSTRACT]自己回帰モデルでは、エンコーダーとデコーダー間のハードアライメントを行うために、テキストトークンの追加の期間が必要です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Neural Spatio-Temporal Beamformer for Target Speech Separation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_16.html">
      Neural Spatio-Temporal Beamformer for Target Speech Separation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらなる改善には、実数値マスクの複素数値マスクへの置き換え、および複素マスクNNの合同トレーニングが含まれます。純粋にニューラルネットワーク（NN）ベースの音声分離および拡張方法ですが、必然的に良好な客観的スコアを達成できます。自動音声認識（ASR）に有害な非線形音声歪みを引き起こします。このホワイトペーパーでは、音声の分離と強化のために、複素数値マスクを備えたマルチタップMVDRビームフォーマーを提案します。 
[要約] nn-予測マスクを備えた最小変動歪みなし応答（mvdr）ビームフォーマーは、音声歪みを大幅に削減できます。新しいシステムは、状態に基づいています-nn-マスクベースのmvdr
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-08">
        <br>2020-05-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Cyclical Post-filtering Approach to Mismatch Refinement of Neural
  Vocoder for Text-to-speech Systems -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_17.html">
      A Cyclical Post-filtering Approach to Mismatch Refinement of Neural
  Vocoder for Text-to-speech Systems
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらの問題に対処するために、トレーニングのための時間的に一致する疑似VCデータと、ニューラルボコーダーをテストするための音響的に一致する拡張データを生成するために、巡回音声変換（VC）モデルを採用します。この論文では、提案された方法を状態2つの異なるTTSシステム用の最新のWaveNetボコーダー、および客観的および主観的な実験結果により、提案されたフレームワークの有効性が確認されます。より経済的なアプローチは、既存のTTSシステムによって生成される音声を強化するニューラルボコーダーを開発することです。 
[ABSTRACT]これらの高度なシステムはゼロから構築できます。データをテストするには、これらの高度なシステムをたとえば使用できます。このモデルは他のボコーダーに適用できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Acoustic Integrity Codes: Secure Device Pairing Using Short-Range
  Acoustic Communication -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_18.html">
      Acoustic Integrity Codes: Secure Device Pairing Using Short-Range
  Acoustic Communication
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最初のペアリングには短距離音響通信を使用することを提案します。Androidスマートフォンでのオープンソースの概念実証実装を使用して、異なるスマートフォンモデル間のペアリングを示します。このSDPスキームのセキュリティと堅牢性の評価では、信号対雑音比（SNR）が14 dBで、ネットビットレートが100 bpsの場合、ビットエラー率は0.1％未満です。 
[ABSTRACT]これには既存のsdpシステムの使用を制限する共通のハードウェアインターフェイスが必要です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Towards Robust Neural Vocoding for Speech Generation: A Survey -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_19.html">
      Towards Robust Neural Vocoding for Speech Generation: A Survey
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      すべてのボコーダーの自然さをもたらす主観的なMOSの結果は、今後の研究で提示されます。このホワイトペーパーでは、WaveNet、WaveRNN、FFTNet、Parallel WaveGANを含む4つの一般的なニューラルボコーダーを5つの異なるデータセットで交互にトレーニングします。話者の多様性は、言語よりもユニバーサルボコーダーを実現するためにはるかに重要です。 
[ABSTRACT]実際のデータでトレーニングされたボコーダーは、多くの場合、目に見えないシナリオの音声品質を低下させます。研究によると、ボコーダーは主観的研究のために低下することがよくあります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br>2019-12-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Unconditional Audio Generation with Generative Adversarial Networks and
  Cycle Regularization -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_20.html">
      Unconditional Audio Generation with Generative Adversarial Networks and
  Cycle Regularization
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      3番目に、歌声の生成だけでなく、スピーチ音声の生成についても、新しいモデルのパフォーマンスを評価します。2番目に、モードの崩壊を回避するために、ジェネレーターにサイクル正則化メカニズムを導入します。モデルのジェネレーターは、ノイズベクトルの可変長シーケンスを入力として受け取るために、可変長のメルスペクトログラムを生成できます。 
[ABSTRACT]モデルのジェネレータは、メル-可変長のスペクトログラムを生成するように設計されています。現在の論文は、次の点で以前の研究を拡張および拡張しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Attentron: Few-Shot Text-to-Speech Utilizing Attention-Based
  Variable-Length Embedding -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_21.html">
      Attentron: Few-Shot Text-to-Speech Utilizing Attention-Based
  Variable-Length Embedding
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、モデルは、任意の数の参照オーディオにスケールアウトして、合成音声の品質を向上させることができます。人間の評価を含む私たちの実験によると、提案されたモデルは、生成時に最先端のモデルを大幅に上回りますこの問題に対処するために、我々は、アッテントロン、トレーニング中に見えない話者の声を複製する数ショットのTTSモデルを提案します。 
[ABSTRACT] attentronは少数です-トレーニング中に見えない話者の声を複製するショットttsモデルです。このモデルは、話者の類似性と品質の点で、見えない話者のスピーチを生成するときに、最先端のモデルを大幅に上回ります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Audio-visual Multi-channel Recognition of Overlapped Speech -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_22.html">
      Audio-visual Multi-channel Recognition of Overlapped Speech
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      重なり合った音声の自動音声認識（ASR）は、今までのところ非常に困難な課題です。音響信号の破損に対する視覚モダリティの不変性に動機付けられて、この論文は、緊密に統合された分離フロントを備えた、オーディオビジュアルマルチチャネル重複音声認識システムを提示しますエンドおよび認識バックエンド.. \ textit {TFマスキング}、\ textit {filter \＆sum}および\ textit {mask-based MVDR}ビームフォーミングアプローチに基づく一連の視聴覚マルチチャネル音声分離フロントエンドコンポーネント開発されました。 
[要約]提案されたマルチチャネルavsrシステムは、ベースラインオーディオよりもパフォーマンスが優れています。asrシステムのみが最大6.81％（26. 83％相対）および22. 22. 22. 87％です。提案されたマルチトールavsrシステム35,000ドルのコストで開発されました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: The NTNU System at the Interspeech 2020 Non-Native Children's Speech ASR
  Challenge -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_23.html">
      The NTNU System at the Interspeech 2020 Non-Native Children's Speech ASR
  Challenge
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちのASRシステムのすべてのバリアントは、RNNベースの言語モデルを採用して、主催者がリリースしたテキストデータセットのみでトレーニングされた最初のパスの認識仮説を再評価しました。このホワイトペーパーでは、Interspeech 2020 Non- ISCAのSIG-CHILDグループがサポートするNative Children&#39;s Speech ASRチャレンジ。このリソース不足の問題を回避するために、CNN-TDNNFベースの音響モデルの上にASRシステムを構築し、その間にさまざまなデータ増強の相乗効果を利用しましたシンプルでありながら効果的なデータクレンジングアプローチとともに、発話レベルと単語レベルの速度摂動とスペクトログラムの増強を含む戦略。 
[要約]最適な構成のシステムが2位になり、その結果、単語エラー率（wer）は17になりました。59％
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: An Effective End-to-End Modeling Approach for Mispronunciation Detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_24.html">
      An Effective End-to-End Modeling Approach for Mispronunciation Detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      一方、提案されたフレームワークとの連携を強化するために、2つのMD決定方法を採用しています。1）認識信頼度に基づく意思決定、または2）音声認識結果に基づく単純な方法です。2番目に、結果として得られるE2EモデルをMDタスクに合わせて調整するためのテキストプロンプト情報。ASRでE2Eモデリングフレームワークが広く採用されているにもかかわらず、コンピューター支援発音学習（CAPT）で使用するE2Eフレームワークを調査する作業はまだ不足しています。 ）、特に誤発音検出（MD）の場合。 
[要約] asrでe2eモデリングフレームワークが広く採用されているにもかかわらず、コンピュータ支援の発音学習で使用するためのe2eフレームワークの調査はまだ不十分です。代わりに、テキストプロンプト情報を使用して入力増強を実行し、結果のモデルを作成しますmdタスクに合わせた調整
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Quaternion Neural Networks for Multi-channel Distant Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_25.html">
      Quaternion Neural Networks for Multi-channel Distant Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらのマルチチャネルオーディオ録音には、各信号間の特定の内部関係が含まれます。四元数代数は、標準のドット積をハミルトンのものに置き換えます。したがって、要素間の依存関係をモデル化するシンプルでエレガントな方法を提供します。この問題を軽減する一般的なアプローチは、さまざまな視点から音響シーンをキャプチャする複数のマイクを録音デバイスに装備する方法。 
[要約]この論文では、これらの構造内および構造内依存性をクォータニオンニューラルネットワークでキャプチャすることを提案しています。クォータニオンレイヤーは、時間領域での長い依存関係を学習できる反復的なやりがいのあるニューラルネットワークと組み合わされます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Audio ALBERT: A Lite BERT for Self-supervised Learning of Audio
  Representation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_26.html">
      Audio ALBERT: A Lite BERT for Self-supervised Learning of Audio
  Representation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      オーディオALBERTは、91 \％少ないパラメーターを使用しながら、ダウンストリームタスクでこれらの巨大なモデルを使用して競争力のあるパフォーマンスを達成できることを示しています。実験を調査すると、潜在表現は音素とスピーカーの両方の豊富な情報をエンコードすることがわかりました。最後の層。2つの下流タスク、話者識別、および音素分類の表現を使用します。 
[ABSTRACT]最近の作品では、モデルのサイズを大きくすることで、音響モデルのトレーニングでより良いパフォーマンスを実現するために使用されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Learning Deep Models from Synthetic Data for Extracting Dolphin Whistle
  Contours -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-19/eess.AS/paper_27.html">
      Learning Deep Models from Synthetic Data for Extracting Dolphin Whistle
  Contours
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      提案されたLearn-from-Synthesisメソッドをパブリックな検出、分類、ローカリゼーション、および密度推定（DCLDE）2011ワークショップデータのサブセットに適用して、ホイッスル信頼マップを抽出し、既存の等高線抽出器で処理してホイッスルアノテーションを生成しました..また、バックグラウンド環境からトレーニングサンプルを合成し、最小限の人間の注釈の労力でネットワークをトレーニングする包括的な方法を開発しました。水中聴音器の録音で歯クジラ（Odontoceti）の笛を抽出するための学習ベースの方法を提示します。 
[ABSTRACT]オーディオツアーに基づいて、各スペクトログラムを時間のセットに分解する方法-周波数パッチ。トレーニングサンプルを合成して周波数をトレーニングする包括的な方法も開発しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="">
      
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本日更新された論文はありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="">
        <br>
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
