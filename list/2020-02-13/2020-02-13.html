<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-02-13の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Audio-Visual Calibration with Polynomial Regression for 2-D Projection
  Using SVD-PHAT -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.SD/paper_0.html">
      Audio-Visual Calibration with Polynomial Regression for 2-D Projection
  Using SVD-PHAT
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      低コストのマイクアレイと市販のカメラを使用して、多項式回帰が非線形カメラの歪みを効率的に処理できることと、リアルタイム処理用に最近提案された音源定位法SVD-PHATを示します。この論文では、音響画像を生成して光学画像の上に重ねることにより、カメラの視野をアレイマイクの聴覚領域で空間的に較正する簡単な2次元法を提案します。 
[概要]低コストのマイクフィールドとカメラのアレイを使用して、音響が非線形カメラの歪みを効率的に処理できることを示します。リアルタイム処理のための最近提案された音源定位法svd-phatは、このタスクに適合
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-04">
        <br>2020-02-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Speech-Based Parameter Estimation of an Asymmetric Vocal Fold
  Oscillation Model and Its Application in Discriminating Vocal Fold
  Pathologies -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.SD/paper_1.html">
      Speech-Based Parameter Estimation of an Asymmetric Vocal Fold
  Oscillation Model and Its Application in Discriminating Vocal Fold
  Pathologies
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      そのような場合、ケースごとにモデルパラメータを調整するよりスケーラブルな方法を見つけることが望ましいです。この論文では、音声信号から声帯モデルパラメータを決定するための新しい代替方法を提示します。声帯の弾力性、抵抗など、これらのモデルのパラメータ。
[要約]多くの場合、声帯の特性は一般的な値から逸脱します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-20">
        <br>2019-10-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: x-vectors meet emotions: A study on dependencies between emotion and
  speaker recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.SD/paper_2.html">
      x-vectors meet emotions: A study on dependencies between emotion and
  speaker recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最後に、感情が話者照合に及ぼす影響に関する結果を示します。IEMOCAP、MSP-Podcast、およびCrema-の3種類のデータセットでの実験を評価しました。 D.微調整により、事前トレーニングなしのベースラインモデルと比較して、IEMOCAP、MSP-Podcast、およびCrema-Dでそれぞれ30.40％、7.99％、8.61％の絶対改善が得られました。 
[概要]まず、話者認識のために学習した知識を、転移学習を通じて感情認識に再利用できることを示します。話者検証パフォーマンスは、テスト話者の感情の変化を起こしやすいことがわかりました。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Attentional Speech Recognition Models Misbehave on Out-of-domain
  Utterances -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.SD/paper_3.html">
      Attentional Speech Recognition Models Misbehave on Out-of-domain
  Utterances
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      同じデータでトレーニングされたフレーム同期ハイブリッド（DNN-HMM）モデルでは、これらの異常に長いトランスクリプトは生成されません。出力内のワードピースの正しい数を予測するために別の長さ予測モデルを作成します。 LibriSpeechタスクでワードエラー率を上げることなく、問題のあるデコード結果が得られます。500文字を超えるデコード出力を生成する5秒の録音が多数あることがわかります（つまり、
[要約] librispeech corpus.decodingの問題のみでトレーニングされた.decoderモデルは、アーキテクチャの音声変換モデルで再現されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: AlignNet: A Unifying Approach to Audio-Visual Alignment -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.SD/paper_4.html">
      AlignNet: A Unifying Approach to Audio-Visual Alignment
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ダンスミュージックアライメントとスピーチリップアライメントに関する定性的、定量的、主観的評価の結果は、本手法が最先端の手法よりもはるかに優れていることを示しています。 AlignNetは、ビデオとオーディオの各フレーム間のエンドツーエンドの密な対応を学習します。 
[ABSTRACT] maskは、ビデオとaudio.syncnetのよく知られた対応を学習します。syncnetは、video.dance50とdance50の各フレーム間のよく認識された対応を学習します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Deep Feature Embedding and Hierarchical Classification for Audio Scene
  Classification -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.SD/paper_5.html">
      Deep Feature Embedding and Hierarchical Classification for Audio Scene
  Classification
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      その後、階層分類は、トリプレット損失関数に関連付けられたディープニューラルネットワーク分類子を使用して実行されます。一方、シーンカテゴリの構造を活用するために、元のシーン分類の問題は、類似のカテゴリがメタカテゴリにグループ化される階層に構造化されます。 
[要旨]深い畳み込みニューラルネットワークは、シーンのオーディオ信号から埋め込まれる特徴を学習するように訓練されます。提案されたシステムは、両方のdcase 2018データセットで良好なパフォーマンスを達成します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Audiogmenter: a MATLAB Toolbox for Audio Data Augmentation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.SD/paper_6.html">
      Audiogmenter: a MATLAB Toolbox for Audio Data Augmentation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ツールボックスとそのドキュメントは、https：//github.com/LorisNanni/Audiogmenterからダウンロードできます。生のオーディオデータ用に15種類、スペクトログラム用に8種類の拡張アルゴリズムを提供しています。自由に利用できる音声データ拡張ライブラリ。 
[ABSTRACT] audiogmenterはmatgmenterの新しいオーディオデータ拡張ライブラリです。我々は、その有用性が文献で広く証明されているいくつかの拡張技術を効率的に実装しました。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-11">
        <br>2019-12-11
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Active Learning for Sound Event Detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.SD/paper_7.html">
      Active Learning for Sound Event Detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      コンテキストとしての記録を使用したトレーニングは、注釈付きセグメントのみを使用したトレーニングよりも優れています。驚くべきことに、ターゲットサウンドイベントがまれなデータセットでは、必要な注釈の労力を大幅に削減できます。提案されたシステムは、評価に使用される2つのデータセット（TUT Rare Sound 2017およびTAU Spatial Sound 2019）の参照方法より明らかに優れています。 
[要約]提案されたシステムは、限られた注釈の努力で学習したsedモデルの精度を最大化することを目指しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Content Based Singing Voice Extraction From a Musical Mixture -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.SD/paper_8.html">
      Content Based Singing Voice Extraction From a Musical Mixture
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      システムの性質上、従来の客観的評価指標とは一致しませんが、リスニングテストによる主観的評価を使用して、方法論を最新の深層学習ベースのソース分離アルゴリズムと比較します。この方法論を使用すると、トレーニング中に歌手が見えない処理済みの混合データセットに対しても、混合から未処理の生の音声信号を抽出することができます。 
[概要]このモデルは、音楽の混合物のエンコーダモデルに基づいています。ボーカルとの混合物のスペクトログラムの振幅成分を取ります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Europarl-ST: A Multilingual Corpus For Speech Translation Of
  Parliamentary Debates -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.SD/paper_9.html">
      Europarl-ST: A Multilingual Corpus For Speech Translation Of
  Parliamentary Debates
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この記事では、合計30の異なる翻訳方向について、6つのヨーロッパ言語からのSLTの音声テキストサンプルを含む新しい多言語SLTコーパスであるEuroparl-STを紹介します。コーパスは、クリエイティブコモンズライセンスの下でリリースされ、このペーパーでは、コーパスの作成プロセスについて説明し、この新しいリソースの可能性を強調する一連の自動音声認識、機械翻訳、音声言語翻訳の実験を紹介します。 
[要旨]論文では、このリソースの可能性を強調する一連の言語を紹介しています。これは、ヨーロッパおよびヨーロッパの研究者の研究です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-08">
        <br>2019-11-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: CIF: Continuous Integrate-and-Fire for End-to-End Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.SD/paper_10.html">
      CIF: Continuous Integrate-and-Fire for End-to-End Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらのメソッドの共同動作により、CIFベースのモデルは競争力のあるパフォーマンスを示します。特に、Librispeechのテストクリーンで2.86％のワードエラー率（WER）を達成し、新しい最先端の結果を作成します。マンダリン電話のASRベンチマークについて。このペーパーでは、シーケンス変換に使用される新しいソフトで単調なアライメントメカニズムを提案します。 
[概要]システムは、統合に基づいています-火災モデル。それは、エンコーダーに基づいています-デコーダーフレームワークは、連続関数で構成されています。テスト-librispeechを削除し、マンダリン電話asrベンチマークで新しい最先端の結果を作成します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-05-27">
        <br>2019-05-27
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Word Embeddings for Entity-annotated Texts -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.CL/paper_0.html">
      Word Embeddings for Entity-annotated Texts
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      トレーニングコーパスの名前付きエンティティに注釈を付けると、下流のタスクでよりインテリジェントな単語機能が得られることは直感的に思えますが、一般的な埋め込みアプローチをエンティティ注釈付きコーパスに単純に適用すると、パフォーマンスの問題が発生します。これらの結果に基づいて、テキストの共起グラフ表現のノード埋め込みがパフォーマンスを復元する方法とタイミングについて説明します。 
[ABSTRACT]人気の単語埋め込みモデルは、名前付きエンティティをファーストクラスの市民として含めることができませんでした。ただし、それらはファーストクラスの市民として含まれません。非エンティティワード埋め込みのパフォーマンスは、トレーニングを受けたエンティティと比較して低下します自然で注釈のないコーパス
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-02-06">
        <br>2019-02-06
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Constructing a Highlight Classifier with an Attention Based LSTM Neural
  Network -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.CL/paper_1.html">
      Constructing a Highlight Classifier with an Attention Based LSTM Neural
  Network
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この研究では、データを選別する際に市場研究者を支援する教師付き学習モデルに基づいたNLPベースのハイライト識別および抽出のための新しいアプローチを提示します。ハイライトの抽出と識別におけるパフォーマンス。この尽きることのない情報の供給をふるい分けて分析するのを支援する技術に対する需要の増加を期待するのは当然です。 
[概要]ビデオデータを分析するための標準的な方法は人件費です。業界の最先端のアプローチは2.2-ビデオコンテンツの1時間ごとに2.2時間の人材が必要です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Learning to Compare for Better Training and Evaluation of Open Domain
  Natural Language Generation Models -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.CL/paper_2.html">
      Learning to Compare for Better Training and Evaluation of Open Domain
  Natural Language Generation Models
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ストーリー生成とチットチャット対話応答生成の両方でアプローチを評価します。実験結果は、以前の自動評価アプローチと比較して、モデルが人間の好みとよく相関することを示しています。訓練済みモデルの評価に加えて、モデルをトレーニング中のパフォーマンスインジケータにより、ハイパーパラメーターの調整と早期停止が改善されます。 
[要旨]論文は、微調整されたbert.trainingによって生成された文のペアを、提案されたメトリックと比較する学習により、自然言語生成モデルを評価することを提案します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Attentional Speech Recognition Models Misbehave on Out-of-domain
  Utterances -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.CL/paper_3.html">
      Attentional Speech Recognition Models Misbehave on Out-of-domain
  Utterances
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      同じデータでトレーニングされたフレーム同期ハイブリッド（DNN-HMM）モデルでは、これらの異常に長いトランスクリプトは生成されません。 LibriSpeechタスクのワードエラー率を上げることなく、問題のあるデコード結果が得られます。1秒あたり100文字を超える）。 
[要約] librispeech corpusのみでトレーニングされた注意深いencoder.decoderモデルで英国国コーパスから音声をデコードします。decoding問題は、アーキテクチャからの音声変換モデルで再現されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Joint Embedding in Named Entity Linking on Sentence Level -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.CL/paper_4.html">
      Joint Embedding in Named Entity Linking on Sentence Level
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      文献には、最近の埋め込み手法がドキュメントレベルでトレーニングデータセットからエンティティのベクトルを学習する多くの報告された研究があります。知識グラフから学習した関係を最大化することにより、新しい統合埋め込み手法を提案します。文レベルでメンションのエンティティをリンクする方法に焦点を当てています。これにより、使用する情報が不十分であるという犠牲を払って、ドキュメント内の同じメンションのさまざまな出現によって導入されるノイズが低減されます。 
[ABSTRACT]ドキュメント内のメンションに複数の候補エンティティがあるという事実を考えると、名前付きエンティティリンクは困難です。メンションを手動でリンクする理由により、特定のトレーニングデータセットが小さいため困難です。そのマッピングエンティティ
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Global Greedy Dependency Parsing -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.CL/paper_5.html">
      Global Greedy Dependency Parsing
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ほとんどの構文依存性解析モデルは、遷移ベースおよびグラフベースのモデルの2つのカテゴリのいずれかに分類されます。提案されたグローバルな貪欲なパーサーは、射影解析に2つのアーク構築アクション、左右のアークのみを使用します。 Penn Treebank（PTB）、CoNLL-Xツリーバンク、Universal Dependency Treebanksを含め、パーサーを評価し、提案された新規パーサーがより速いトレーニングとデコードで良好なパフォーマンスを達成することを示します。 
[概要]前者のモデルは、線形時間の複雑さで高い尤度効率を享受します。しかし、部分的に構築された解析ツリーのスタッキングまたはリバンクに依存しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-20">
        <br>2019-11-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: FALCON 2.0: An Entity and Relation Linking Tool over Wikidata -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.CL/paper_6.html">
      FALCON 2.0: An Entity and Relation Linking Tool over Wikidata
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      自然言語処理（NLP）ツールとフレームワークは、エンティティと関係を抽出し、それらを関連する知識グラフにリンクする問題の解決に大きく貢献しました。Falcon2.0は公開されており、コミュニティで再利用できます。は英語の短い自然言語テキストです。 
[概要]既存のツールの大部分は、1つの知識graph.falcon 2. 0のみを抽出するために利用可能であり、その背景知識ベースはリソースとして利用可能です。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-24">
        <br>2019-12-24
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Pseudo-Bidirectional Decoding for Local Sequence Transduction -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.CL/paper_7.html">
      Pseudo-Bidirectional Decoding for Local Sequence Transduction
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本論文では、LSTタスクの特性に動機付けられ、LSTタスク用の疑似双方向復号化（PBD）と呼ばれるシンプルだが汎用性の高いアプローチを提案します。さらに、双方向復号化スキームとLSTタスクの特性により、 seq2seqモデルのエンコーダーとデコーダー。ローカルシーケンストランスダクション（LST）タスクは、Grammatical Error Correction（GEC）やスペルまたはOCR修正など、ソースシーケンスとターゲットシーケンスの間に大規模な重複が存在するシーケンストランスダクションタスクです。 
[要約]提案されたpbdアプローチは、デコーダーに右側のコンテキスト情報を提供します。lstタスクの誘導バイアスをモデル化し、パラメーター数を半分に減らし、良好な正則化効果を提供します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-31">
        <br>2020-01-31
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Using Chinese Glyphs for Named Entity Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.CL/paper_8.html">
      Using Chinese Glyphs for Named Entity Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本書では、中国語のNERシステムについて、これらの従来の機能は使用しませんが、中国語文字の辞書編集機能を使用します。ほとんどの名前付きエンティティ認識（NER）システムは、品詞（POS）タグ、浅い解析、このセマンティック情報を組み込んだCNNベースのモデルを提案し、NERに使用します。 
[ABSTRACT]新しい情報には、ラベルのないテキストや訓練されたタガーなどの外部知識が必要です。これらのシステムでは、広範なデータクリーニングが必要
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-22">
        <br>2019-09-22
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Adversarial Attacks on GMM i-vector based Speaker Verification Systems -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.CL/paper_9.html">
      Adversarial Attacks on GMM i-vector based Speaker Verification Systems
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      等しいエラー率と誤認率の低下によってシステムの脆弱性を測定します。これらの敵対的なサンプルは、GMM i-vectorシステムとx-vectorシステムの両方を攻撃するために使用されます。 
[概要]実験結果は、gmm i-スピーカーシステムが敵の攻撃に対して脆弱であることを示しています。細工された敵のサンプルは転送可能で、i-4システムに脅威を与えることが判明しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-08">
        <br>2019-11-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: On Layer Normalization in the Transformer Architecture -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.CL/paper_10.html">
      On Layer Normalization in the Transformer Architecture
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      Transformerは、自然言語処理タスクで広く使用されています。ウォームアップステージは、この問題を回避するために実際に役立ちます。我々の実験では、ウォームアップステージのないPre-LNトランスフォーマーは、幅広いアプリケーションでのトレーニング時間の大幅な短縮とハイパーパラメーターの調整。 
[要約]これは、事前変圧器のトレーニングのためのウォームアップ段階を削除する動機付けです。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Multi-task Learning Model for Chinese-oriented Aspect Polarity
  Classification and Aspect Term Extraction -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.CL/paper_11.html">
      A Multi-task Learning Model for Chinese-oriented Aspect Polarity
  Classification and Aspect Term Extraction
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      既存のモデルと比較して、このモデルはアスペクト用語を抽出し、アスペクト用語の極性を同期的に推定する機能を備えています。さらに、このモデルは中国語と英語の両方のコメントを同時に分析するのに効果的であり、多言語混合データセットの実験がその可用性を証明しました。ベースの感情分析（ABSA）タスクは、自然言語処理のマルチグレインタスクであり、アスペクト用語抽出（ATE）とアスペクト極性分類（APC）の2つのサブタスクで構成されています。既存の作業のほとんどは、アスペクト用語のサブタスクに焦点を当てています極性を推測し、アスペクト用語抽出の重要性を無視します。 
[ABSTRACT]既存の作業のほとんどは、アスペクト用語極性推論のサブタスクに焦点を当てています。インパクト用語抽出の重要性を無視します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-17">
        <br>2019-12-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: ConvLab-2: An Open-Source Toolkit for Building, Evaluating, and
  Diagnosing Dialogue Systems -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.CL/paper_12.html">
      ConvLab-2: An Open-Source Toolkit for Building, Evaluating, and
  Diagnosing Dialogue Systems
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      分析ツールは豊富な統計情報を提示し、シミュレートされたダイアログからの一般的な誤りを要約し、エラー分析とシステム改善を促進します。研究者が最先端のタスク指向の対話システムを構築できるオープンソースのツールキットConvLab-2を提示します-芸術モデル、エンドツーエンド評価を実行し、システムの弱点を診断します。インタラクティブツールは、システムと対話し、各システムコンポーネントの出力を変更することにより、開発者が組み立てられたダイアログシステムを診断できるユーザーインターフェイスを提供します。 
[概要] convlabの後継として、convlabのフレームワークを継承しますが、より強力な対話モデルを統合し、より多くのデータセットをサポートします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Self-Adversarial Learning with Comparative Discrimination for Text
  Generation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.CL/paper_13.html">
      Self-Adversarial Learning with Comparative Discrimination for Text
  Generation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この自己改善報酬メカニズムにより、モデルはより簡単にクレジットを受け取り、限られた数の実際のサンプルへの崩壊を避けることができ、報酬の希薄性の問題を緩和するだけでなく、モード崩壊のリスクも軽減します。提案されたアプローチは、品質と多様性の両方を大幅に改善し、テキスト生成の以前のGANと比較してより安定したパフォーマンスをもたらすことを示します。サンプル。 
[要約]たとえば、テキスト生成におけるガンのパフォーマンスを改善するための新しい自己敵対学習（sal）イネーターを提案します。代わりに、salは、現在の生成された文が以前に生成されたサンプルよりも良い場合
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-31">
        <br>2020-01-31
      </time>
    </span>
  </h3>
</article>
<!-- paper0: DeepMutation: A Neural Mutation Tool -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.CL/paper_14.html">
      DeepMutation: A Neural Mutation Tool
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この論文では、ディープラーニングモデルを、ディープラーニングモデルを実際の障害から学習した変異体を生成、注入、およびテストできる完全に自動化されたツールチェーンにラッピングするツールであるDeepMutationを紹介します。 Recurrent Neural Network Encoder-Decoderアーキテクチャを使用して、実際のプログラムからマイニングされた〜787kフォールトからミュータントを学習するアプローチ。ミューテーションテストを使用して、特定のテストスイートのフォールト検出機能を評価できます。 
[ABSTRACT]テストポイントはテストポイントの目的です。自動的に突然変異体を生成、注入、テストできるツールチェーンを提供する必要があります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Europarl-ST: A Multilingual Corpus For Speech Translation Of
  Parliamentary Debates -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.CL/paper_15.html">
      Europarl-ST: A Multilingual Corpus For Speech Translation Of
  Parliamentary Debates
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このコーパスは、Creative Commonsライセンスの下でリリースされており、自由にアクセスおよびダウンロードできます。このコーパスは、2008年から2012年の間に欧州議会で開催された議論を使用して編集されました。この新しいリソースの可能性を際立たせる自動音声認識、機械翻訳、音声言語翻訳の実験を行っています。 
[要旨]論文では、このリソースの可能性を強調する一連の言語を紹介しています。これは、ヨーロッパおよびヨーロッパの研究者の研究です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-08">
        <br>2019-11-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Utilizing BERT Intermediate Layers for Aspect Based Sentiment Analysis
  and Natural Language Inference -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.CL/paper_16.html">
      Utilizing BERT Intermediate Layers for Aspect Based Sentiment Analysis
  and Natural Language Inference
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、提案されたアプローチの有効性と一般性を示しています。一般性を示すために、自然言語推論タスクにもこのアプローチを適用します。事前トレーニング済みBERTの微調整は、このタスクで優れたパフォーマンスを発揮し、最新のアートパフォーマンス。 
[ABSTRACT]このペーパーでは、bert中間層を使用する可能性を探ります。一般性を示すために、このアプローチを自然言語の可能性タスクにも適用します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: CIF: Continuous Integrate-and-Fire for End-to-End Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/cs.CL/paper_17.html">
      CIF: Continuous Integrate-and-Fire for End-to-End Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらの方法の共同作用により、CIFベースのモデルは競争力のあるパフォーマンスを示します。特に、Librispeechのテストクリーンで2.86％のワードエラー率（WER）を達成し、新しい最先端の結果を作成します。マンダリン電話ASRベンチマークについて。CIFベースのモデルの固有の問題を軽減するために、いくつかのサポート戦略も提案されています。 
[概要]システムは、統合に基づいています-火災モデル。それは、エンコーダーに基づいています-デコーダーフレームワークは、連続関数で構成されています。テスト-librispeechを削除し、マンダリン電話asrベンチマークで新しい最先端の結果を作成します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-05-27">
        <br>2019-05-27
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Audio-Visual Calibration with Polynomial Regression for 2-D Projection
  Using SVD-PHAT -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/eess.AS/paper_0.html">
      Audio-Visual Calibration with Polynomial Regression for 2-D Projection
  Using SVD-PHAT
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      低コストのマイクアレイと市販のカメラを使用して、多項式回帰が非線形カメラの歪みを効率的に処理できることと、リアルタイム処理用に最近提案された音源定位法SVD-PHATを示します。この論文では、音響画像を生成して光学画像の上に重ねることにより、カメラの視野をアレイマイクの聴覚領域で空間的に較正する簡単な2次元法を提案します。 
[概要]低コストのマイクフィールドとカメラのアレイを使用して、音響が非線形カメラの歪みを効率的に処理できることを示します。リアルタイム処理のための最近提案された音源定位法svd-phatは、このタスクに適合
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-04">
        <br>2020-02-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Speech-Based Parameter Estimation of an Asymmetric Vocal Fold
  Oscillation Model and Its Application in Discriminating Vocal Fold
  Pathologies -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/eess.AS/paper_1.html">
      Speech-Based Parameter Estimation of an Asymmetric Vocal Fold
  Oscillation Model and Its Application in Discriminating Vocal Fold
  Pathologies
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      非対称モデルに焦点を当て、そのようなモデルの場合、推定されたパラメーターの違いを使用して、さまざまな潜在的な声帯病変の特徴である音声を区別できることを示します。多くの場合、特に病理学では、音声の特性折り畳みは一般的な値から逸脱することがよくあります---非対称である場合があり、同じ個人に対して2つの声帯の特性が異なります。そのような場合、ケースごとにモデルパラメータを調整するよりスケーラブルな方法を見つけることが望ましい基礎。 
[要約]多くの場合、声帯の特性は一般的な値から逸脱することが多い
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-20">
        <br>2019-10-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: x-vectors meet emotions: A study on dependencies between emotion and
  speaker recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/eess.AS/paper_2.html">
      x-vectors meet emotions: A study on dependencies between emotion and
  speaker recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最後に、感情が話者照合に及ぼす影響に関する結果を示します。IEMOCAP、MSP-Podcast、およびCrema-の3種類のデータセットでの実験を評価しました。 D.微調整により、事前トレーニングなしのベースラインモデルと比較して、IEMOCAP、MSP-Podcast、およびCrema-Dでそれぞれ30.40％、7.99％、8.61％の絶対改善が得られました。 
[概要]まず、話者認識のために学習した知識を、転移学習を通じて感情認識に再利用できることを示します。話者検証パフォーマンスは、テスト話者の感情の変化を起こしやすいことがわかりました。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Attentional Speech Recognition Models Misbehave on Out-of-domain
  Utterances -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/eess.AS/paper_3.html">
      Attentional Speech Recognition Models Misbehave on Out-of-domain
  Utterances
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      出力内のワードピースの正しい数を予測するために個別の長さ予測モデルを作成します。これにより、LibriSpeechタスクでのワードエラー率を増加させることなく、問題のあるデコード結果を特定および切り捨てることができます。フレーム同期ハイブリッド（DNN-HMM）モデルこれらのデコードの問題は、ESPnetの音声トランスフォーマーモデルで再現可能であり、自己注意CTCモデルではそれほどではありません。これらの問題は、注意メカニズム。 
[要約] librispeech corpusのみでトレーニングされた注意深いencoder.decoderモデルで英国国コーパスから音声をデコードします。decoding問題は、アーキテクチャからの音声変換モデルで再現されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multimodal active speaker detection and virtual cinematography for video
  conferencing -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/eess.AS/paper_4.html">
      Multimodal active speaker detection and virtual cinematography for video
  conferencing
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このシステムは、4KワイドFOVカメラ、深度カメラ、およびマイクアレイを使用します。各モダリティから特徴を抽出し、非常に効率的でリアルタイムで実行されるAdaBoost機械学習システムを使用してASDをトレーニングします。部屋の参加者の気を散らすのを防ぎ、スイッチングレイテンシを減らすために、システムには可動部分がありません-VCは4KワイドFOVビデオストリームのトリミングとズーム。アクティブスピーカー検出（ASD）と仮想シネマトグラフィー（VC）は、ビデオ会議カメラの自動パン、チルト、ズームにより、ユーザーの主観的な操作を大幅に改善します。専門家のビデオ撮影技師のビデオを、未編集のビデオよりも格段に高く評価します。 
[概要] asdとvcは、専門のシネマトグラファーの0.3 mos以内で実行されます。システムは、クラウドソーシングテクニックを使用して調整および評価されました。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-10">
        <br>2020-02-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: AlignNet: A Unifying Approach to Audio-Visual Alignment -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/eess.AS/paper_5.html">
      AlignNet: A Unifying Approach to Audio-Visual Alignment
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ダンスミュージックアライメントとスピーチリップアライメントに関する定性的、定量的、主観的評価の結果は、本手法が最先端の手法よりもはるかに優れていることを示しています。 AlignNetは、ビデオとオーディオの各フレーム間のエンドツーエンドの密な対応を学習します。 
[ABSTRACT] maskは、ビデオとaudio.syncnetのよく知られた対応を学習します。syncnetは、video.dance50とdance50の各フレーム間のよく認識された対応を学習します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Deep Feature Embedding and Hierarchical Classification for Audio Scene
  Classification -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/eess.AS/paper_6.html">
      Deep Feature Embedding and Hierarchical Classification for Audio Scene
  Classification
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      学習済みの畳み込みニューラルネットワークを介して、学習した埋め込みは、入力を埋め込み特徴空間に埋め込み、それを表現のための高レベルの特徴ベクトルに変換します。その後、トリプレット損失関数に関連付けられたディープニューラルネットワーク分類子を使用して階層分類が行われます。一方、シーンカテゴリの構造を活用するために、元のシーン分類問題は、類似のカテゴリがメタカテゴリにグループ化される階層に構造化されます。 
[要旨]深い畳み込みニューラルネットワークは、シーンのオーディオ信号から埋め込まれる特徴を学習するように訓練されます。提案されたシステムは、両方のdcase 2018データセットで良好なパフォーマンスを達成します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Audiogmenter: a MATLAB Toolbox for Audio Data Augmentation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/eess.AS/paper_7.html">
      Audiogmenter: a MATLAB Toolbox for Audio Data Augmentation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの知る限り、これは無料で利用できる最大のMATLABオーディオデータ増強ライブラリです。オーディオデータ増強は、オーディオ分類タスクを解決するためのディープニューラルネットワークのトレーニングにおける重要なステップです。ツールボックスとそのドキュメントはhttps： //github.com/LorisNanni/Audiogmenter。 
[ABSTRACT] audiogmenterはmatgmenterの新しいオーディオデータ拡張ライブラリです。我々は、その有用性が文献で広く証明されているいくつかの拡張技術を効率的に実装しました。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-11">
        <br>2019-12-11
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Adversarial Attacks on GMM i-vector based Speaker Verification Systems -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/eess.AS/paper_8.html">
      Adversarial Attacks on GMM i-vector based Speaker Verification Systems
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      等しいエラー率と誤認率の低下によってシステムの脆弱性を測定します。これらの敵対的なサンプルは、GMM i-vectorシステムとx-vectorシステムの両方を攻撃するために使用されます。 
[概要]実験結果は、gmm i-スピーカーシステムが敵の攻撃に対して脆弱であることを示しています。細工された敵のサンプルは転送可能で、i-4システムに脅威を与えることが判明しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-08">
        <br>2019-11-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Active Learning for Sound Event Detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/eess.AS/paper_9.html">
      Active Learning for Sound Event Detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      注目すべきは、ターゲットのサウンドイベントがまれなデータセットでは、必要な注釈の労力を大幅に削減できることです。トレーニングデータの2％のみに注釈を付けることにより、達成されるSEDパフォーマンスはすべてのトレーニングデータに注釈を付けることに似ています..SEDモデルのトレーニング中提案されたシステムは、評価に使用される2つのデータセット（TUT Rare Sound 2017およびTAU Spatial Sound 2019）の参照方法より明らかに優れています。 
[要約]提案されたシステムは、限られた注釈の努力で学習したsedモデルの精度を最大化することを目指しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Content Based Singing Voice Extraction From a Musical Mixture -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/eess.AS/paper_10.html">
      Content Based Singing Voice Extraction From a Musical Mixture
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      モデルのエンコーダー部分は、教師ネットワークを使用して知識の蒸留を介して学習し、コンテンツの埋め込みを学習します。これは、対応するボコーダー機能を生成するためにデコードされます。また、再現性のある健全な例とソースコードも提供します。従来の客観的評価指標と矛盾するため、リスニングテストによる主観的評価を使用して、方法論を最新の深層学習ベースのソース分離アルゴリズムと比較します。 
[概要]このモデルは、音楽の混合物のエンコーダモデルに基づいています。ボーカルとの混合物のスペクトログラムの振幅成分を取ります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Europarl-ST: A Multilingual Corpus For Speech Translation Of
  Parliamentary Debates -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/eess.AS/paper_11.html">
      Europarl-ST: A Multilingual Corpus For Speech Translation Of
  Parliamentary Debates
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このコーパスは、クリエイティブコモンズライセンスの下でリリースされており、自由にアクセスおよびダウンロードできます。このコーパスは、2008年から2012年の間に欧州議会で開催された議論を使用して編集されました。 6つのヨーロッパ言語との間のSLTのペアのオーディオテキストサンプルを含む多言語SLTコーパス、合計30の異なる翻訳方向。 
[要旨]論文では、このリソースの可能性を強調する一連の言語を紹介しています。これは、ヨーロッパおよびヨーロッパの研究者の研究です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-08">
        <br>2019-11-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: CIF: Continuous Integrate-and-Fire for End-to-End Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-13/eess.AS/paper_12.html">
      CIF: Continuous Integrate-and-Fire for End-to-End Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ASRタスクに適用されるCIFは、簡潔な計算を表示するだけでなく、オンライン認識と音響境界位置決めをサポートするため、さまざまなASRシナリオに適しています。特に、テストで2.86％の単語エラー率（WER）を達成します。 Librispeechを削除し、マンダリン電話ASRベンチマークで最新の結果を作成します。CIFベースモデルの固有の問題を軽減するために、いくつかのサポート戦略も提案されています。 
[概要]システムは、統合に基づいています-火災モデル。それは、エンコーダーに基づいています-デコーダーフレームワークは、連続関数で構成されています。テスト-librispeechを削除し、マンダリン電話asrベンチマークで新しい最先端の結果を作成します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-05-27">
        <br>2019-05-27
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="">
      
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本日更新された論文はありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="">
        <br>
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
