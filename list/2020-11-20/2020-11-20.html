<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-11-20の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging,
  audio, and lip videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.SD/paper_0.html">
      <font color="black">TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging,
  audio, and lip videos</font>
    </a>
  </h2>
  <font color="black">TaLコーパスは、CC BY-NC 4.0ライセンスの下で公開されています。TaLは2つの部分で構成されています。TaL1は、英語を母国語とする男性1人のプロの声優による6つの録音セッションのセットです。 TaL80は、声優の経験のない英語のネイティブスピーカー81人の録音セッションのセットです。全体として、コーパスには24時間の並列超音波、ビデオ、およびオーディオデータが含まれ、そのうち約13.5時間は音声です。 
[ABSTRACT] tal1は、英語を母国語とする男性の1人のプロの声優による録音セッションのセットです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Uncertainty-Aware Multi-Modal Ensembling for Severity Prediction of
  Alzheimer's Dementia -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.SD/paper_1.html">
      <font color="black">Uncertainty-Aware Multi-Modal Ensembling for Severity Prediction of
  Alzheimer's Dementia</font>
    </a>
  </h2>
  <font color="black">不確実性の推定に基づいてさまざまなモダリティを比較検討し、被験者に依存しないバランスの取れたデータセットであるベンチマークADReSSデータセットを実験して、システムの全体的なエントロピーを低減しながら、この方法が最先端の方法よりも優れていることを示します。 ..この作業は、公正で意識の高いモデルを奨励することを目的としています。ソースコードはhttps://github.com/wazeerzulfikar/alzheimers-dementia 
[ABSTRACT]で入手できます。この作業では、アルツハイマーを予測するための音響ブースティング手法を提案します。 。この方法は、システムの全体的な不安定性を低減しながら、最先端の方法よりも優れています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-03">
        <br><font color="black">2020-10-03</font>
      </time>
    </span>
</section>
<!-- paper0: Universal MelGAN: A Robust Neural Vocoder for High-Fidelity Waveform
  Generation in Multiple Domains -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.SD/paper_2.html">
      <font color="black">Universal MelGAN: A Robust Neural Vocoder for High-Fidelity Waveform
  Generation in Multiple Domains</font>
    </a>
  </h2>
  <font color="black">モデルは、グラウンドトゥルースメルスペクトログラムを入力として使用して、ほとんどのシナリオで最高の平均オピニオン評点（MOS）を達成しました。これにより、モデルは、高周波での過度の平滑化の問題を軽減することにより、マルチスピーカーのリアルな波形を生成できます。大フットプリントモデルの帯域..複数のドメインで忠実度の高い音声を合成するボコーダーであるUniversalMelGANを提案します。 
[概要]メルガンベースの構造は、地面に近い信号を生成します-真実のデータ。モデルはスペクトログラムスピーカーを使用して22を超えるスピーカーを生成しました。外部ドメイン情報なしで達成された結果は、ユニバーサルボコーダーとして提案されたモデルの可能性を強調しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Residual Local Feature Learning for Speech Emotion Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.SD/paper_3.html">
      <font color="black">Deep Residual Local Feature Learning for Speech Emotion Recognition</font>
    </a>
  </h2>
  <font color="black">EMODBとRAVDESSの2つの利用可能な公開データセットに基づいて、提案されたDeepResLFLBは、標準メトリック（精度、適合率、再現率、およびF1スコア）で評価すると、パフォーマンスを大幅に向上させることができます。DeepResLFLBは、LFLB、残差局所特徴学習ブロックの3つのカスケードブロックで構成されます。 （ResLFLB）、および多層パーセプトロン（MLP）。したがって、このペーパーでは、既存のローカル特徴学習ブロック（LFLB）の再設計を提案しました。 
[概要]新しい設計は、深残余局所特徴学習ブロック（deepreslflb）と呼ばれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: End-To-End Dilated Variational Autoencoder with Bottleneck
  Discriminative Loss for Sound Morphing -- A Preliminary Study -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.SD/paper_4.html">
      <font color="black">End-To-End Dilated Variational Autoencoder with Bottleneck
  Discriminative Loss for Sound Morphing -- A Preliminary Study</font>
    </a>
  </h2>
  <font color="black">話し言葉とドラム音の両方のモーフィングの例を示します。音のモーフィングのためのエンドツーエンドの変分オートエンコーダ（VAE）に関する予備調査を示します。パラメータ化とデータセットのこれらの結果は、DC-VAEがより優れていることを示しています。 DC-VAEデコーダーは、オーディオドメインから潜在空間にマッピングするときにトポロジをより適切に保持するため、CC-VAEよりもサウンドモーフィングに適しています。 
[概要] 2つのmme-vaeバージョンは、拡張レイヤー（dc-vae）を備えたvaeと比較されます。vaeは2年以上にわたってcc-vaeを上回ります。たとえば、サウンドクラスはボトルネックレイヤーで分離します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Recursive Deep Prior Video: a Super Resolution algorithm for Time-Lapse
  Microscopy of organ-on-chip experiments -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.IV/paper_0.html">
      <font color="black">Recursive Deep Prior Video: a Super Resolution algorithm for Time-Lapse
  Microscopy of organ-on-chip experiments</font>
    </a>
  </h2>
  <font color="black">さらに、DIP損失関数は、2つの異なる全変動（TV）ベースの項によってペナルティが課せられます。DIPネットワークアーキテクチャの重みは、効率的な早期停止基準と組み合わされた新しい再帰更新ルールに従って、フレームごとに初期化されます。この方法は、合成、つまり人工的に生成されたもの、および腫瘍と免疫の相互作用に関連するOOC実験からの実際のビデオで検証されています。 
[要約]この方法は、人工的、人工的に生成された、および腫瘍と免疫の相互作用に関連するooc実験からの実際のビデオで検証されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Semi-supervised Task-driven Data Augmentation for Medical Image
  Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.IV/paper_1.html">
      <font color="black">Semi-supervised Task-driven Data Augmentation for Medical Image
  Segmentation</font>
    </a>
  </h2>
  <font color="black">この作業では、合成データジェネレータがセグメンテーションタスク用に最適化されている、限られたラベル付きデータで学習するための新しいタスク駆動型データ拡張方法を提案します。提案された方法のジェネレータは、2セットの変換を使用して強度と形状の変化をモデル化します。 、加法強度変換および変形フィールドとして..コードはhttps://github.com/krishnabits001/task$\_$driven$\_$data$\_$augmentationで公開されています。 
[要約]提案されたアプローチは、ランダムなデータ拡張よりもまだ大幅な向上をもたらしていません。しかし、調査によると、新しいアプローチは標準的な拡張を大幅に上回っています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-09">
        <br><font color="black">2020-07-09</font>
      </time>
    </span>
</section>
<!-- paper0: Mini-DDSM: Mammography-based Automatic Age Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.IV/paper_2.html">
      <font color="black">Mini-DDSM: Mammography-based Automatic Age Estimation</font>
    </a>
  </h2>
  <font color="black">このデータセットの元の画像は、残念ながら壊れたソフトウェアでしか取得できません。年齢推定は、さまざまな医療アプリケーションで注目を集めています。サンプルのランダム選択での10回のテストの平均エラー値は約8年でした。 
[概要]この研究の目的は、マンモグラム画像から年齢を再分類するためのaiベースのモデルを考案することです。マンモグラム画像は、壊れたソフトウェアによってのみ取得できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Foreground-Aware Relation Network for Geospatial Object Segmentation in
  High Spatial Resolution Remote Sensing Imagery -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.IV/paper_3.html">
      <font color="black">Foreground-Aware Relation Network for Geospatial Object Segmentation in
  High Spatial Resolution Remote Sensing Imagery</font>
    </a>
  </h2>
  <font color="black">一方、最適化の観点から、前景を意識した最適化は、バランスの取れた最適化のためのトレーニング中に前景の例と背景の難しい例に焦点を当てることが提案されています。しかし、一般的なセマンティックセグメンテーション方法は主に自然シーンのスケール変動に焦点を当てており、不十分です。大面積の地球観測シーンで通常発生する他の2つの問題の検討..コードは\ url {https://github.com/Z-Zheng/FarSeg}で入手できます。 
[概要]提案された方法は、最先端の一般的なセマンティックセグメンテーション方法よりも優れています。これらには、大面積の地球観測シーンで通常発生する他の2つの問題の不適切な考慮が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging,
  audio, and lip videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.IV/paper_4.html">
      <font color="black">TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging,
  audio, and lip videos</font>
    </a>
  </h2>
  <font color="black">TaLは2つの部分で構成されています。TaL1は、英語を母国語とする男性の1人のプロの声優による6つの録音セッションのセットです。 TaL80は、声優の経験のない英語のネイティブスピーカー81人の録音セッションのセットです。音声、超音波舌イメージング、および唇のビデオのマルチスピーカーコーパスであるTongue and Lipsコーパス（TaL）を紹介します。TaLコーパスCC BY-NC4.0ライセンスの下で公開されています。 
[ABSTRACT] tal1は、英語を母国語とする男性の1人のプロの声優による録音セッションのセットです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Multiclass Yeast Segmentation in Microstructured Environments with Deep
  Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.IV/paper_5.html">
      <font color="black">Multiclass Yeast Segmentation in Microstructured Environments with Deep
  Learning</font>
    </a>
  </h2>
  <font color="black">モデルは堅牢なセグメンテーション結果を達成し、精度と速度の両方で以前の最先端技術を上回ります。微細構造環境の設定では課題が悪化します。微細構造環境で酵母をセグメント化する方法の貢献を典型的な方法で紹介します。合成生物学のアプリケーションを念頭に置いてください。 
[概要]微細構造環境の設定では課題が激化しています。今日は、個々の酵母細胞のマルチクラスセグメンテーション用にトレーニングされた畳み込みニューラルネットワークを紹介します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning for Automated Screening of Tuberculosis from Indian Chest
  X-rays: Analysis and Update -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.IV/paper_6.html">
      <font color="black">Deep Learning for Automated Screening of Tuberculosis from Indian Chest
  X-rays: Analysis and Update</font>
    </a>
  </h2>
  <font color="black">したがって、インドのデータセットでの結核の正確で自動化された診断のための深層学習は、依然として重要な研究対象です。提案された方法は、インドと深センのデータセットでの最先端を上回ります。結核の自動診断は、医療専門家の迅速化と特に、訓練を受けた医療専門家や放射線技師が不足しているインドのような発展途上国では、診断を改善します。 
[ABSTRACT]胸部X線写真から結核を検出するための深層学習ベースの方法が提案されています。この方法は、文献に記載されている手法に対してもテストされています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Deep LF-Net: Semantic Lung Segmentation from Indian Chest Radiographs
  Including Severely Unhealthy Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.IV/paper_7.html">
      <font color="black">Deep LF-Net: Semantic Lung Segmentation from Indian Chest Radiographs
  Including Severely Unhealthy Images</font>
    </a>
  </h2>
  <font color="black">DeepLabアーキテクチャ、エンコーダデコーダ、拡張畳み込みを統合したエンドツーエンドのDeepLabv3 +ネットワークを、高速トレーニングと高精度でセマンティック肺セグメンテーションに使用します。また、日本放射線技術学会などの一般的に使用されるベンチマークデータセットでも実験しました。米国モンゴメリー郡;提案された方法は、その堅牢性を検証するために、深刻な異常所見を伴う画像を含む、インドのCxRデータセットの688枚の画像でテストされます。 
[要約]提案された作業は、cxrからの肺の正確なセグメンテーションのための効率的な深い畳み込みニューラルネットワークの使用に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: A Preliminary Comparison Between Compressive Sampling and Anisotropic
  Mesh-based Image Representation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.IV/paper_8.html">
      <font color="black">A Preliminary Comparison Between Compressive Sampling and Anisotropic
  Mesh-based Image Representation</font>
    </a>
  </h2>
  <font color="black">結果は、同じサンプル密度で、AMA表現がテストされたアルゴリズムに基づいてCSよりも優れた再構成品質を提供できることを示しています。したがって、CSはデジタル画像の表現にも広く適用されています。通常の画像はそれ自体ではまばらではありませんが、多くはウェーブレット変換ドメインでまばらに表すことができます。 
[ABSTRACT] csと最近開発されたmbirメソッドであるama表現がテストされました。amarepresentativeとして知られるこのメソッドは効果的であるとは見なされていませんが、これらの画像を制限する代わりに、スパースにすることができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Self-Supervised Poisson-Gaussian Denoising -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.IV/paper_9.html">
      <font color="black">Self-Supervised Poisson-Gaussian Denoising</font>
    </a>
  </h2>
  <font color="black">パフォーマンスを向上させるために、デノイザーをテストデータに適合させる方法を示します。新しい戦略では、損失関数からハイパーパラメーターを排除します。これは、ハイパーパラメーターの調整をガイドするためのグラウンドトゥルースデータが利用できない自己監視体制で重要です。顕微鏡画像の標準ノイズモデルであるポアソンガウスノイズを処理するための新しいトレーニング戦略を導入します。 
[概要]私たちの新しい戦略は、損失関数からハイパーパラメータを排除します。これは、自己監視体制で重要です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-21">
        <br><font color="black">2020-02-21</font>
      </time>
    </span>
</section>
<!-- paper0: Noise adaptive beamforming for linear array photoacoustic imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.IV/paper_10.html">
      <font color="black">Noise adaptive beamforming for linear array photoacoustic imaging</font>
    </a>
  </h2>
  <font color="black">残念ながら、これらの既存の適応ベースのアルゴリズムは、処理ハードウェアの計算能力の向上を要求するため、計算コストが高くなります。この記事では、ノイズレベルの変動を考慮した変動コヒーレンス係数（VCF）という名前の新しい適応重み係数を紹介します。この提案された技術は、画像解像度、サイドローブ低減、信号対雑音、およびコントラストレベルの改善に関して優れた結果を提供します。 
[ABSTRACT] dasメソッドは、適切な遅延と合計を使用してビーム形成信号を計算することに依存しています。これらのアルゴリズムは、画質を向上させるために導入されました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Spectral Response Function Guided Deep Optimization-driven Network for
  Spectral Super-resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.IV/paper_11.html">
      <font color="black">Spectral Response Function Guided Deep Optimization-driven Network for
  Spectral Super-resolution</font>
    </a>
  </h2>
  <font color="black">従来のSSR手法には、モデル駆動型アルゴリズムと深層学習が含まれます。完全データ駆動型CNNとは異なり、補助スペクトル応答関数（SRF）を使用して、CNNをガイドし、スペクトル関連性のあるバンドをグループ化します。リモートセンシングでの分類結果データセットはまた、提案された方法によって強化された情報の有効性を検証しました。 
[ABSTRACT]ハイパースペクトル画像は、ハイパースペクトル画像を作成するために使用される方法です。この方法は、高空間解像度（hr）のハイパースペクトル画像を取得するために使用されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: All-in-Focus Iris Camera With a Great Capture Volume -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.IV/paper_12.html">
      <font color="black">All-in-Focus Iris Camera With a Great Capture Volume</font>
    </a>
  </h2>
  <font color="black">私たちの虹彩イメージング被写界深度拡張システムは、機械的な動きを必要とせず、非常に高速で焦点面を調整することができます。さらに、電動反射ミラーは、アクティブな状態で水平および垂直の視野を拡張するために光ビームを適応的に操縦します本研究では、焦点調整可能レンズと2Dステアリングミラーを使用して、時空間多重化法によりキャプチャボリュームを大幅に拡張する、新しいオールインフォーカスアイリスイメージングシステムを開発します。 
[概要]提案されたオールインフォーカスアイリスカメラは、被写界深度を最大3.9 mまで拡大します。これは、従来の長レンズと比較して37.5倍です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Latent-Separated Global Prediction for Learned Image Compression -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.IV/paper_13.html">
      <font color="black">Latent-Separated Global Prediction for Learned Image Compression</font>
    </a>
  </h2>
  <font color="black">明らかに、参照ポイントと現在のデコードポイント間のグローバル相関を示す予測ベクトルを送信するためのビットオーバーヘッドが生成されます。最近学習された画像コーデックは、最初に画像を低次元の潜在表現にエンコードし、次に自動エンコーダに基づいています。再構成のためにそれらをデコードします。最初のグループがデコードされると、提案されたモジュールは既知のグループを活用して、未知のグループのグローバル予測を導く空間相関をモデル化し、より効率的なエントロピー推定を実現します。 
[概要]最近学習した画像コーデックはオートエンコーダに基づいています。最初に画像を致命的でない潜在表現にエンコードし、次に再構成のためにデコードしますが、効果的な長距離依存関係をモデル化するのは困難です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: LIFI: Towards Linguistically Informed Frame Interpolation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.IV/paper_14.html">
      <font color="black">LIFI: Towards Linguistically Informed Frame Interpolation</font>
    </a>
  </h2>
  <font color="black">この動機で、特に音声ビデオ補間の問題を対象とした言語情報に基づく新しいメトリックのセットを提供します。また、音声理解のコンピュータービジョンビデオ生成モデルをテストするためのいくつかのデータセットをリリースします。この作業では、音声ビデオのフレーム補間の新しい問題。 
[概要]音声理解のコンピュータービジョンビデオ生成モデルをテストするためのいくつかのデータセットもリリースします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-30">
        <br><font color="black">2020-10-30</font>
      </time>
    </span>
</section>
<!-- paper0: Noise2Inpaint: Learning Referenceless Denoising by Inpainting Unrolling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.IV/paper_15.html">
      <font color="black">Noise2Inpaint: Learning Referenceless Denoising by Inpainting Unrolling</font>
    </a>
  </h2>
  <font color="black">重要なのは、これらの1つを使用して、展開されたネットワークにデータの忠実度を課し、もう1つは損失を定義することです。これにより、必要に応じてノイズのさまざまな統計的特性を組み込むことができる目的関数を使用できます。これらの自己監視アプローチに基づいて、ノイズ除去の問題を正規化された画像修復フレームワークに再キャストするトレーニングアプローチであるNoise2Inpaint（N2I）を紹介します。 
[概要]以前は、これらのメソッドは教師ありの方法でトレーニングされていました。ノイズの多い入力とクリーンなターゲット画像のセットが必要です。これにより、必要に応じてノイズのさまざまな統計的特性を組み込むことができる目的関数を使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-16">
        <br><font color="black">2020-06-16</font>
      </time>
    </span>
</section>
<!-- paper0: Perceptron Synthesis Network: Rethinking the Action Scale Variances in
  Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.IV/paper_16.html">
      <font color="black">Perceptron Synthesis Network: Rethinking the Action Scale Variances in
  Videos</font>
    </a>
  </h2>
  <font color="black">パスの相互作用の豊富さと情報容量を保証するために、新しい\ textit {最適化された特徴融合レイヤー}を設計します。このレイヤーは、現在の特徴融合手法（チャネルシャッフルなど）のほとんどをカバーするのに十分な原理的なユニバーサルパラダイムを確立します。 、およびチャネルドロップアウト）を初めて.. \ textit {synthesizer}を挿入することにより、この方法では、従来の2D CNNを、わずかな追加計算コストで、アクション認識などのビデオ理解タスクに簡単に適合させることができます。 
[ABSTRACT]モーションパーセプトロンシンセサイザーは、スロットパスによって相互作用される固定サイズのカーネルのバッグからカーネルを生成するために提案されています。このレイヤーは、現在の機能融合技術のほとんどを初めてカバーするのに十分な原理的なユニバーサルツールを設定します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-22">
        <br><font color="black">2020-07-22</font>
      </time>
    </span>
</section>
<!-- paper0: PhaseGAN: A deep-learning phase-retrieval approach for unpaired datasets -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.IV/paper_17.html">
      <font color="black">PhaseGAN: A deep-learning phase-retrieval approach for unpaired datasets</font>
    </a>
  </h2>
  <font color="black">ここでは、生成的敵対的ネットワークに基づく新しいDLアプローチであるPhaseGANを紹介します。これにより、対になっていないデータセットの使用が可能になり、画像形成の物理学が含まれます。DLに基づく位相回復アプローチは、強度ホログラムから位相情報を取得するためのフレームワークを提供します。回折パターンをロバストな方法でリアルタイムに..したがって、PhaseGANは、位相再構成が利用できない場合に位相問題に対処する機会を提供しますが、オブジェクトの優れたシミュレーションまたは他の実験からのデータが利用可能であり、以前は可能でした。 
[概要]現在のdlアーキテクチャは、データセットに依存する位相問題に適用されます。これらは、満足のいく解決策が見つかった場合にのみ適用されます。代わりに、それらのほとんどがイメージングプロセスの物理を無視するという事実に基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-15">
        <br><font color="black">2020-11-15</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Recursive Deep Prior Video: a Super Resolution algorithm for Time-Lapse
  Microscopy of organ-on-chip experiments -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_0.html">
      <font color="black">Recursive Deep Prior Video: a Super Resolution algorithm for Time-Lapse
  Microscopy of organ-on-chip experiments</font>
    </a>
  </h2>
  <font color="black">達成された結果は、卓越したパフォーマンスを示すいくつかの最先端の訓練された深層学習SRアルゴリズムと比較されます。この方法は、合成、つまり人工的に生成されたもの、および腫瘍と免疫の相互作用に関連するOOC実験からの実際のビデオで検証されています。 ..さらに、DIP損失関数は、2つの異なる合計変動（TV）ベースの項によってペナルティが課せられます。 
[要約]この方法は、人工的、人工的に生成された、および腫瘍と免疫の相互作用に関連するooc実験からの実際のビデオで検証されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Semi-supervised Task-driven Data Augmentation for Medical Image
  Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_1.html">
      <font color="black">Semi-supervised Task-driven Data Augmentation for Medical Image
  Segmentation</font>
    </a>
  </h2>
  <font color="black">コードはhttps://github.com/krishnabits001/task$\_$driven$\_$data$\_$augmentationで公開されています。教師あり学習ベースのセグメンテーション方法では、通常、多数の注釈付きトレーニングデータが必要です。テスト時によく一般化するために..この作業では、合成データジェネレーターがセグメンテーションタスク用に最適化されている、限られたラベル付きデータで学習するための新しいタスク駆動型データ拡張方法を提案します。 
[要約]提案されたアプローチは、ランダムなデータ拡張よりもまだ大幅な向上をもたらしていません。しかし、調査によると、新しいアプローチは標準的な拡張を大幅に上回っています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-09">
        <br><font color="black">2020-07-09</font>
      </time>
    </span>
</section>
<!-- paper0: Everybody Sign Now: Translating Spoken Language to Photo Realistic Sign
  Language Video -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_2.html">
      <font color="black">Everybody Sign Now: Translating Spoken Language to Photo Realistic Sign
  Language Video</font>
    </a>
  </h2>
  <font color="black">放送映像から抽出された8つの異なる手話通訳者のデータセットを使用して、SignGANが定量的指標と人間の知覚研究のすべてのベースライン方法を大幅に上回っていることを示します。次に、ポーズ条件付きの人間合成モデルを導入して、フォトリアリスティックな手話を生成します骨格ポーズシーケンスからのビデオ..聴覚障害者のコミュニティに真に理解され受け入れられるためには、自動手話制作（SLP）システムが写真のようにリアルな署名者を生成する必要があります。 
[概要]混合密度ネットワークを備えたトランスアーキテクチャを採用して、話し言葉から骨格ポーズへの翻訳を処理します。サインビデオは、書かれたテキストから直接翻訳して作成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised
  Visual Representation Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_3.html">
      <font color="black">Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised
  Visual Representation Learning</font>
    </a>
  </h2>
  <font color="black">さらに、最先端のアプローチを大幅に上回り、より良い結果を生み出すピクセルから伝搬への整合性タスクを提案します。具体的には、に転送すると60.2 AP、41.4 / 40.5 mAP、77.2mIoUを達成します。 Pascal VOCオブジェクト検出（C4）、COCOオブジェクト検出（FPN / C4）、およびResNet-50バックボーンネットワークを使用したCityscapesセマンティックセグメンテーション。これらは、インスタンス上に構築された以前の最良の方法よりも2.6 AP、0.8 / 1.0 mAP、1.0mIoU優れています-レベルの対照的な学習..これらの結果は、ピクセルレベルで口実タスクを定義する強力な可能性を示しており、監視されていない視覚表現学習の新しい道を示唆しています。 
[概要]現在の方法はインスタンスレベルの口実タスクでのみトレーニングされているため、対照的な学習の力はまだ完全には解き放たれていません。その結果、高密度のピクセル予測を必要とするダウンストリームタスクには最適ではない表現につながる可能性があります。ピクセルレベルで口実タスクを定義する可能性-しかし、教師なし視覚表現学習の新しい道を示唆する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Unifying Instance and Panoptic Segmentation with Dynamic Rank-1
  Convolutions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_4.html">
      <font color="black">Unifying Instance and Panoptic Segmentation with Dynamic Rank-1
  Convolutions</font>
    </a>
  </h2>
  <font color="black">重要なのは、DR1Maskと呼ばれる提案された新しい方法は、単一のレイヤーを追加することでパノラマセグメンテーションを実行できることです。フレームワークがはるかに効率的であるだけでなく、以前の最良の2ブランチアプローチの2倍の速度であるだけでなく、統合されたフレームワークが機会を開きます。同じコンテキストモジュールを使用して両方のタスクのパフォーマンスを向上させるため。コードは次のURLで入手できます。https：//git.io/AdelaiDet 
[ABSTRACT] dr1maskは、インスタンスとセマンティックの両方で共有機能マップを活用する最初のパノラマセグメンテーションフレームワークです。副産物としてのセグメンテーション。dr1maskは、以前に利用可能な最先端のサンプル例よりも10％速く、マップ内の1ポイントがより正確です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Abnormal Event Detection in Urban Surveillance Videos Using GAN and
  Transfer Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_5.html">
      <font color="black">Abnormal Event Detection in Urban Surveillance Videos Using GAN and
  Transfer Learning</font>
    </a>
  </h2>
  <font color="black">その中で、深層学習ベースの方法が最良の結果を示します。実験結果は、提案された方法が群集シーンの異常イベントを効果的に検出および特定できることを示しています。モデルの効率は、ビデオのオプティカルフロー情報を処理することによってさらに改善されます。 
[概要]これはディープラーニング手法に基づいており、ビデオ内の異常なイベントを検出して特定する効果的な方法を提供します。モデルは、ビデオのオプティカルフロー情報を処理することによってさらに改善されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Mini-DDSM: Mammography-based Automatic Age Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_6.html">
      <font color="black">Mini-DDSM: Mammography-based Automatic Age Estimation</font>
    </a>
  </h2>
  <font color="black">続いて、収集したデータセットから深層学習の特徴を抽出し、ランダムフォレストリグレッサを使用してモデルを構築して年齢を自動的に推定しました。別の独立したデータセットに対してロジスティック回帰モデルと線形回帰モデルを実行して、提案した利点をさらに検証しました。作業..このデータセットの元の画像は、残念ながら、壊れたソフトウェアによってのみ取得できます。 
[概要]この研究の目的は、マンモグラム画像から年齢を再分類するためのaiベースのモデルを考案することです。マンモグラム画像は、壊れたソフトウェアによってのみ取得できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Bidirectional RNN-based Few Shot Learning for 3D Medical Image
  Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_7.html">
      <font color="black">Bidirectional RNN-based Few Shot Learning for 3D Medical Image
  Segmentation</font>
    </a>
  </h2>
  <font color="black">私たちのモデルは、最先端の数ショットセグメンテーションモデルよりも大幅にパフォーマンスが向上し、より多くのターゲットトレーニングデータでトレーニングされた完全な教師ありモデルに匹敵しました。また、ターゲット画像の特性を適応させるための転移学習法を導入します。任意のサポートでテストする前にモデルを更新し、サポートデータからサンプリングしたクエリデータを使用して臓器を作成します。この論文では、ターゲット臓器注釈の限られたトレーニングサンプルを使用して正確な臓器セグメンテーションを行うための3D数ショットセグメンテーションフレームワークを提案します。 
[概要] au-ネットのようなネットワークは、サポートデータの2Dスライスと検索画像の関係を学習することでセグメンテーションを予測するように設計されています。これには、隣接するスライス間のエンコードされた特徴の一貫性を学習する双方向ゲート付き回帰ユニット（gru）が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Generalized Few-Shot Semantic Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_8.html">
      <font color="black">Generalized Few-Shot Semantic Segmentation</font>
    </a>
  </h2>
  <font color="black">GFS-Segを扱いやすくするために、元のモデルの構造を変更せずに適切なパフォーマンスを実現するGFS-Segベースラインを設定しました。以前の最先端のFS-Segメソッドは、GFS-Segとパフォーマンスの不一致では不十分です。主にFS-Segの制約付きトレーニング設定に由来します。この論文では、セグメンテーションモデルの一般化能力を分析して新しいカテゴリを同時に認識するために、Generalized Few-Shot Semantic Segmentation（GFS-Seg）と呼ばれる新しいベンチマークを紹介します。非常に少数の例と、十分な例のある基本カテゴリ。 
[ABSTRACT] fs --seg（fs --siota）は、多くの制約でこの問題に取り組みます。このようなメソッドの以前の例は、gfs --segでは不十分です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-11">
        <br><font color="black">2020-10-11</font>
      </time>
    </span>
</section>
<!-- paper0: SAM: Squeeze-and-Mimic Networks for Conditional Visual Driving Policy
  Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_9.html">
      <font color="black">SAM: Squeeze-and-Mimic Networks for Conditional Visual Driving Policy
  Learning</font>
    </a>
  </h2>
  <font color="black">CARLAシミュレーターを使用してアプローチをテストします。特に、サイドタスクを達成したり、その機能を学習したりすることは目的としていません。代わりに、模倣損失を介して、運転に直接役立つサイドタスクアノテーションの表現を学習することを目指しています。次に、画像のみを入力として使用して運転するように模倣ネットワークをトレーニングし、スクイーズネットワークの潜在的な表現を使用して模倣ネットワークを監視します。損失を模倣することによって。 
[概要]画像のみを入力として使用して運転するようにスクイーズネットワークをトレーニングします。スクイーズネットワークの潜在的な表現を使用して、模倣ネットワークを監視します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-06">
        <br><font color="black">2019-12-06</font>
      </time>
    </span>
</section>
<!-- paper0: Using Text to Teach Image Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_10.html">
      <font color="black">Using Text to Teach Image Retrieval</font>
    </a>
  </h2>
  <font color="black">これに対処するために、幾何学的に整列されたテキストで多様なサンプルを補強し、それによって画像について教えるために多数の文を使用します。画像検索に役立つテキストの力を示す標準データセットに関する広範な結果に加えて、新しい公開データセットCLEVRに基づいて、視覚データとテキストデータ間の意味的類似性を定量化するために導入されました。実験結果は、共同埋め込みマニホールドが堅牢な表現であり、画像とテキストのみが与えられた場合に画像検索を実行するためのより良い基盤となることを示しています。画像に対する望ましい修正に関する指示
[要約]最初に、ニューラルネットワークによって学習された画像の特徴空間をグラフとして表すことを提案します。広範な結果に加えて、新しい公開データセットを導入して、視覚間の意味的類似性を定量化します。データとテキストデータ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Foreground-Aware Relation Network for Geospatial Object Segmentation in
  High Spatial Resolution Remote Sensing Imagery -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_11.html">
      <font color="black">Foreground-Aware Relation Network for Geospatial Object Segmentation in
  High Spatial Resolution Remote Sensing Imagery</font>
    </a>
  </h2>
  <font color="black">一方、最適化の観点から、前景を意識した最適化を提案し、バランスの取れた最適化のためのトレーニング中に前景の例と背景の難しい例に焦点を当てます。この論文では、問題は前景モデリングの欠如にあると主張し、提案します。関係ベースおよび最適化ベースの前景モデリングの観点からの前景認識関係ネットワーク（FarSeg）は、上記の2つの問題を軽減します。関係の観点から、FarSegは、前景とシーンの関係を学習します。 
[概要]提案された方法は、最先端の一般的なセマンティックセグメンテーション方法よりも優れています。これらには、大面積の地球観測シーンで通常発生する他の2つの問題の不適切な考慮が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Deep Video Stabilization without Optical Flow -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_12.html">
      <font color="black">Learning Deep Video Stabilization without Optical Flow</font>
    </a>
  </h2>
  <font color="black">私たちの方法は、最先端のビデオ安定化方法によって生成された結果よりも定性的および定量的に優れた結果を提供します。そのために、新しいビデオ安定化データセットを提供し、競争力のある安定化結果をわずかで生成できる効率的なネットワークをトレーニングします。最近の反復フレーム補間スキーマで同じことを行うのにかかる時間。従来の光フロー支援アプローチよりも純粋なRGBベースの生成タスクとしてビデオ安定化を扱うことの主な利点は、コンテンツと解像度の保存です。後者のアプローチ。 
[概要]これは、ビデオ安定化をフレーム間モーションメジャーに置き換えるのではなく、ディープラーニングの問題として定式化する際の遠近法の重要性を示す唯一の作品です。私たちの知る限り、これは実証された最初の作品ではありません。視点の必要性</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging,
  audio, and lip videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_13.html">
      <font color="black">TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging,
  audio, and lip videos</font>
    </a>
  </h2>
  <font color="black">TaLコーパスは、CC BY-NC 4.0ライセンスの下で公開されています。全体として、コーパスには24時間の並列超音波、ビデオ、およびオーディオデータが含まれ、そのうち約13.5時間は音声です。このペーパーでは、コーパスについて説明し、ベンチマークを示します。音声認識、音声合成（関節から音響へのマッピング）、および超音波から音声への自動同期のタスクの結果。 
[ABSTRACT] tal1は、英語を母国語とする男性の1人のプロの声優による録音セッションのセットです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Multiclass Yeast Segmentation in Microstructured Environments with Deep
  Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_14.html">
      <font color="black">Multiclass Yeast Segmentation in Microstructured Environments with Deep
  Learning</font>
    </a>
  </h2>
  <font color="black">モデルは堅牢なセグメンテーション結果を実現し、精度と速度の両方で以前の最先端技術を上回ります。ネットワークのトレーニング、検証、テストのために記録されたデータセットの概要と、一般的な使用例を示します。 。微細構造環境の設定では、課題はさらに悪化します。 
[概要]微細構造環境の設定では課題が激化しています。今日は、個々の酵母細胞のマルチクラスセグメンテーション用にトレーニングされた畳み込みニューラルネットワークを紹介します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning for Automated Screening of Tuberculosis from Indian Chest
  X-rays: Analysis and Update -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_15.html">
      <font color="black">Deep Learning for Automated Screening of Tuberculosis from Indian Chest
  X-rays: Analysis and Update</font>
    </a>
  </h2>
  <font color="black">背景と目的：結核（TB）は重大な公衆衛生問題であり、世界中の主要な死因です。したがって、インドのデータセットで結核を正確かつ自動診断するための深い学習は、依然として重要な研究対象です。結果：提案された方法が達成されましたインド人の結核を診断するための98.60％の感度で93.40％の精度。 
[ABSTRACT]胸部X線写真から結核を検出するための深層学習ベースの方法が提案されています。この方法は、文献に記載されている手法に対してもテストされています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Deep LF-Net: Semantic Lung Segmentation from Indian Chest Radiographs
  Including Severely Unhealthy Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_16.html">
      <font color="black">Deep LF-Net: Semantic Lung Segmentation from Indian Chest Radiographs
  Including Severely Unhealthy Images</font>
    </a>
  </h2>
  <font color="black">DeepLabアーキテクチャ、エンコーダデコーダ、拡張畳み込みを統合したエンドツーエンドのDeepLabv3 +ネットワークを試み、高速トレーニングと高精度でセマンティック肺セグメンテーションを実現します。結核、慢性閉塞性肺疾患、間質性肺疾患、胸膜滲出液、および肺癌の患者。また、日本放射線技術学会などの一般的に使用されるベンチマークデータセットで実験を行いました。米国モンゴメリー郡;最先端の比較のために中国の深セン。 
[要約]提案された作業は、cxrからの肺の正確なセグメンテーションのための効率的な深い畳み込みニューラルネットワークの使用に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Top-Related Meta-Learning Method for Few-Shot Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_17.html">
      <font color="black">Top-Related Meta-Learning Method for Few-Shot Detection</font>
    </a>
  </h2>
  <font color="black">分類タスクおよびカテゴリベースのグループ化メカニズム用のTCL-C）。TCLは、トゥルーラベルおよび最も類似したクラスを利用して、少数ショットクラスの検出パフォーマンスを向上させます。したがって、Top-C分類損失（つまり、 
[概要] tclはtrue-ラベルと最も類似したクラスを悪用して、少数のショットクラスの検出パフォーマンスを向上させます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Geometry-Inspired Top-k Adversarial Perturbations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_18.html">
      <font color="black">Geometry-Inspired Top-k Adversarial Perturbations</font>
    </a>
  </h2>
  <font color="black">他の敵対的な例の作成手法と比較することで、その有効性と効率を評価します。実験的に、私たちのアプローチがベースライン手法よりも優れており、ユニバーサル敵対的摂動を生成する既存の手法を改善することを示しています。ただし、多くの現実のシナリオでは、特にデジタル画像、上位$ k $の予測がより重要です。 
[要約]既存の敵対的摂動の主なターゲットは、主に正しいトップを変更することに限定されています-1つの予測クラスを誤ったクラスで変更します。これはトップを変更することを意図していません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-28">
        <br><font color="black">2020-06-28</font>
      </time>
    </span>
</section>
<!-- paper0: FATNN: Fast and Accurate Ternary Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_19.html">
      <font color="black">FATNN: Fast and Accurate Ternary Neural Networks</font>
    </a>
  </h2>
  <font color="black">三元ニューラルネットワーク（TNN）は、推論が桁違いに高速であり、完全精度のものよりも電力効率が高いため、多くの注目を集めています。次に、パフォーマンスギャップを緩和するために、実装に依存する設計を精巧に行います。三元量子化アルゴリズム..その結果、従来のTNNは、標準の2ビットモデルと比較して同様のメモリ消費と速度を持ちますが、表現能力は劣ります。 
[ABSTRACT] 3つの量子化レベルのみを活用して3値表現をエンコードするには、2ビットが必要です。ただし、tnnとフルビットネットワークの間には依然として大きな精度のギャップがあり、表現が妨げられています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-12">
        <br><font color="black">2020-08-12</font>
      </time>
    </span>
</section>
<!-- paper0: Semantic Scene Completion using Local Deep Implicit Functions on LiDAR
  Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_20.html">
      <font color="black">Semantic Scene Completion using Local Deep Implicit Functions on LiDAR
  Data</font>
    </a>
  </h2>
  <font color="black">この連続表現は、空間離散化を必要とせずに、広範な屋外シーンの幾何学的およびセマンティックプロパティをエンコードするのに適していることを示します（したがって、シーンの詳細レベルとカバーできるシーンの範囲との間のトレードオフを回避します）。グローバルシーン完了関数は、その後、ローカライズされた関数パッチから組み立てられます。私たちの方法のパフォーマンスは、幾何学的および意味的完了の交差オーバーユニオン（IoU）の両方の点で、セマンティックKITTIシーン完了ベンチマークの最先端を超えています。 
[ABSTRACT]私たちの方法は、ボクセル化に基づかない連続的なシーン表現を生成します。グローバルシーン完了関数は、その後、ローカライズされたパッチから組み立てられます。私たちの方法のパフォーマンスは、セマンティックキティシーン完了ベンチマークの最先端を超えています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Heterogeneous Contrastive Learning: Encoding Spatial Information for
  Compact Visual Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_21.html">
      <font color="black">Heterogeneous Contrastive Learning: Encoding Spatial Information for
  Compact Visual Representations</font>
    </a>
  </h2>
  <font color="black">さらに重要なことは、私たちのアプローチが視覚表現の効率を高め、それによって自己教師あり視覚表現学習の将来の研究を刺激する重要なメッセージを提供することを示します。HCLの有効性を示すことにより、（i）より高い効率を達成するインスタンス識別の精度と（ii）一連のダウンストリームタスクで既存の事前トレーニング方法を上回り、事前トレーニングコストを半分に削減します。このペーパーでは、空間情報を追加する効果的なアプローチである異種教師あり学習（HCL）を紹介します。対照的な目的と強力なデータ拡張操作の間の学習の不一致を軽減するためのエンコード段階。 
[概要]この論文は、異種対照学習（hcl）を提示しました。これは、メインステージに空間情報を追加する効果的なアプローチです。また、自己監視視覚表現の将来の研究への重要なメッセージを提供します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: A Preliminary Comparison Between Compressive Sampling and Anisotropic
  Mesh-based Image Representation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_22.html">
      <font color="black">A Preliminary Comparison Between Compressive Sampling and Anisotropic
  Mesh-based Image Representation</font>
    </a>
  </h2>
  <font color="black">結果は、同じサンプル密度で、AMA表現がテストされたアルゴリズムに基づいてCSよりも優れた再構成品質を提供できることを示しています。この論文では、CSと最近開発されたMbIRメソッドであるAMA表現との予備比較を行います。徹底的な比較を行うには、最近のアルゴリズムによるさらなる調査が必要です。 
[ABSTRACT] csと最近開発されたmbirメソッドであるama表現がテストされました。amarepresentativeとして知られるこのメソッドは効果的であるとは見なされていませんが、これらの画像を制限する代わりに、スパースにすることができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Reducing the Teacher-Student Gap via Spherical Knowledge Disitllation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_23.html">
      <font color="black">Reducing the Teacher-Student Gap via Spherical Knowledge Disitllation</font>
    </a>
  </h2>
  <font color="black">具体的には、ResNet18を73.0の精度でトレーニングします。これは、以前のSOTAに比べて大幅に改善されており、resnet34とほぼ2倍の生徒サイズです。教師と生徒の信頼のギャップを調査することでこの問題を調査します。したがって、生徒のパフォーマンスキャパシティギャップ問題と呼ばれる特大の教師から蒸留すると、予期せず低下します。 
[要約]生徒の能力が限られているため、生徒は教師に不適合になります。これにより、不適合の問題が緩和されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-15">
        <br><font color="black">2020-10-15</font>
      </time>
    </span>
</section>
<!-- paper0: An Experimental Study of Semantic Continuity for Deep Learning Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_24.html">
      <font color="black">An Experimental Study of Semantic Continuity for Deep Learning Models</font>
    </a>
  </h2>
  <font color="black">深層学習モデルには、意味の不連続性の問題があります。入力空間の小さな摂動は、モデル出力に意味レベルの干渉を引き起こす傾向があります。最初にデータ分析を実行して、既存の深層学習モデルの意味の不連続性の証拠を提供し、次に設計します。モデルが滑らかな勾配を取得し、セマンティック指向の機能を学習できるようにする単純なセマンティック連続性制約。定性的および定量的実験により、セマンティック連続モデルが非セマンティック情報の使用を減らすことに成功し、敵対的ロバスト性の向上にさらに貢献します。解釈可能性、モデル転送、およびマシンバイアス。 
[要約]研究は、意味的に連続的なモデルが非意味情報の使用をうまく減らすことを示しています。不適切なトレーニングターゲットからの結果は、敵対的なロバスト性、解釈可能性などの問題に貢献します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Geography-Aware Self-Supervised Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_25.html">
      <font color="black">Geography-Aware Self-Supervised Learning</font>
    </a>
  </h2>
  <font color="black">時間の経過とともに空間的に整列した画像を活用して、対照的な学習と地理的位置で時間的ポジティブペアを構築し、プレテキストタスクを設計します。ギャップを埋めるために、リモートセンシングデータの時空間構造を活用する新しいトレーニング方法を提案します。それらの異なる特性のために、標準的なベンチマークでの対照的な学習と監視された学習の間に重要なギャップが持続することを示しています。 
[概要]提案された方法は、自己満足と教師あり学習の間のギャップを埋めます。これらには、対照的、教師あり学習、および意味情報が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: A Dual Iterative Refinement Method for Non-rigid Shape Matching -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_26.html">
      <font color="black">A Dual Iterative Refinement Method for Non-rigid Shape Matching</font>
    </a>
  </h2>
  <font color="black">データ適応型の補完情報の効果的な組み合わせのおかげで、DIRは効率的であるだけでなく、数回の反復で正確な結果をレンダリングするために堅牢でもあります。これらの選択されたアンカーペアは、スペクトル特徴（または他の適切なグローバル特徴）を整列させるために使用されます）その寸法は、選択したアンカーペアの容量に適応的に一致します。適切なデュアル機能を選択することにより、DIRはパッチおよび部分一致も処理する柔軟性を備えています。 
[概要]重要なアイデアは、gpsなどの二重情報を使用して、より正確な情報を抽出することです。この方法を使用すると、dirにはパッチと部分一致を処理する柔軟性があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-26">
        <br><font color="black">2020-07-26</font>
      </time>
    </span>
</section>
<!-- paper0: HMFlow: Hybrid Matching Optical Flow Network for Small and Fast-Moving
  Objects -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_27.html">
      <font color="black">HMFlow: Hybrid Matching Optical Flow Network for Small and Fast-Moving
  Objects</font>
    </a>
  </h2>
  <font color="black">提案されたHMFlowは、高精度と小さなモデルサイズを維持するだけでなく、グローバルマッチング機能を適用して、ローカルマッチング機能と一致しない小さくて動きの速いオブジェクトを検出するようにネットワークをガイドできます。また、Small andFastという名前の新しいデータセットを構築します。評価用の可動椅子（SFChairs）..実験結果は、提案されたネットワークが、特に小さくて動きの速いオブジェクトがある領域で、かなりのパフォーマンスを達成することを示しています。 
[ABSTRACT]提案されたハイブリッドマッチングオプティカルフローネットワークは将来使用される可能性がありますが、現在のネットワークは小さくて高速で移動するオブジェクトをキャプチャできません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Domain Adaptation without Source Data by Casting a BAIT -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_28.html">
      <font color="black">Unsupervised Domain Adaptation without Source Data by Casting a BAIT</font>
    </a>
  </h2>
  <font color="black">実験結果は、提案された方法が既存のUDAおよびSFUDA方法と比較して、いくつかのベンチマークデータセットで最先端のパフォーマンスを達成することを示しています。具体的には、ソース分類器のヘッドが固定されたソースモデルのみを前提として、新しい学習可能な分類器を導入します。 ..ターゲットドメインに適応する場合、新しく追加された分類子のクラスプロトタイプが餌として機能します。 
[ABSTRACT] sfudaに対処するためにbaitという名前の新しいメソッドを提案します。たとえば、新しく追加された分類子のクラスプロトタイプは餌として機能します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-23">
        <br><font color="black">2020-10-23</font>
      </time>
    </span>
</section>
<!-- paper0: Unmixing Convolutional Features for Crisp Edge Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_29.html">
      <font color="black">Unmixing Convolutional Features for Crisp Edge Detection</font>
    </a>
  </h2>
  <font color="black">実験は、提案されたCATSを最新のディープエッジ検出器に統合してローカリゼーションの精度を向上させることができることを示しています。バニラVGG16バックボーンを使用すると、BSDS500データセットの観点から、CATSはRCFおよびBDCNディープエッジ検出器のFメジャー（ODS）を向上させます。エッジ検出に形態学的な非最大抑制スキームを使用せずに評価すると、それぞれ12％と6％増加します。このペーパーでは、ディープエッジ検出器を使用した鮮明なエッジ検出のためのコンテキスト認識トレース戦略（CATS）を示します。ディープエッジ検出器のローカリゼーションのあいまいさは、主に畳み込み神経ネットワークの混合現象によって引き起こされます。エッジ分類での特徴の混合と、融合側の予測中の側の混合です。 
[概要]猫は2つのモジュールで構成されています：境界をトレースすることによって機能のアンミキシングを実行する新しいトレース損失。学習されたサイドエッジの補完的なメリットを集約することによってサイドミキシングに取り組む融合ブロック</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Self-Supervised Poisson-Gaussian Denoising -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_30.html">
      <font color="black">Self-Supervised Poisson-Gaussian Denoising</font>
    </a>
  </h2>
  <font color="black">私たちの新しい戦略は、損失関数からハイパーパラメータを排除します。これは、ハイパーパラメータの調整をガイドするグラウンドトゥルースデータが利用できない自己監視体制で重要です。標準のノイズモデルであるポアソンガウスノイズを処理するための新しいトレーニング戦略を導入します。顕微鏡画像の場合..顕微鏡画像のノイズ除去ベンチマークに関する私たちの評価は、私たちのアプローチを検証します。 
[概要]私たちの新しい戦略は、損失関数からハイパーパラメータを排除します。これは、自己監視体制で重要です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-21">
        <br><font color="black">2020-02-21</font>
      </time>
    </span>
</section>
<!-- paper0: AQD: Towards Accurate Quantized Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_31.html">
      <font color="black">AQD: Towards Accurate Quantized Object Detection</font>
    </a>
  </h2>
  <font color="black">レイテンシと精度のトレードオフの改善を示すために、RetinaNetとFCOSで提案された方法を適用します。このホワイトペーパーでは、浮動小数点計算を完全に排除するために、AQDと呼ばれる正確な量子化オブジェクト検出ソリューションを提案します。特に、MS-COCOデータセットの実験結果は、私たちのAQDが、非常に低いビットスキームの下で、完全精度の対応物と比較して同等またはそれ以上のパフォーマンスを達成することを示しています。 
[概要]これに加えて、畳み込みネットワーク、正規化層、スキップ接続など、あらゆる種類の層で固定小数点演算を使用することを目標としています。この目的のために、固定小数点演算を使用して、固定小数点演算を使用することを目指しています。 、すべてのタイプのレイヤーで</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Threats to DeepFake Detection: A Practical Perspective -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_32.html">
      <font color="black">Adversarial Threats to DeepFake Detection: A Practical Perspective</font>
    </a>
  </h2>
  <font color="black">顔を操作した画像や動画、またはDeepFakeを悪意を持って使用して、誤った情報を煽ったり、個人を中傷したりする可能性があります。どの敵対的摂動が異なるモデル間で伝達され、敵対的例の伝達可能性を改善するための手法を提案します。 
[概要]ディープフェイク検出チャレンジ（dfdc）は、敵対的攻撃に基づいています。実際の攻撃シナリオでは簡単に回避できます。勝者のエントリに対して評価を行います。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Improved and Interpretable Deep Metric Learning via Attentive
  Grouping -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_33.html">
      <font color="black">Towards Improved and Interpretable Deep Metric Learning via Attentive
  Grouping</font>
    </a>
  </h2>
  <font color="black">学習可能なクエリと各空間位置の間の注意スコアは、その位置の重要性として解釈できます。グループ化は、多様な特徴を計算するためのディープメトリック学習で一般的に使用されています。さまざまなデータセット、評価指標、基本モデル、および損失関数にわたって大幅に。 
[概要]私たちの方法は、グループごとに学習可能な検索を行う注意メカニズムに基づいていますが、現在の方法は過剰適合しやすく、神経リードが不足しています。これは、私たちの方法が自然に解釈可能性を強化することを意味します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Toward Metrics for Differentiating Out-of-Distribution Sets -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_34.html">
      <font color="black">Toward Metrics for Differentiating Out-of-Distribution Sets</font>
    </a>
  </h2>
  <font color="black">ただし、この基準を直接最適化して最も効果的なOODセットを選択すると、膨大な計算コストが発生します。バニラCNNは、未校正の分類器として、分布外（OOD）サンプルを分布内サンプルとほぼ同じくらい自信を持って分類することに苦しんでいます。 、これらの作業では重要な質問が未解決のままです。目に見えないOODセットで高い検出率でそのようなCNNのトレーニングを誘発する最も効果的なものを選択するためにOODセットを区別する方法は？ 
[概要]これらの作品がトレーニングに利用可能なフードセットを活用することの利点を実証したのはこれが初めてです。cnnによると、最も保護的なフードセットでトレーニングされたcnnはブラックボックスfgsの敵対的な例も検出できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-18">
        <br><font color="black">2019-10-18</font>
      </time>
    </span>
</section>
<!-- paper0: Multigrid-in-Channels Architectures for Wide Convolutional Neural
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_35.html">
      <font color="black">Multigrid-in-Channels Architectures for Wide Convolutional Neural
  Networks</font>
    </a>
  </h2>
  <font color="black">この目的のために、一般的なCNNの各畳み込み層を、構造化された（つまり、グループ化された）畳み込みで構成されるマルチレベル層に置き換えます。チャネル数に関するパラメーター数の2次関数的成長に対抗するマルチグリッドアプローチを提示します。標準の畳み込みニューラルネットワーク（CNN）で。標準のCNNには冗長性があることが示されています。これは、畳み込み演算子が非常に少ないネットワークでは、完全なネットワークと同様のパフォーマンスが得られるためです。 
[概要]標準のcnnsには冗長性があります。これは、畳み込み演算子がまばらなネットワークでは、完全なネットワークと同様のパフォーマンスが得られるためです。チャネルの完全性を実現するcnnアーキテクチャを構築するためのマルチグリッドインチャネルアプローチを紹介します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-11">
        <br><font color="black">2020-06-11</font>
      </time>
    </span>
</section>
<!-- paper0: Spectral Response Function Guided Deep Optimization-driven Network for
  Spectral Super-resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_36.html">
      <font color="black">Spectral Response Function Guided Deep Optimization-driven Network for
  Spectral Super-resolution</font>
    </a>
  </h2>
  <font color="black">最後に、自然とリモートセンシング画像を含む2種類のデータセットでの実験は、提案された方法のスペクトル増強効果を示しています。さらに、チャネルアテンションモジュール（CAM）と再定式化されたスペクトル角度マッパー損失関数を適用して効果を達成します。再構成モデル..そして、リモートセンシングデータセットの分類結果も、提案された方法によって強化された情報の有効性を検証しました。 
[ABSTRACT]ハイパースペクトル画像は、ハイパースペクトル画像を作成するために使用される方法です。この方法は、高空間解像度（hr）のハイパースペクトル画像を取得するために使用されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: MCUNet: Tiny Deep Learning on IoT Devices -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_37.html">
      <font color="black">MCUNet: Tiny Deep Learning on IoT Devices</font>
    </a>
  </h2>
  <font color="black">TinyEngineは、レイヤーごとの最適化ではなく、ネットワークトポロジ全体に従ってメモリスケジューリングを適応させ、メモリ使用量を4.8倍削減し、TF-Lite MicroおよびCMSIS-NNと比較して推論を1.7〜3.3倍高速化します。コードとモデルhttps://tinyml.mit.edu ..私たちの調査は、IoTデバイスでの常時オンの小さなマシン学習の時代が到来したことを示唆しています。 
[ABSTRACT] mcunetは、効率的なニューラルアーキテクチャ（tinynas）と軽量の可能性エンジン（tinyengine）を共同で設計し、マイクロコントローラーでimagenetスケールの可能性を実現するフレームワークです。mcunetは、商用マイクロコントローラーで70％を超えるimagenettop1精度を達成した最初の製品です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-20">
        <br><font color="black">2020-07-20</font>
      </time>
    </span>
</section>
<!-- paper0: Scene text removal via cascaded text stroke detection and erasing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_38.html">
      <font color="black">Scene text removal via cascaded text stroke detection and erasing</font>
    </a>
  </h2>
  <font color="black">これらの2つのサブ問題を別々に解決するために、テキストストローク検出ネットワークとテキスト除去生成ネットワークを設計します。実験結果は、提案された方法がシーンテキストを見つけて消去するための最先端のアプローチを大幅に上回っていることを示しています。 、これら2つのネットワークを処理ユニットとして結合し、このユニットをカスケードして、テキスト削除の最終モデルを取得します。 
[概要]テキスト削除の問題をテキストストローク検出とストローク削除に分離します。このシステムは、これら2つのネットワークを処理ユニットとして組み合わせることで機能します。次に、結果を組み合わせて、テキスト削除の最終モデルを作成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: All-in-Focus Iris Camera With a Great Capture Volume -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_39.html">
      <font color="black">All-in-Focus Iris Camera With a Great Capture Volume</font>
    </a>
  </h2>
  <font color="black">提案されたオールインフォーカスアイリスカメラは、被写界深度を最大3.9 mまで拡大し、従来の長焦点レンズと比較して37.5倍になります。さらに、電動反射ミラーが光ビームを適応的に操縦して水平および垂直に拡張します。本研究では、焦点調整可能レンズと2Dステアリングミラーを使用して、時空間多重化法によりキャプチャボリュームを大幅に拡張する、新しいオールインフォーカス虹彩イメージングシステムを開発します。 
[概要]提案されたオールインフォーカスアイリスカメラは、被写界深度を最大3.9 mまで拡大します。これは、従来の長レンズと比較して37.5倍です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Face Forgery Detection by 3D Decomposition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_40.html">
      <font color="black">Face Forgery Detection by 3D Decomposition</font>
    </a>
  </h2>
  <font color="black">具体的には、顔画像を3D形状、共通テクスチャ、アイデンティティテクスチャ、環境光、直接光に解きほぐすことで、直接光とアイデンティティテクスチャに悪魔が横たわっていることを発見します。さらに、操作された領域を注意深く強調します。メカニズムと2ストリーム構造を導入して、顔の画像と顔の詳細の両方をマルチモダリティタスクとして一緒に活用します。この観察に基づいて、直接光とアイデンティティテクスチャの組み合わせである顔の詳細を微妙な偽造パターンを検出する手がかり。 
[ABSTRACT]直接光とアイデンティティテクスチャの組み合わせである顔の詳細は、偽造パターンを検出するための手がかりです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Differentiable Data Augmentation with Kornia -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_41.html">
      <font color="black">Differentiable Data Augmentation with Kornia</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、空間（2D）テンソルと体積（3D）テンソルの両方のKornia微分可能データ拡張（DDA）モジュールのレビューを示します。さらに、さまざまなDAフレームワークを比較するベンチマークといくつかの簡単なレビューを提供します。このモジュールは、Korniaの差別化可能なコンピュータービジョンソリューションを活用して、データ拡張（DA）パイプラインと戦略を既存のPyTorchコンポーネントに統合することを目的としています（例：
[ABSTRACT]モジュールはkorniaのコンピュータービジョンソリューションを組み合わせたものです） .itは、データ拡張（da）パイプラインと戦略を既存のpytorchコンポーネントに結合することを目的としています。さらに、さまざまなdaフレームワークを比較するベンチマークを提供します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: The Cube++ Illumination Estimation Dataset -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_42.html">
      <font color="black">The Cube++ Illumination Estimation Dataset</font>
    </a>
  </h2>
  <font color="black">このように、それはほとんどのデジタルカメラの画像処理パイプラインの重要な部分です。この論文では、言及された問題の多くを軽減し、照明推定研究を支援することを目的とした新しい照明推定データセットを提案します。照明推定法が提案されており、その精度は通常、公開されているデータセットの画像で取得されたエラーメトリックの値を提供することによって報告されます。 
[概要]データセットは、ほとんどのデジタルカメラの画像処理パイプラインの重要な部分です。既知のイラストカラーの4890枚の画像と、学習プロセスをさらに正確にすることができる追加のセマンティックデータで構成されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Remix: Rebalanced Mixup -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_43.html">
      <font color="black">Remix: Rebalanced Mixup</font>
    </a>
  </h2>
  <font color="black">実験結果により、Remixが以前の方法に比べて一貫性のある大幅な改善を提供することが確認されました。クラス不均衡な体制下でMixup、Manifold Mixup、CutMixなどの最先端の正則化手法を研究し、提案されたRemixが大幅に改善されることを示しました。 CIFAR-10、CIFAR-100、およびCINIC-10によって構築された不均衡なデータセットで、これらの最先端の技術やいくつかの再重み付けおよび再サンプリング手法よりも優れています。また、実際の世界でRemixを評価しました。大規模な不均衡なデータセット、iNaturalist2018。
[要約]新しい手法であるリミックスは、混合の形を緩和します。これにより、混合係数を解きほぐすことができます。分類器は、決定の境界を押し上げることを学習します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-08">
        <br><font color="black">2020-07-08</font>
      </time>
    </span>
</section>
<!-- paper0: Cylindrical and Asymmetrical 3D Convolution Networks for LiDAR
  Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_44.html">
      <font color="black">Cylindrical and Asymmetrical 3D Convolution Networks for LiDAR
  Segmentation</font>
    </a>
  </h2>
  <font color="black">この調査を動機として、屋外LiDARセグメンテーションの新しいフレームワークを提案します。このフレームワークでは、円筒形のパーティションと非対称の3D畳み込みネットワークが、これらの固有の特性を維持しながら3Dの幾何学的パターンを探索するように設計されています。損失の多いボクセルベースのラベルエンコーディングの干渉を軽減するために導入されました。さらに、提案された3Dフレームワークは、LiDARパノラマセグメンテーションおよびLiDAR3D検出にも一般化されています。 
[概要]この会社は点群で競争力を示していますが、必然的に3D配置を変更して放棄します。ただし、屋外の点群では、この方法で得られる改善は非常に限られていることがわかりました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Defocus Blur Detection via Salient Region Detection Prior -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_45.html">
      <font color="black">Defocus Blur Detection via Salient Region Detection Prior</font>
    </a>
  </h2>
  <font color="black">デジタル一眼レフカメラ（DSLR）で写真を撮るとき、常に写真にデフォーカスブレが発生し、顕著な領域と美的喜びを与えます。さらに、デフォーカスブレ検出のための新しいネットワークを提案します。ボケ効果のある画像では、ほとんどの場合、顕著な領域とフィールドの深さの領域が重なっていることは明らかです。 
[概要]この作品では、焦点ぼけ検出と顕著な領域検出の関係を再考します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Self-Gradient Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_46.html">
      <font color="black">Self-Gradient Networks</font>
    </a>
  </h2>
  <font color="black">ディープニューラルネットワークの敵対的な脆弱性の問題が発見されて以来、敵対的な防御メカニズムが提案されてきましたが、この問題を完全に理解して対処するには長い道のりがあります。勾配フロー情報は、自己勾配ネットワーク内で活用され、摂動の安定性を超えます。標準のトレーニングプロセスで何を達成できるか..実験結果は、PGDおよびCWの敵対的摂動下でCIFAR10データセットが10％改善され、最先端の敵対的学習戦略と比較した場合の自己勾配ネットワークの有効性を示しています。 
[ABSTRACT]ディープニューラルネットワークアーキテクチャは、敵対的な摂動に対してより堅牢になるように設計されています。自己膨潤ネットワークの概念は、ディープニューラルネットワークの敵対的な脆弱性の問題の発見に触発されました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Learning in School: Multi-teacher Knowledge Inversion for Data-Free
  Quantization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_47.html">
      <font color="black">Learning in School: Multi-teacher Knowledge Inversion for Data-Free
  Quantization</font>
    </a>
  </h2>
  <font color="black">ただし、この偽のデータは他のモデルに簡単に適用できず、不変の目的によって最適化されるため、一般化可能性と多様性が不足しますが、これらのプロパティは自然画像データセットにあります。広範な実験により、LIS画像が自然画像に似ていることが証明されています。これらの問題に対処するために、複数の教師の知識を反転させることにより、すべてのモデルに適した画像を生成できるLearning in School〜（LIS）アルゴリズムを提案します。 
[ABSTRACT]データ-ユーザーデータを必要とせずにモデル圧縮を実行するための有望な方法として、無料の量子化が登場しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: SHAD3S: A model to Sketch, Shade and Shadow -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_48.html">
      <font color="black">SHAD3S: A model to Sketch, Shade and Shadow</font>
    </a>
  </h2>
  <font color="black">私たちのアプローチの目新しさは、入力が3D形状を表すこと以外は仮定しないという事実にありますが、照明とテクスチャのコンテキスト情報が与えられると、アクセスなしでスケッチ上に正確なハッチパターンを合成します。 3Dまたは疑似3Dに..このプロセスでは、a）タスクに関連する十分に大きな高忠実度のデータセットを合成するための安価で効果的な方法に貢献します。 b）条件付き生成的敵対的ネットワーク（CGAN）を使用してパイプラインを作成する。 c）GIMPを使用してインタラクティブなユーティリティを作成します。これは、アーティストが自動ハッチングやフォーム探索の演習を行うためのツールです。ツールのユーザー評価は、モデルのパフォーマンスが、両方の観点から、さまざまな入力に対して十分に一般化されていることを示しています。スタイルだけでなく、形。 
[要約]開始スコアの単純な比較は、生成された分布がグラウンドトゥルースと同じくらい多様であることを示唆しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-13">
        <br><font color="black">2020-11-13</font>
      </time>
    </span>
</section>
<!-- paper0: Creative Sketch Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_49.html">
      <font color="black">Creative Sketch Generation</font>
    </a>
  </h2>
  <font color="black">実際、Creative Birdsでは、被験者は人間が描いたスケッチよりもDoodlerGANが生成したスケッチを好みます。定量的評価と人間の研究により、私たちのアプローチによって生成されたスケッチは、既存のアプローチよりも創造的で高品質であることが示されています。この作業では、創造的なスケッチの2つのデータセット（CreativeBirdsとCreativeCreatures）を紹介します。パーツの注釈とともに。 
[概要]私たちは、新しいパーツの外観の目に見えない構成を生成するために、doodlerganを提案します。創造的な鳥では、被験者は、人間が描いたスケッチよりも、doodlerganによって生成されたスケッチを好みます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Watch and Learn: Mapping Language and Noisy Real-world Videos with
  Self-supervision -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_50.html">
      <font color="black">Watch and Learn: Mapping Language and Noisy Real-world Videos with
  Self-supervision</font>
    </a>
  </h2>
  <font color="black">この論文では、明示的な注釈なしで文とノイズの多いビデオスニペットの間のマッピングを学習することにより、視覚と自然言語を理解するための機械を教えます。トレーニングと評価のために、多数のオンラインビデオを含む新しいデータセット「ApartmenTour」を提供します。サブタイトル..最初に、クロスモーダル情報をキャプチャする自己監視学習フレームワークを定義します。 
[概要]データはリリースされずにリリースされます。データは2国間で開発されました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Latent-Separated Global Prediction for Learned Image Compression -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_51.html">
      <font color="black">Latent-Separated Global Prediction for Learned Image Compression</font>
    </a>
  </h2>
  <font color="black">したがって、オーバーヘッドの送信を回避するために、潜在性を2つのチャネルグループに分離する3Dグローバルコンテキストモデルを提案します。明らかに、参照ポイントとの間のグローバル相関を示す予測ベクトルを送信するためのビットオーバーヘッドを生成します。現在のデコードポイント..潜在空間の空間依存性をキャプチャするために、以前の作業では、ハイパープライアおよび空間コンテキストモデルを利用してエントロピー推定を容易にします。 
[概要]最近学習した画像コーデックはオートエンコーダに基づいています。最初に画像を致命的でない潜在表現にエンコードし、次に再構成のためにデコードしますが、効果的な長距離依存関係をモデル化するのは困難です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: LIFI: Towards Linguistically Informed Frame Interpolation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_52.html">
      <font color="black">LIFI: Towards Linguistically Informed Frame Interpolation</font>
    </a>
  </h2>
  <font color="black">この動機付けにより、音声ビデオ補間の問題を特に対象とした、言語情報に基づいた新しいメトリックのセットを提供します。この作業では、音声ビデオのフレーム補間の新しい問題を調査します。このようなコンテンツは、今日、オンラインコミュニケーション。 
[概要]音声理解のコンピュータービジョンビデオ生成モデルをテストするためのいくつかのデータセットもリリースします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-30">
        <br><font color="black">2020-10-30</font>
      </time>
    </span>
</section>
<!-- paper0: Tracklets Predicting Based Adaptive Graph Tracking -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_53.html">
      <font color="black">Tracklets Predicting Based Adaptive Graph Tracking</font>
    </a>
  </h2>
  <font color="black">TPAGTの適応グラフニューラルネットワークは、位置、外観、履歴情報を融合するために採用されており、さまざまなオブジェクトを区別する上で重要な役割を果たします。さらに、特徴を抽出するときは、位置関係も情報も使用せず、外観情報のみが使用されます。トラックレットの特徴が考慮されます。これは、動きの予測に基づいて現在のフレーム内のトラックレットの特徴を再抽出します。これは、特徴の不整合の問題を解決するための鍵です。 
[概要]マルチオブジェクト追跡のための正確でエンドツーエンドの学習フレームワークを提示します。システムは、場所、外観、および履歴情報を組み合わせます。場所と外観を融合してオブジェクトを識別するために使用されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-18">
        <br><font color="black">2020-10-18</font>
      </time>
    </span>
</section>
<!-- paper0: Noise2Inpaint: Learning Referenceless Denoising by Inpainting Unrolling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_54.html">
      <font color="black">Noise2Inpaint: Learning Referenceless Denoising by Inpainting Unrolling</font>
    </a>
  </h2>
  <font color="black">これらの方法は、画像ピクセルを2つの互いに素なセットに分割するマスキングアプローチに依存しています。一方はネットワークへの入力として使用され、もう一方は損失を定義するために使用されます。最近では、ノイズ除去を学習するために自己監視アプローチが提案されています。ノイズの多い画像のみから..ただし、これらの以前の自己監視アプローチは、マスキングモデルを明示的に考慮せずに、純粋にデータ駆動型の正規化ニューラルネットワークに依存しています。 
[概要]以前は、これらのメソッドは教師ありの方法でトレーニングされていました。ノイズの多い入力とクリーンなターゲット画像のセットが必要です。これにより、必要に応じてノイズのさまざまな統計的特性を組み込むことができる目的関数を使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-16">
        <br><font color="black">2020-06-16</font>
      </time>
    </span>
</section>
<!-- paper0: Multigrid-in-Channels Neural Network Architectures -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_55.html">
      <font color="black">Multigrid-in-Channels Neural Network Architectures</font>
    </a>
  </h2>
  <font color="black">この目的のために、軽量畳み込みの階層を利用して、各畳み込みブロックをそのMGICブロックに置き換えます。たとえば、MobileNetV3と同様のパラメーターとFLOPを備えた軽量ネットワークを使用してImageNetで76.1％のトップ1精度を取得します。 CNNアーキテクチャの構築は、標準のCNNの場合と同様に、チャネルの完全な結合を維持しながら、ネットワークの幅に対して線形にスケーリングします。 
[概要] cnnアーキテクチャを構築するためのアプローチは、標準のcnnのようにチャネルの完全な結合を維持しながら、ネットワークの幅に対して線形にスケーリングします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Rethinking the competition between detection and ReID in Multi-Object
  Tracking -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_56.html">
      <font color="black">Rethinking the competition between detection and ReID in Multi-Object
  Tracking</font>
    </a>
  </h2>
  <font color="black">この問題を解決するために、個別のブランチにタスク依存の表現を効果的に学習させることができる新しい相互相関ネットワークを提案します。繊細に設計されたネットワークを、CSTrackと呼ばれるワンショットオンラインMOTシステムに統合します。ホイッスル、私たちのモデルは、MOT16とMOT17で新しい最先端のパフォーマンスを実現します。 
[概要]ワンショット追跡体制における2つのタスクの違いは、無意識のうちに見過ごされています。これにより、2段階の方法よりもパフォーマンスが低下します。また、識別的な埋め込みを学習する共同の認識ネットワークを導入します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-23">
        <br><font color="black">2020-10-23</font>
      </time>
    </span>
</section>
<!-- paper0: Monitoring and Diagnosability of Perception Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_57.html">
      <font color="black">Monitoring and Diagnosability of Perception Systems</font>
    </a>
  </h2>
  <font color="black">LGSVL自動運転シミュレーターとApolloAuto自律ソフトウェアスタックを使用した現実的なシミュレーションで、PerSySと呼ばれる監視システムを示し、PerSySが困難なシナリオ（自動運転車の事故を引き起こしたシナリオを含む）で障害を検出できることを示します近年）、最小限の計算オーバーヘッド（シングルコアCPUで5ms未満）を伴う一方で障害を正しく特定することができます。この目標に向けて、マルチプロセッサシステムの診断可能性に関する文献とのつながりを描き、それを一般化します。時間の経過とともに相互作用する異種出力を持つモジュールを説明します。知覚システムの最重要性にもかかわらず、現在、システムレベルの監視のための正式なアプローチはありません。 
[ABSTRACT]システムシステムシステムは、モニターと障害検出の数学モデルです。認識モジュールの一貫性を説明するためのフレームワークを提供します。これにより、障害識別のアルゴリズムが改善される可能性があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-11">
        <br><font color="black">2020-11-11</font>
      </time>
    </span>
</section>
<!-- paper0: Pruning neural networks without any data by iteratively conserving
  synaptic flow -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_58.html">
      <font color="black">Pruning neural networks without any data by iteratively conserving
  synaptic flow</font>
    </a>
  </h2>
  <font color="black">最初に、初期化時の既存の勾配ベースのプルーニングアルゴリズムがレイヤーの崩壊、ネットワークをトレーニング不能にするレイヤー全体の時期尚早なプルーニングに悩まされる理由を説明する保存則を数学的に定式化し、実験的に検証します。したがって、データに依存しないプルーニングアルゴリズムは既存のプルーニングアルゴリズムに挑戦します。初期化時に、どのシナプスが重要であるかを定量化するためにデータを使用する必要があるというパラダイム。最近の研究では、トレーニングと剪定のサイクルの費用のかかるシーケンスを通じて、初期化時に勝利の宝くじチケットまたはまばらなトレーニング可能なサブネットワークの存在が特定されました。 
[概要]以前の作品では、当選した宝くじの存在が確認されています。これは、トレーニングと剪定のサイクルの激しいプロセスを通じて説明できます。これは、レイヤーの崩壊を完全に回避する方法を説明しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-09">
        <br><font color="black">2020-06-09</font>
      </time>
    </span>
</section>
<!-- paper0: DCT-Mask: Discrete Cosine Transform Mask Representation for Instance
  Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_59.html">
      <font color="black">DCT-Mask: Discrete Cosine Transform Mask Representation for Instance
  Segmentation</font>
    </a>
  </h2>
  <font color="black">さらに、マスク表現の品質の観点からメソッドのパフォーマンスを分析します。ベルやホイッスルがない場合、DCT-Maskは、さまざまなフレームワーク、バックボーン、データセット、およびトレーニングスケジュールで大幅な向上をもたらします。DCTの主な理由-マスクがうまく機能するのは、複雑さの少ない高品質のマスク表現を取得できることです。 
[ABSTRACT] mask r-cnnは、$ 28 /回28 $バイナリグリッドでマスクを予測します。これは、代表的なインスタンス化を作成するための最初のステップです。ベルやホイッスルなしで、dct-マスクは、さまざまなフレームワーク、バックボーン、データセットで大幅な向上をもたらします。 、およびトレーニングスケジュール</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Deep correction of breathing-related artifacts in MR-thermometry -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_60.html">
      <font color="black">Deep correction of breathing-related artifacts in MR-thermometry</font>
    </a>
  </h2>
  <font color="black">このため、畳み込みニューラルネットワーク（CNN）は、温熱療法の前の準備学習段階で取得された画像から見かけの温度摂動を学習するように設計されました。その後の温熱療法の手順では、最近のマグニチュード画像がCNN-の入力として使用されます。現在の温度マップのオンライン補正を生成するためのモデル。補正しないままにすると、これらのアーティファクトは温度推定値に重大なエラーを引き起こし、治療ガイダンスを損ないます。 
[ABSTRACT]温度アーチファクトが呼吸および生理学的運動によって誘発されるため、移動するターゲットの温度測定は依然として困難です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-10">
        <br><font color="black">2020-11-10</font>
      </time>
    </span>
</section>
<!-- paper0: Perceptron Synthesis Network: Rethinking the Action Scale Variances in
  Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_61.html">
      <font color="black">Perceptron Synthesis Network: Rethinking the Action Scale Variances in
  Videos</font>
    </a>
  </h2>
  <font color="black">特に、私たちの低解像度モデルは、最近の強力なベースライン手法、つまりTSMとGSTを上回り、計算コストの30 \％未満です。相互作用の豊富さとパスの情報容量を保証するために、新しい\を設計します。 textit {optimized feature Fusion layer} ..この制限を克服するために、データから最適なスケールのカーネルを学習することを提案します。 
[ABSTRACT]モーションパーセプトロンシンセサイザーは、スロットパスによって相互作用される固定サイズのカーネルのバッグからカーネルを生成するために提案されています。このレイヤーは、現在の機能融合技術のほとんどを初めてカバーするのに十分な原理的なユニバーサルツールを設定します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-22">
        <br><font color="black">2020-07-22</font>
      </time>
    </span>
</section>
<!-- paper0: Continuous and Diverse Image-to-Image Translation via Signed Attribute
  Vectors -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_62.html">
      <font color="black">Continuous and Diverse Image-to-Image Translation via Signed Attribute
  Vectors</font>
    </a>
  </h2>
  <font color="black">連続翻訳結果の視覚的品質を向上させるために、2つの符号対称属性ベクトル間の軌道を生成し、その軌道に沿って補間された結果のドメイン情報を敵対的トレーニングに活用します。最先端の方法に対する高品質の連続翻訳結果。中間結果のスムーズなシーケンスを生成することで、2つの異なるドメインのギャップを埋め、ドメイン間のモーフィング効果を促進します。 
[概要]連続翻訳の問題は十分に研究されていません。既存のi2iアプローチは連続翻訳に限定されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-02">
        <br><font color="black">2020-11-02</font>
      </time>
    </span>
</section>
<!-- paper0: Dense Label Encoding for Boundary Discontinuity Free Rotation Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_63.html">
      <font color="black">Dense Label Encoding for Boundary Discontinuity Free Rotation Detection</font>
    </a>
  </h2>
  <font color="black">航空画像の大規模な公開データセットに関する広範な実験と視覚的分析。ソースコードはhttps://github.com/Thinklab-SJTU/DCL_RetinaNet_Tensorflowで入手でき、オープンソースの回転検出ベンチマークにも統合されています：https：//github.com/yangxue0827/RotationDetection ..回転検出は基本的な役割を果たします航空画像、シーンテキスト、顔などを含む多くのビジュアルアプリケーションのビルディングブロック。
[概要] 2つの側面でフロンティアをプッシュする新しい手法を提案します。角度距離、シーン比率に敏感な重み付け（adarsw）を提案し、特に検出精度を向上させます。正方形の場合-オブジェクトのように</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Style Intervention: How to Achieve Spatial Disentanglement with
  Style-based Generators? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_64.html">
      <font color="black">Style Intervention: How to Achieve Spatial Disentanglement with
  Style-based Generators?</font>
    </a>
  </h2>
  <font color="black">具体的には、任意の入力画像に適応し、柔軟な目的の下で自然な翻訳効果をレンダリングできる軽量の最適化ベースのアルゴリズムである「スタイル介入」を提案します。広範な定性的結果は、私たちの方法の有効性を示し、定量的測定は、提案されたアルゴリズムは、さまざまな面で最先端のベンチマークを上回っています。フォトリアリズムと一貫性の両方が要求される高解像度画像の顔属性編集で提案されたフレームワークのパフォーマンスを検証します。 
[ABSTRACT] stylegan（stylegan）は、画像合成のセマンティック制御を正常に有効にします。しかし、最近の研究では、編集を使用して最近述べることができることも明らかになっています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Open-sourced Dataset Protection via Backdoor Watermarking -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_65.html">
      <font color="black">Open-sourced Dataset Protection via Backdoor Watermarking</font>
    </a>
  </h2>
  <font color="black">いくつかのベンチマークデータセットで実験が行われ、提案された方法の有効性が検証されます。データセットのウォーターマークに古典的な中毒ベースのバックドア攻撃（$ eg $、BadNets）を採用します。つまり、特定のトリガー（$たとえば、ローカルパッチ）を、事前定義されたターゲットクラスでラベル付けされたいくつかの良性サンプルに適用します。具体的には、提案されたメソッドには、\ emph {データセットのウォーターマーク}と\ emph {データセットの検証}を含む2つの主要なプロセスが含まれます。 
[概要]迅速なデータセットでは、商業目的ではなく学術目的または教育目的でのみ採用できる必要がありますが、それらを保護するための適切な方法はまだありません。提案された方法には、データセットを保護するために使用される2つの主要なプロセスが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-12">
        <br><font color="black">2020-10-12</font>
      </time>
    </span>
</section>
<!-- paper0: DeepMorph: A System for Hiding Bitstrings in Morphable Vector Drawings -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_66.html">
      <font color="black">DeepMorph: A System for Hiding Bitstrings in Morphable Vector Drawings</font>
    </a>
  </h2>
  <font color="black">実世界の画像キャプチャ条件に対する堅牢性を追加するために、画像の破損がソフトラスタライザとデコーダの間に注入されます。この方法は、共同でトレーニングされる2つのニューラルネットワークで構成されます。ビットストリングを摂動に変換するエンコーダネットワーク描画プリミティブ、およびモーフィングされた描画の画像からビットストリングを復元するデコーダネットワーク。バックプロパゲーションによるエンドツーエンドのトレーニングを可能にするために、描画プリミティブの摂動に関して区別可能なソフトラスタライザを導入します。 
[概要]ユースケースは、十分に編集されたqrコードのユースケースと似ていますが、私たちのソリューションは、クリエイティブに芸術的な自由を提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: gradSLAM: Automagically differentiable SLAM -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_67.html">
      <font color="black">gradSLAM: Automagically differentiable SLAM</font>
    </a>
  </h2>
  <font color="black">この作業では、勾配ベースの学習とSLAMを統合するSLAMシステムを微分可能な計算グラフとして提示する方法論であるgradSLAMを提案します。TL; DR：自動微分フレームワークの力を活用して高密度SLAMを微分可能にします。精度を犠牲にすることなく、微分可能な信頼領域オプティマイザ、表面測定と融合スキーム、およびレイキャスティング。 
[ABSTRACT] slamは、生のセンサーをロボットベースの状態（s）全体の分布に変換する操作です。これにより、異なるシステムから学習できますが、一般的な高密度プロジェクトのいくつかのコンポーネントは区別できません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-23">
        <br><font color="black">2019-10-23</font>
      </time>
    </span>
</section>
<!-- paper0: Interval-valued aggregation functions based on moderate deviations
  applied to Motor-Imagery-Based Brain Computer Interface -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_68.html">
      <font color="black">Interval-valued aggregation functions based on moderate deviations
  applied to Motor-Imagery-Based Brain Computer Interface</font>
    </a>
  </h2>
  <font color="black">そのために、区間値の中程度の偏差関数の概念を導入し、特に入力間隔の幅を保持する区間値の中程度の偏差関数を研究します。次に、これらの関数を適用して区間を構築する方法を研究します。価値のある集計関数..この作業では、中程度の偏差関数を使用して、指定された区間値データのセット間の類似性と非類似性を測定します。 
[概要] 2つの運動イメージブレインコンピューターインターフェースフレームワークの意思決定フェーズでそれらを適用しました。他の数値および間隔偏差を使用して得られた結果よりも優れた結果が得られました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Attention-Based Transformers for Instance Segmentation of Cells in
  Microstructures -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_69.html">
      <font color="black">Attention-Based Transformers for Instance Segmentation of Cells in
  Microstructures</font>
    </a>
  </h2>
  <font color="black">オブジェクトインスタンスの検出とセグメント化は、生物医学アプリケーションの一般的なタスクです。セグメンテーションのパフォーマンスは最先端のインスタンスセグメンテーション方法と同等ですが、Cell-DETRはよりシンプルで高速です。この方法の貢献をシステムまたは合成生物学で一般的に使用される、微細構造環境での酵母のセグメント化の典型的な使用例。 
[概要]これらの例は、機能的磁気共鳴画像での病変の検出から、組織病理学的画像での腫瘍の検出にまで及びます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Plane Program Induction with 3D Box Priors -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_70.html">
      <font color="black">Multi-Plane Program Induction with 3D Box Priors</font>
    </a>
  </h2>
  <font color="black">ニューラルネットワークを使用して、消失点やワイヤーフレームラインなどの視覚的な手がかりを推測し、検索ベースのアルゴリズムをガイドして、画像を最もよく説明するプログラムを見つけます。画像の理解と編集では、通常のプログラムのような2つの重要な側面を考慮します。 2D平面のテクスチャまたはパターン、およびシーン内のこれらの平面の3Dポーズ。このような全体的で構造化されたシーン表現により、欠落したピクセルの修復、カメラパラメータの変更、画像コンテンツの推定など、3D対応のインタラクティブな画像編集操作が可能になります。 
[概要]ボックスプログラム帰納法（bpi）を提示します。これは、複数の2D平面上で繰り返される構造を同時にモデル化するシーン表現のようなプログラムを推測します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: DeepRepair: Style-Guided Repairing for DNNs in the Real-world
  Operational Environment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_71.html">
      <font color="black">DeepRepair: Style-Guided Repairing for DNNs in the Real-world
  Operational Environment</font>
    </a>
  </h2>
  <font color="black">したがって、これはDNNの実際のアプリケーションにとってかなり重要な問題を引き起こします。つまり、展開された運用環境で障害サンプルを修正するために展開されたDNNを修復する方法であり、通常のデータまたはクリーンなデータを処理する機能を損なうことはありません。 ..したがって、収集できる限られた障害サンプルに基づいて、より類似した障害を修復する方法はかなり困難です。実際に収集できる障害サンプルの数は、運用環境のノイズ要因によって制限されることがよくあります。 
[ABSTRACT] dnnは、運用環境での実際の使用中にエラーを発生させることがよくあります。そのため、特定のエラーを予測することが難しい場合があります。代わりに、システムはトレーニングデータセットの不一致に基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Predict the 3D Layout of a Scene -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_72.html">
      <font color="black">Learning to Predict the 3D Layout of a Scene</font>
    </a>
  </h2>
  <font color="black">トレーニングにはKITTIデータセットを使用します。このデータセットは、クラスラベル、2Dバウンディングボックス、7自由度の3Dアノテーションを備えた道路交通シーンで構成されています。単一のRGB画像のみを使用する方法を提案し、デバイスや車両でのアプリケーションを可能にします。 LiDARセンサーを備えていない。さらに、サブ問題と実装の詳細が全体的な予測結果にどのように影響するかを評価します。 
[概要] 3D検出への最近のアプローチの多くは、視覚にLIDARポイントクラウドを使用しています。2D検出器を3D検出ヘッドで拡張することにより、最近の2Dオブジェクト検出器の成熟度と成功を活用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Multi-view Depth Estimation with Predicted Uncertainty -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_73.html">
      <font color="black">Deep Multi-view Depth Estimation with Predicted Uncertainty</font>
    </a>
  </h2>
  <font color="black">特に、DRNには、深層フィーチャをリファインすることにより、反復での深さの精度を向上させる反復リファインメントモジュール（IRM）が含まれています。この論文では、ディープニューラルネットワークを使用して一連の画像から密な深さを推定する問題に対処します。ただし、ポイントクラウドの一部は、一般的な観測値がないか、ベースラインと深度の比率が小さいため、他の部分よりも精度が低い場合があります。 
[ABSTRACT] depth-リファインメントネットワーク（drn）は、画像のコンテキストキューに基づいて初期デプスマップを最適化します。drnは、リファインされた深度の不確実性も予測します。これは、シーン再構成の測定選択などのアプリケーションで望ましいことです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Spatio-Temporal Video Scene Text Detection via Temporal
  Clustering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CV/paper_74.html">
      <font color="black">Towards Spatio-Temporal Video Scene Text Detection via Temporal
  Clustering</font>
    </a>
  </h2>
  <font color="black">STVText4には、106本の動画の161,347本の動画フレームから140万を超えるテキストインスタンスが含まれています。各インスタンスには、空間境界ボックスと時間範囲だけでなく、読みやすさ、密度、スケール、ライフサイクルなどの4つの固有の属性が注釈として付けられ、コミュニティを促進します。 ..データセットとコードはまもなく利用可能になります..ビデオシーケンス内の同一のテキストの継続的な伝播により、TCはテキストの空間的な四辺形および時間範囲を正確に出力でき、ST-VSTDの強力なベースラインを設定します。 
[ABSTRACT] stvtext textsには、106本の動画のビデオフレームからの140万を超えるテキストが含まれています。stvtext4は、適切に設計された空間検出メトリック（stdm）であり、新しいクラスタリングベースのベースラインメソッドです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Everybody Sign Now: Translating Spoken Language to Photo Realistic Sign
  Language Video -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_0.html">
      <font color="black">Everybody Sign Now: Translating Spoken Language to Photo Realistic Sign
  Language Video</font>
    </a>
  </h2>
  <font color="black">ろうコミュニティに真に理解され受け入れられるためには、自動手話制作（SLP）システムがフォトリアリスティックな署名者を生成する必要があります。次に、ポーズ条件付きの人間合成モデルを導入して、骨格からフォトリアリスティックな手話ビデオを生成します。ポーズシーケンス..この論文では、話された言語から直接フォトリアリスティックな連続手話ビデオを生成する最初のSLPモデルであるSignGANを提案します。 
[概要]混合密度ネットワークを備えたトランスアーキテクチャを採用して、話し言葉から骨格ポーズへの翻訳を処理します。サインビデオは、書かれたテキストから直接翻訳して作成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: A Bilingual Generative Transformer for Semantic Sentence Embedding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_1.html">
      <font color="black">A Bilingual Generative Transformer for Semantic Sentence Embedding</font>
    </a>
  </h2>
  <font color="black">まず、変分確率フレームワークを使用することで、ソースの分離を促進する事前分布を導入し、モデルの後方を使用して、テスト時の単一言語データの文の埋め込みを予測できます。実験では、私たちのアプローチは最先端を大幅に上回っています。監視されていないセマンティック類似性評価の標準スイートについて..提案されたアプローチは、セマンティックセンテンスエンコーディングに関する過去の研究とは2つの点で異なります。 
[要約]提案されたアプローチは、言語データを奨励するように設計されていますが、意味文に関する以前の研究とは関係ありません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-10">
        <br><font color="black">2019-11-10</font>
      </time>
    </span>
</section>
<!-- paper0: TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging,
  audio, and lip videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_2.html">
      <font color="black">TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging,
  audio, and lip videos</font>
    </a>
  </h2>
  <font color="black">TaLコーパスは、CC BY-NC 4.0ライセンスの下で公開されています。TaLは2つの部分で構成されています。TaL1は、英語を母国語とする男性1人のプロの声優による6つの録音セッションのセットです。 TaL80は、声優の経験のない英語のネイティブスピーカー81人の録音セッションのセットです。音声、超音波舌イメージング、および唇のビデオのマルチスピーカーコーパスであるTongue and Lipsコーパス（TaL）を紹介します。 
[ABSTRACT] tal1は、英語を母国語とする男性の1人のプロの声優による録音セッションのセットです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Position-Aware Tagging for Aspect Sentiment Triplet Extraction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_3.html">
      <font color="black">Position-Aware Tagging for Aspect Sentiment Triplet Extraction</font>
    </a>
  </h2>
  <font color="black">また、モデルの有効性と堅牢性を調査するために広範な実験を実施しました。この作業では、トリプレットを共同で抽出できる新しい位置認識タグ付けスキームを備えた最初のエンドツーエンドモデルを提案します。トリプレット抽出プロセスをいくつかの段階に分割するパイプラインアプローチを使用して、この問題を解決します。 
[要約]研究努力は主にパイプラインアプローチを使用して問題を解決します。これらのアプローチはトリプレット抽出プロセスをいくつかの段階に分割します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-06">
        <br><font color="black">2020-10-06</font>
      </time>
    </span>
</section>
<!-- paper0: Generating Semantically Valid Adversarial Questions for TableQA -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_4.html">
      <font color="black">Generating Semantically Valid Adversarial Questions for TableQA</font>
    </a>
  </h2>
  <font color="black">元の質問の意味を維持するために、SIMILEとエンティティの非語彙化による最小リスクトレーニングを適用します。Gumbel-Softmaxを使用して、エンドツーエンドトレーニングの敵対的損失を組み込みます。最後に、SAGE拡張データを使用した敵対的トレーニングが改善できることを示します。 TableQAシステムのパフォーマンスと堅牢性。 
[概要]元の質問の意味を維持するために最小リスクトレーニングを使用します。最大の敵対者、直喩とエンティティの非語彙化による最小リスクテストを適用します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-26">
        <br><font color="black">2020-05-26</font>
      </time>
    </span>
</section>
<!-- paper0: Exploring Text Specific and Blackbox Fairness Algorithms in Multimodal
  Clinical NLP -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_5.html">
      <font color="black">Exploring Text Specific and Blackbox Fairness Algorithms in Multimodal
  Clinical NLP</font>
    </a>
  </h2>
  <font color="black">私たちの論文が臨床NLPと公平性の重要な交差点で将来の貢献を刺激することを願っています。この目的のために、モダリティに依存しない公平性アルゴリズム（処理後の等化オッズ）を調査し、テキスト固有の公平性アルゴリズムと比較します：偏りのない臨床単語の埋め込み..完全なソースコードはこちらから入手できます：https：//github.com/johntiger1/multimodal_fairness 
[ABSTRACT]マルチモーダル臨床データセットの公平性を調査する新しいタスクを提案し、ダウンストリームに等化オッズを採用します。テキスト-公平性への特定のアプローチは、パフォーマンスと公平性の古典的な概念のバランスを同時に達成する可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: oLMpics -- On what Language Model Pre-training Captures -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_6.html">
      <font color="black">oLMpics -- On what Language Model Pre-training Captures</font>
    </a>
  </h2>
  <font color="black">これに対処するために、ゼロショット評価（微調整なし）と、微調整されたLMの学習曲線を複数のコントロールの学習曲線と比較することの両方を含む評価プロトコルを提案します。 LM機能..ただし、LM表現がシンボリック推論タスクに役立つかどうかを理解するための取り組みは限られており、散在しています。主な調査結果は次のとおりです。（a）異なるLMは、質的に異なる推論能力を示します。たとえば、RoBERTaは推論タスクに成功します。 BERTが完全に失敗する場合。 （b）LMは抽象的な方法で推論せず、コンテキストに依存します。たとえば、RoBERTaは年齢を比較できますが、年齢が人間の年齢の典型的な範囲内にある場合にのみ比較できます。 （c）推論タスクの半分で、すべてのモデルが完全に失敗します。 
[要約] lm表現がシンボリック推論タスクに役立つかどうかを理解するための努力は限られており、散在しています。目的は、タスクでのlmのパフォーマンスが接続詞または微調整の言語に起因するかどうかを理解することです。タスクデータ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-31">
        <br><font color="black">2019-12-31</font>
      </time>
    </span>
</section>
<!-- paper0: The Volctrans Machine Translation System for WMT20 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_7.html">
      <font color="black">The Volctrans Machine Translation System for WMT20</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、WMT20共有ニュース翻訳タスクでのVolcTransシステムについて説明します。8つの翻訳方向に参加しました。基本システムはTransformerに基づいており、いくつかのバリエーションがあります（より広いまたはより深いTransformer、動的畳み込み）。 
[概要]最終的なシステムには、テキストの事前処理、データの選択、合成データの生成、および多言語の事前トレーニングが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-28">
        <br><font color="black">2020-10-28</font>
      </time>
    </span>
</section>
<!-- paper0: Neural Semi-supervised Learning for Text Classification Under
  Large-Scale Pretraining -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_8.html">
      <font color="black">Neural Semi-supervised Learning for Text Classification Under
  Large-Scale Pretraining</font>
    </a>
  </h2>
  <font color="black">私たちの仕事は、大規模な事前トレーニングのコンテキストでの半教師あり学習モデルの動作を理解するための最初のステップです。半教師あり学習戦略を使用すると、わずか50のトレーニングデータで約93.8％の精度のパフォーマンスを達成できます。 IMDBデータセットのポイント、および完全なIMDBデータセットとの96.6％の競争力のあるパフォーマンス。この論文では、大規模なLM事前トレーニングのコンテキストで、テキスト分類のタスクにおける半教師あり学習に関する包括的な研究を行います。 
[概要]私たちの仕事は、大規模な事前トレーニングのコンテキストでの半教師あり学習モデルの動作を理解するための最初のステップです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Living Machines: A study of atypical animacy -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_9.html">
      <font color="black">Living Machines: A study of atypical animacy</font>
    </a>
  </h2>
  <font color="black">これに対処するために、19世紀の英語の文に基づいて、機械が生物または無生物として表される、非定型の有生性検出用の最初のデータセットを作成しました。このペーパーでは、有生性検出への新しいアプローチを提案します。エンティティはテキスト内で有生性として表されます。特に非常に複雑な形式の言語使用に適用した場合、私たちの方法が非定型の有生性の実質的により正確な特性を提供することを示します。 
[概要]私たちの方法は、言語モデリングにおける最近の革新に基づいています。私たちの方法が、非定型の有生性の実質的により正確な定義を提供することを示します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-22">
        <br><font color="black">2020-05-22</font>
      </time>
    </span>
</section>
<!-- paper0: SentiLSTM: A Deep Learning Approach for Sentiment Analysis of Restaurant
  Reviews -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_10.html">
      <font color="black">SentiLSTM: A Deep Learning Approach for Sentiment Analysis of Restaurant
  Reviews</font>
    </a>
  </h2>
  <font color="black">インターネットへの簡単なアクセスとさまざまなWeb2.0アプリケーションの進化により、テキストデータの生成量は大幅に増加しました。顧客レビューの影響は、レストランに対する顧客の態度を認識する上で重要です。レビューからの感情は、レストランの所有者、またはサービスプロバイダーと顧客が、意思決定やサービスをより満足のいくものにするために有利です。 
[概要]顧客レビューの影響は、レストランに対する顧客の態度を認識するために重要です。このペーパーでは、レストランのクライアントから提供されたレビューを正と負の極性に分類するためのディープラーニングベースの手法を提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: WNUT-2020 Task 1 Overview: Extracting Entities and Relations from Wet
  Lab Protocols -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_11.html">
      <font color="black">WNUT-2020 Task 1 Overview: Extracting Entities and Relations from Wet
  Lab Protocols</font>
    </a>
  </h2>
  <font color="black">タスク、データ注釈プロセス、コーパス統計の概要を説明し、各サブタスクの参加システムの概要を示します。このペーパーでは、WNUT2020でのウェットラボ情報抽出タスクの結果を示します。このタスクは以下で構成されています。 2つのサブタスク：（1）13人の参加者がいる名前付きエンティティ認識（NER）タスクと（2）2人の参加者がいる関係抽出（RE）タスク。 
[概要] 51歳のタスクには、13人の参加者による名前付きエンティティ認識タスクが含まれていました。2人の参加者による関係抽出-re）タスクが含まれていました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-27">
        <br><font color="black">2020-10-27</font>
      </time>
    </span>
</section>
<!-- paper0: Universal MelGAN: A Robust Neural Vocoder for High-Fidelity Waveform
  Generation in Multiple Domains -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_12.html">
      <font color="black">Universal MelGAN: A Robust Neural Vocoder for High-Fidelity Waveform
  Generation in Multiple Domains</font>
    </a>
  </h2>
  <font color="black">複数の領域で忠実度の高い音声を合成するボコーダーであるUniversalMelGANを提案します。特に、話者、感情、言語に関して、目に見えない領域で優れたパフォーマンスを示しました。これにより、モデルはマルチスピーカーのリアルな波形を生成できます。 、大規模フットプリントモデルの高周波数帯域での過度の平滑化の問題を軽減することによって。 
[概要]メルガンベースの構造は、地面に近い信号を生成します-真実のデータ。モデルはスペクトログラムスピーカーを使用して22を超えるスピーカーを生成しました。外部ドメイン情報なしで達成された結果は、ユニバーサルボコーダーとして提案されたモデルの可能性を強調しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Do We Need Online NLU Tools? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_13.html">
      <font color="black">Do We Need Online NLU Tools?</font>
    </a>
  </h2>
  <font color="black">最後に、選択したパブリックNLUサービスを選択したオープンソースアルゴリズムと比較して、インテント認識を行います。現在、APIとしていくつかのインテント認識サービスを利用できます。または、多くのオープンソースの選択肢から選択します。評価用のデータセットを示します。 
[ABSTRACT]インテント認識サービスは現在APIとして利用可能です。または、多くのオープンソースの選択肢から選択します。多くの要因により、実際には適切なアプリケーションの選択が困難になります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Residual Local Feature Learning for Speech Emotion Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_14.html">
      <font color="black">Deep Residual Local Feature Learning for Speech Emotion Recognition</font>
    </a>
  </h2>
  <font color="black">ただし、ディープラーニングの効率はレイヤーの数に依存します。つまり、レイヤーが深いほど効率が高くなります。公開されている2つのデータセットEMODBとRAVDESSに基づいて、提案されたDeepResLFLBは、標準の指標で評価した場合にパフォーマンスを大幅に向上させることができます。 、精度、再現率、およびF1スコア..したがって、このペーパーでは、既存のローカル特徴学習ブロック（LFLB）の再設計を提案しました。 
[概要]新しい設計は、深残余局所特徴学習ブロック（deepreslflb）と呼ばれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Relation Extraction with Contextualized Relation Embedding (CRE) -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_15.html">
      <font color="black">Relation Extraction with Contextualized Relation Embedding (CRE)</font>
    </a>
  </h2>
  <font color="black">このモデルは、文をコンテキスト化されたリレーション埋め込みにエンコードする新しいアプローチを適用します。これは、パラメータ化されたエンティティ埋め込みと一緒に使用して、リレーションインスタンスをスコアリングできます。ソースコードが利用可能になりました。KBモデリングをリレーションに内部化するモデルアーキテクチャを紹介します。抽出。 【概要】本論文では、意味情報と知識ベースモデリングを斬新な方法で統合するアーキテクチャを提案する。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: When Does Unsupervised Machine Translation Work? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_16.html">
      <font color="black">When Does Unsupervised Machine Translation Work?</font>
    </a>
  </h2>
  <font color="black">ソースコーパスとターゲットコーパスが異なるドメインからのものである場合、パフォーマンスが急速に低下し、ランダムな単語埋め込みの初期化がダウンストリームの翻訳パフォーマンスに劇的な影響を与える可能性があることがわかりました。教師なしMTシステムの広範な経験的評価を提唱して、障害点を強調し、最も有望なパラダイム..さらに、ソース言語とターゲット言語が異なるスクリプトを使用すると、教師なしMTのパフォーマンスが低下し、本物の低リソース言語ペアでパフォーマンスが非常に低下することがわかります。 
[概要]異なる言語ペア、異なるドメイン、多様なデータセット、および本物の低リソース言語を使用して、教師なしmtの広範な評価を実施します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-12">
        <br><font color="black">2020-04-12</font>
      </time>
    </span>
</section>
<!-- paper0: Theoretical Knowledge Graph Reasoning via Ending Anchored Rules -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_17.html">
      <font color="black">Theoretical Knowledge Graph Reasoning via Ending Anchored Rules</font>
    </a>
  </h2>
  <font color="black">結果は、EARDictモデルが、WN18RRで80.38パーセントのHits @ 10スコアを含む、ベンチマーク知識グラフ完了タスクで新しい最先端のパフォーマンスを達成することを示しています。私たちの理論は、トリプルが正しい理由またはそうでない理由に答える正確な理由を提供します。 ..知識グラフから正確で具体的なルールを見つけることは重要な課題と見なされており、多くのダウンストリームタスクのパフォーマンスを向上させ、自然言語処理の研究トピックにアプローチする新しい方法を提供することさえできます。 
[概要]新しい研究は新しい研究チームによって実施されました。それは知識グラフ推論の基礎理論を提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-12">
        <br><font color="black">2020-11-12</font>
      </time>
    </span>
</section>
<!-- paper0: An Integrated Approach for Improving Brand Consistency of Web Content:
  Modeling, Analysis and Recommendation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_18.html">
      <font color="black">An Integrated Approach for Improving Brand Consistency of Web Content:
  Modeling, Analysis and Recommendation</font>
    </a>
  </h2>
  <font color="black">分類子は、企業の使命とビジョンと一致しないWeb記事を自動的に識別し、さらに一貫性を維持できない条件を発見するのに役立ちます。消費者に依存する（企業から消費者への）組織は、会社のブランドパーソナリティと呼ばれる一連の人間的資質を持っていることを示します。一貫したブランドは、規則性と共通のパターンへの親和性を発達させるにつれて、信頼を生み出し、長期にわたって顧客を維持します。 
[概要]この問題は、作成してインターネットにプッシュする必要のあるコンテンツの量と一致しています。ブランドの不整合の問題に対処するために、変更が必要な上位3つの文を配信する文ランキングシステムを開発しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Fact-level Extractive Summarization with Hierarchical Graph Mask on BERT -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_19.html">
      <font color="black">Fact-level Extractive Summarization with Hierarchical Graph Mask on BERT</font>
    </a>
  </h2>
  <font color="black">CNN / DaliyMailデータセットでの実験は、モデルが最先端の結果を達成することを示しています。さらに、階層グラフマスクを使用してモデルをBERTに組み込みます。これにより、BERTの自然言語理解とモデルのスケールを大きくせずに構造情報。 
[要約]人間の間にギャップがあります-書かれた金の要約とオラクルの文のラベル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Combining Determinism and Indeterminism -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_20.html">
      <font color="black">Combining Determinism and Indeterminism</font>
    </a>
  </h2>
  <font color="black">ここでは、バイイミュニティセットが証明されています。目的は、バイイミュニティを維持する操作です。この新しい非可算サブグループは、バイイミュニティ対称群と呼ばれます。正式には、計算可能に列挙可能な（ce）に適用される操作に関するいくつかの結果
[ABSTRACT]これらの操作は、計算可能に列挙可能な（ce）に適用されます。これらの操作は、自然数に数えられないグループを生成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-02">
        <br><font color="black">2020-09-02</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Persona-Based Empathetic Conversational Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_21.html">
      <font color="black">Towards Persona-Based Empathetic Conversational Models</font>
    </a>
  </h2>
  <font color="black">最後に、共感的反応に対するペルソナの影響を調査するために広範な実験を行います。具体的には、最初に、ペルソナベースの共感的会話のための新しい大規模マルチドメインデータセットを提示します。特に、私たちの結果は、ペルソナが共感的反応をより改善することを示していますCoBERTが非共感的な会話よりも共感的な会話について訓練されている場合、人間の会話におけるペルソナと共感の間に経験的なリンクを確立します。 
[概要]心理学では、ペルソナは性格と高い相関関係があることが示され、それが共感に影響を与えます。さらに、ペルソナベースの共感的な会話に向けた新しいタスクを提案します。これは、共感に対するペルソナの影響に関する最初の研究です。応答する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-26">
        <br><font color="black">2020-04-26</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Variational Word Masks to Improve the Interpretability of
  Neural Text Classifiers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_22.html">
      <font color="black">Learning Variational Word Masks to Improve the Interpretability of
  Neural Text Classifiers</font>
    </a>
  </h2>
  <font color="black">この制限に対処するために、タスク固有の重要な単語を自動的に学習し、分類に関する無関係な情報を削減する変分単語マスク（VMASK）法を提案します。これにより、モデル予測の解釈可能性が最終的に向上します。実験により、両方のモデルの改善におけるVMASKの有効性が示されます。予測の精度と解釈可能性..提案された方法は、7つのベンチマークテキスト分類データセットで3つのニューラルテキスト分類器（CNN、LSTM、およびBERT）を使用して評価されます。 
[概要]モデルの解釈可能性を改善するための新しい作業が始まったばかりです。多くの既存の方法では、事前情報または人間の注釈が必要です。提案された方法は、3つのニューラルテキスト分類子で評価されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Are Pre-trained Language Models Knowledgeable to Ground Open Domain
  Dialogues? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_23.html">
      <font color="black">Are Pre-trained Language Models Knowledgeable to Ground Open Domain
  Dialogues?</font>
    </a>
  </h2>
  <font color="black">ベンチマークに関する広範な実験を通じて、知識を含むいくつかのダイアログで微調整することにより、事前にトレーニングされた言語モデルが、自動評価と人間の判断で外部の知識を必要とする最先端のモデルよりも優れていることがわかりました。私たちが提起した質問に対する肯定的な答え..事前に訓練された言語モデルを使用して知識に基づいた対話の生成を研究します..ベンチマークで新しい最先端を追求する代わりに、知識がパラメータに格納されているかどうかを理解しようとします事前にトレーニングされたモデルは、オープンドメインの対話を開始するのにすでに十分であり、したがって、生成中の外部の知識ソースへの依存を取り除くことができます。 
[概要]事前にトレーニングされたモデルのパラメータに保存されている知識が、オープンドメインの話し合いを開始するのにすでに十分であるかどうかを理解しようとします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Entity Recognition and Relation Extraction from Scientific and Technical
  Texts in Russian -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_24.html">
      <font color="black">Entity Recognition and Relation Extraction from Scientific and Technical
  Texts in Russian</font>
    </a>
  </h2>
  <font color="black">これらのタスクのテキストコレクションは英語用に存在し、科学界で積極的に使用されていますが、現在、ロシア語のこのようなデータセットは公開されていません。キーワード抽出方法、語彙方法、およびを比較した実験の結果も含まれています。ニューラルネットワークに基づくいくつかの方法..この論文では、ロシア語の方法のいくつかの修正が提案されています。 
[要約]この論文では、ロシア語の科学的テキストのコーパスを紹介します。データセットと研究データセットはwwwで入手できます。 github。 com / iis-研究チーム</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Persuasive Dialogue Understanding: the Baselines and Negative Results -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/cs.CL/paper_25.html">
      <font color="black">Persuasive Dialogue Understanding: the Baselines and Negative Results</font>
    </a>
  </h2>
  <font color="black">説得は、説得者の戦略を含む一連の説得力のあるメッセージを介して自分の意見と行動を形成することを目的としています。2つの否定的な結果を観察します。このモデルでは、話者間および話者内のコンテキストセマンティック機能とラベルの依存関係を活用して、認識。 
[概要]トランスフォーマーベースのアプローチの制限を示しました。このモデルは説得力のあるラベルの依存関係をキャプチャできます。このアーキテクチャはスピーカーをしのぐことができません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging,
  audio, and lip videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.AS/paper_0.html">
      <font color="black">TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging,
  audio, and lip videos</font>
    </a>
  </h2>
  <font color="black">TaLコーパスは、CC BY-NC 4.0ライセンスの下で公開されています。全体として、コーパスには24時間の並列超音波、ビデオ、およびオーディオデータが含まれ、そのうち約13.5時間は音声です。TaLは2つの部分で構成されます。英語を母国語とする男性1人のプロの声優による6回の録音セッションのセット。 TaL80は、声優の経験がない英語のネイティブスピーカー81人による録音セッションのセットです。 
[ABSTRACT] tal1は、英語を母国語とする男性の1人のプロの声優による録音セッションのセットです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Uncertainty-Aware Multi-Modal Ensembling for Severity Prediction of
  Alzheimer's Dementia -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.AS/paper_1.html">
      <font color="black">Uncertainty-Aware Multi-Modal Ensembling for Severity Prediction of
  Alzheimer's Dementia</font>
    </a>
  </h2>
  <font color="black">不確実性の推定に基づいてさまざまなモダリティを比較検討し、被験者に依存しないバランスの取れたデータセットであるベンチマークADReSSデータセットを実験して、システムの全体的なエントロピーを低減しながら、この方法が最先端の方法よりも優れていることを示します。 ..ニューラルネットワーク（NN）の信頼性は、ヘルスケアなどの安全性が重要なアプリケーションで重要です。不確実性の推定は、展開におけるNNの信頼性を強調するために広く研究されている方法です。この作業は、公正で意識の高いモデルを奨励することを目的としています。 
[概要]この研究では、アルツハイマー病を予測するための音響ブースティング技術を提案します。この方法は、システムの全体的な不安定性を低減しながら、最先端の方法よりも優れています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-03">
        <br><font color="black">2020-10-03</font>
      </time>
    </span>
</section>
<!-- paper0: Universal MelGAN: A Robust Neural Vocoder for High-Fidelity Waveform
  Generation in Multiple Domains -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.AS/paper_2.html">
      <font color="black">Universal MelGAN: A Robust Neural Vocoder for High-Fidelity Waveform
  Generation in Multiple Domains</font>
    </a>
  </h2>
  <font color="black">これにより、大きなフットプリントモデルの高周波帯域での過度の平滑化の問題を軽減することにより、モデルがマルチスピーカーのリアルな波形を生成できるようになります。複数のドメインで忠実度の高い音声を合成するボコーダーであるUniversalMelGANを提案します。特に、話者、感情、言語の面で、目に見えない領域で優れたパフォーマンスを示しました。 
[概要]メルガンベースの構造は、地面に近い信号を生成します-真実のデータ。モデルはスペクトログラムスピーカーを使用して22を超えるスピーカーを生成しました。外部ドメイン情報なしで達成された結果は、ユニバーサルボコーダーとして提案されたモデルの可能性を強調しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Residual Local Feature Learning for Speech Emotion Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.AS/paper_3.html">
      <font color="black">Deep Residual Local Feature Learning for Speech Emotion Recognition</font>
    </a>
  </h2>
  <font color="black">新しい設計は、深部残余局所特徴学習ブロック（DeepResLFLB）と呼ばれます。DeepResLFLBは、LFLB、残余局所特徴学習ブロック（ResLFLB）、多層パーセプトロン（MLP）の3つのカスケードブロックで構成されます。ただし、深層学習の効率。層の数に依存します。つまり、層が深いほど効率が高くなります。 
[概要]新しい設計は、深残余局所特徴学習ブロック（deepreslflb）と呼ばれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: End-To-End Dilated Variational Autoencoder with Bottleneck
  Discriminative Loss for Sound Morphing -- A Preliminary Study -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.AS/paper_4.html">
      <font color="black">End-To-End Dilated Variational Autoencoder with Bottleneck
  Discriminative Loss for Sound Morphing -- A Preliminary Study</font>
    </a>
  </h2>
  <font color="black">次の損失関数を組み合わせます：1）入力信号を再構築するための時間領域平均二乗誤差、2）ボトルネック層の標準正規分布へのカルバックライブラー発散、および3）ボトルネックから計算された分類損失表現..パラメータ化とデータセットのこれらの結果は、DC-VAEデコーダがオーディオドメインから潜在空間にマッピングするときにトポロジをより適切に保持するため、DC-VAEがCC-VAEよりもサウンドモーフィングに適していることを示しています。サウンドモーフィングのためのエンドツーエンドの変分オートエンコーダ（VAE）に関する予備研究を提示します。 
[概要] 2つのmme-vaeバージョンは、拡張レイヤー（dc-vae）を備えたvaeと比較されます。vaeは2年以上にわたってcc-vaeを上回ります。たとえば、サウンドクラスはボトルネックレイヤーで分離します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-stage Speaker Extraction with Utterance and Frame-Level Reference
  Signals -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.AS/paper_5.html">
      <font color="black">Multi-stage Speaker Extraction with Utterance and Frame-Level Reference
  Signals</font>
    </a>
  </h2>
  <font color="black">さらに、信号融合スキームを提案して、複数のスケールでデコードされた信号を自動的に学習された重みと組み合わせます。さらに、初めて、フレームレベルの順次音声埋め込みをターゲットスピーカーの参照として使用します。スピーカー抽出では、ターゲットスピーカー抽出の参照信号として事前に録音された参照音声。 
[ABSTRACT] wsj0-2mixとそのノイズの多いバージョンの研究者は、spexが他の最先端のベースラインを一貫して上回っていることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-end anti-spoofing with RawNet2 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-20/eess.AS/paper_6.html">
      <font color="black">End-to-end anti-spoofing with RawNet2</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、RawNet2のスプーフィング防止への最初の適用について報告します。スプーフィング防止に適用できるように元のRawNet2アーキテクチャに加えられた変更について説明します。結果はオープンソースソフトウェアで再現可能です。 
[概要]最新のasvspoof2019評価の結果は、ほとんどの形態の攻撃を検出する大きな可能性を示しています。rawnet2は生の音声を取り込み、検出できない手がかりを学習する可能性があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-02">
        <br><font color="black">2020-11-02</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
