<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
<title>Akari-2019-12-25の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
  <div class="header-logo">
    <a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
  </div>
</header>
<input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">

<div class="menubar">
<span class="bar"></span>
<span class="bar"></span>
<span class="bar"></span>
</div>

<ul>
<li><a id="home" href="../../index.html">Home</a></li>
<li><a id="about" href="../../teamAkariとは.html">About</a></li>
<li><a id="contact" href="../../contact.html">Contact</a></li>
<li><a id="contact" href="../../list/newest.html">New Papers</a></li>
<li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
</ul>

</label>
<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Audio-Visual Embodied Navigation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-25/cs.SD/paper_0.html">
      Audio-Visual Embodied Navigation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      見ることと聞くことの両方によって、エージェントは音声ベースのターゲットへのナビゲートを学習する必要があります。新しいセンサーをサポートするために利用可能な3Dアセットと機器AI-Habitatを使用して、アパート、オフィス、ホテルの環境に任意の音源を挿入することができます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-24">
        <br>2019-12-24
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Audio-based automatic mating success prediction of giant pandas -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-25/cs.SD/paper_1.html">
      Audio-based automatic mating success prediction of giant pandas
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      次に、オーディオセグメントから音響特徴を抽出し、その特徴をディープニューラルネットワークに送り込み、交配を成功または失敗に分類します。提案されたディープニューラルネットワークでは、畳み込み層に続いて双方向ゲートリカレントユニットを使用してボーカル特徴を抽出し、適用します。ネットワークが最も関連性のある機能に集中するように強制する注意メカニズム
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-24">
        <br>2019-12-24
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving EEG based Continuous Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-25/cs.SD/paper_2.html">
      Improving EEG based Continuous Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      CTCモデルのエンコーダーでリカレントレイヤーの重みをランダムな重みではなく、より意味のある重みで初期化する手法を導入し、外部言語モデルを使用してデコード時のビーム検索を改善します。この論文では、脳波（EEG）特徴に基づく連続音声認識（CSR）システムのパフォーマンスを改善するためのさまざまな手法を紹介します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-24">
        <br>2019-11-24
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: FALCON 2.0: An Entity and Relation Linking framework over Wikidata -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-25/cs.CL/paper_0.html">
      FALCON 2.0: An Entity and Relation Linking framework over Wikidata
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      Falcon 2.0は、英語の形態の基本原則（N-GramタイリングやN-Gram分割など）と、学習した知識グラフから得られたラベルアラインメントの背景知識を使用して出力として返します。結果のエンティティと関係リソースは、DBpediaまたはWikidataナレッジグラフのいずれかです。Falcon2.0は公開されており、コミュニティで再利用できます。.Wikidataのみを使用してFalcon 2.0の影響を経験的に調査し、知識グラフに依存しないことを確認しましたつまり、Falcon 2.0のパフォーマンスと動作は、背景知識として使用される知識グラフの影響を受けません。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-24">
        <br>2019-12-24
      </time>
    </span>
  </h3>
</article>
<!-- paper0: KnowIT VQA: Answering Knowledge-Based Questions about Videos -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-25/cs.CL/paper_1.html">
      KnowIT VQA: Answering Knowledge-Based Questions about Videos
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このデータセットは、視覚的、テキスト的、時間的一貫性の推論と知識ベースの質問を組み合わせたものであり、シリーズの視聴から得られる経験が必要です。主な調査結果は次のとおりです。ビデオのVQA、および（ii）KnowIT VQAのパフォーマンスは、現在のビデオモデリングの制限を研究するための有用性を示す、人間の精度よりもまだ遅れています。ショーについて。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-23">
        <br>2019-10-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving Abstractive Text Summarization with History Aggregation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-25/cs.CL/paper_2.html">
      Improving Abstractive Text Summarization with History Aggregation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちのモデルは、履歴情報を確認してエンコーダーのメモリ容量を増やすことができます。経験的に、Transformerモデルに集約メカニズムを適用し、CNN / DailyMailデータセットで実験して、ROUGEメトリックのいくつかの強力なベースラインモデルと比較して、より高品質のサマリーを実現します。最近のニューラルシーケンスシーケンスモデルは、抽象的要約の実行可能なソリューションを提供しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-24">
        <br>2019-12-24
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Audio-Visual Embodied Navigation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-25/eess.AS/paper_0.html">
      Audio-Visual Embodied Navigation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      視覚と聴覚の両方で、エージェントは音声ベースのターゲットへのナビゲートを学習する必要があります。複雑で、音響的および視覚的に現実的な3D環境向けの視聴覚ナビゲーションを導入します。 。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-24">
        <br>2019-12-24
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Audio-based automatic mating success prediction of giant pandas -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-25/eess.AS/paper_1.html">
      Audio-based automatic mating success prediction of giant pandas
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      次に、オーディオセグメントから音響特性を抽出し、その特性をディープニューラルネットワークに送り込み、交配を成功または失敗に分類します。過去9年間に収集されたデータセットに対する評価実験は、有望な結果を得て、オーディオの可能性を証明しますジャイアントパンダの繁殖を支援するための自動ベースの交配成功予測方法。この論文は、彼らの声に基づいてジャイアントパンダの交尾成功を予測する自動方法を考案する最初の試みをする。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-24">
        <br>2019-12-24
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Continuous Speech Recognition using EEG and Video -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-25/eess.AS/paper_2.html">
      Continuous Speech Recognition using EEG and Video
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この論文では、脳波（EEG）機能を使用して連続視覚音声認識システムのパフォーマンスを改善できるかどうかを調査します。この結果は、EEG機能が連続視覚音声認識システムのパフォーマンス向上に役立つことを示しています。認識を実行するための時間分類（CTC）ベースのエンドツーエンド自動音声認識（ASR）モデル。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-16">
        <br>2019-12-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving EEG based Continuous Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-25/eess.AS/paper_3.html">
      Improving EEG based Continuous Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本論文では、EEG特徴から調音特徴を予測する問題を最終的に研究する。本論文では、脳波（EEG）特徴に基づく連続音声認識（CSR）システムの性能を改善するための様々な手法を紹介する。ランダムな重みではなく、より意味のある重みを持つCTCモデルのエンコーダーの再帰層の重み。外部言語モデルを使用して、デコード時のビーム検索を改善します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-24">
        <br>2019-11-24
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="">
      
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本日更新された論文はありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="">
        <br>
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
