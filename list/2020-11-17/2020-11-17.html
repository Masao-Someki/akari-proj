<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-11-17の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Environment Sound Classification using Multiple Feature Channels and
  Attention based Deep Convolutional Neural Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.SD/paper_0.html">
      <font color="black">Environment Sound Classification using Multiple Feature Channels and
  Attention based Deep Convolutional Neural Network</font>
    </a>
  </h2>
  <font color="black">ESC-10およびESC-50データセットの場合、提案されたモデルによって達成される精度は、人間の精度であるそれぞれ95.7％および81.3％を超えています。このような複数の機能は、これまで信号またはオーディオ処理に使用されたことはありません。チャネルと空間の注意を一緒に実行するモジュール。 
[概要]複数の機能チャネルには、mel-周波数ケプストラム度（mfcc）、ガンマトーン周波数ケプストラムエントリ（gfcc）、定数q-変換（cqt）、およびクロマトグラムが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-28">
        <br><font color="black">2019-08-28</font>
      </time>
    </span>
</section>
<!-- paper0: The SLT 2021 children speech recognition challenge: Open datasets, rules
  and baselines -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.SD/paper_1.html">
      <font color="black">The SLT 2021 children speech recognition challenge: Open datasets, rules
  and baselines</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、データセット、ルール、評価方法、およびベースラインを紹介します。これらの問題に対処するために、IEEE SLT 2021ワークショップの主力衛星イベントとしてChildrenSpeech Recognition Challenge（CSRC）を開始します。登録されたチームの約400時間のマンダリン音声データをリリースし、2つのチャレンジトラックを設定し、CSRパフォーマンスをベンチマークするための共通のテストベッドを提供します。 
[概要]子供の音声認識チャレンジ（csrc）は、ワイヤレスslt2021ワークショップの主力衛星イベントです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-13">
        <br><font color="black">2020-11-13</font>
      </time>
    </span>
</section>
<!-- paper0: Insertion-Based Modeling for End-to-End Automatic Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.SD/paper_2.html">
      <font color="black">Insertion-Based Modeling for End-to-End Automatic Speech Recognition</font>
    </a>
  </h2>
  <font color="black">この定式化は、非自己回帰方式で挿入ベースのトークン生成に依存するようにすることでCTCを強化します。エンドツーエンド（E2E）モデルは、自動音声認識（ASR）の研究分野で注目を集めています。挿入ベースのモデルとCTCの共同トレーニングの新しい定式化を紹介します。 
[概要]これまでに提案された多くのe2eモデルは、出力トークンシーケンスの左から右への自己回帰生成を想定しています。非左から右へのモデルの1つは、非自己回帰トランスフォーマー（nat）として知られており、ニューラル機械翻訳の領域</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-27">
        <br><font color="black">2020-05-27</font>
      </time>
    </span>
</section>
<!-- paper0: Conversational End-to-End TTS for Voice Agent -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.SD/paper_3.html">
      <font color="black">Conversational End-to-End TTS for Voice Agent</font>
    </a>
  </h2>
  <font color="black">次に、補助エンコーダーと会話コンテキストエンコーダーを備えた会話コンテキスト対応のエンドツーエンドTTSアプローチを提案し、会話内の現在の発話とそのコンテキストに関する情報も強化します。実験結果は、提案されたものがメソッドは、会話のコンテキストに応じてより自然なプロソディを生成し、発話レベルと会話レベルの両方で大幅な優先度の向上を実現します。まず、音声エージェント向けに設計された自発的な会話型音声コーパスを、両方の録音品質を保証する新しい録音スキームで構築します。会話スタイル。 
[ABSTRACT]ヨーク大学の研究者は、音声エージェント向けに設計された会話型音声コーパスを構築しています。提案された方法は、会話のコンテキストに応じてより自然な韻律を生成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-21">
        <br><font color="black">2020-05-21</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: AMPA-Net: Optimization-Inspired Attention Neural Network for Deep
  Compressed Sensing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.IV/paper_0.html">
      <font color="black">AMPA-Net: Optimization-Inspired Attention Neural Network for Deep
  Compressed Sensing</font>
    </a>
  </h2>
  <font color="black">私たちのコードはhttps://github.com/puallee/AMPA-Netで入手できます。AMP-Netは、近似メッセージパッシング（AMP）アルゴリズムとニューラルネットワークの融合を実現します。さらに、AMPA-Netを使用して提案します。 AMP-Netの表現能力を向上させる3つのアテンションネットワーク。 
[ABSTRACT] amp-netおよびampa-netは、4つの標準cs再構成ベンチマークデータセットにあります。これらは、amp-netまたはampa-netの有効性をテストするために使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-14">
        <br><font color="black">2020-10-14</font>
      </time>
    </span>
</section>
<!-- paper0: DANA: Dimension-Adaptive Neural Architecture for Multivariate Sensor
  Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.IV/paper_1.html">
      <font color="black">DANA: Dimension-Adaptive Neural Architecture for Multivariate Sensor
  Data</font>
    </a>
  </h2>
  <font color="black">DATは、フォワードパス中の次元のランダムな選択と、いくつかのバックワードパスの累積勾配による最適化で構成されます。ディープニューラルネットワーク（DNN）はセンサーデータ分類で競争力のある精度を実現しますが、現在のDNNアーキテクチャはセンサーの固定セットからのデータのみを処理します。固定サンプリングレートで、入力の次元の変更は、電力管理またはデータ共有の制御のために、かなりの精度の損失、不必要な計算、または操作の失敗を引き起こします。 
[ABSTRACT]モーションセンサー（dap）レイヤーを使用すると、dnnsはデータの変更に応答できます。dapとdatは、「ビジョン」を作成するように設定します。dapは、処理用のセンサーのdnnsを堅牢にするレイヤーです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-05">
        <br><font color="black">2020-08-05</font>
      </time>
    </span>
</section>
<!-- paper0: Anonymization of labeled TOF-MRA images for brain vessel segmentation
  using generative adversarial networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.IV/paper_2.html">
      <font color="black">Anonymization of labeled TOF-MRA images for brain vessel segmentation
  using generative adversarial networks</font>
    </a>
  </h2>
  <font color="black">これにより、医用画像におけるデータ不足と匿名化の課題を克服する道が開かれます。脳血管セグメンテーションを分析し、画像ラベル生成用の飛行時間型（TOF）磁気共鳴血管造影（MRA）パッチで3つのGANをトレーニングしました：1 ）深い畳み込みGAN、2）勾配ペナルティを伴うWasserstein-GAN（WGAN-GP）、および3）スペクトル正規化を伴うWGAN-GP（WGAN-GP-SN）。生成的敵対的ネットワーク（GAN）は、匿名画像を提供する可能性があります。予測特性の保持。 
[概要]事前トレーニングの有無にかかわらず、実際のデータでモデルのパフォーマンスを評価した患者数が増加しました。最大15人の患者で、合成データでモデルのパフォーマンスを評価しました。たとえば、gp-snは次のように示しています。血管を予測するための最高のパフォーマンス（dsc / 95hd 0 .82 / 28 .97）</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Constrained Nonnegative Matrix Factorization for Blind Hyperspectral
  Unmixing incorporating Endmember Independence -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.IV/paper_3.html">
      <font color="black">Constrained Nonnegative Matrix Factorization for Blind Hyperspectral
  Unmixing incorporating Endmember Independence</font>
    </a>
  </h2>
  <font color="black">過去数十年にわたって、これらの混合スペクトルを効果的に混合解除するために、従来の非負行列因子分解（NMF）フレームワークに補助制約を課すことに多くの試みが集中してきました。提案されたアルゴリズムは、特にハイパースペクトルデータからエンドメンバースペクトルを抽出するという点で優れたパフォーマンスを示します。 ;したがって、端成分スペクトルを存在量抽出の監視入力データとして利用する最近の深層学習HUメソッドのパフォーマンスを向上させる可能性があります。従来のNMFフレームワークにこの制約を課すと、独立した端成分の抽出が促進され、パーツベースの表現がさらに強化されます。データ。 
[ABSTRACT]ハイパースペクトル画像（hsi）は、独立したソースによって生成され、混合スペクトルとしてイメージング分光計のセンサー要素に到達する前に巨視的な程度で混合される可能性が高くなります。尖度ベースの滑らかな非負行列因子分解として知られる新しい方法、端成分スペクトルの確率密度関数に基づく新しい制約を組み込んでいます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-02">
        <br><font color="black">2020-03-02</font>
      </time>
    </span>
</section>
<!-- paper0: Training Strategies and Data Augmentations in CNN-based DeepFake Video
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.IV/paper_4.html">
      <font color="black">Training Strategies and Data Augmentations in CNN-based DeepFake Video
  Detection</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、同じデータセットまたは異なるデータセット間でトレーニングとテストを行う場合に、さまざまなトレーニング戦略とデータ拡張手法がCNNベースのディープフェイク検出器にどのように影響するかを分析します。ディープフェイクビデオの数と品質の急速かつ継続的な増加には、ソーシャルメディアやインターネット上のユーザーに、そのようなコンテンツの潜在的な不正について自動的に警告できる信頼性の高い検出システム。アルゴリズム、ソフトウェア、スマートフォンアプリは、操作されたビデオの生成と顔の交換において日々向上していますが、自動システムの精度はビデオでの顔の偽造検出はまだかなり制限されており、特定の検出システムの設計とトレーニングに使用されるデータセットに一般的に偏っています。 
[概要]ビデオでの顔の偽造検出のための自動システムの精度はまだかなり制限されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Mode Penalty Generative Adversarial Network with adapted Auto-encoder -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.IV/paper_5.html">
      <font color="black">Mode Penalty Generative Adversarial Network with adapted Auto-encoder</font>
    </a>
  </h2>
  <font color="black">この問題に対処するために、エンコードされた空間で生成されたデータサンプルと実際のデータサンプルを明示的に表現するために、事前にトレーニングされたオートエンコーダと組み合わせたモードペナルティGANを提案します。この空間では、全体を見つけることによって実際の多様体に従うように生成多様体を作成します。ターゲット分布のモード..提案された方法をGANに適用すると、実験的評価を通じて、ジェネレータの最適化がより安定し、収束が速くなるのに役立つことを示します。 
[ABSTRACT] ganのジェネレータネットワークは、候補生成サンプルを使用した分類から実際のデータセットの暗黙的な分布を学習します。これは、エンコードされた空間での明示的な表現のために、事前にトレーニングされた自動エンコーダと組み合わせたモードペナルティganを示唆しています。全体的なターゲット分布を見つけることを奨励するジェネレータにターゲット分布の</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Fast and Robust Cascade Model for Multiple Degradation Single Image
  Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.IV/paper_6.html">
      <font color="black">Fast and Robust Cascade Model for Multiple Degradation Single Image
  Super-Resolution</font>
    </a>
  </h2>
  <font color="black">SISRの3つの最先端（SOTA）データセットでモデルをチェックし、結果をSOTAモデルと比較します。このアプローチは非ブラインドであり、ブラーカーネルの推定が必要ですが、ブラーカーネル推定エラーに対するロバスト性を示しています。 、ブラインドモデルの優れた代替手段になります。新しい密に接続されたCNNアーキテクチャが提案され、各サブモジュールの出力は、特定のタスクに集中するために外部の知識を使用して制限されます。 
[概要] cnnアーキテクチャのネットワークが提案され、サブモジュールの知識が特定のタスクに集中するように制限されています。最高のモデルに合わせるために、最後のサブモジュールが前のサブモジュールによって伝播された残留エラーを処理します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Robust building footprint extraction from big multi-sensor data using
  deep competition network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.IV/paper_7.html">
      <font color="black">Robust building footprint extraction from big multi-sensor data using
  deep competition network</font>
    </a>
  </h2>
  <font color="black">この研究では、非常に高い空間分解能の光学リモートセンシング画像とLiDARデータを融合して堅牢なBFEを実現するディープコンペティションネットワーク（DCN）を開発および評価します。DCNは、分類されたエンコーダベクトル量子化を使用したディープスーパーピクセル単位の畳み込みエンコーダ-デコーダアーキテクチャです。構造..精度評価の比較結果は、DCNが他のディープセマンティックバイナリセグメンテーションアーキテクチャと比較して競争力のあるBFEパフォーマンスを持っていることを示しました。 
[ABSTRACT] dcnは、複数の建物シーンを持つ米国のインディアナ州から取得した大きなマルチセンサーデータセットでトレーニングおよびテストされています。提案されたモデルは、大きなマルチセンサー評価からの堅牢なbfeに対する適切なソリューションです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Deep learning in magnetic resonance prostate segmentation: A review and
  a new perspective -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.IV/paper_8.html">
      <font color="black">Deep learning in magnetic resonance prostate segmentation: A review and
  a new perspective</font>
    </a>
  </h2>
  <font color="black">したがって、前立腺セグメンテーションの分野を探索し、一般化可能な解決策を発見するために、MR前立腺セグメンテーションにおける最先端の深層学習アルゴリズムを確認します。それらの制限と長所を議論することにより、フィールドに洞察を提供します。そして、MR前立腺セグメンテーション用に最適化された2D U-Netを提案します。ただし、MRIデータから前立腺を正確に描写するために必要な時間は、時間のかかるプロセスです。深部学習は、正確な前立腺セグメンテーションが癌の検出と治療に役立つ前立腺癌。 
[要約] mriデータから前立腺を描写するのに必要な時間は時間のかかるプロセスです。ただし、取得プロトコルが異なり、公開されているデータセットが限られているため、潜在的なモデルが制限される可能性があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Natural Disaster Classification using Aerial Photography Explainable for
  Typhoon Damaged Feature -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.IV/paper_9.html">
      <font color="black">Natural Disaster Classification using Aerial Photography Explainable for
  Typhoon Damaged Feature</font>
    </a>
  </h2>
  <font color="black">台風後の千葉地域で記録された航空写真に適用された事例研究を示します。近年、気候変動により台風被害が社会問題になっています。本論文では、台風災害の特徴に焦点を当てた被害地域を視覚化する実用的な方法を提案します。航空写真。 【概要】2019年9月9日、令和元年房が日本の千葉を通過しました。復旧までに18日かかりました。この方法では、被害のない土地と被災地を含む8つのクラスに分類できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br><font color="black">2020-04-21</font>
      </time>
    </span>
</section>
<!-- paper0: FRDet: Balanced and Lightweight Object Detector based on Fire-Residual
  Modules for Embedded Processor of Autonomous Driving -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.IV/paper_10.html">
      <font color="black">FRDet: Balanced and Lightweight Object Detector based on Fire-Residual
  Modules for Embedded Processor of Autonomous Driving</font>
    </a>
  </h2>
  <font color="black">したがって、提案されたFRDetは、自動運転での実用化のためのバランスのとれた効率的な物体検出器であり、精度、リアルタイム推論、および軽量モデルサイズのすべての基準を満たすことができます。さらに、リアルタイム検出速度は31.3に達しました。組み込みGPUボード（NVIDIA Xavier）上のFPS。当社のネットワークは、YOLOv3レベルの精度を達成または上回る一方で、モデルの圧縮を最大化することを目的としています。 
[ABSTRACT]提案されたfrdetは、自動運転用の軽量の1ステージオブジェクト検出器です。これは、組み込みGPUプロセッサでの精度、モデルサイズ、およびリアルタイム処理のすべての制約を満たすことを目的としています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Improving distribution and flexible quantization for DCT coefficients -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.IV/paper_11.html">
      <font color="black">Improving distribution and flexible quantization for DCT coefficients</font>
    </a>
  </h2>
  <font color="black">両方を最適化すると、ここでは自動化されたテール処理により、ほぼ均一な量子化が得られます。歪みのみに対して$ q $を最適化すると、大幅な改善が得られますが、分布がより均一になるため、エントロピーが増加します。 JPEG画像圧縮のDCT-IIのようなフーリエ関連変換のAC係数は、ラプラス分布からのものであり、より一般的なEPD（指数パワー分布）$ \ rho \ sim \ exp（-（| x- \ mu | /）がテストされました。 \ sigma）^ {\ kappa}）$ファミリー、最尤推定（MLE）につながるラプラス分布$ \ kappa = 1 $の代わりに$ \ kappa \約0.5 $-このような置換により、$ \ upperx 0.1 $ビット/値の平均節約（グレースケールの場合はピクセルあたり、RGBの場合は最大$ 3 \ times $）。 
[概要] dctカクテルの分布の予測についても説明します。これらは、dctブロックのすでにデコエグされた領域からのものです。これらの要因には、最適化された連続およびmu mu muemphが含まれます。これらには、最適化された「不均一」量子化が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video
  Captioning and Video Question Answering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.IV/paper_12.html">
      <font color="black">iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video
  Captioning and Video Question Answering</font>
    </a>
  </h2>
  <font color="black">さらに、DVCおよびVideoQAでのこれまでの作業のほとんどは視覚情報のみに依存していますが、音声や音声などの他のモダリティは、人間の観察者が環境を認識するために不可欠です。ActivityNetキャプションでiPerceiveDVCおよびiPerceiveVideoQAのパフォーマンスを評価することによりTVQAデータセットはそれぞれ、私たちのアプローチが最先端技術を促進することを示しています。私たちを人間として定義し、機械とは根本的に異なるものの一部は、関連の背後にある因果関係を追求するという本能です。イベントXの直接の結果。
[要約]ビデオキャプションとビデオ質問回答（e evc）.videoを使用して、私たちの手法の有効性を実証しました。 。 videoqaタスクはvideog.comとvideoqaを使用して作成されました。videog-trackingテクニックは人間のためのツールとして使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Multiclass Yeast Segmentation in Microstructured Environments with Deep
  Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.IV/paper_13.html">
      <font color="black">Multiclass Yeast Segmentation in Microstructured Environments with Deep
  Learning</font>
    </a>
  </h2>
  <font color="black">モデルは堅牢なセグメンテーション結果を達成し、精度と速度の両方で以前の最先端技術を上回ります。微細構造環境の設定では課題が悪化します。微細構造環境で酵母をセグメント化する方法の貢献を典型的な方法で紹介します。合成生物学のアプリケーションを念頭に置いてください。 
[概要]微細構造環境の設定で課題が激化しています。今日、個々の酵母細胞のマルチクラスセグメンテーションのためにトレーニングされた畳み込みニューラルネットワークを紹介します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Fast Uncertainty Quantification for Deep Object Pose Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_0.html">
      <font color="black">Fast Uncertainty Quantification for Deep Object Pose Estimation</font>
    </a>
  </h2>
  <font color="black">学習済みメトリックを含む4つの不一致メトリックを提案し、平均距離（ADD）が学習なしの最良のメトリックであり、ラベル付きターゲットデータを必要とする学習済みメトリックよりもわずかに悪いことを示します。提案されたUQを評価します。不確実性の定量化により、ベースラインよりもポーズ推定誤差との相関がはるかに強くなる3つのタスクの方法。この方法には、従来の技術と比較していくつかの利点があります。1）この方法ではトレーニングプロセスやモデル入力を変更する必要がありません。 2）モデルごとに1つのフォワードパスのみが必要です。 
[ABSTRACT]ポーズ推定器の効率的でロバストな不確実性の定量化（uq）は、多くのロボットタスクで非常に必要です。異なるニューラルネットワークアーキテクチャとトレーニングデータソースを使用して2〜3の事前トレーニング済みモデルをアンサンブルし、それらの平均ペアワイズ不一致を計算します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: AMPA-Net: Optimization-Inspired Attention Neural Network for Deep
  Compressed Sensing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_1.html">
      <font color="black">AMPA-Net: Optimization-Inspired Attention Neural Network for Deep
  Compressed Sensing</font>
    </a>
  </h2>
  <font color="black">私たちのコードはhttps://github.com/puallee/AMPA-Netで入手できます。最後に、4つの標準CS再構築ベンチマークデータセットに対するAMP-NetとAMPA-Netの有効性を示します。圧縮センシング（CS）は限られた測定からほぼ完全な画像を再構成するため、画像処理における困難な問題。 
[ABSTRACT] amp-netおよびampa-netは、4つの標準cs再構成ベンチマークデータセットにあります。これらは、amp-netまたはampa-netの有効性をテストするために使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-14">
        <br><font color="black">2020-10-14</font>
      </time>
    </span>
</section>
<!-- paper0: A Primer on Large Intelligent Surface (LIS) for Wireless Sensing in an
  Industrial Setting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_2.html">
      <font color="black">A Primer on Large Intelligent Surface (LIS) for Wireless Sensing in an
  Industrial Setting</font>
    </a>
  </h2>
  <font color="black">これは、屋内環境では、感知された現象の近くに配置できる一方で、高解像度は、広い領域に配置された密集した小さなアンテナによって提供されるためです。高スループットとワイヤレスリンクの効率的な多重化の可能性に加えて、LISは、伝搬環境の高解像度レンダリングを提供できます。産業用ロボットが事前定義されたルートから逸脱しているかどうかを検出する必要があるシナリオで、これらのメソッドをテストします。 
[概要]このペーパーでは、通信の可能性について説明します。大規模なintelligentxing.lisのセンシング統合は、センシングされた現象の近くに配置できますが、高解像度は、広いエリアのテレックスによって提供されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-11">
        <br><font color="black">2020-06-11</font>
      </time>
    </span>
</section>
<!-- paper0: Relative Drone -- Ground Vehicle Localization using LiDAR and Fisheye
  Cameras through Direct and Indirect Observations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_3.html">
      <font color="black">Relative Drone -- Ground Vehicle Localization using LiDAR and Fisheye
  Cameras through Direct and Indirect Observations</font>
    </a>
  </h2>
  <font color="black">私たちの方法は完全に自動化されています。LiDARを使用したドローンの検出と追跡のための動的適応カーネルベースの方法を提案します。この作業では、ドローンと地上車両の間のLiDARカメラベースの相対姿勢推定方法を提示します。 LiDARセンサーと車両の屋根にあるフィッシュアイカメラ、およびドローンの下に取り付けられた別のフィッシュアイカメラを使用します。 
[概要]ライダーセンサーがドローンを直接観測し、その位置を測定します。2台のカメラが周囲の物体を間接的に観測して物体を推定します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-13">
        <br><font color="black">2020-11-13</font>
      </time>
    </span>
</section>
<!-- paper0: MP-ResNet: Multi-path Residual Network for the Semantic segmentation of
  High-Resolution PolSAR Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_4.html">
      <font color="black">MP-ResNet: Multi-path Residual Network for the Semantic segmentation of
  High-Resolution PolSAR Images</font>
    </a>
  </h2>
  <font color="black">コードはhttps://github.com/ggsDing/SARSegで入手できます。トレーニングデータが不足しているため、高解像度の偏光合成開口レーダー（PolSAR）画像のセマンティックセグメンテーションに関する研究は限られています。スペックルノイズ..また、全体的な精度（OA）、平均F1およびfwIoUの点で、いくつかの従来の最先端の方法を上回っていますが、計算コストはそれほど増加していません。 
[概要]高分コンテストは、高品質のpolsarセマンティックセグメンテーションデータセットへのオープンアクセスを提供しました。mp--resnetは、並列マルチネットブランチを使用してセマンティックコンテキストを学習します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-10">
        <br><font color="black">2020-11-10</font>
      </time>
    </span>
</section>
<!-- paper0: Drone LAMS: A Drone-based Face Detection Dataset with Large Angles and
  Many Scenarios -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_5.html">
      <font color="black">Drone LAMS: A Drone-based Face Detection Dataset with Large Angles and
  Many Scenarios</font>
    </a>
  </h2>
  <font color="black">重複率、注釈方法などの重要な要素がデータセットのパフォーマンスにどのように影響するかについての詳細な分析も提供され、顔検出でのドローンのさらなる使用が容易になりました。ドローンLAMSは、現在利用可能なドローンベースの顔検出データセットよりも大幅に改善されました。特にピッチとヨー角が大きい場合の検出性能の観点から..この作業では、大きな角度などのシナリオでのドローンベースの顔検出のパフォーマンス低下の問題を解決するために、新しいドローンベースの顔検出データセットDroneLAMSを提示しました。ドローンが高く飛ぶときの主な作業条件。 
[ABSTRACT]提案されたデータセットは、43,000を超える注釈と4.0kのピッチまたはヨー角の画像を含む261のシナリオシナリオから画像をキャプチャしました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Incorporating Learnable Membrane Time Constant to Enhance Learning of
  Spiking Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_6.html">
      <font color="black">Incorporating Learnable Membrane Time Constant to Enhance Learning of
  Spiking Neural Networks</font>
    </a>
  </h2>
  <font color="black">この論文では、膜関連パラメータが脳領域間で異なるという観察からインスピレーションを得て、シナプスの重みだけでなく、SNNの膜時定数も学習できるトレーニングアルゴリズムを提案します。学習可能な膜時定数により、ネットワークの初期値に対する感度が低下し、学習が高速化されます。従来の静的MNIST、Fashion-MNIST、CIFAR-10データセット、およびニューロモーフィックN-MNISTの両方で、画像分類タスクの提案された方法を評価します。 CIFAR10-DVS、DVS128ジェスチャデータセット。 
[ABSTRACT]ニューヨークを拠点とする研究では、snnの注目度の高い学習アルゴリズムは依然として困難であることが示されていますが、脳のニューロンの信頼性は依然として課題です。提案された方法に加えて、新しい方法を開発するために使用できます。システム</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-11">
        <br><font color="black">2020-07-11</font>
      </time>
    </span>
</section>
<!-- paper0: GCNNMatch: Graph Convolutional Neural Networks for Multi-Object Tracking
  via Sinkhorn Normalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_7.html">
      <font color="black">GCNNMatch: Graph Convolutional Neural Networks for Multi-Object Tracking
  via Sinkhorn Normalization</font>
    </a>
  </h2>
  <font color="black">ネットワークは、MOTタスクに固有の制約を考慮して、オブジェクトの関連付けを予測するようにトレーニングされています。実験結果は、最先端のオンラインアプローチの中でMOT17チャレンジで最高のパフォーマンスを達成する上で提案されたアプローチの有効性を示しています。提案されたフレームワークの中心的な革新は、モデルトレーニング中にオブジェクト間の関連付けをエンドツーエンドで学習するためのSinkhornアルゴリズムの使用です。 
[概要]新しい方法では、グラフベースのシステムを使用して学習することを提案しています。この概念は、外観やcomなどの機能の開発にも使用できます。また、エンドツーエンドの学習にシンクホーンアルゴリズムを使用することもできます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Does Visual Self-Supervision Improve Learning of Speech Representations
  for Emotion Recognition? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_8.html">
      <font color="black">Does Visual Self-Supervision Improve Learning of Speech Representations
  for Emotion Recognition?</font>
    </a>
  </h2>
  <font color="black">自己教師あり学習は、最近多くの研究関心を集めています。この作業（1）は、音声表現の学習をガイドするために、顔の再構成による視覚的な自己教師ありを調査します。 （2）音声表現学習のための音声のみの自己監視アプローチを提案します。 （3）提案された視覚と音声の自己監視のマルチタスクの組み合わせは、ノイズの多い条件でより堅牢なより豊富な機能を学習するのに有益であることを示しています。 （4）は、自己教師あり事前トレーニングが完全教師ありトレーニングよりも優れており、小さいサイズのデータセットでの過剰適合を防ぐのに特に役立つことを示しています。ただし、音声の自己教師ありのほとんどの作業は通常単峰性であり、相互作用を研究する作業は限られています。クロスモーダル自己監視のためのオーディオモダリティとビジュアルモダリティの間。 
[概要]これは、音声の自己制御学習が不足しているためです。これらは自己に対して機能します。音声の監視は通常、単峰性です。これは、視覚と音声の共同自己監視が、音声と感情の認識のためのより有益な音声表現につながることを示唆しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-04">
        <br><font color="black">2020-05-04</font>
      </time>
    </span>
</section>
<!-- paper0: Application of Computer Vision Techniques for Segregation of
  PlasticWaste based on Resin Identification Code -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_9.html">
      <font color="black">Application of Computer Vision Techniques for Segregation of
  PlasticWaste based on Resin Identification Code</font>
    </a>
  </h2>
  <font color="black">最初のケースでは、シャム損失ネットワークとトリプレット損失ネットワークを使用したワンショット学習手法の使用を提案します。提案されたアプローチでは、データベースのサイズを増やすために拡張を必要とせず、99.74％の高精度を達成しました。使用済みプラスチック廃棄物の効率的なリサイクルを提供するために、樹脂識別コードに基づいてプラスチック廃棄物を識別する方法を示します。 
[概要]システムのトレーニング時に、プラスチック廃棄物の既知のカテゴリに属するプラスチック廃棄物を特定するためのさまざまな機械学習手法の設計、トレーニング、テストを提案します。提案されたアプローチでは、サイズを大きくするための拡張は必要ありません。データベースと99の高精度を達成しました。74％</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Zero Cost Improvements for General Object Detection Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_10.html">
      <font color="black">Zero Cost Improvements for General Object Detection Network</font>
    </a>
  </h2>
  <font color="black">SA-FPNモジュールと呼ばれる、より少ないパラメータでマルチレベルの特徴マップを効率的に融合するために、スケールアテンションメカニズムを採用しています。分類ヘッドと回帰ヘッドの相関を考慮して、広く使用されている並列の代わりにシーケンシャルヘッドを使用します。 Seq-HEADモジュールと呼ばれるhead ..現代の物体検出ネットワークは、一般的な物体検出データセットでより高い精度を追求すると同時に、精度の向上とともに計算負荷も増加しています。 
[概要]ゼロコストで検出精度を向上させるために2つのモジュールが提案されています。これはcocoデータセットの研究者の仕事です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Online Monitoring of Object Detection Performance Post-Deployment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_11.html">
      <font color="black">Online Monitoring of Object Detection Performance Post-Deployment</font>
    </a>
  </h2>
  <font color="black">提案されたカスケードネットワークは、オブジェクト検出器のディープニューラルネットワークの内部機能を活用します。この問題に対処し、オブジェクト検出器の平均精度（mAP）の品質を予測することによってオブジェクト検出器のパフォーマンスを監視するカスケードニューラルネットワークを導入します。入力フレームのスライディングウィンドウ..自律駆動データセットとオブジェクト検出器のさまざまな組み合わせを使用して、提案されたアプローチを評価します。 
[概要]オブジェクト検出器のパフォーマンスは変動する可能性があり、警告なしに場合によっては大幅に低下する可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Constrained Nonnegative Matrix Factorization for Blind Hyperspectral
  Unmixing incorporating Endmember Independence -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_12.html">
      <font color="black">Constrained Nonnegative Matrix Factorization for Blind Hyperspectral
  Unmixing incorporating Endmember Independence</font>
    </a>
  </h2>
  <font color="black">提案されたアルゴリズムは、特にハイパースペクトルデータから端成分スペクトルを抽出するという点で優れたパフォーマンスを示します。したがって、端成分スペクトルを存在量抽出の監視入力データとして利用する最近の深層学習HUメソッドのパフォーマンスを向上させる可能性があります。端成分を抽出するための最適な制約を見つけるための有望なステップとして、このペーパーでは、参照される新しいブラインドHUアルゴリズムを紹介します。端成分スペクトルの確率密度関数の統計的独立性に基づく新しい制約を組み込んだKurtosisベースのSmoothNonnegative Matrix Factorization（KbSNMF）として。過去数十年にわたって、多くの試みが従来の非負に補助制約を課すことに焦点を当ててきました。これらの混合スペクトルを効果的に混合解除するためのマトリックス因数分解（NMF）フレームワーク。 
[ABSTRACT]ハイパースペクトル画像（hsi）は、独立したソースによって生成され、混合スペクトルとしてイメージング分光計のセンサー要素に到達する前に巨視的な程度で混合される可能性が高くなります。尖度ベースの滑らかな非負行列因子分解として知られる新しい方法、端成分スペクトルの確率密度関数に基づく新しい制約を組み込んでいます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-02">
        <br><font color="black">2020-03-02</font>
      </time>
    </span>
</section>
<!-- paper0: Monitoring and Diagnosability of Perception Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_13.html">
      <font color="black">Monitoring and Diagnosability of Perception Systems</font>
    </a>
  </h2>
  <font color="black">LGSVL自動運転シミュレーターとApolloAuto自律ソフトウェアスタックを使用した現実的なシミュレーションで、PerSySと呼ばれる監視システムを示し、PerSySが困難なシナリオ（自動運転車の事故を引き起こしたシナリオを含む）で障害を検出できることを示します近年）、最小限の計算オーバーヘッド（シングルコアCPUで5ms未満）を伴う一方で、障害を正しく特定することができます。知覚は、ロボット工学や自動運転システムなどの自律システムの高整合性アプリケーションの重要なコンポーネントです。自動運転車..この目標に向けて、マルチプロセッサシステムの診断可能性に関する文献との関連性を引き出し、時間の経過とともに相互作用する異種出力のモジュールを説明するために一般化します。 
[ABSTRACT]システムシステムシステムは、モニターと障害検出の数学モデルです。認識モジュールの一貫性を説明するためのフレームワークを提供します。これにより、障害識別のアルゴリズムが改善される可能性があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-11">
        <br><font color="black">2020-11-11</font>
      </time>
    </span>
</section>
<!-- paper0: A Large-Scale Database for Graph Representation Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_14.html">
      <font color="black">A Large-Scale Database for Graph Representation Learning</font>
    </a>
  </h2>
  <font color="black">MalNetの前例のない規模と多様性は、グラフ表現学習のフロンティアを前進させる刺激的な機会を提供します---不均衡な分類、説明可能性、およびクラスの硬さの影響に関する新しい発見と研究を可能にします。MalNetには120万を超えるグラフが含まれ、平均で17kノードを超えます。 47タイプと696ファミリの階層全体で、グラフあたり39kのエッジ。既存のグラフデータベースを注意深く分析することにより、グラフ表現学習の分野を前進させるために重要な3つの重要なコンポーネントを特定します。（1）大きなグラフ、（2）多くのグラフ、および（3）クラスの多様性。 
[概要]これまでに構築された最大のパブリックグラフデータベースであるmalnetを紹介します。これは、ソフトウェア関数呼び出しグラフの大規模データベースを表します。malnetは、105倍のグラフ、平均44倍のネットワーク、63倍のクラスを提供します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Place Recognition in Forests with Urquhart Tessellations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_15.html">
      <font color="black">Place Recognition in Forests with Urquhart Tessellations</font>
    </a>
  </h2>
  <font color="black">この手紙では、森林内の木の位置から導出されたウルクハートテッセレーションに基づく新しい記述子を提示します。シミュレーションでループ閉鎖検出実験を実行し、無人航空機（UAV）のさまざまなフライトから実際のデータマップをマージします。 ）松林で、私たちの方法が精度と堅牢性において最先端のアプローチよりも優れていることを示します。部分的なオーバーラップとノイズがある場合でも、これらの記述子を使用して以前に見た観測とランドマークの対応を検出するフレームワークを提案します。 
[ABSTRACT]記述子を使用してデータを検出するフレームワークを提案します。理論をテストするための概念を提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Adaptive Future Frame Prediction with Ensemble Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_16.html">
      <font color="black">Adaptive Future Frame Prediction with Ensemble Network</font>
    </a>
  </h2>
  <font color="black">提案された適応更新フレームワークは、事前訓練された予測ネットワーク、連続更新予測ネットワーク、および重み推定ネットワークで構成されています。学習ベースの将来のフレーム予測アプローチが、さまざまな文献で提案されています。次に、適応を提案します。将来のフレーム予測タスクのフレームワークを更新します。 
[ABSTRACT]将来のフレーム予測アプローチの誤用がさまざまな文献で提案されています。既存のテストシステムと機能前モデルにより、既存の最先端のアプローチに対するパフォーマンスが実現されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-13">
        <br><font color="black">2020-11-13</font>
      </time>
    </span>
</section>
<!-- paper0: BiTraP: Bi-directional Pedestrian Trajectory Prediction with Multi-modal
  Goal Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_17.html">
      <font color="black">BiTraP: Bi-directional Pedestrian Trajectory Prediction with Multi-modal
  Goal Estimation</font>
    </a>
  </h2>
  <font color="black">BiTraPは、軌道の目標（エンドポイント）を推定し、新しい双方向デコーダーを導入して、長期的な軌道予測の精度を向上させます。このペーパーでは、目標条件付き双方向マルチモーダル軌道予測方法であるBiTraPを紹介します。 CVAE ..これらの結果は、衝突回避やナビゲーションシステムなどのロボットアプリケーションの軌道予測子の設計に関するガイダンスを提供します。 
[ABSTRACT]ロボットは、リカレントニューラルネットワーク（rnns）を備えた条件付き変分オートエンコーダー（cvae）を使用して、観測された軌道をエンコードし、マルチモーダル未来軌道をデコードします。bitrap、目標-条件付きbi-条件付きマルチモーダル軌道予測方法、bitrap（fpv）に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-29">
        <br><font color="black">2020-07-29</font>
      </time>
    </span>
</section>
<!-- paper0: Per-frame mAP Prediction for Continuous Performance Monitoring of Object
  Detection During Deployment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_18.html">
      <font color="black">Per-frame mAP Prediction for Continuous Performance Monitoring of Object
  Detection During Deployment</font>
    </a>
  </h2>
  <font color="black">これを行うには、検出器の内部機能を使用して、フレームごとの平均平均精度が臨界しきい値を下回る時期を予測します。アラームを発生させて不在にすることで誤った決定を行うことと引き換えに、リスクを軽減する方法の能力を定量的に評価および実証します。検出..実際には、この仮定は成り立たず、パフォーマンスは展開条件の関数として変動します。 
[概要]現在、オブジェクト検出器は、将来のすべての展開条件を表すと想定される単一のデータセットに基づく要約メトリックを使用して評価されます。この問題に対処するために、グラウンドトゥルースを必要とせずに展開中のパフォーマンス監視へのイントロスペクションアプローチを提案します。データ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-18">
        <br><font color="black">2020-09-18</font>
      </time>
    </span>
</section>
<!-- paper0: Mode Penalty Generative Adversarial Network with adapted Auto-encoder -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_19.html">
      <font color="black">Mode Penalty Generative Adversarial Network with adapted Auto-encoder</font>
    </a>
  </h2>
  <font color="black">この空間では、ターゲット分布のモード全体を見つけることにより、実際の多様体に従うジェネレータ多様体を作成します。この問題に対処するために、生成された実際のデータサンプルを明示的に表現するための事前トレーニング済みオートエンコーダと組み合わせたモードペナルティGANを提案します。提案された方法をGANに適用すると、実験的評価を通じて、ジェネレータの最適化がより安定し、収束が速くなるのに役立つことを示します。 
[ABSTRACT] ganのジェネレータネットワークは、候補生成サンプルを使用した分類から実際のデータセットの暗黙的な分布を学習します。これは、エンコードされた空間での明示的な表現のために、事前にトレーニングされた自動エンコーダと組み合わせたモードペナルティganを示唆しています。全体的なターゲット分布を見つけることを奨励するジェネレータにターゲット分布の</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Ensemble of Models Trained by Key-based Transformed Images for
  Adversarially Robust Defense Against Black-box Attacks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_20.html">
      <font color="black">Ensemble of Models Trained by Key-based Transformed Images for
  Adversarially Robust Defense Against Black-box Attacks</font>
    </a>
  </h2>
  <font color="black">提案されたアンサンブルでは、さまざまなキーとブロックサイズで変換された画像を使用して多数のモデルがトレーニングされ、投票アンサンブルがモデルに適用されます。画像分類実験では、提案された防御が状態を防御するために示されます。最先端の攻撃..したがって、モデルの投票アンサンブルを使用して、ブラックボックス攻撃に対する堅牢性を強化することを目指しています。 
[ABSTRACT]キーベースの敵対的防御は、特定の特定のタイプの攻撃に対して優れていることが実証されました。ただし、キーベースの敵対的防御は、モデルの投票アンサンブルを使用することで堅牢性を強化することができました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: A Review of Uncertainty Quantification in Deep Learning: Techniques,
  Applications and Challenges -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_21.html">
      <font color="black">A Review of Uncertainty Quantification in Deep Learning: Techniques,
  Applications and Challenges</font>
    </a>
  </h2>
  <font color="black">次に、UQメソッドのいくつかの重要なアプリケーションの概要を説明します。さらに、強化学習（RL）におけるこれらのメソッドのアプリケーションについても調査します。この研究では、深層学習で使用されるUQメソッドの最近の進歩を確認します。 
[ABSTRACT] uqメソッドは、科学と工学におけるさまざまな実世界のアプリケーションを解決するために使用できます。研究者は、さまざまなuqメソッドを提案し、そのパフォーマンスを調査しました。また、強化された学習に加えて、これらのメソッドのアプリケーションを調査します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-12">
        <br><font color="black">2020-11-12</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-view Sensor Fusion by Integrating Model-based Estimation and Graph
  Learning for Collaborative Object Localization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_22.html">
      <font color="black">Multi-view Sensor Fusion by Integrating Model-based Estimation and Graph
  Learning for Collaborative Object Localization</font>
    </a>
  </h2>
  <font color="black">私たちのアプローチは、新しい時空間グラフ表現を使用して複雑なオブジェクトの関係をモデル化し、ベイズ方式でマルチビュー観測を融合して、不確実性の下での位置推定を改善します。協調オブジェクトローカリゼーションは、複数のビューまたは視点から観測されたオブジェクトの位置を協調的に推定することを目的としています。接続された車両などのマルチエージェントシステムの重要な能力..接続された自律運転と複数の歩行者の位置特定のアプリケーションにおけるアプローチを評価します。 
[概要]これらのモデルベースのモデルベースのモデルベースのツールの使用が開発されました。これらには、学習ベースの複数の歩行者のローカリゼーションと複数の歩行者の位置が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Fast and Robust Cascade Model for Multiple Degradation Single Image
  Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_23.html">
      <font color="black">Fast and Robust Cascade Model for Multiple Degradation Single Image
  Super-Resolution</font>
    </a>
  </h2>
  <font color="black">新しい密に接続されたCNNアーキテクチャが提案され、各サブモジュールの出力は、特定のタスクに焦点を当てるために外部の知識を使用して制限されます。SISRの3つの最先端（SOTA）データセットでモデルをチェックし、比較します。 SOTAモデルでの結果..さらに、私たちのモデルは、変形の標準セットに対する現在のすべてのSOTAメソッドを克服します。 
[概要] cnnアーキテクチャのネットワークが提案され、サブモジュールの知識が特定のタスクに集中するように制限されています。最高のモデルに合わせるために、最後のサブモジュールが前のサブモジュールによって伝播された残留エラーを処理します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: DSIC: Dynamic Sample-Individualized Connector for Multi-Scale Object
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_24.html">
      <font color="black">DSIC: Dynamic Sample-Individualized Connector for Multi-Scale Object
  Detection</font>
    </a>
  </h2>
  <font color="black">さらに、これら2つのコンポーネントは両方ともプラグアンドプレイであり、任意のバックボーンに埋め込むことができます。これらは調整できず、各種類のデータと互換性がありません。ISGは、機能統合の入力としてバックボーンからマルチレベル機能を適応的に抽出します。 
[ABSTRACT]統合マルチレベル機能は、問題を軽減するために提示されます。これらのモデルは、さまざまなサンプルが供給されると、固定アーキテクチャとデータフローパスを持ちます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Going in circles is the way forward: the role of recurrence in visual
  inference -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_25.html">
      <font color="black">Going in circles is the way forward: the role of recurrence in visual
  inference</font>
    </a>
  </h2>
  <font color="black">有限時間リカレントニューラルネットワーク（RNN）は、時間とともに展開して同等のフィードフォワードニューラルネットワーク（FNN）を生成できます。これとは逆に、FNNはRNNの特殊なケースであり、計算神経科学者とエンジニアは脳と機械がどのように（1）より大きくより柔軟な計算深度を達成できるか、（2）複雑な計算を限られたハードウェアに圧縮する、（3）期待と注意を通じて視覚的推論に優先順位と優先順位を統合する、（4）シーケンシャルを活用する方法を理解するためにリカレントを利用します推論と予測を改善するためのデータの依存関係、および（5）反復計算の能力を活用します。生物学的視覚システムは、豊富なリカレント接続を示します。 
[ABSTRACT]視覚認識用のニューラルネットワークモデルは、フィードフォワードに大きくまたは排他的に依存しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-26">
        <br><font color="black">2020-03-26</font>
      </time>
    </span>
</section>
<!-- paper0: Robust building footprint extraction from big multi-sensor data using
  deep competition network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_26.html">
      <font color="black">Robust building footprint extraction from big multi-sensor data using
  deep competition network</font>
    </a>
  </h2>
  <font color="black">この研究では、非常に高い空間分解能の光学リモートセンシング画像をLiDARデータと融合して堅牢なBFEを実現するディープコンペティションネットワーク（DCN）を開発および評価します。DCNは、次の状態から取得した大きなマルチセンサーデータセットでトレーニングおよびテストされます。複数の建物シーンがある米国のインディアナ..DCNは、堅牢なバイナリ表現（スーパーピクセル）学習のための畳み込み重みを持つ5つのエンコード/デコードブロックで構成されています。 
[ABSTRACT] dcnは、複数の建物シーンを持つ米国のインディアナ州から取得した大きなマルチセンサーデータセットでトレーニングおよびテストされています。提案されたモデルは、大きなマルチセンサー評価からの堅牢なbfeに対する適切なソリューションです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Object Detection based on Region Decomposition and Assembly -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_27.html">
      <font color="black">Object Detection based on Region Decomposition and Assembly</font>
    </a>
  </h2>
  <font color="black">提案されたR-DADでは、最初にオブジェクト領域を複数の小さな領域に分解します。R-DADをいくつかの特徴抽出器に統合し、最近の畳み込み検出器と比較してPASCAL07 / 12とMSCOCO18で明確なパフォーマンスの向上を証明します。領域ベースのオブジェクト検出は、画像内の1つ以上のカテゴリのオブジェクト領域を推測します。 
[ABSTRACT]より正確なオブジェクト検出のために、領域分解およびアセンブリ検出器（r-dad）を提案します。オブジェクト領域全体および分解された領域内のcnn特徴を抽出します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-01-24">
        <br><font color="black">2019-01-24</font>
      </time>
    </span>
</section>
<!-- paper0: Vision-Aided Radio: User Identity Match in Radio and Video Domains Using
  Machine Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_28.html">
      <font color="black">Vision-Aided Radio: User Identity Match in Radio and Video Domains Using
  Machine Learning</font>
    </a>
  </h2>
  <font color="black">これは、通信におけるコンピュータービジョンツールの実用化に不可欠なステップです。この作業では、ディープニューラルネットワークとランダムフォレスト分類器の使用を比較し、前者がすべての実験でパフォーマンスが向上し、99％を超える分類精度を達成したことを示しています。現在の文献にはメカニズムがありません。 
[概要]ディープラーニングとコンピュータービジョンツールを使用すると、視覚データからの情報でネットワークの環境意識を高めることができます。ただし、ネットワークには、視覚システムと無線システムの両方でユーザーのIDを照合するメカニズムが必要です。これらのツールがネットワークに接続できるネットワークの開発に使用されるように設計されたのはこれが初めてです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-14">
        <br><font color="black">2020-10-14</font>
      </time>
    </span>
</section>
<!-- paper0: General Data Analytics with Applications to Visual Information Analysis:
  A Provable Backward-Compatible Semisimple Paradigm over T-Algebra -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_29.html">
      <font color="black">General Data Analytics with Applications to Visual Information Analysis:
  A Provable Backward-Compatible Semisimple Paradigm over T-Algebra</font>
    </a>
  </h2>
  <font color="black">t代数では、すべてではないにしても、多くのアルゴリズムがこの新しい半単純パラダイムを使用して簡単な方法で一般化されます。公開データセットでの実験は、一般化されたアルゴリズムが標準的なアルゴリズムと比べて遜色がないことを示しています。複素数の固定サイズの多方向配列によってt代数の要素を表すことによるt代数と、直接積構成要素のコレクションによるt代数上の代数構造。 
[概要]たとえば、視覚的なパターン分析のための一連の新しいアルゴリズムを一般化します。新しいツールのパフォーマンスとその下位互換性を示します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-31">
        <br><font color="black">2020-10-31</font>
      </time>
    </span>
</section>
<!-- paper0: Natural Disaster Classification using Aerial Photography Explainable for
  Typhoon Damaged Feature -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_30.html">
      <font color="black">Natural Disaster Classification using Aerial Photography Explainable for
  Typhoon Damaged Feature</font>
    </a>
  </h2>
  <font color="black">台風後の千葉地域で記録された航空写真に適用した事例研究を示します。迅速な復旧には迅速な対応が重要です。本稿では、台風災害の特徴に焦点を当てた被害地域を航空写真を使用して視覚化する実用的な方法を提案します。 【概要】2019年9月9日、令和元年房が日本の千葉を通過しました。復旧までに18日かかりました。この方法では、被害のない土地と被災地を含む8つのクラスに分類できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br><font color="black">2020-04-21</font>
      </time>
    </span>
</section>
<!-- paper0: SqueezeFacePoseNet: Lightweight Face Verification Across Different Poses
  for Mobile Platforms -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_31.html">
      <font color="black">SqueezeFacePoseNet: Lightweight Face Verification Across Different Poses
  for Mobile Platforms</font>
    </a>
  </h2>
  <font color="black">したがって、はるかに大きなモデルと比較して十分な精度で動作できる、わずか数メガバイトの軽量顔認識ネットワークを開発するという課題に対処します。モバイルプラットフォームを介した仮想アプリケーションは、AIで最も重要で成長を続ける分野の1つです。 、モバイルデバイスを介して提供されるすべてのサービスの突破口の後、ユビキタスでリアルタイムの個人認証が重要になっています。このペーパーでは、わずか4.4MBの軽量SqueezeNetモデルを採用して、クロスポーズの顔認識を効果的に提供します。 
[ABSTRACT]顔認証テクノロジーは、これらのデバイスでカメラが利用可能であり、日常のアプリケーションで広く使用されていることを考えると、信頼性が高く堅牢なユーザー認証を提供できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-16">
        <br><font color="black">2020-07-16</font>
      </time>
    </span>
</section>
<!-- paper0: An HVS-Oriented Saliency Map Prediction Modeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_32.html">
      <font color="black">An HVS-Oriented Saliency Map Prediction Modeling</font>
    </a>
  </h2>
  <font color="black">この論文は、人間の低レベル視覚野機能に触発された新しい顕著性予測アーキテクチャを提案します。モデルは、画像の特徴を抽出するための対戦相手のカラーチャネル、ウェーブレットエネルギーマップ、およびコントラスト感度関数と、実際の視覚野ネットワーク機能への最大のアプローチを考慮しました。脳..提案されたモデルは、MIT1003、MIT300、TORONTO、SID4VAMなどのいくつかのデータセットを評価して、その効率を説明します。 
[要約]提案されたモデルの結果は、他の最先端の顕著性予測モデルと比較した評価であり、視覚的顕著性予測の達成されたパフォーマンスです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-08">
        <br><font color="black">2020-11-08</font>
      </time>
    </span>
</section>
<!-- paper0: DARE: AI-based Diver Action Recognition System using Multi-Channel CNNs
  for AUV Supervision -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_33.html">
      <font color="black">DARE: AI-based Diver Action Recognition System using Multi-Channel CNNs
  for AUV Supervision</font>
    </a>
  </h2>
  <font color="black">DAREは、分類パフォーマンスを向上させるために体系的にトレーニングされたツリートポロジディープニューラルネットワーク分類器でサポートされるマルチチャネル畳み込みニューラルネットワークを使用したカメラ画像のステレオペアの融合に基づいています。この点で、このペーパーでは、ダイバーであるDAREを紹介します。 Cognitive Autonomous Driving Buddy（CADDY）データセットに基づいてトレーニングされたアクション認識システム。これは、さまざまなダイバーのジェスチャーやポーズの画像を含む豊富なデータセットであり、いくつかの異なる現実的な水中環境で使用されます。したがって、ダイバーアクションベースの監視は便利で、使いやすく、高速で、費用対効果が高いため、ますます人気が高まっています。 
[ABSTRACT] dareは、マルチチャネル畳み込みニューラルネットワークを使用したステレオペアのカメラ画像の融合に基づいています。ダイバーアクション認識に適した分類器のパフォーマンスに取って代わります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Gram Regularization for Multi-view 3D Shape Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_34.html">
      <font color="black">Gram Regularization for Multi-view 3D Shape Retrieval</font>
    </a>
  </h2>
  <font color="black">ギャップを埋めるために、この論文では、グラム正則化と呼ばれる新しい正則化用語を提案します。これは、重みカーネルが対応する特徴マップ上のさまざまな情報を抽出するように促すことによって、ネットワークの学習能力を強化します。 3D形状は、3D形状検索タスクの重要な課題です。さらに、既存の既製のアーキテクチャに簡単に接続できます。 
[ABSTRACT] l2正則化は、既存の深層学習フレームワークに広く適用されています。提案されたグラム正則化はデータに依存せず、ベルやホイッスルなしで安定して迅速に収束できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Blur Removal via Blurred-Noisy Image Pair -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_35.html">
      <font color="black">Blur Removal via Blurred-Noisy Image Pair</font>
    </a>
  </h2>
  <font color="black">合成データと実世界データの両方での広範な実験は、堅牢性、視覚的品質、および定量的メトリックの点で、私たちの方法が最先端の技術よりも優れていることを示しています。2つの画像間のオプティカルフローを分析することにより、パッチの対応を計算します。 ..シャープな特徴を維持するために、Mステップの目的関数に追加の二国間項を追加します。 
[概要]本論文では、ぼけた画像を推定する必要のない新しい画像ぼけ除去法を提案します。ノイズの多い画像を拡張して、対応するパッチを使用して各パッチの基礎となる強度分布をモデル化します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-03-26">
        <br><font color="black">2019-03-26</font>
      </time>
    </span>
</section>
<!-- paper0: iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video
  Captioning and Video Question Answering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_36.html">
      <font color="black">iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video
  Captioning and Video Question Answering</font>
    </a>
  </h2>
  <font color="black">さらに、DVCおよびVideoQAでのこれまでの作業のほとんどは視覚情報のみに依存していますが、音声や音声などの他のモダリティは、人間の観察者が環境を認識するために不可欠です。ActivityNetキャプションでiPerceiveDVCおよびiPerceiveVideoQAのパフォーマンスを評価することによりTVQAデータセットそれぞれ、私たちのアプローチが最先端を促進することを示します。この目的のために、常識的な知識ベースを構築することにより、ビデオ内のイベント間の「理由」を理解できるフレームワークであるiPerceiveを提案します。コンテキストキューを使用して、ビデオ内のオブジェクト間の因果関係を推測します。 
[概要]ビデオキャプションとビデオ質問応答（e evc）.video gを使用して、この手法の有効性を示しました。 。 videoqaタスクはvideog.comとvideoqaを使用して作成されました。videog-trackingテクニックは人間のためのツールとして使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: DTGAN: Dual Attention Generative Adversarial Networks for Text-to-Image
  Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CV/paper_37.html">
      <font color="black">DTGAN: Dual Attention Generative Adversarial Networks for Text-to-Image
  Generation</font>
    </a>
  </h2>
  <font color="black">提案されたモデルは、グローバルセンテンスベクトルに基づいてテキスト関連のチャネルとピクセルに焦点を合わせ、注意の重みを使用して元の特徴マップを微調整するようにジェネレータをガイドできるチャネル認識およびピクセル認識の注意モジュールを導入します。生成された画像の鮮明な形状と知覚的に均一な色分布を確保することにより、画像解像度を向上させるために、一種の視覚的損失が利用されます。単一のジェネレータ/ディスクリミネータペアのみを使用します。 
[ABSTRACT]新しい方法を使用して、入力自然言語による形状とテクスチャの変化を制御できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-05">
        <br><font color="black">2020-11-05</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: A More Comprehensive Method for Using The Target-side Monolingual Data
  to Improve Low Resource Neural Machine Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CL/paper_0.html">
      <font color="black">A More Comprehensive Method for Using The Target-side Monolingual Data
  to Improve Low Resource Neural Machine Translation</font>
    </a>
  </h2>
  <font color="black">単一言語のターゲットデータ（合成並列データ）の逆翻訳を使用してニューラル機械翻訳（NMT）モデルを改善することは、現在、改善された翻訳システムをトレーニングするための最先端のアプローチです。一方、アプローチの成功は大きく依存しています。追加の並列データ生成モデル（後方モデル）のアプローチの目的は、前方モデルの改善のみを目的としています。ターゲット側の単一言語データのみを使用して、前方翻訳と前方モデルを通じて後方モデルを改善することを検討しました。逆翻訳を通じて。 
[要約]新しい研究は、後方モデルと前方モデルの両方を改善するために、ターゲット（後方全体のサイドデータ）変換アプローチの使用を示唆しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br><font color="black">2020-06-04</font>
      </time>
    </span>
</section>
<!-- paper0: Modeling the Music Genre Perception across Language-Bound Cultures -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CL/paper_1.html">
      <font color="black">Modeling the Music Genre Perception across Language-Bound Cultures</font>
    </a>
  </h2>
  <font color="black">この作業では、言語固有のセマンティック表現、つまり分散概念の埋め込みとオントロジーのみに基づいて、関連する言語間、文化固有の音楽ジャンル注釈を取得する可能性を研究します。音楽ジャンルを研究するこのアプローチは、これまでで最も広範囲ですまた、音楽学と音楽情報の取得に多くの影響を及ぼします。さらに、最先端の多言語の事前トレーニング済み埋め込みモデルをベンチマークするために、ドメインに依存する新しいクロスリンガルコーパスを紹介します。 
[概要] 6つの言語に焦点を当てた調査では、教師なしの言語横断的な音楽ジャンルの注釈が高精度で実現可能であることが示されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-13">
        <br><font color="black">2020-10-13</font>
      </time>
    </span>
</section>
<!-- paper0: HoVer: A Dataset for Many-Hop Fact Extraction And Claim Verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CL/paper_2.html">
      <font color="black">HoVer: A Dataset for Many-Hop Fact Extraction And Claim Verification</font>
    </a>
  </h2>
  <font color="black">HoVerデータセットをhttps://hover-nlp.github.ioで公開しています。推論ホップの数が増えると、既存の最先端のセマンティックマッチングモデルのパフォーマンスがデータセットで大幅に低下することを示します。したがって、強力な結果を達成するには、多くのホップの推論が必要であることを示しています。このやりがいのあるデータセットの導入とそれに伴う評価タスクは、メニーホップの事実検索と情報検証の研究を促進します。 
[要約]いくつかのウィキペディアの記事から事実を抽出するためにモデルに挑戦します。3/ 4ホップの主張のほとんどは複数の文で書かれています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-05">
        <br><font color="black">2020-11-05</font>
      </time>
    </span>
</section>
<!-- paper0: Does Visual Self-Supervision Improve Learning of Speech Representations
  for Emotion Recognition? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CL/paper_3.html">
      <font color="black">Does Visual Self-Supervision Improve Learning of Speech Representations
  for Emotion Recognition?</font>
    </a>
  </h2>
  <font color="black">自己教師あり学習は、最近多くの研究関心を集めています。私たちは、離散感情認識、継続的影響認識、自動音声認識のために学習した音声表現を評価します。この作業（1）は、学習を導くために顔の再構成による視覚的自己監視を調査します。音声表現の; （2）音声表現学習のための音声のみの自己監視アプローチを提案します。 （3）提案された視覚と音声の自己監視のマルチタスクの組み合わせは、ノイズの多い条件でより堅牢なより豊富な機能を学習するのに有益であることを示しています。 （4）は、自己教師あり事前トレーニングが完全教師ありトレーニングよりも優れている可能性があり、小さいサイズのデータセットでの過剰適合を防ぐのに特に役立つことを示しています。 
[概要]これは、音声の自己制御学習が不足しているためです。これらは自己に対して機能します。音声の監視は通常、単峰性です。これは、視覚と音声の共同自己監視が、音声と感情の認識のためのより有益な音声表現につながることを示唆しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-04">
        <br><font color="black">2020-05-04</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Ignore: Long Document Coreference with Bounded Memory Neural
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CL/paper_4.html">
      <font color="black">Learning to Ignore: Long Document Coreference with Bounded Memory Neural
  Networks</font>
    </a>
  </h2>
  <font color="black">すべてのエンティティをメモリに保持する必要はないと主張し、一度に少数の制限されたエンティティのみを追跡するメモリ拡張ニューラルネットワークを提案します。これにより、ドキュメントの長さの線形ランタイムが保証されます。（a）このモデルは、OntoNotesおよびLitBankでメモリと計算の要件が高いモデルとの競争力を維持し、（b）モデルは、ルールベースの戦略を簡単に上回る効率的なメモリ管理戦略を学習します。長いドキュメントの共参照の解決は、現在のモデルのメモリとランタイムの要件。 
[要約]エンティティのグローバル表現のみを使用して行われた調査は、実用的な利点を示していますが、すべてのエンティティをメモリに保持する必要があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-06">
        <br><font color="black">2020-10-06</font>
      </time>
    </span>
</section>
<!-- paper0: Non-Autoregressive Machine Translation with Latent Alignments -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CL/paper_5.html">
      <font color="black">Non-Autoregressive Machine Translation with Latent Alignments</font>
    </a>
  </h2>
  <font color="black">さらに、Imputerモデルを非自己回帰機械翻訳に適合させ、わずか4世代のステップでImputerが自己回帰Transformerベースラインのパフォーマンスに匹敵することを示します。競争力のあるWMT&#39;14 En $ \ rightarrow $ Deタスクでは、CTCモデルは単一生成ステップで25.7BLEUを達成し、Imputerは2生成ステップで27.5 BLEUを達成し、4生成ステップで28.0 BLEUを達成します。機械翻訳のためにCTCを再検討し、単純なCTCモデルが最新の状態を達成できることを示します。以前の研究が示していることに反して、シングルステップの非自己回帰機械翻訳のための芸術。 
[概要]単純なctcモデルは、単一ステップの非自己回帰機械翻訳を実現できます。これまでの作業で示されたものとは異なり、ターゲットの長さの予測や自己回帰は必要ありません。たとえば、潜在的なアライメントモデルは既存の多くのモデルよりも単純です。非自己回帰翻訳ベースライン</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-16">
        <br><font color="black">2020-04-16</font>
      </time>
    </span>
</section>
<!-- paper0: Beneath the Tip of the Iceberg: Current Challenges and New Directions in
  Sentiment Analysis Research -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CL/paper_6.html">
      <font color="black">Beneath the Tip of the Iceberg: Current Challenges and New Directions in
  Sentiment Analysis Research</font>
    </a>
  </h2>
  <font color="black">現在の関連性の原因となる大きな飛躍を分析します。フィールドとしての感情分析は、20年近く前にタスクとして最初に導入されて以来、長い道のりを歩んできました。多くの見落とされ、答えられていない質問。 
[ABSTRACT]センチメントは、マーケティング、リスク管理、極性、および政治に広く行き渡っています。マーケティングやリスク管理などのさまざまな分野で幅広い商用アプリケーションがあります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-01">
        <br><font color="black">2020-05-01</font>
      </time>
    </span>
</section>
<!-- paper0: BERTs of a feather do not generalize together: Large variability in
  generalization across models with similar test set performance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CL/paper_7.html">
      <font color="black">BERTs of a feather do not generalize together: Large variability in
  generalization across models with similar test set performance</font>
    </a>
  </h2>
  <font color="black">たとえば、主語と目的語の交換の単純なケース（たとえば、「医師が弁護士を訪問した」と判断しても「弁護士が医師を訪問した」とは限らない）の場合、精度は0.00％から66.2％の範囲でした。同じモデルの一般化パフォーマンスは大きく異なります。MNLI開発セットでは、すべてのインスタンスの動作は非常に一貫しており、精度は83.6％から84.8％の範囲でした。 
[概要]同じモデルは一般化のパフォーマンスが大きく異なります。まったく対照的に、同じモデルは一般化が大きく異なります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-07">
        <br><font color="black">2019-11-07</font>
      </time>
    </span>
</section>
<!-- paper0: IIT_kgp at FinCausal 2020, Shared Task 1: Causality Detection using
  Sentence Embeddings in Financial Reports -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CL/paper_8.html">
      <font color="black">IIT_kgp at FinCausal 2020, Shared Task 1: Causality Detection using
  Sentence Embeddings in Financial Reports</font>
    </a>
  </h2>
  <font color="black">実験では、財務テキストとレポートの文の因果関係を検出するタスクで、BERT（Large）が最高のパフォーマンスを示し、F1スコアは0.958でした。クラスの不均衡は、より良いメトリックを提供するために修正された損失関数で処理されました。評価のスコア..このペーパーでは、チームがFinCausal 2020 SharedTaskに提出した作業について説明します。 
[要約]この作業は、文の因果関係を特定する最初のサブタスクに関連付けられています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: The Curse of Performance Instability in Analysis Datasets: Consequences,
  Source, and Suggestions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CL/paper_9.html">
      <font color="black">The Curse of Performance Instability in Analysis Datasets: Consequences,
  Source, and Suggestions</font>
    </a>
  </h2>
  <font color="black">（2）この不安定性はどこから来るのですか？次に、2番目の質問に答えるために、不安定性の原因に関する理論的説明と経験的証拠の両方を示し、不安定性は主に分析セット内の高い例間の相関に起因することを示します。最初の質問では、徹底的な経験的分析を行います。分析セットを調べてみると、不安定な最終パフォーマンスに加えて、トレーニング曲線全体に不安定性が存在することがわかります。 
[要約]これは、不安定性がこれらの分析セットに基づいて導き出された結論の信頼性にどのように影響するかについて3つの質問を提起します。分析検証セットと標準検証セットの間の予想よりも低い相関も観察されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-28">
        <br><font color="black">2020-04-28</font>
      </time>
    </span>
</section>
<!-- paper0: Word Rotator's Distance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CL/paper_10.html">
      <font color="black">Word Rotator's Distance</font>
    </a>
  </h2>
  <font color="black">いくつかのテキスト類似性データセットでは、これらの単純な提案された方法の組み合わせは、アラインメントベースのアプローチだけでなく強力なベースラインよりも優れていました。アラインメントベースのアプローチはそれらを区別しませんが、文ベクトルアプローチは自動的にノルムを単語の重要性として使用します。したがって、最初に単語ベクトルをそれらのノルムと方向に分離し、次に、単語回転子の距離と呼ばれる土工の距離（すなわち、最適な輸送コスト）を使用してアライメントベースの類似性を計算する方法を提案します。 
[概要]単語「トランスポーター」の規範と方向性を高める方法を見つけます。これは、文から派生した新しい体系的なアプローチです-文字列推定方法</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-30">
        <br><font color="black">2020-04-30</font>
      </time>
    </span>
</section>
<!-- paper0: An Atlas of Cultural Commonsense for Machine Reasoning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CL/paper_11.html">
      <font color="black">An Atlas of Cultural Commonsense for Machine Reasoning</font>
    </a>
  </h2>
  <font color="black">また、普遍的な（そしておそらく西洋に偏った）常識知識の厳格なフレームワークを想定せず、文脈的および文化的に敏感な方法で推論する能力を備えたマシンの構築に向けて一歩近づきます。私たちの希望はこの種の文化的知識は、質問応答（QA）やテキストの理解と生成などのNLPタスクでより人間らしいパフォーマンスにつながるでしょう。AIおよびNLPタスクの既存の常識的な推論データセットは、人間の生活の重要な側面に対処できません。文化の違い。 
[要約]私たちの研究は、ありふれた出来事の常識的な推論の分野での既存の研究によって特定されたさまざまなタイプの関係を調査します。私たちの希望は、この種の文化的知識が、質問応答などの自然言語処理タスクのパフォーマンスのように、より人間的なものにつながることです（ qa）およびテキストの理解と生成</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-11">
        <br><font color="black">2020-09-11</font>
      </time>
    </span>
</section>
<!-- paper0: Reinforced Medical Report Generation with X-Linear Attention and
  Repetition Penalty -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CL/paper_12.html">
      <font color="black">Reinforced Medical Report Generation with X-Linear Attention and
  Repetition Penalty</font>
    </a>
  </h2>
  <font color="black">2つの公開データセットで広範な実験的研究が行われ、その結果は、ReMRG-XRがすべてのメトリックの点で最先端のベースラインを大幅に上回っていることを示しています。したがって、この作業では、強化された医療レポートの生成を提案します。これらの問題を克服するためのx-linearアテンションおよび繰り返しペナルティメカニズム（ReMRG-XR）を使用したソリューション。モデルのトレーニングプロセス中に繰り返される用語にペナルティを適用します。 
[概要]これらの最先端のソリューションには、2つの欠点があります。これらには、x-光学的注意モジュールとtf-idfベースの報酬関数が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Shallow Fusion for RNN-T Personalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CL/paper_13.html">
      <font color="black">Deep Shallow Fusion for RNN-T Personalization</font>
    </a>
  </h2>
  <font color="black">この作業では、希少なWordPiecesをモデル化し、エンコーダーに追加情報を注入し、代替の書記素発音の使用を可能にし、パーソナライズされた言語モデルとの深い融合を実行してより堅牢なバイアスをかけるRNN-Tの機能を向上させる新しい手法を紹介します。 RNN-Tパーソナライズの限界を押し広げ、バイアスとエンティティ認識が重要なユースケースでハイブリッドシステムとのギャップを埋めるのに役立ちます。一般にエンドツーエンドモデル、特にリカレントニューラルネットワークトランスデューサー（RNN-T）、自動音声認識コミュニティでは、その単純さ、コンパクトさ、および一般的な転写タスクでの優れたパフォーマンスにより、ここ数年で大きな注目を集めています。 
[概要]新しいモデルは、外部言語モデルがないため、従来のハイブリッドシステムよりもパーソナライズが困難です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: GGPONC: A Corpus of German Medical Text with Rich Metadata Based on
  Clinical Practice Guidelines -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CL/paper_14.html">
      <font color="black">GGPONC: A Corpus of German Medical Text with Rich Metadata Based on
  Clinical Practice Guidelines</font>
    </a>
  </h2>
  <font color="black">ドイツ語のテキストに既存の医療情報抽出パイプラインを適用して評価することにより、医療言語の使用を他のコーパス、医療および非医療のものと比較することができます。この作業では、GGPONC（ドイツ腫瘍学ガイドラインプログラム）を紹介します。 NLP Corpus）、腫瘍学の臨床診療ガイドラインに基づいて自由に配布可能なドイツ語コーパス。臨床文書とは異なり、臨床ガイドラインには患者関連の情報が含まれていないため、データ保護の制限なしで使用できます。 
[概要]コーパスは、ドイツの医療文書から構築された史上最大のコーパスの1つです。ggponcは、大規模な医療サブフィールドのさまざまな条件をカバーするドイツ語の最初のコーパスです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: AI-lead Court Debate Case Investigation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CL/paper_15.html">
      <font color="black">AI-lead Court Debate Case Investigation</font>
    </a>
  </h2>
  <font color="black">この作業では、革新的なエンドツーエンドの質問生成モデルであるTrial Brain Model（TBM）を提案して、Trial Brainを構築します。これにより、原告と被告の間の歴史的な対話を通じて、裁判官が尋ねたい質問を生成できます。 。質問の生成は、自然言語の生成における重要なタスクです。.司法裁判では、裁判官が事件をより明確に理解できるように、裁判官が効率的な質問を提起するのに役立ちます。 
[概要]私たちのモデルは、事前定義された知識を通じて裁判官の質問意図を学ぶことができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Beyond I.I.D.: Three Levels of Generalization for Question Answering on
  Knowledge Bases -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/cs.CL/paper_16.html">
      <font color="black">Beyond I.I.D.: Three Levels of Generalization for Question Answering on
  Knowledge Bases</font>
    </a>
  </h2>
  <font color="black">代わりに、KBQAモデルには、iid、compositional、zero-shotの3つのレベルの組み込み一般化が必要であることをお勧めします。さらに、新しいBERTベースのKBQAモデルを提案します。データセットとモデルの組み合わせにより、 KBQAの一般化におけるBERTのような事前にトレーニングされたコンテキスト埋め込みの重要な役割を初めて徹底的に調べて実証すること。 
[ABSTRACT] iii dは、大規模なkbsでは合理的に達成可能でも望ましいものでもない可能性があります。64、495の質問、grailqaを含む新しい高操作データセットを構築してリリースします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: Does Visual Self-Supervision Improve Learning of Speech Representations
  for Emotion Recognition? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.AS/paper_0.html">
      <font color="black">Does Visual Self-Supervision Improve Learning of Speech Representations
  for Emotion Recognition?</font>
    </a>
  </h2>
  <font color="black">自己教師あり学習は、最近多くの研究関心を集めています。この作業（1）は、音声表現の学習をガイドするために、顔の再構成による視覚的な自己教師ありを調査します。 （2）音声表現学習のための音声のみの自己監視アプローチを提案します。 （3）提案された視覚と音声の自己監視のマルチタスクの組み合わせは、ノイズの多い条件でより堅牢なより豊富な機能を学習するのに有益であることを示しています。 （4）は、自己教師あり事前トレーニングが完全教師ありトレーニングよりも優れており、小さいサイズのデータセットでの過剰適合を防ぐのに特に役立つことを示しています。テストしたすべてのダウンストリームタスクで既存の自己教師あり方法よりも優れています。 
[概要]これは、音声の自己制御学習が不足しているためです。これらは自己に対して機能します。音声の監視は通常、単峰性です。これは、視覚と音声の共同自己監視が、音声と感情の認識のためのより有益な音声表現につながることを示唆しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-04">
        <br><font color="black">2020-05-04</font>
      </time>
    </span>
</section>
<!-- paper0: Block-Online Guided Source Separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.AS/paper_1.html">
      <font color="black">Block-Online Guided Source Separation</font>
    </a>
  </h2>
  <font color="black">CHiME-6コーパスと会議コーパスの評価では、提案されたアルゴリズムが従来のオフラインGSSアルゴリズムとほぼ同じパフォーマンスを達成したが、リアルタイムアプリケーションには十分な32倍高速な計算であることが示されました。提案されたアルゴリズムでは、ブロック-賢明な入力サンプルと対応する時間注釈は、前のコンテキストのものと連結され、パラメーターを更新するために使用されます。また、各ブロックとそのコンテキストのアクティブなスピーカーのパラメーターのみを更新することにより、計算コストを削減します。 
[ABSTRACT]ブロック単位の入力サンプルと対応する時間注釈は、前述のコンテキストのものと連結されます。提案されたアルゴリズムを使用して、ブロック単位の出力サンプルが連結され、パラメーターの更新に使用されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Environment Sound Classification using Multiple Feature Channels and
  Attention based Deep Convolutional Neural Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.AS/paper_2.html">
      <font color="black">Environment Sound Classification using Multiple Feature Channels and
  Attention based Deep Convolutional Neural Network</font>
    </a>
  </h2>
  <font color="black">それに加えて、チャネルと空間の注意を一緒に実行する注意モジュールを使用します。私たちのモデルは、3つのベンチマーク環境のサウンド分類データセットすべてで最先端のパフォーマンスを実現できます。 ESC-10およびESC-50データセットの場合、提案されたモデルによって達成される精度は、それぞれ95.7％および81.3％の人間の精度を超えています。 
[概要]複数の機能チャネルには、mel-周波数ケプストラム度（mfcc）、ガンマトーン周波数ケプストラムエントリ（gfcc）、定数q-変換（cqt）、およびクロマトグラムが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-28">
        <br><font color="black">2019-08-28</font>
      </time>
    </span>
</section>
<!-- paper0: A General Network Architecture for Sound Event Localization and
  Detection Using Transfer Learning and Recurrent Neural Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.AS/paper_3.html">
      <font color="black">A General Network Architecture for Sound Event Localization and
  Detection Using Transfer Learning and Recurrent Neural Network</font>
    </a>
  </h2>
  <font color="black">提案されたSELDネットワークアーキテクチャのソースコードはGithubで入手できます。リカレントレイヤーは、アップストリームのSEDおよびDOA推定アルゴリズムによってこれらの出力がどのように生成されるかを認識せずに、サウンドクラスとサウンドイベントのDOAの間の調整を行います。シンプルなネットワークアーキテクチャは、さまざまな既存のSEDおよびDOA推定アルゴリズムと互換性があります。 
[概要]このシンプルなネットワークアーキテクチャは、さまざまなローカルsedおよびdoaの機能強化と互換性があります。調査によると、提案されたネットワークアーキテクチャのパフォーマンスは、他の最先端のセルドアルゴリズムと競合します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Audio-visual Multi-channel Integration and Recognition of Overlapped
  Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.AS/paper_4.html">
      <font color="black">Audio-visual Multi-channel Integration and Recognition of Overlapped
  Speech</font>
    </a>
  </h2>
  <font color="black">これは、音声分離フロントエンドと認識バックエンドの緊密な統合の恩恵を受けます。どちらも追加のビデオ入力を組み込んでいます。音響信号の破損に対する視覚モダリティの不変性と、ターゲットスピーカーを分離するために提供する追加の手がかりに動機付けられています。干渉する音源から、この論文は、重複した音声のための視聴覚マルチチャネルベースの認識システムを提示します。しかし、重複した音声の認識は、今日まで非常に挑戦的なタスクのままです。 
[要約]新しい論文は、重複した音声のためのオーディオ-ビジュアルマルチ-チャネルベースの認識システムを提示しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Shallow Fusion for RNN-T Personalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.AS/paper_5.html">
      <font color="black">Deep Shallow Fusion for RNN-T Personalization</font>
    </a>
  </h2>
  <font color="black">この作業では、希少なWordPiecesをモデル化し、エンコーダーに追加情報を注入し、代替の書記素発音の使用を可能にし、パーソナライズされた言語モデルとの深い融合を実行してより堅牢なバイアスをかけるRNN-Tの機能を向上させる新しい手法を紹介します。 RNN-Tパーソナライズの限界を押し広げ、バイアスとエンティティ認識が重要なユースケースでハイブリッドシステムとのギャップを埋めるのに役立ちます。これらの組み合わせた手法により、強力なワードエラー率と比較して15.4％〜34.5％の相対的なワードエラー率が向上することを示します。浅い融合とテキストから音声への拡張を使用するRNN-Tベースライン。 
[概要]新しいモデルは、外部言語モデルがないため、従来のハイブリッドシステムよりもパーソナライズが困難です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: The SLT 2021 children speech recognition challenge: Open datasets, rules
  and baselines -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.AS/paper_6.html">
      <font color="black">The SLT 2021 children speech recognition challenge: Open datasets, rules
  and baselines</font>
    </a>
  </h2>
  <font color="black">特に、子供の音声認識（CSR）のパフォーマンスは、1）子供の声の音声と言語の特性が大人のそれと大幅に異なること、および2）子供の音声のかなりのオープンデータセットがまだ研究で利用できないため、まだ遅れています。コミュニティ..チャレンジは、登録されたチームの約400時間のマンダリン音声データをリリースし、2つのチャレンジトラックを設定し、CSRパフォーマンスをベンチマークするための共通のテストベッドを提供します。ベースラインとして。 
[概要]子供の音声認識チャレンジ（csrc）は、ワイヤレスslt2021ワークショップの主力衛星イベントです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-13">
        <br><font color="black">2020-11-13</font>
      </time>
    </span>
</section>
<!-- paper0: Insertion-Based Modeling for End-to-End Automatic Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.AS/paper_7.html">
      <font color="black">Insertion-Based Modeling for End-to-End Automatic Speech Recognition</font>
    </a>
  </h2>
  <font color="black">この定式化は、非自己回帰方式で挿入ベースのトークン生成に依存するようにすることでCTCを強化します。1つのNATモデルであるマスク予測がASRに適用されましたが、モデルには、長さを推定するためのヒューリスティックまたは追加コンポーネントが必要です。出力トークンシーケンス..このペーパーでは、元々NMT用に提案された挿入ベースモデルと呼ばれる別のタイプのNATをASRタスクに適用することを提案します。 
[概要]これまでに提案された多くのe2eモデルは、出力トークンシーケンスの左から右への自己回帰生成を想定しています。非左から右へのモデルの1つは、非自己回帰トランスフォーマー（nat）として知られており、ニューラル機械翻訳の領域</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-27">
        <br><font color="black">2020-05-27</font>
      </time>
    </span>
</section>
<!-- paper0: Conversational End-to-End TTS for Voice Agent -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-17/eess.AS/paper_8.html">
      <font color="black">Conversational End-to-End TTS for Voice Agent</font>
    </a>
  </h2>
  <font color="black">次に、補助エンコーダーと会話コンテキストエンコーダーを備えた会話コンテキスト対応のエンドツーエンドTTSアプローチを提案し、会話内の現在の発話とそのコンテキストに関する情報も強化します。実験結果は、提案されたものがメソッドは、会話のコンテキストに応じてより自然なプロソディを生成し、発話レベルと会話レベルの両方で大幅な優先度の向上を実現します。エンドツーエンドのニューラルTTSは、リーディングスタイルの音声合成で優れたパフォーマンスを実現しました。 
[ABSTRACT]ヨーク大学の研究者は、音声エージェント向けに設計された会話型音声コーパスを構築しています。提案された方法は、会話のコンテキストに応じてより自然な韻律を生成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-21">
        <br><font color="black">2020-05-21</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
