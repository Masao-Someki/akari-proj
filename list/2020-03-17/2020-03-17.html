<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-03-17の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: DNN-Based Distributed Multichannel Mask Estimation for Speech
  Enhancement in Microphone Arrays -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/cs.SD/paper_0.html">
      DNN-Based Distributed Multichannel Mask Estimation for Speech
  Enhancement in Microphone Arrays
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      2つのノードの配列で、この追加信号を効率的に考慮してマスクを予測し、マスク推定がローカル信号のみに依存している場合よりも優れた音声強調パフォーマンスにつながることを示します。分散適応ノード固有の信号推定アプローチをニューラルネットワークフレームワークに拡張します。マルチチャネル処理は音声強調に広く使用されていますが、これらのソリューションを実世界に展開しようとするといくつかの制限が現れます。 
[概要]いくつかのマイクを備えた複数のデバイスは、展開の実行可能な代替手段です。グローバルマルチチャネルウィナーフィルターを計算するには、マイクの配列にアクセスできます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-13">
        <br>2020-02-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multi-modal Multi-channel Target Speech Separation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/cs.SD/paper_1.html">
      Multi-modal Multi-channel Target Speech Separation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、提案されたマルチモーダルフレームワークが、シングルモーダルおよびバイモーダルの音声分離アプローチを大幅に上回る一方で、リアルタイム処理をサポートできることを示しています。この方法は、最初に混合オーディオを音響部分空間に分解し、次にこれらの部分空間音響埋め込みを学習可能なアテンションスキームで強化するための、他のモダリティからのターゲットの情報。 
[要約]実験は大規模な視覚データセットで行われます。センサーを使用して、さまざまな種類の音声を再現できます。この方法は、音声の代わりに使用できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-16">
        <br>2020-03-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Semantic Mask for Transformer based End-to-End Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/cs.SD/paper_2.html">
      Semantic Mask for Transformer based End-to-End Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      Librispeech 960hおよびTedLium2データセットで実験を行い、E2Eモデルの範囲内のテストセットで最先端のパフォーマンスを達成します。アイデアは、特定の出力トークンに対応する入力機能をマスクすることです。 、コンテキスト情報に基づいてモデルがトークンを埋めるように奨励するために、単語またはワードピース。このアプローチは、あらゆるタイプのニューラルネットワークアーキテクチャを備えたエンコーダ/デコーダフレームワークに適用可能ですが、この作品のASRのベースモデル。 
[要約]これは、エンコーダーの新しいパターンに基づいています-デコーダー。代わりに、ニューラルネットワークの記憶能力を利用します。このモデルは、このような華やかな形式の音声を訓練するために使用されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-06">
        <br>2019-12-06
      </time>
    </span>
  </h3>
</article>
<!-- paper0: TRANS-BLSTM: Transformer with Bidirectional LSTM for Language
  Understanding -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/cs.SD/paper_3.html">
      TRANS-BLSTM: Transformer with Bidirectional LSTM for Language
  Understanding
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      トランスフォーマー（BERT）からの双方向エンコーダー表現は、最近、文の分類、機械翻訳、質問への回答を含む広範囲のNLPタスクで最先端のパフォーマンスを達成しました。 SQuAD 1.1開発データセットでは、最新の結果に匹敵します。各トランスフォーマブロックに統合されたBLSTMレイヤを備えたTransformer with BLSTM（TRANS-BLSTM）と呼ばれる新しいアーキテクチャを提案します。トランスフォーマーとBLSTMの共同モデリングフレームワーク。 
[概要]バートモデルアーキテクチャのメモリはトランスフォーマーから派生します。これら2つのモデリング手法を組み合わせて、より強力なモデルアーキテクチャを作成できます。trans-blstmモデルは一貫して精度の向上につながります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-16">
        <br>2020-03-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Interfacing PDM MEMS microphones with PFM spiking systems: Application
  for Neuromorphic Auditory Sensors -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/cs.SD/paper_4.html">
      Interfacing PDM MEMS microphones with PFM spiking systems: Application
  for Neuromorphic Auditory Sensors
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ニューロモーフィックエンジニアリングでは、計算は一般に非同期で実行され、神経系が情報を処理する方法を模倣します。スパイクごと。この変換は、PDMマイクのインターフェイスとしてVHDLで行われ、パルス周波数に続くパルスを時間分散スパイクに変換します「スパイクインターフェースへのPDM」（PSI）として知られる正確なスパイク間間隔を備えた変調（PFM）スキーム。PSIは、-39.51dBの全高調波歪み（THD）とS / N比（ SNR）は59.12dBであり、Spartan-6 FPGAのリソースの1 \％未満を要求し、消費電力は5mW未満です。 
[概要]ニューロモーフィック聴覚センサー（nas）は、スパイクベースの信号処理ブロックを使用して、スパイクベースのスパイクをシミュレートします。システムは、低電力パルス密度補正（pdm）マイクロエレクトロメカニカルシステム（mems）マイクから音声情報をレートに変換するために使用されますコード化されたスパイク周波数
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-30">
        <br>2019-04-30
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: A Machine Learning Application for Raising WASH Awareness in the Times
  of Covid-19 Pandemic -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/cs.CL/paper_0.html">
      A Machine Learning Application for Raising WASH Awareness in the Times
  of Covid-19 Pandemic
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      しかし、新しい情報を提供することによる継続的な意識向上と誤った情報のリスクとの間には微妙なバランスがあります。最先端のテキストを音声エンジンに使用することにより、ナレーション付きのコンテンツをヒンディー語で配信します。 WASHに関連する病気を予防するためのWashKaroアプリケーションの有効性は、インドのデリーで2019年に350万件の相談を行ったMohallaクリニックで実施される予定です。 
[概要]アプリケーションは、インドで最も広く使用されている言語であるヒンディー語でユーザーに本格的なレポートを配信します。最新のテキスト音声読み上げエンジンを使用して、ナレーション付きのコンテンツをヒンディー語で配信します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-16">
        <br>2020-03-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: EDA: Enriching Emotional Dialogue Acts using an Ensemble of Neural
  Annotators -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/cs.CL/paper_1.html">
      EDA: Enriching Emotional Dialogue Acts using an Ensemble of Neural
  Annotators
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      感情と対話行為のラベルの共起を分析し、特定の関係を発見しました。ただし、テキストおよびマルチモーダルの会話感情コーパスのほとんどは、感情ラベルのみを含み、対話行為は含みません。これらの神経モデルは、感情コーパスに対話で注釈を付けます。行為ラベル、およびアンサンブルアノテーターが最終的な対話行為ラベルを抽出します。 
[ABSTRACT]感情の解釈は感情を理解させます。対話行為は発話の意図と実行機能を反映します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-02">
        <br>2019-12-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Stanza: A Python Natural Language Processing Toolkit for Many Human
  Languages -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/cs.CL/paper_2.html">
      Stanza: A Python Natural Language Processing Toolkit for Many Human
  Languages
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、Stanzaには、広く使用されているJava Stanford CoreNLPソフトウェアへのネイティブPythonインターフェイスが含まれており、機能をさらに拡張して、相互参照解決や関係抽出などの他のタスクをカバーします。UniversalDependenciesを含む合計112のデータセットでStanzaをトレーニングしましたツリーバンクおよび他の多言語コーパス。同じニューラルアーキテクチャが一般化され、テストされたすべての言語で競争力のあるパフォーマンスを達成していることを示します。66言語のソースコード、ドキュメント、および事前トレーニングモデルはhttps://stanfordnlp.github.io/stanzaで入手できます。 
[ABSTRACT] wexlerのプロジェクトには、トークン化、マルチワードトークン拡張、レンマタイゼーション、品詞およびトピック機能が含まれますtagging.projectには、JavaスタンフォードcorenlpソフトウェアへのネイティブPythonインターフェイスが含まれます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-16">
        <br>2020-03-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Learning from Easy to Complex: Adaptive Multi-curricula Learning for
  Neural Dialogue Generation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/cs.CL/paper_3.html">
      Learning from Easy to Complex: Adaptive Multi-curricula Learning for
  Neural Dialogue Generation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本書では、子供が簡単な対話から複雑な対話まで学習し、学習の進捗を動的に調整する会話学習の人間の行動に触発されて、最初に5つの対話属性を分析して、3つの公的に利用可能なコーパスの複数の観点で対話の複雑さを測定します。現在の最先端の神経対話システムは、主にデータ駆動型であり、人間が生成する応答について訓練されています。5つの最先端のモデルで行われた広範な実験は、13の自動に関する学習効率と有効性を実証します評価指標と人間の判断。 
[要旨]統一された対話の複雑さの測定値はなく、対話の複雑さは主観性と開かれた対話に基づいています。しかし、主観性のため、対話の複雑さは大きく異なります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-02">
        <br>2020-03-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Semantic Mask for Transformer based End-to-End Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/cs.CL/paper_4.html">
      Semantic Mask for Transformer based End-to-End Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      Librispeech 960hおよびTedLium2データセットで実験を行い、E2Eモデルの範囲内のテストセットで最先端のパフォーマンスを達成します。アイデアは、特定の出力トークンに対応する入力機能をマスクすることです。 、コンテキスト情報に基づいてモデルがトークンを埋めるように奨励するために、単語またはワードピース。このアプローチは、あらゆるタイプのニューラルネットワークアーキテクチャを備えたエンコーダ/デコーダフレームワークに適用可能ですが、この作品のASRのベースモデル。 
[要約]これは、エンコーダーの新しいパターンに基づいています-デコーダー。代わりに、ニューラルネットワークの記憶能力を利用します。このモデルは、このような華やかな形式の音声を訓練するために使用されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-06">
        <br>2019-12-06
      </time>
    </span>
  </h3>
</article>
<!-- paper0: TRANS-BLSTM: Transformer with Bidirectional LSTM for Language
  Understanding -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/cs.CL/paper_5.html">
      TRANS-BLSTM: Transformer with Bidirectional LSTM for Language
  Understanding
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      トランスフォーマーとBLSTMの共同モデリングフレームワークにつながるBLSTMレイヤーを各トランスフォーマブロックに統合した、トランスフォーマーとBLSTM（TRANS-BLSTM）として示される新しいアーキテクチャを提案します。トランスフォーマーからの双方向エンコーダー表現（BERT）は最近状態を達成しました文の分類、機械翻訳、質問への回答など、広範なNLPタスクでの最先端のパフォーマンス。TRANS-BLSTMモデルは、SQuAD 1.1開発データセットで94.01％のF1スコアを取得します。これは、最先端の結果。 
[概要]バートモデルアーキテクチャのメモリはトランスフォーマーから派生します。これら2つのモデリング手法を組み合わせて、より強力なモデルアーキテクチャを作成できます。trans-blstmモデルは一貫して精度の向上につながります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-16">
        <br>2020-03-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data
  In Your Machine Translation System? -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/cs.CL/paper_6.html">
      Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data
  In Your Machine Translation System?
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      メンバーシップ推論攻撃の問題に焦点を当てます。データサンプルとモデルのAPIへのブラックボックスアクセスを与え、サンプルがモデルのトレーニングデータに存在するかどうかを判断します。シーケンス生成のためのメンバーシップ推論問題を定義し、オープンデータセットを提供最先端の機械翻訳モデルに基づいており、これらのモデルがいくつかの種類のメンバーシップ推論攻撃に対して個人情報を漏らすかどうかの初期結果を報告します。私たちの貢献は、シーケンスからシーケンスのコンテキストでのこの問題の調査です機械翻訳やビデオキャプションなどのアプリケーションで重要なモデル。 
[ABSTRACT]データサンプルとモデルのAPIへのブラックボックスアクセスを見つけることができます。データサンプルはモデルのトレーニングデータに存在しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-11">
        <br>2019-04-11
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Key Phrase Classification in Complex Assignments -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/cs.CL/paper_7.html">
      Key Phrase Classification in Complex Assignments
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      事前学習済みの言語モデルと単純なTFIDF SVM分類子はどちらも同様の結果を生成し、前者は後者よりも平均が0.6 F1高くなります。そのため、レビューに必要な重要なコンテンツを識別することを期待して、この作業では最初の作業を提示します伝統的および最新の言語モデリングアプローチに関する詳細な実証的研究によるキーフレーズ分類。最終的に、将来の教育レポートからのキーフレーズ分類に興味がある人のための広範な経験的およびモデル解釈可能性結果から実用的なアドバイスを導き出します。 
[要旨]キーフレーズの分類のタスクは、新しいデータセットで0. 77のコーエンのカッパを生成する人間レベルではあいまいです。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-16">
        <br>2020-03-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: CompLex --- A New Corpus for Lexical Complexity Predicition from Likert
  Scale Data -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/cs.CL/paper_8.html">
      CompLex --- A New Corpus for Lexical Complexity Predicition from Likert
  Scale Data
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      特定のターゲット母集団について理解しにくい単語を予測することは、テキストの単純化などの多くのNLPアプリケーションで重要なステップです。以前の研究では、システムがテキスト内のターゲット単語のセットの複雑度値（複雑対非複雑）を予測するバイナリ分類タスクとしてタスクにアプローチしました。 
[要約]このタスクは、一般に複雑な単語の識別（cwi）と呼ばれます。単語の欠如は、すべてのcwiデータセットに注釈が付けられているという事実に起因します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-16">
        <br>2020-03-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Survey on Contextual Embeddings -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/cs.CL/paper_9.html">
      A Survey on Contextual Embeddings
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      コンテキスト埋め込みにより、各単語にそのコンテキストに基づいた表現が割り当てられるため、さまざまなコンテキストで単語の使用をキャプチャし、言語間で伝達される知識をエンコードします。ELMoやBERTなどのコンテキスト埋め込みは、Word2Vecのようなグローバルな単語表現を超えて画期的な成果を達成しますこの調査では、既存のコンテキスト埋め込みモデル、クロスリンガルポリグロットの事前トレーニング、ダウンストリームタスクでのコンテキスト埋め込みの適用、モデル圧縮、モデル分析を確認します。 
[ABSTRACT] embeddingsは各単語のパフォーマンスに基づいて表現を割り当てます。したがって、さまざまな状況での単語の使用をキャプチャします。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-16">
        <br>2020-03-16
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: DNN-Based Distributed Multichannel Mask Estimation for Speech
  Enhancement in Microphone Arrays -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/eess.AS/paper_0.html">
      DNN-Based Distributed Multichannel Mask Estimation for Speech
  Enhancement in Microphone Arrays
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      2つのノードの配列で、この追加信号を効率的に考慮してマスクを予測し、マスク推定がローカル信号のみに依存している場合よりも優れた音声強調パフォーマンスにつながることを示します。これらのソリューションを実世界に展開しようとすると、いくつかの制限が表示されます。このコンテキストでは、分散適応ノード固有の信号推定アプローチをニューラルネットワークフレームワークに拡張することを提案します。 
[概要]いくつかのマイクを備えた複数のデバイスは、展開の実行可能な代替手段です。グローバルマルチチャネルウィナーフィルターを計算するには、マイクの配列にアクセスできます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-13">
        <br>2020-02-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multi-modal Multi-channel Target Speech Separation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/eess.AS/paper_1.html">
      Multi-modal Multi-channel Target Speech Separation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、提案されたマルチモーダルフレームワークが、シングルモーダルおよびバイモーダルの音声分離アプローチを大幅に上回る一方で、リアルタイム処理をサポートできることを示しています。この方法は、最初に混合オーディオを音響部分空間に分解し、次にこれらの部分空間音響埋め込みを学習可能なアテンションスキームで強化するための、他のモダリティからのターゲットの情報。 
[要約]実験は大規模な視覚データセットで行われます。センサーを使用して、さまざまな種類の音声を再現できます。この方法は、音声の代わりに使用できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-16">
        <br>2020-03-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Semantic Mask for Transformer based End-to-End Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/eess.AS/paper_2.html">
      Semantic Mask for Transformer based End-to-End Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      Librispeech 960hおよびTedLium2データセットで実験を行い、E2Eモデルの範囲でテストセットで最先端のパフォーマンスを実現します。アイデアは、特定の出力トークンに対応する入力機能をマスクすることです。 、コンテキスト情報に基づいてモデルがトークンを埋めるようにモデルを奨励するために、単語または単語ピース。SpecAugmentとBERTに触発されて、この論文では、このような種類のエンドエンド（E2E）モデル。 
[要約]これは、エンコーダーの新しいパターンに基づいています-デコーダー。代わりに、ニューラルネットワークの記憶能力を利用します。このモデルは、このような華やかな形式の音声を訓練するために使用されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-06">
        <br>2019-12-06
      </time>
    </span>
  </h3>
</article>
<!-- paper0: TRANS-BLSTM: Transformer with Bidirectional LSTM for Language
  Understanding -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/eess.AS/paper_3.html">
      TRANS-BLSTM: Transformer with Bidirectional LSTM for Language
  Understanding
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちは、各トランスフォーマブロックに統合されたBLSTMレイヤーを備えたトランスフォーマーBLSTM（TRANS-BLSTM）と呼ばれる新しいアーキテクチャを提案します。 （BLSTM）は、ニューラル機械翻訳と質問応答の主要なモデリングアーキテクチャでした。TRANS-BLSTMモデルは、SQuAD 1.1開発データセットで94.01％のF1スコアを取得します。これは、最先端の結果に匹敵します。 。 
[概要]バートモデルアーキテクチャのメモリはトランスフォーマーから派生します。これら2つのモデリング手法を組み合わせて、より強力なモデルアーキテクチャを作成できます。trans-blstmモデルは一貫して精度の向上につながります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-16">
        <br>2020-03-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Interfacing PDM MEMS microphones with PFM spiking systems: Application
  for Neuromorphic Auditory Sensors -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/eess.AS/paper_4.html">
      Interfacing PDM MEMS microphones with PFM spiking systems: Application
  for Neuromorphic Auditory Sensors
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらのスパイクは、NASの入力信号を表し、アナログまたはデジタルからスパイクへの変換を回避するため、NASの時間応答が向上します。スパイクドメインでの計算には、アナログまたはデジタル表現からスパイクドメインへの信号の変換が必要です。このペーパーでは、低電力パルス密度変調（PDM）MicroElectroMechanical Systems（MEMS）マイクからの音声情報をレートコーディングされたスパイク周波数に変換するスパイクベースのシステムを紹介します。 
[概要]ニューロモーフィック聴覚センサー（nas）は、スパイクベースの信号処理ブロックを使用して、スパイクベースのスパイクをシミュレートします。システムは、低電力パルス密度補正（pdm）マイクロエレクトロメカニカルシステム（mems）マイクから音声情報をレートに変換するために使用されますコード化されたスパイク周波数
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-30">
        <br>2019-04-30
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Differential TAM receptor regulation of hepatic physiology and injury -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-17/biorxiv.physiology/paper_0.html">
      Differential TAM receptor regulation of hepatic physiology and injury
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最後に、Axlが慢性肝障害のモデルで発症する線維症を悪化させることを示します。さらに、Merシグナル伝達は、急性肝障害中に生成されるアポトーシス肝細胞の食作用に重要であり、MerとAxlは協調して作用するTAM受容体チロシンキナーゼ（RTK）MerおよびAxlは肝疾患に関与しているが、肝恒常性および傷害におけるそれらの役割の理解は限られている。 
[概要]加齢中のmerおよびaxl変異マウスの性能を調べ、肝臓損傷の4つのモデルで、merとaxkが協調して傷害を阻害することを発見しました-サイトカイン産生を誘発しました。これらの多様な効果は、 Tamの設計と実装
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-16">
        <br>2020-03-16
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
