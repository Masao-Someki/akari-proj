<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-03-18の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Multi-modal Dense Video Captioning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/cs.SD/paper_0.html">
      Multi-modal Dense Video Captioning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      具体的には、音声と音声のモダリティが高密度のビデオキャプションモデルを改善する方法を示します。ただし、特に音声と音声は、環境を理解する人間の観察者にとって重要な手がかりになります。また、これらのモダリティにビデオフレームの実質的な補完情報が含まれていることを示唆する音声コンポーネント。 
[ABSTRACT]高密度ビデオキャプションの以前の作品のほとんどは、視覚情報のみに基づいており、オーディオトラックを完全に無視します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-17">
        <br>2020-03-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Deep Attention Fusion Feature for Speech Separation with End-to-End
  Post-filter Method -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/cs.SD/paper_1.html">
      Deep Attention Fusion Feature for Speech Separation with End-to-End
  Post-filter Method
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これは完全な畳み込み音声分離ネットワークであり、入力特徴として波形を使用します。事前分離段階の目的は、混合物を事前に分離することです。ディープアテンションフュージョン機能を備えたエンドツーエンドのポストフィルター（E2EPF）が提案されています。 
[要旨]提案された方法は、時間の64％を測定するために提案されています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-17">
        <br>2020-03-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: High-Resolution Speaker Counting In Reverberant Rooms Using CRNN With
  Ambisonics Features -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/cs.SD/paper_2.html">
      High-Resolution Speaker Counting In Reverberant Rooms Using CRNN With
  Ambisonics Features
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ネットワークは、フレーム解像度でスピーカーの数を高い精度で予測できます。そのために、短期のフレーム解像度で推定を行うマルチチャネル畳み込みリカレントニューラルネットワークを使用して、スピーカーのカウント問題に対処します。音源とマイクの位置、残響、およびノイズに関する多くの異なる条件を含むシミュレートされたデータを使用して、マルチチャンネル混合で最大5人の同時スピーカーを予測します。 
[ABSTRACT]スピーカーのダイアライゼーション、分離、ローカリゼーション、およびトラッキングが前提条件です。各タイムステップでのスピーカーの数を知ることが前提条件になる場合があります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-17">
        <br>2020-03-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: High-Accuracy and Low-Latency Speech Recognition with Two-Head
  Contextual Layer Trajectory LSTM Model -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/cs.SD/paper_3.html">
      High-Accuracy and Low-Latency Speech Recognition with Two-Head
  Contextual Layer Trajectory LSTM Model
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      低レイテンシを実現するには、LSTMと比較して、1つのヘッドのレイテンシがゼロで、もう1つのヘッドのレイテンシが小さい2ヘッドcltLSTMを設計します。高精度を実現するには、コンテキストレイヤートラジェクトリLSTM（cltLSTM）を使用しますこれは、時間モデリングとターゲット分類タスクを分離し、将来のコンテキストフレームを組み込んで、正確な音響モデリングの詳細情報を取得します。Microsoftの65,000時間の匿名トレーニングデータでトレーニングし、180万語のテストセットで評価した場合提案されたトレーニング戦略を備えた-head cltLSTMモデルでは、従来のLSTM音響モデルと比較して28.2 \％の相対的なWER削減が得られ、同様の遅延が知覚されます。 
[ABSTRACT]提案されたトレーニング戦略を備えた2ヘッドcltlstmモデルを提案すると、従来のlstm音響モデルと比較して28.2％の相対的な削減が得られ、同様の知覚レイテンシー
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-17">
        <br>2020-03-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Sensitivity to Haptic-Audio Envelope Asynchrony -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/cs.SD/paper_4.html">
      Sensitivity to Haptic-Audio Envelope Asynchrony
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      すべての実験では、一定の刺激と階段法の組み合わせを使用して2つの刺激を提示し、参加者の（N = 12）タスクは2つの刺激のどちらが同期したかを特定することでした。 ADSR（Attack Decay Sustain Release）エンベロープを使用しています。また、結果は25 \％のJNDの振幅減少を示しています。 
[要旨] 4つの心理物理実験を実施しました。 3つの触覚信号を使用して2つの刺激を提示しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-27">
        <br>2019-06-27
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Emotions Don't Lie: A Deepfake Detection Method using Audio-Visual
  Affective Cues -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/cs.SD/paper_5.html">
      Emotions Don't Lie: A Deepfake Detection Method using Audio-Visual
  Affective Cues
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      シャムのネットワークアーキテクチャとトリプレット損失に触発されたディープラーニングネットワークを提案します。このアプローチをいくつかのSOTAディープフェイク検出方法と比較し、DFDCで84.4％、DF-TIMITデータセットで96.6％のビデオごとのAUCを報告しますモデルを検証するために、2つの大規模な視聴覚ディープフェイク検出データセット、DeepFake-TIMIT DatasetおよびDFDCのAUCメトリックを報告します。 
[要約]同じvideo.we内から2つのモダリティ間の類似性を抽出して分析します-ビデオauc 84. dfdcで4％、dfで96. 6％-timitデータセット
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-14">
        <br>2020-03-14
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Multi-modal Dense Video Captioning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/cs.CL/paper_0.html">
      Multi-modal Dense Video Captioning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      アブレーション研究は、これらのモダリティがビデオフレームに実質的な補足情報を含むことを示唆する音声および音声コンポーネントからのかなりの貢献を示しています。説明..高密度ビデオキャプションの以前の作品のほとんどは、視覚情報のみに基づいており、オーディオトラックを完全に無視します。 
[ABSTRACT]高密度ビデオキャプションの以前の作品のほとんどは、視覚情報のみに基づいており、オーディオトラックを完全に無視します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-17">
        <br>2020-03-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: XPersona: Evaluating Multilingual Personalized Chatbot -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/cs.CL/paper_1.html">
      XPersona: Evaluating Multilingual Personalized Chatbot
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、多言語訓練されたモデルが翻訳パイプラインよりも優れており、単一言語モデルと同等であり、複数の言語で単一のモデルを使用できるという利点があることを示しています。 ..多言語およびクロスリンガルの両方のトレーニング済みベースラインを実験し、自動評価と人間評価の両方を使用して、単一言語モデルおよび翻訳パイプラインモデルに対して評価します。 
[概要]データセットには、多言語パーソナライズエージェントを構築および評価するための英語以外の6つの異なる言語のペルソナ会話が含まれています。多言語トレーニングモデルは、単一言語モデルと同等であり、複数の言語で単一のモデルを使用できるという利点があります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-17">
        <br>2020-03-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Overview of the TREC 2019 deep learning track -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/cs.CL/paper_2.html">
      Overview of the TREC 2019 deep learning track
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ドキュメント検索タスクには、367千のトレーニングクエリを含む320万のドキュメントのコーパスがあり、43のクエリの再利用可能なテストセットを生成します。この結果の可能な説明は、大きなトレーニングデータを導入し、そのようなトレーニングを受けたディープモデルを含めたことです今年の15グループは、ディープラーニング、トランスファーラーニング、および従来のIRランキング手法のさまざまな組み合わせを使用して、合計75回の実行を提出しました。 
[概要]ディープラーニングは、人間ベースの大規模なトレーニングセットを備えた最初のトラックです。これらには2つのタスクに対応する2つのセットが含まれます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-17">
        <br>2020-03-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Adapting Deep Learning Methods for Mental Health Prediction on Social
  Media -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/cs.CL/paper_3.html">
      Adapting Deep Learning Methods for Mental Health Prediction on Social
  Media
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、モデルの限界を探り、モデルの単語レベルの注意の重みを調べることにより、分類に関連するフレーズを分析します。精神的健康は、個人の幸福にとって大きな課題となります。ソーシャルメディアユーザーの検出という課題に取り組みます。ディープラーニングベースのモデルによる精神状態。従来のアプローチからタスクへの移行。 
[要約]ソーシャルメディアのテキスト分析は、病気の深い理解に貢献し、病気の早期発見の手段を提供します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-17">
        <br>2020-03-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Rethinking Batch Normalization in Transformers -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/cs.CL/paper_4.html">
      Rethinking Batch Normalization in Transformers
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      特に、PNはIWSLT14 / WMT14で0.4 / 0.6 BLEU、PTB / WikiText-103で5.6 / 3.0 PPLでLNを上回っています。 、私たちは、（i）BNのゼロ平均正規化を緩和し、（ii）変動を安定させるためにバッチごとの統計の代わりに実行中の二次平均を組み込むことにより、この問題を解決する新しい正規化スキームであるPower Normalization（PN）を提案しますフォワードパスに実行中の統計を組み込むために、おおよそのバックプロパゲーションを使用します。 
[ABSTRACT]これは、コンピュータビジョンで広く使用されているバッチ正規化（bn）とは異なります。bnがシステム全体で単純に使用されている場合、lnと比較してbnのパフォーマンスが低い理由を理解するために、これは不安定になります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-17">
        <br>2020-03-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic
  Emotions in German and English Poetry -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/cs.CL/paper_5.html">
      PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic
  Emotions in German and English Poetry
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最後に、BERTに基づいて最初の感情分類実験を行い、ドイツ語のサブセットで最大.52 F1-microのデータを使用して、美的感情の識別が困難であることを示します。 70、将来の大規模分析のための一貫したデータセットが得られます。慎重に訓練された専門家とクラウドソーシングの両方を使用して、注釈実験でこの新しい設定を評価します。 
[要約]アート（文学など）は、より複雑で微妙な感情への関与を可能にします。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-17">
        <br>2020-03-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: High-Accuracy and Low-Latency Speech Recognition with Two-Head
  Contextual Layer Trajectory LSTM Model -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/cs.CL/paper_6.html">
      High-Accuracy and Low-Latency Speech Recognition with Two-Head
  Contextual Layer Trajectory LSTM Model
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      低レイテンシを実現するために、2つのヘッドのcltLSTMを設計します。LSTMに比べて、1つのヘッドのレイテンシはゼロで、もう1つのヘッドのレイテンシは小さくなります。トレーニングストラテジを備えた2ヘッドcltLSTMモデルは、従来のLSTM音響モデルに比べて28.2 \％の相対WER削減を実現し、同様の知覚遅延を備えています。シーケンスレベルでトレーニング戦略をさらに改善します。教師と生徒の学習。 
[ABSTRACT]提案されたトレーニング戦略を備えた2ヘッドcltlstmモデルを提案すると、従来のlstm音響モデルと比較して28.2％の相対的な削減が得られ、同様の知覚レイテンシー
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-17">
        <br>2020-03-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multi-label natural language processing to identify diagnosis and
  procedure codes from MIMIC-III inpatient notes -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/cs.CL/paper_7.html">
      Multi-label natural language processing to identify diagnosis and
  procedure codes from MIMIC-III inpatient notes
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このモデルは、上位10コードについて、87.08％の全体的な精度、85.82％のF1スコア、91.76％のAUCを達成しました。米国では、25％または2000億ドル以上の病院支出が管理費を占めています。医療コーディングと請求のためのサービスが含まれます。MIMIC-IIIデータベースからの救命救急患者の匿名化されたデータを使用し、データをサブセット化して、最も一般的な10（トップ10）および50（トップ50すべての入場の47.45％と74.12％をそれぞれカバーする手順。 
[ABSTRACT]実行されるコードの手動割り当ては、膨大で時間がかかり、エラーが発生しやすいため、請求エラーが発生します。これらの検死は、マルチラベル分類を実行することにより、臨床メモから適切な診断および手順コードを識別するために使用できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-17">
        <br>2020-03-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Recent Advances and Challenges in Task-oriented Dialog System -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/cs.CL/paper_8.html">
      Recent Advances and Challenges in Task-oriented Dialog System
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この調査は、タスク指向ダイアログシステムの今後の研究に光を当てることができると信じています。タスク指向ダイアログシステムの3つの重要なトピックについて説明します。 2）ダイアログポリシー学習のためのマルチターンダイナミクスのモデリングによるタスク完了パフォーマンスの向上、および（3）パイプラインモデルとエンドツーエンドモデルの両方でのドメインオントロジーの知識のダイアログモデルへの統合。ダイアログ評価といくつかの広く使用されているコーパス。 
[ABSTRACT]特定の方法で進歩と課題を調査します。また、人体への注目度を高める最近の進捗状況も確認します。また、将来に関する詳細情報を取得する方法も探しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-17">
        <br>2020-03-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Sensitive Data Detection and Classification in Spanish Clinical Text:
  Experiments with BERT -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/cs.CL/paper_9.html">
      Sensitive Data Detection and Classification in Spanish Clinical Text:
  Experiments with BERT
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらの進歩は、2018年にGoogleが提案したモデルであるBERTと、数百万のドキュメントで事前トレーニングされた共有言語モデルによって最も顕著に導かれました。大規模なデジタルデータ処理は、幅広い機会と利点を提供します個人データのプライバシーを危険にさらす..実験は、一般的なドメインの事前トレーニングを備えた単純なBERTベースのモデルが、ドメイン固有の機能エンジニアリングなしで非常に競争力のある結果を得ることを示しています。 
[ABSTRACT]匿名化は、データから機密情報を削除または置換することで構成され、個人のプライバシーを保護しながら、さまざまな目的での悪用を可能にします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-06">
        <br>2020-03-06
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Video2Commonsense: Generating Commonsense Descriptions to Enrich Video
  Captioning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/cs.CL/paper_10.html">
      Video2Commonsense: Generating Commonsense Descriptions to Enrich Video
  Captioning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      および効果（アクションによって世界がどのように変化するか、他のエージェントに対するアクションの効果）。意図などの潜在的な側面を記述するために、ビデオから直接\ textit {commonsense}キャプションを生成する最初の作品を紹介します。属性、およびエフェクト。生成タスクとQAタスクの両方を使用して、ビデオキャプションを充実させることができます。 
[ABSTRACT] video-to-commonsense（v2c）には、さまざまなアクションを実行する人間エージェントの9kビデオが含まれます。このビデオは、新しいデータセットに基づいています。ビデオ
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-11">
        <br>2020-03-11
      </time>
    </span>
  </h3>
</article>
<!-- paper0: FRAGE: Frequency-Agnostic Word Representation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/cs.CL/paper_11.html">
      FRAGE: Frequency-Agnostic Word Representation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      単語の類似性、言語モデリング、機械翻訳、テキスト分類など、4つの自然言語処理タスクにわたって10個のデータセットに関する包括的な調査を実施しました。 ..結果は、FRAGEを使用すると、すべてのタスクでベースラインよりも高いパフォーマンスを達成することを示しています。 
[要約]高頻度語と低頻度語の埋め込みは、埋め込みスペースの異なるサブ領域にあります。埋め込みは、それらが意味的に類似していても、互いに遠くなる可能性があります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-09-18">
        <br>2018-09-18
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Multi-modal Dense Video Captioning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/eess.AS/paper_0.html">
      Multi-modal Dense Video Captioning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      高密度ビデオキャプションは、トリミングされていないビデオから興味深いイベントをローカライズし、ローカライズされた各イベントのテキスト記述（キャプション）を生成するタスクです。機械翻訳の問題として、最近提案されたTransformerアーキテクチャを利用して、マルチモーダル入力データをテキスト記述に変換します。 
[ABSTRACT]高密度ビデオキャプションの以前の作品のほとんどは、視覚情報のみに基づいており、オーディオトラックを完全に無視します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-17">
        <br>2020-03-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Deep Attention Fusion Feature for Speech Separation with End-to-End
  Post-filter Method -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/eess.AS/paper_1.html">
      Deep Attention Fusion Feature for Speech Separation with End-to-End
  Post-filter Method
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      事前分離法と比較して、提案された方法は、スケール不変の信号源対雑音比（SI-SNR）、信号対歪み比（64.1％、60.2％、25.6％、7.5％の相対的な改善を得ることができます。 SDR）、音声品質の知覚的評価（PESQ）および短時間の客観的了解度（STOI）の測定値、それぞれ。時間領域の信号。完全な畳み込み音声分離ネットワークであり、入力特徴として波形を使用します。 
[要旨]提案された方法は、時間の64％を測定するために提案されています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-17">
        <br>2020-03-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: High-Resolution Speaker Counting In Reverberant Rooms Using CRNN With
  Ambisonics Features -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/eess.AS/paper_2.html">
      High-Resolution Speaker Counting In Reverberant Rooms Using CRNN With
  Ambisonics Features
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ネットワークは、フレーム解像度でスピーカーの数を高い精度で予測できます。そのため、短期のフレーム解像度で推定を行うマルチチャネル畳み込みリカレントニューラルネットワークを使用して、スピーカーのカウント問題に対処します。オーディオ録音で同時に話している人の数を推定するタスク。 
[ABSTRACT]スピーカーのダイアライゼーション、分離、ローカリゼーション、およびトラッキングが前提条件です。各タイムステップでのスピーカーの数を知ることが前提条件になる場合があります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-17">
        <br>2020-03-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: High-Accuracy and Low-Latency Speech Recognition with Two-Head
  Contextual Layer Trajectory LSTM Model -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/eess.AS/paper_3.html">
      High-Accuracy and Low-Latency Speech Recognition with Two-Head
  Contextual Layer Trajectory LSTM Model
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      低レイテンシを実現するには、LSTMと比較して、1つのヘッドのレイテンシがゼロで、もう1つのヘッドのレイテンシが小さい2ヘッドcltLSTMを設計します。これは、時間モデリングとターゲット分類タスクを分離し、将来のコンテキストフレームを組み込んで、正確な音響モデリングの詳細情報を取得します。シーケンスレベルの教師と生徒の学習により、トレーニング戦略をさらに改善します。 
[ABSTRACT]提案されたトレーニング戦略を備えた2ヘッドcltlstmモデルを提案すると、従来のlstm音響モデルと比較して28.2％の相対的な削減が得られ、同様の知覚レイテンシー
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-17">
        <br>2020-03-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Sensitivity to Haptic-Audio Envelope Asynchrony -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/eess.AS/paper_4.html">
      Sensitivity to Haptic-Audio Envelope Asynchrony
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      聴覚触覚刺激は、ADSR（Attack Decay Sustain Release）エンベロープを備えたステレオ音声信号を使用して定義されました。また、結果は25 \％の振幅減少JNDを示しています。 4番目の実験では、信号の中央で振幅の減少を測定しました。 
[要旨] 4つの心理物理実験を実施しました。 3つの触覚信号を使用して2つの刺激を提示しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-27">
        <br>2019-06-27
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: From work stress to disease: A computational model -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-18/biorxiv.physiology/paper_0.html">
      From work stress to disease: A computational model
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      具体的には、研究1の結果は、モデルがコルチゾールレベル（グループレベルと個人レベルの両方）の毎日の変動に関する確立された調査結果を正確に再現できることを示唆しました。仕事のストレスと病気の因果関係を理解するために、計算この関係のモデル。シミュレーション研究により、その後、以前の実証研究からの主要な調査結果を再現するモデルの能力を調べました。 
[要約]研究は、仕事のストレスが病気を引き起こす可能性があることを示しています。これは、就業週と病気の発症との間のリンクのためです。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-17">
        <br>2020-03-17
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
