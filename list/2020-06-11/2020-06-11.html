<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-06-11の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Robust End-to-End Speaker Verification Using EEG -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/cs.SD/paper_0.html">
      Robust End-to-End Speaker Verification Using EEG
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最新のエンドツーエンドのディープラーニングモデルを使用して話者検証を実行し、ノイズの多いスピーチの結果を示します。この結果は、EEG信号が特にノイズ環境での話者検証システムの堅牢性を向上できることを示しています..このホワイトペーパーでは、脳波記録（EEG）信号機能を音声信号機能と連結するか、EEG信号機能のみを使用することにより、話者検証システムのパフォーマンスを向上できることを示します。 
[ABSTRACT]米国では、雑音の多いスピーチの結果を示しました。話者確認を実行するために、状態（eeg）深層学習モデルを使用します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-17">
        <br>2019-06-17
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Misinformation Has High Perplexity -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/cs.CL/paper_0.html">
      Misinformation Has High Perplexity
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最初に、主張との文の類似性に従って、科学およびニュースソースから信頼できる証拠を抽出します。COVID-19に関連する2つの新しいテストセットを構築します。既存のシステムと比較して有利です。第2に、抽出された証拠を使用して言語モデルを準備し、最後に、まき散らし時の混乱のスコアに基づいて、指定されたクレームの正当性を評価します。 
[ABSTRACT]バンキング解除は、covid-19アウトブレイクなどの新しいイベントの速い時間枠には適していません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-08">
        <br>2020-06-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Text Processing Like Humans Do: Visually Attacking and Shielding NLP
  Systems -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/cs.CL/paper_1.html">
      Text Processing Like Humans Do: Visually Attacking and Shielding NLP
  Systems
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ただし、シールド方法は、非攻撃シナリオで達成されたパフォーマンスよりも依然として遅れており、視覚的攻撃に対処することの難しさを示しています。次に、現在のNLPシステムに対する視覚的敵対的攻撃が文字、単語、および文に与える影響を調査します。人間とは対照的に、神経モデルと非神経モデルの両方がそのような攻撃に非常に敏感であり、パフォーマンスが最大82 \％低下していることを示すレベルタスク。次に、3つのシールド方法-ビジュアルキャラクターの埋め込み、敵対的トレーニング、およびルールベースのリカバリ---モデルの堅牢性を大幅に向上させます。 
[要約]これを、人間が非常に堅牢な設定であるnlpの新しいタイプの敵対的攻撃と見なします。単純な視覚入力の摂動とより困難な視覚入力の摂動の両方を示す実験
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-03-27">
        <br>2019-03-27
      </time>
    </span>
  </h3>
</article>
<!-- paper0: X-Stance: A Multilingual Multi-Target Dataset for Stance Detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/cs.CL/paper_2.html">
      X-Stance: A Multilingual Multi-Target Dataset for Stance Detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ターゲット全体での学習を可能にするために、各インスタンスにターゲットを表す自然な質問を付加します（たとえば、多言語BERTのベースライン結果は、ゼロショットのクロスリンガルおよびクロスターゲットのスタンス検出の転送がこのアプローチで適度に成功することを示しています。 「Xをサポートしていますか？」）。 
[ABSTRACT]データセットはドイツ語、フランス語、イタリア語のテキストで構成されています。これにより、スタンス検出のクロスリンガル評価が可能になります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-18">
        <br>2020-03-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting
  BERT -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/cs.CL/paper_3.html">
      Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting
  BERT
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの方法は、調査タスクからの直接の監視を必要とせず、調査プロセスに追加のパラメーターを導入する必要もありません。BERTに関する実験は、この方法を使用してBERTから回復された構文木が、言語学的に情報のないベースラインよりもはるかに優れていることを示しています。これらの作品では、事前トレーニング済みの言語モデル（BERTなど）を分析するためのパラメーターなしのプローブ手法を提案します。 
[ABSTRACT]単純な単純なプローブには、事前トレーニング済みの言語モデルを分析するためのツールの導入が含まれます。直接プローブツールなど、より完全にトレーニング済みの言語モデルを調べるツールが必要です。この方法は、事前トレーニング済みのトレインモデルは言語知識をエンコードします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-30">
        <br>2020-04-30
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Automatic Translating Between Ancient Chinese and Contemporary Chinese
  with Limited Aligned Corpora -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/cs.CL/paper_4.html">
      Automatic Translating Between Ancient Chinese and Contemporary Chinese
  with Limited Aligned Corpora
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      モデルの文レベルの並列トレーニングデータを構築するために、整列された文のペアが多くのトークンを共有するという事実を使用して、文に整列された古代-現代のペアを構築する教師なしアルゴリズムを提案します。整列されたコーパスに基づいて、コピーメカニズムを備えたエンドツーエンドのニューラルモデルと、古代と現代の中国語を翻訳するためのローカルな注意。実験では、教師なしアルゴリズムの提案により、文の配置について99.4％のF1スコアが達成され、翻訳モデルは古代から現代に26.95 BLEUを達成している、そして現代から古代までの36.34 BLEU。 
[ABSTRACT]現在の古代-現代中国語の対訳コーパスは文レベルと文で整列されていません-から学ぶ必要があります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-03-05">
        <br>2018-03-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: InterBERT: An Effective Multi-Modal Pretraining Approach via
  Vision-and-Language Interaction -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/cs.CL/paper_5.html">
      InterBERT: An Effective Multi-Modal Pretraining Approach via
  Vision-and-Language Interaction
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      MGMと従来の画像テキストマッチングを使用してモデルを事前トレーニングし、キャプションベースの画像検索、ゼロショット画像検索、視覚的常識推論など、一連の視覚と言語のダウンストリームタスクでモデルを微調整します。単一の単語またはオブジェクトの代わりにスパンまたは領域をモデル化するモデル。一般的なコンテキストから学習するモデルが必要です。分析は、提案されたMGMが事前トレーニングに効果的であり、マルチモーダル事前トレーニングの方法がBERTベースのモデルと比較してパフォーマンスを大幅に低下させることなく、単一モードのタスクに適応します。 
[要約]提案されたモデルにはマルチモーダル事前トレーニングが必要です。提案された設計は、異なるモダリティの情報フロー間の相互作用をモデル化できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-30">
        <br>2020-03-30
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Adaptive Transformers for Learning Multimodal Representations -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/cs.CL/paper_6.html">
      Adaptive Transformers for Learning Multimodal Representations
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、これらのアプローチが、ネットワークが入力シーケンスの複雑さ、さまざまなモダリティのスパース性の好み、およびその他の関連する現象をどのように認識するかを知るのに役立つことをさらに示します。 ..この作業では、モデルの解釈可能性と計算効率についてさらに学ぶために、適応アプローチを拡張します。 
[ABSTRACT]これらのアーキテクチャは多くの場合、過剰にパラメータ化されており、大量の計算が必要です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-15">
        <br>2020-05-15
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Listen to What You Want: Neural Network-based Universal Sound Selector -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/eess.AS/paper_0.html">
      Listen to What You Want: Neural Network-based Universal Sound Selector
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この問題は、音源分離とそれに続くAE分類の組み合わせで対処できますが、これは問題を解決する最適な方法ではありません。提案されたフレームワークは、複数の目的のAEクラスから同時に音を選択するように明示的に最適化できます。この論文では、代わりに、ユーザー指定のターゲットAEクラスが与えられた混合物からAEサウンドを直接選択できるユニバーサルサウンド選択ニューラルネットワークを提案します。 
[要約]提案されたフレームワークは、複数の望ましいaeクラスからの音を聞くことができます。これにより、1つまたは複数の構造に属するすべての音を抽出できます。これは、aesが機能しないことを確認するために使用できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-10">
        <br>2020-06-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Speaker Diarization: Using Recurrent Neural Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/eess.AS/paper_1.html">
      Speaker Diarization: Using Recurrent Neural Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このプロジェクトでは、2つのチャネルと2つのスピーカー（別のチャネル上）を使用して、指定されたオーディオファイルを分析します。具体的には、異なるタイプのニューラルネットワークを使用します。具体的には、単層パーセプトロン（SLP）、多層パーセプトロン（MLP）、リカレントニューラルネットワーク（RNN）です。 ）およびConvolution Neural Network（CNN）では、RNNを使用して$ \ sim $ 92 \％の精度を実現しています。スピーカーのダイアライゼーションは、オーディオのスピーカーを分離する問題です。 
[要旨]プロジェクトはcnn.com / githubで利用できます。システムを使用して、1人あたりの発言時間を学習します。このコードは、このプロジェクトで利用できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-10">
        <br>2020-06-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Exploring Quality and Generalizability in Parameterized Neural Audio
  Effects -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/eess.AS/paper_2.html">
      Exploring Quality and Generalizability in Parameterized Neural Audio
  Effects
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      16ビットの解像度で44.1 kHzのサンプリングレート..提示された方法のほとんどは、データセットの操作を除いて、元のモデルに比べて出力の精度がわずかであるか、まったくありません。計算効率の向上、S / N比の低下、さまざまな非線形オーディオエフェクトへの拡張など、最適化の変更。 
[ABSTRACT]このペーパーの結果は、アーキテクチャによるこれらの効果のモデリングの進歩を強調しています。これには、アナログ機器で見られるカスタマイズされた設定をエミュレートする方法に関する以前の研究が含まれます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-10">
        <br>2020-06-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Robust End-to-End Speaker Verification Using EEG -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/eess.AS/paper_3.html">
      Robust End-to-End Speaker Verification Using EEG
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの結果は、EEG信号が、特にノイズ環境において、話者検証システムの堅牢性を改善できることを示しています。最新のエンドツーエンドのディープラーニングモデルを使用して話者検証を実行し、ノイズの多い音声の結果を示します..このホワイトペーパーでは、脳波記録（EEG）信号機能を音声信号機能と連結するか、EEG信号機能のみを使用することにより、話者検証システムのパフォーマンスを向上できることを示します。 
[ABSTRACT]米国では、雑音の多いスピーチの結果を示しました。話者確認を実行するために、状態（eeg）深層学習モデルを使用します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-17">
        <br>2019-06-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Integrated Replay Spoofing-aware Text-independent Speaker Verification -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/eess.AS/paper_4.html">
      Integrated Replay Spoofing-aware Text-independent Speaker Verification
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      提案されたバックエンドアプローチは、従来の話者検証システムと比較して、統合試行の等しいエラー率の観点から21.77％の相対的な改善を示しています。最初のアプローチは、話者識別、リプレイスプーフィング検出、およびマルチを使用する統合システムを同時にトレーニングそのため、登録とテスト発話から抽出された話者の埋め込みと、テスト発話の再生検出予測を入力として使用するディープニューラルネットワークを使用したバックエンドアプローチを提案します。 
[ABSTRACT]最初のアプローチは、話者の識別、リプレイスポット検出、および共通機能を備えたマルチタスク学習を使用した統合システムをトレーニングします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-10">
        <br>2020-06-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Exploring Automatic Diagnosis of COVID-19 from Crowdsourced Respiratory
  Sound Data -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/eess.AS/paper_5.html">
      Exploring Automatic Diagnosis of COVID-19 from Crowdsourced Respiratory
  Sound Data
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      人体によって生成されたオーディオ信号（例えば、ため息、呼吸、心臓、消化、振動音）は、臨床医によって、疾患および疾患の発症の診断または進行の指標として日常的に使用されています。この論文では、大規模なデータ分析について説明します。 COVID-19の診断を支援するために収集された呼吸音の大規模クラウドソーシングデータセット。私たちの結果は、単純なバイナリ機械学習分類器でさえ、健康でCOVID-19の音を正しく分類できることを示しています。 
[要約]いくつかの初期の研究は、声と咳からのcovid-19の診断信号の検出に有望です。さらに、咳と呼吸を使用して、識別機能が呼吸器疾患からどのようにしているかを理解しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-10">
        <br>2020-06-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: HiFi-GAN: High-Fidelity Denoising and Dereverberation Based on Speech
  Deep Features in Adversarial Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/eess.AS/paper_6.html">
      HiFi-GAN: High-Fidelity Denoising and Dereverberation Based on Speech
  Deep Features in Adversarial Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      提案されたモデルは、新しい話者、新しいスピーチコンテンツ、新しい環境に一般化します。強化されたスピーチの知覚品質を改善するために、弁別器の深い特徴マッチング損失に依存します。エンドツーエンドのフィードフォワードを使用します。 WaveNetアーキテクチャ。時間ドメインと時間周波数ドメインの両方でマルチスケールの敵対的な弁別器を使用してトレーニングされています。 
[ABSTRACT]新しい用紙にhifi-gan-スタジオで録音されたような音声に録音された音声を変換するディープラーニング手法が導入されました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-10">
        <br>2020-06-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Description and Discussion on DCASE2020 Challenge Task2: Unsupervised
  Anomalous Sound Detection for Machine Condition Monitoring -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/eess.AS/paper_7.html">
      Description and Discussion on DCASE2020 Challenge Task2: Unsupervised
  Anomalous Sound Detection for Machine Condition Monitoring
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      チャレンジの提出期限後に、チャレンジの結果と提出の分析が追加されます。このタスクの主な課題は、トレーニングデータとして通常のサウンドサンプルのみが提供されているという条件下で、未知の異常なサウンドを検出することです。このペーパーでは、 DCASE 2020チャレンジタスク2の詳細。機械状態監視のための異常音の教師なし検出。 
[ABSTRACT]異常な音の検出の目的は、ターゲットマシンから放出される音が正常か異常かを識別することです
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-10">
        <br>2020-06-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Gender in Danger? Evaluating Speech Translation Technology on the
  MuST-SHE Corpus -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/eess.AS/paper_8.html">
      Gender in Danger? Evaluating Speech Translation Technology on the
  MuST-SHE Corpus
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      音声翻訳におけるジェンダーバイアスの最初の徹底的な調査を示します。i）将来の研究に役立つベンチマークのリリース、ii）2つの言語の方向に関するさまざまなテクノロジー（カスケードとエンドツーエンド）の比較（英語-イタリア語/フランス語）。音声は、性別による偏見を減らすための追加情報を提供できますか。英語のような生産的な文法上の性別のない言語から性別でマークされた言語への翻訳は、機械にとってよく知られた困難です。 
[ABSTRACT]モデルが構築されるトレーニングデータは通常、自然言語の非対称性を反映しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-10">
        <br>2020-06-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Uniphore's submission to Fearless Steps Challenge Phase-2 -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/eess.AS/paper_9.html">
      Uniphore's submission to Fearless Steps Challenge Phase-2
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      フィアレスステップチャレンジフェーズ2の音声アクティビティ検出（SAD）および話者識別（SID）タスクの監視対象システムを提案します。SIDタスクの2レベルの識別方法を提案します。上位5件の検索精度82.07％および82.42 ％は、SIDタスクのdevおよびevalセットで取得されます。 
[ABSTRACT]両方のタスクに提案されたシステムは、共通の畳み込みニューラルネットワーク（cnn）アーキテクチャを共有します。sidタスクに使用されるネットワークアーキテクチャとトレーニングステップは、悲しいタスクのものと同様です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-10">
        <br>2020-06-10
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Calculating the optimal hematocrit under the constraint of constant cardiac power -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/biorxiv.physiology/paper_0.html">
      Calculating the optimal hematocrit under the constraint of constant cardiac power
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この研究では、2つの異なる制約の下で最適なヘマトクリットを計算します-一定の駆動圧力と一定の心拍数の下で-一定の心拍数の下での最適なヘマトクリット値が0.5-0.7の範囲の通常の値より高いことを示しています。ヘマトクリット理論は、約0.3-0.5のヘマトクリット値の予測に成功しており、人間および多くの動物種で観察される通常の値と非常によく一致しています。人間および高等動物では、酸素を結合するのに十分高い赤血球濃度間のトレードオフ進化の間に急速な血流を可能にする十分に低い血液粘度が達成されました。 
[ABSTRACT]最適なヘマトクリット理論は0. 3-0. 5.5の予測に成功しています。これは、人間や多くの動物種で観察された通常の値と非常によく一致しています。高性能スポーツでの血液ドーパと血液ブーストの増加引用されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-10">
        <br>2020-06-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Visualizing ATP Dynamics in Live Mice -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/biorxiv.physiology/paper_1.html">
      Visualizing ATP Dynamics in Live Mice
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これは、ATPレベルの摂動に対する分子および形態学的応答の予測、ならびにATPホメオスタシスを制御する生理学的メカニズムの解明を通知します。これらの調査結果は、GO-ATeamマウスモデルが多種多様な機能を調査する強力なツールであるという説得力のある証拠を提供しますリアルタイムで前例のない時空間解像度を持つin vivoでの細胞ATPの測定。このモデルを使用して、3つの異なる条件下でのATPダイナミクスの生体内イメージングを容易に実施しました。心筋梗塞の進行中;心毒性薬の適用に応じて。 
[要約]バイオセンサーgo-ateamは、バイオセンサーを使用してatpレベルを制御します。これにより、定量的、高感度、非侵襲的、および異なることが可能になります。これらの調査結果は、go-eatamマウスモデルが、多種多様な機能を調査する強力なツールであるという説得力のある証拠を提供しますセルラーatp
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-10">
        <br>2020-06-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Eukarion-134 attenuates endoplasmic reticulum stress-induced mitochondrial dysfunction in human skeletal muscle cells -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-11/biorxiv.physiology/paper_2.html">
      Eukarion-134 attenuates endoplasmic reticulum stress-induced mitochondrial dysfunction in human skeletal muscle cells
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      細胞のROS生成酸化ストレスの増加は、ミトコンドリアのスーパーオキシドの変化が目立たなかったとしても、EUK-134の存在下で改善されたERストレスに応答して観察されました。全体的に、この研究は、慢性のERストレスの例では、筋炎では、ROS生成の抑制は筋力低下と機能障害の有望な治療法となる可能性があります。ERストレスはミトコンドリアの展開タンパク質応答を引き起こし、ミトコンドリア機能障害を促進します。 、EUK-134によって改善された呼吸のリン酸化、およびカップリング効率。 
[要約]研究では、骨格筋におけるerストレスのin vitroモデルにおけるros生成の影響を調べました。スーパーオキシドジスムターゼの存在下または非存在下でのerストレスインデューサーツニカマイシンの使用
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-10">
        <br>2020-06-10
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
