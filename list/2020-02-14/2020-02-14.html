<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-02-14の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Continuous Silent Speech Recognition using EEG -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.SD/paper_0.html">
      Continuous Silent Speech Recognition using EEG
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      30のユニークな文で構成される限られた英語の語彙の結果を示します。この論文では、電気脳波計（EEG）信号を使用した連続無音音声認識を検討します。連続無音音声認識の実行にEEG信号を使用する可能性を示します。 
[要約] 30個のユニークなセンテンスの限られた英語の語彙の結果を示しました。コネクショニストの時間分類（ctc）自動音声認識（asr）モデルを使用して、並列に記録されたeeg信号を翻訳しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-06">
        <br>2020-02-06
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Limitations of weak labels for embedding and tagging -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.SD/paper_1.html">
      Limitations of weak labels for embedding and tagging
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、弱いラベルは、通常、マルチラベル、不均衡なクラス、重複するイベントなど、さまざまな他の課題と混ざり合っています。.に焦点を当てたデータセットを作成します。強いラベルと弱いラベルの違い。 
[ABSTRACT]注釈を付けるにはコストが高すぎるため、通常は弱いラベルが使用されます。一部のユースケースでは、強いラベルではより良い結果が得られないことがあります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-05">
        <br>2020-02-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Dataset for measuring reading levels in India at scale -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.SD/paper_2.html">
      A Dataset for measuring reading levels in India at scale
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ASER調査の対象は50万人であるため、このデータセットはそれらの規模にまで拡大できます。インドの言語では、6〜14歳の年齢層の子供のASERデータセットを提示します。 
[概要]子どもたちの発話のデータセットはまれであるだけでなく、主に英語です。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-27">
        <br>2019-11-27
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Speech Emotion Recognition with Dual-Sequence LSTM Architecture -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.SD/paper_3.html">
      Speech Emotion Recognition with Dual-Sequence LSTM Architecture
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      各発声は、異なる時間周波数分解能でMFCC機能と2つのメルスペクトログラムに前処理されます。提案モデルでは、平均で72.7％の重み付き精度と73.3％の重みなし精度を実現します。最先端のユニモーダルモデル---そして、音声情報だけでなくテキスト情報を活用するマルチモーダルモデルに匹敵します。標準のLSTMはMFCC機能を処理しますが、デュアルシーケンスLSTM（ DS-LSTM）、2つのメルスペクトログラムを同時に処理します。 
[ABSTRACT] mfccの新しいデュアルレベルモデルは、mfcc機能と生の音声信号からのスペクトログラムの両方に基づいて感情を予測します。新しいモデルは、平均で、72。7％の重み付き精度と73.3％の重みなし精度を実現します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-20">
        <br>2019-10-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Self-supervised learning for audio-visual speaker diarization -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.SD/paper_4.html">
      Self-supervised learning for audio-visual speaker diarization
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最後に、中国語のオーディオビデオデータセットの空きを埋めるように設計された新しい大規模なオーディオビデオコーパスを導入します。それらを実際の人間とコンピューターの相互作用システムでテストすると、最良のモデルが顕著な利益をもたらすことがわかります+ 8％F1-scoresasとダイアリゼーションエラー率の削減。2つの新しい損失関数、動的トリプレット損失と多項損失を導入することにより、以前のアプローチを改善します。 
[要約]私たちは、大規模なラベリング作業なしで話者のダイアライゼーションの問題に対処するために、自己監視型のオーディオビデオアプリケーションを提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-13">
        <br>2020-02-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Within-sample variability-invariant loss for robust speaker recognition
  under noisy environments -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.SD/paper_5.html">
      Within-sample variability-invariant loss for robust speaker recognition
  under noisy environments
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、クリーンでノイズの多い発話ペアをオンザフライで生成するためのデータ準備戦略を調査します。VoxCeleb1の実験は、提案されたトレーニングフレームワークがクリーンでノイズの多い条件で話者検証システムのパフォーマンスを向上させることを示していますネットワークは、元の話者識別損失と、サンプル内の変動不変損失の補助で訓練されます。 
[要旨]論文では、「きれいな」埋め込みを学習するために話者埋め込みネットワークをトレーニングします。この戦略は、各トレーニングステップで同じクリーンな発声に対して異なるノイズの多いコピーを生成します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Deja-vu: Double Feature Presentation and Iterated Loss in Deep
  Transformer Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.SD/paper_6.html">
      Deja-vu: Double Feature Presentation and Iterated Loss in Deep
  Transformer Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このモデルの中間出力仮説を訓練するために、特徴を再利用する直前に各層に目的関数を適用します。Librispeechと大規模なビデオデータセットの両方で結果を示します。Librispeechおよび3.2では10〜20％の相対的な改善があります。 -動画の13％。私たちの動機は、部分的な仮説に照らして音響モデルが入力特性を再検討できるようにすることであるため、中間モデルの頭部と損失関数を導入します。 
[要約]たとえば、音響モデルの複数の深さで入力フィーチャを供給することを提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-23">
        <br>2019-10-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Efficient And Scalable Neural Residual Waveform Coding With
  Collaborative Quantization -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.SD/paper_7.html">
      Efficient And Scalable Neural Residual Waveform Coding With
  Collaborative Quantization
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      CQは、以前のモデルよりもはるかに高い品質を9 kbpsで達成し、さらにモデルの複雑さも低くなることを実証します。また、CQがAMR-WBおよびOpusを上回る24 kbpsまで拡張できることを示します。ニューラルネットワークですが、高度なニューラルネットワークモデルと従来の、まだ効率的でドメイン固有のデジタル信号処理方法の計算能力を統合的に橋渡しします。 
[要約] lpcの容量のコードブックと対応する残差.cqは、9 kbpsでその前身よりもはるかに高い品質を達成できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-13">
        <br>2020-02-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Evaluating Voice Conversion-based Privacy Protection against Informed
  Attackers -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.SD/paper_8.html">
      Evaluating Voice Conversion-based Privacy Protection against Informed
  Attackers
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      変換された音声の有用性は、自動音声認識によって達成される単語誤り率によって測定されますが、プライバシー保護は、最先端のiベクトルまたはxベクトルベースの話者検証によって達成される等しい誤り率の増加によって評価されます。 。3つの攻撃シナリオで、2つの周波数ワーピングベースの変換方法とディープラーニングベースの方法を比較します。結果から、音声変換スキームは、変換の種類とその方法について豊富な知識を持つ攻撃者から効果的に保護できないことがわかります。適用されますが、知識の少ない攻撃者に対する保護を提供する場合があります。 
[ABSTRACT]これらの属性は、悪意のある目的で推測および悪用される可能性があります。これらには、音声クローニング、art、spoofing、またはxが含まれます。これらの属性は、最新の情報に使用されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-10">
        <br>2019-11-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Identifying Audio Adversarial Examples via Anomalous Pattern Detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.SD/paper_9.html">
      Identifying Audio Adversarial Examples via Anomalous Pattern Detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ディープニューラルネットワークに基づく音声処理モデルは、敵の音声波形が良性サンプルと99.9％類似している場合でも、敵の攻撃を受けやすい。これらのモデルの活性化空間で異常パターン検出技術を適用することにより、オーディオ処理システムに対する現在の最先端の攻撃は、ノードの一部のサブセットで予想よりも高い活性化を体系的に引き起こし、良性サンプルのパフォーマンスを低下させることなく、最大0.98のAUCでこれらを検出できます。 。DNNベースの音声認識システムの幅広いアプリケーションを考えると、敵対的な例の存在を検出することは、実用的に重要です。 
[ABSTRACT]人間の活性化の敵対的な例の存在を検出します。これらは成功することが期待されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-13">
        <br>2020-02-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Comparison of user models based on GMM-UBM and i-vectors for speech,
  handwriting, and gait assessment of Parkinson's disease patients -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.SD/paper_10.html">
      Comparison of user models based on GMM-UBM and i-vectors for speech,
  handwriting, and gait assessment of Parkinson's disease patients
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      一方、ガウス混合モデルに基づくユーザーモデル-ユニバーサルバックグラウンドモデル（GMM-UBM）およびiベクトルは、特定のスピーカー特性をモデル化できるため、スピーカー検証などの生体認証アプリケーションの最先端と見なされます..結果は、患者の神経学的状態の評価における信号の各タイプからの異なる特徴セットの重要性を示しています。パーキンソン病は、異なる運動障害の存在を特徴とする神経変性障害です。 
[要旨]音声、手書き、歩行モデルからの情報は、パーキンソン病患者の神経学的状態を評価するために考慮されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-13">
        <br>2020-02-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: End-to-End Multi-speaker Speech Recognition with Transformer -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.SD/paper_11.html">
      End-to-End Multi-speaker Speech Recognition with Transformer
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      モデルアーキテクチャの改善に加えて、外部残響除去前処理、加重予測誤差（WPE）を組み込んで、モデルが残響信号を処理できるようにします。まず、音声認識モデルのRNNベースのエンコーダーデコーダーをTransformerに置き換えます。アーキテクチャ..次に、マルチチャネルの場合、ニューラルビームフォーマのマスキングネットワークでトランスフォーマーを使用するために、計算を削減するためにシーケンス全体ではなくセグメントに制限されるように自己注意コンポーネントを変更します。 
[概要]この作業では、2つの側面に焦点を当ててこれらのタスクにトランスモデルを使用する方法を検討します。最初に、計算を減らすためにシーケンス全体ではなくセグメントに制限されるように自己注意コンポーネントを変更します。 、自己制御コンポーネントを変更して、ニューラルビームフォーマのマスキングネットワークで使用します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-10">
        <br>2020-02-10
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Variational Template Machine for Data-to-Text Generation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.CL/paper_0.html">
      Variational Template Machine for Data-to-Text Generation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このようなテンプレートを学習することは、多くの場合大きなペアを必要とするため、法外です。 <table, description>コーパスはめったに利用できません。さまざまなドメインのデータセットの実験により、VTMは優れた流encyさと品質を維持しながら、より多様に生成できることがわかります。このペーパーでは、再利用可能な「テンプレート」をペアとペアになっていないデータ。 
[概要]このシステムは、さまざまな異なるドメインのデータセットに基づいています。流andさと品質を維持しながら、より多様に生成できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-04">
        <br>2020-02-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Hybrid Neural Tagging Model for Open Relation Extraction -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.CL/paper_1.html">
      Hybrid Neural Tagging Model for Open Relation Extraction
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このペーパーでは、これらの困難を克服するために、大規模で高品質のトレーニングコーパスを完全に自動化された方法で構築し、OREタスクをシーケンスタギング処理に変換するのに役立つタグ付けスキームを設計します。モデルが文レベルのセマンティクスをグローバルな観点として取り入れ、同時に顕著なローカル機能を実装してスパースアノテーションを実現できるように、相互に補完します。さまざまなテストセットでの実験結果は、モデルが従来の方法または他のニューラルモデルと比較した最新のパフォーマンス。 
[ABSTRACT]従来の方法は非効率的またはエラーです-カスケード。これらには、開放関係抽出の特性に適応する特定のニューラルアーキテクチャを見つけることが含まれます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-07-26">
        <br>2019-07-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: E3: Entailment-driven Extracting and Editing for Conversational Machine
  Reading -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.CL/paper_2.html">
      E3: Entailment-driven Extracting and Editing for Conversational Machine
  Reading
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最近導入されたShARC会話型機械読み取りデータセットで、私たちの含意駆動型抽出および編集ネットワーク（E3）は、新しい最先端の既存システムを上回る新しいBERTベースのベースラインを達成します。 ）会話型機械読み取りシステムは、ユーザーが高レベルの質問に答えるのに役立ちます（たとえば
[要約]これらの規則は手順テキストの形式でのみ提供されることが重要です）
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-12">
        <br>2019-06-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Comparison of Turkish Word Representations Trained on Different
  Morphological Forms -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.CL/paper_3.html">
      Comparison of Turkish Word Representations Trained on Different
  Morphological Forms
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本研究では、形態学的に豊富な言語であるトルコ語で形態学的に異なる形式のテキストを作成し、異なる内因性および外因性タスクの結果を比較しました。一般的に研究されている言語は同様の形態学的構造を持っているため、形態学的に豊富な言語で発生する問題は主に研究では無視されます。形態学的構造の効果を確認するために、補題と接尾辞が異なる方法で扱われるテキストでword2vecモデルをトレーニングしました。 
[ABSTRACT]大きなコーパスでトレーニングされた埋め込みは、さまざまなnlpタスクで使用される有意義な関係を提供します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-13">
        <br>2020-02-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Pre-Training for Query Rewriting in A Spoken Language Understanding
  System -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.CL/paper_4.html">
      Pre-Training for Query Rewriting in A Spoken Language Understanding
  System
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの実験は、事前トレーニングが豊富な事前情報を提供し、QRタスクが強力なパフォーマンスを達成するのを助けることを示しています。音声認識エラー、言語理解エラー、エンティティ解決エラーなどのソース。最後に、事前トレーニング後、すべての完全なトレーニングにより、QRモデルを微調整して強力なベースラインよりも優れたパフォーマンスを発揮するために、書き換えペアの小さなセットで十分であることがわかりましたQRトレーニングデータ。 
[概要]言語言語言語システムは現在、新しいシステムをテストしています。言語システムを使用して再到達することは既に提案されています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-13">
        <br>2020-02-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Non-Autoregressive Neural Dialogue Generation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.CL/paper_5.html">
      Non-Autoregressive Neural Dialogue Generation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ターゲットトークンは非AR生成で独立して生成されるため、各ターゲットワードの$ p（x | y）$は生成されるとすぐに計算でき、シーケンス全体の完了を待つ必要はありません。これは当然です。デコードにおける非グローバル最適問題を解決します。経験的には、$ p（y | x）$が与えられるとN-bestリストが最初に生成され、次に$ p（x | y）$がN-bestリストの再ランク付けに使用されます、必然的に非グローバルに最適なソリューションになります。 
[要旨]論文では、非プロキシモデルを使用してx-グローバルな最適性の問題に対処することを提案しています。これは、直接デコードをデコードする必要があるという事実によるものです。 -回帰（非ar）生成モデル
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-11">
        <br>2020-02-11
      </time>
    </span>
  </h3>
</article>
<!-- paper0: SuperGLUE: A Stickier Benchmark for General-Purpose Language
  Understanding Systems -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.CL/paper_6.html">
      SuperGLUE: A Stickier Benchmark for General-Purpose Language
  Understanding Systems
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このホワイトペーパーでは、GLUEに準拠した新しいベンチマークであるSuperGLUEを紹介します。これは、より難しい言語理解タスクの新しいセット、ソフトウェアツールキット、およびパブリックリーダーボードです。SuperGLUEはsuper.gluebenchmark.comで入手できます。事前学習と転移学習のための新しいモデルと方法は、言語理解タスクの範囲全体で顕著なパフォーマンスの改善を促進しました。 
[概要] 1年ほど前に導入された接着剤のベンチマークは、ベンチマークに関する単一の数値metric.performanceを提供し、最近は非専門家のレベルを超えました。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-05-02">
        <br>2019-05-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Delving Deeper into the Decoder for Video Captioning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.CL/paper_7.html">
      Delving Deeper into the Decoder for Video Captioning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      第二に、検証セットでモデルのパフォーマンスを評価して、テストに最適なチェックポイントを選択するための新しいオンライン手法が提案されます。まず、変動ドロップアウトとレイヤー正規化の組み合わせがリカレントユニットに組み込まれて軽減されますオーバーフィッティングの問題。MicrosoftResearch Video Description Corpus（MSVD）およびMSR-Video to Text（MSR-VTT）データセットの実験で、BLEU、CIDEr、METEOR、ROUGEによって評価されたモデルが最高の結果を達成したことが実証されています-以前の最先端モデルと比較して、MSVDで最大18％、MSR-VTTで3.5％の大幅な増加を伴うLメトリック。 
[要約]エンコーダー-デコーダーフレームワークは、近年このタスクで最も人気のあるツールです。モデルのパフォーマンスを改善するための3つの手法があります。モデルを検証セットに評価する新しいオンライン手法が提案されています。テストに最適なチェックポイント
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-16">
        <br>2020-01-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Deja-vu: Double Feature Presentation and Iterated Loss in Deep
  Transformer Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.CL/paper_8.html">
      Deja-vu: Double Feature Presentation and Iterated Loss in Deep
  Transformer Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      Librispeechと大規模なビデオデータセットの両方で結果を示します。Librispeechの場合は10〜20％、ビデオの場合は3.2〜13％の相対的な改善があります。このモデルの中間出力仮説を訓練するために、特徴の再利用の直前に各層で目的関数を適用します。 
[要約]たとえば、音響モデルの複数の深さで入力フィーチャを供給することを提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-23">
        <br>2019-10-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural
  Language Processing -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.CL/paper_9.html">
      GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural
  Language Processing
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      MXNetエコシステムを活用して、GluonCVおよびGluonNLPのディープラーニングモデルは、異なるプログラミング言語を使用するさまざまなプラットフォームに展開できます。また、効率的なカスタマイズを可能にする柔軟なビルディングブロックを備えたモジュラーAPIも提供しています。 
[概要]ディープラーニングモデルは、異なるプログラミング言語を使用してさまざまなプラットフォームに展開できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-07-09">
        <br>2019-07-09
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Evaluating Voice Conversion-based Privacy Protection against Informed
  Attackers -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.CL/paper_10.html">
      Evaluating Voice Conversion-based Privacy Protection against Informed
  Attackers
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      変換された音声の有用性は、自動音声認識によって達成される単語誤り率によって測定されますが、プライバシー保護は、最先端のiベクトルまたはxベクトルベースの話者検証によって達成される等しい誤り率の増加によって評価されます。従来の作業とは対照的に、匿名化スキームに関する攻撃者の知識に応じて、さまざまなリンケージ攻撃を設計できると主張します。 
[ABSTRACT]これらの属性は、悪意のある目的で推測および悪用される可能性があります。これらには、音声クローニング、art、spoofing、またはxが含まれます。これらの属性は、最新の情報に使用されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-10">
        <br>2019-11-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Keyphrase Extraction with Span-based Feature Representations -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.CL/paper_11.html">
      Keyphrase Extraction with Span-based Feature Representations
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このようにして、私たちのモデルは各キーフレーズの表現を取得し、より良いランキング結果を得るために1つのドキュメント内のキーフレーズ間の相互作用をキャプチャすることをさらに学習します。 。生成方法（シーケンス間ニューラルネットワークモデル）は、これらの欠点を克服するため、広く研究されており、最先端のパフォーマンスを獲得しています。 
[ABSTRACT]キーフレーズ抽出は、情報の管理、分類、および取得を容易にすることができます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-13">
        <br>2020-02-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: DeepMutation: A Neural Mutation Tool -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.CL/paper_12.html">
      DeepMutation: A Neural Mutation Tool
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このホワイトペーパーでは、2番目のポイントに取り組んで、DeepMutationを紹介します。DeepMutationは、ディープラーニングモデルを、実際の障害から学習した変異体を生成、注入、テストできる完全自動化ツールチェーンにラッピングするツールです。実際の障害を代表するミュータントを生成します。ビデオ：https://sites.google.com/view/learning-mutation/deepmutation 
[ABSTRACT]テストポイントを目的としたテストポイント。自動生成できるツールチェーンを提供する必要があります。 、注入し、変異体をテストする
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Sparse and Structured Visual Attention -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.CL/paper_13.html">
      Sparse and Structured Visual Attention
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この論文では、画像構造とテキストをより適切にリンクするために、従来のsoftmax注意メカニズムを2つの代替スパース性促進変換に置き換えます。新たに提案されたTotal-Variation Sparse Attention（TVmax）。これは、隣接する空間位置の共同選択をさらに促進します。関連性、VQA精度、改善された解釈可能性。視覚注意メカニズムは、画像キャプションや視覚質問応答（VQA）などのマルチモーダルタスクで広く使用されています。 
[ABSTRACT]研究者はlstmと変圧器のアーキテクチャを使用しました。彼らは人間の観点から注目を集めています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-13">
        <br>2020-02-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Probability Calibration for Knowledge Graph Embedding Models -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.CL/paper_14.html">
      Probability Calibration for Knowledge Graph Embedding Models
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      また、キャリブレーションされたモデルは、関係固有の決定しきい値を定義することなく、最先端の精度に達することを示します。すべてのキャリブレーション方法からキャリブレーションされていないモデルよりもはるかに良い結果が得られます。確率較正の。 
[要約]人気のある埋め込みモデルが未調整であることを示します。グランドトゥルースネガが利用できない場合にモデルを調整する新しい方法を提示します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-20">
        <br>2019-12-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Exploiting the Matching Information in the Support Set for Few Shot
  Event Classification -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.CL/paper_15.html">
      Exploiting the Matching Information in the Support Set for Few Shot
  Event Classification
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      特に、クエリのサンプルをトレーニング用のサポートセットのサンプルと照合することに加えて、サポートセット内の例をさらに照合するよう努めています。2つのベンチマークECデータセットに関する広範な実験により、提案された方法により、イベント分類の精度が最大10％の数回の学習モデル。 ECモデルが観察されていないイベントタイプに操作を拡張することを可能にしますが、この分野ではほとんどのショット学習は調査されていません。 
[要約]少数-ショット学習はこの分野では調査されていません。これにより、モデルは動作を観察されていないイベントタイプに拡張できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-13">
        <br>2020-02-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Looking Enhances Listening: Recovering Missing Speech Using Images -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.CL/paper_16.html">
      Looking Enhances Listening: Recovering Missing Speech Using Images
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      視覚的コンテキストを統合すると、マスクされた単語の回復が最大35％向上することがわかります。視覚的コンテキストを使用すると、音声がよりよく理解されます。このため、画像を使用して自動音声認識（ASR）システムを適応させる試みが数多く行われています。これらの結果は、視覚コンテキストを活用することで、エンドツーエンドのマルチモーダルASRシステムがノイズに対してより堅牢になることを示しています。 
[ABSTRACT]新しい研究では、視覚的に適応されたasrモデルは、イメージを正則化信号としてのみ使用し、セマンティックコンテンツを完全に無視することを示しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-13">
        <br>2020-02-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: End-to-End Multi-speaker Speech Recognition with Transformer -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.CL/paper_17.html">
      End-to-End Multi-speaker Speech Recognition with Transformer
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      モデルアーキテクチャの改善に加えて、外部残響除去前処理、加重予測誤差（WPE）を組み込んで、モデルが残響信号を処理できるようにします。単一チャネルおよびマルチチャネルタスクの無響状態で、それぞれ25.6％の相対的なWER削減、12.1％および6.4％のWERまで、残響の場合、当社の方法は41.5％および13.8％の相対的なWER削減を達成します2番目に、マルチチャネルの場合にニューラルビームフォーマのマスキングネットワークでトランスフォーマーを使用するために、シーケンス全体ではなくセグメントに制限されるように自己注意コンポーネントを変更します。計算を減らすため。 
[概要]この作業では、2つの側面に焦点を当ててこれらのタスクにトランスモデルを使用する方法を検討します。最初に、計算を減らすためにシーケンス全体ではなくセグメントに制限されるように自己注意コンポーネントを変更します。 、自己制御コンポーネントを変更して、ニューラルビームフォーマのマスキングネットワークで使用します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-10">
        <br>2020-02-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Sentiment Analysis Using Averaged Weighted Word Vector Features -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.CL/paper_18.html">
      Sentiment Analysis Using Averaged Weighted Word Vector Features
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この作業では、異なる種類の単語ベクトルを組み合わせてレビューの極性を学習および推定する2つの方法を開発します。これらのコメントは、製品またはサービスに関連する満足度を測定するために使用できる貴重なソースを作成します。肯定的および否定的な感度タグ付きレビューの単語頻度を使用して、このレビューベクトルに単語ベクトルを追加し、重みを追加します。 
[要約]ポジティブポジティブおよびネガティブポジティブポジティブポジティブセンシティビティを作成する方法があります-タグ付きレビュー
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-13">
        <br>2020-02-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Understanding Knowledge Distillation in Non-autoregressive Machine
  Translation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/cs.CL/paper_19.html">
      Understanding Knowledge Distillation in Non-autoregressive Machine
  Translation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      知識の蒸留により、データセットの複雑さが軽減され、NATが出力データの変動をモデル化できることがわかります。非自己回帰機械翻訳（NAT）システムは、出力トークンのシーケンスを並列に予測し、生成速度の大幅な改善を達成します知識の蒸留は経験的に有用であり、NATモデルの精度が大幅に向上しますが、この成功の理由はまだ明らかではありません。 
[ABSTRACT]既存のnatモデルは通常、知識の蒸留の手法に依存しています。これにより、事前学習済みの自己回帰モデルからトレーニングデータが作成され、パフォーマンスが向上します。natモデルの容量と蒸留モデルの最適な複雑さの間に強い相関が観察されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-07">
        <br>2019-11-07
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Continuous Silent Speech Recognition using EEG -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/eess.AS/paper_0.html">
      Continuous Silent Speech Recognition using EEG
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの結果は、連続的なサイレント音声認識を実行するためにEEG信号を使用する可能性を示しています。30のユニークな文から成る限られた英語の語彙の結果を示します。.コネクショニスト時間分類（CTC）自動音声認識（ASR）モデルを実装しました被験者が音声をテキストに変換することなく心の中で英文を読んでいる間に並行して記録された脳波信号を翻訳します
[要約] 30個のユニークなセンテンスの限られた英語の語彙の結果を示しました。コネクショニストの時間分類（ctc）自動音声認識（asr）モデルを使用して、並列に記録されたeeg信号を翻訳しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-06">
        <br>2020-02-06
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Limitations of weak labels for embedding and tagging -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/eess.AS/paper_1.html">
      Limitations of weak labels for embedding and tagging
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      アンビエントサウンド分析の多くのデータセットとアプローチでは、弱いラベルのデータを使用しますが、強いラベルと比較した場合のパフォーマンスへの弱いラベルの影響は不明のままです。さまざまな実験シナリオについて説明します。データ..さらに、通常、弱いラベルは、マルチラベル、不均衡なクラス、重複するイベントなど、さまざまな他の課題と混在しています。 
[ABSTRACT]注釈を付けるにはコストが高すぎるため、通常は弱いラベルが使用されます。一部のユースケースでは、強いラベルではより良い結果が得られないことがあります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-05">
        <br>2020-02-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Dataset for measuring reading levels in India at scale -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/eess.AS/paper_2.html">
      A Dataset for measuring reading levels in India at scale
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ASER調査は50万人の被験者にまたがることを考慮すると、このデータセットはそれらの規模に成長する可能性があります。これらのラベルは、指定されたレベルで読む子供の能力に関する専門家の意見を表しています。 
[概要]子どもたちの発話のデータセットはまれであるだけでなく、主に英語です。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-27">
        <br>2019-11-27
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Speech Emotion Recognition with Dual-Sequence LSTM Architecture -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/eess.AS/paper_3.html">
      Speech Emotion Recognition with Dual-Sequence LSTM Architecture
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      音声感情認識（SER）は、次世代のヒューマンマシンインターフェーステクノロジーの重要なコンポーネントとして登場しました。出力は、平均化されて発話の最終分類が生成されます。各発話は、MFCC機能と2つのメルスペクトログラムに前処理されます異なる時間周波数分解能で。 
[ABSTRACT] mfccの新しいデュアルレベルモデルは、mfcc機能と生の音声信号からのスペクトログラムの両方に基づいて感情を予測します。新しいモデルは、平均で、72。7％の重み付き精度と73.3％の重みなし精度を実現します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-20">
        <br>2019-10-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Self-supervised learning for audio-visual speaker diarization -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/eess.AS/paper_4.html">
      Self-supervised learning for audio-visual speaker diarization
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最後に、中国語の音声ビデオデータセットの空きを埋めるように設計された新しい大規模音声ビデオコーパスを導入します。動的なトリプレット損失と多項損失の2つの新しい損失関数を導入することにより、以前のアプローチを改善します。特定の話者の音声セグメントを見つけることは、ビデオ会議や人間とコンピュータの相互作用システムなどの人間中心のアプリケーションで広く使用されています。 
[要約]私たちは、大規模なラベリング作業なしで話者のダイアライゼーションの問題に対処するために、自己監視型のオーディオビデオアプリケーションを提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-13">
        <br>2020-02-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Frame-based overlapping speech detection using Convolutional Neural
  Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/eess.AS/paper_5.html">
      Frame-based overlapping speech detection using Convolutional Neural
  Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      提案システムは、GRIDデータセットに基づいて生成された混合音声のデータセットで、84 \％の精度と88 \％のFscoreで重複音声を予測できます。さまざまなスペクトル特徴を使用して検出性能を評価し、pyknogram特徴が優れていることを示します他の一般的に使用される音声機能。この研究では、畳み込みニューラルネットワークを使用して、25 msという短いセグメントで重複する音声の検出を調査します。 
[ABSTRACT]さまざまな色の機能を使用して検出パフォーマンスをチェックします。pyknogram機能は、他の一般的に使用される音声機能よりも優れています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-27">
        <br>2020-01-27
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Within-sample variability-invariant loss for robust speaker recognition
  under noisy environments -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/eess.AS/paper_6.html">
      Within-sample variability-invariant loss for robust speaker recognition
  under noisy environments
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      VoxCeleb1の実験は、提案されたトレーニングフレームワークがクリーンな環境とノイズの多い環境の両方で話者検証システムのパフォーマンスを改善することを示しています。さらに、オンザフライでクリーンなノイズのある発話ペアを生成するためのデータ準備戦略を調査します。各トレーニングステップで同じクリーンな発声に対して異なるノイズのあるコピーを作成し、ノイズの多い環境でスピーカー埋め込みネットワークをより一般化できるようにします。 
[要旨]論文では、「きれいな」埋め込みを学習するために話者埋め込みネットワークをトレーニングします。この戦略は、各トレーニングステップで同じクリーンな発声に対して異なるノイズの多いコピーを生成します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Deja-vu: Double Feature Presentation and Iterated Loss in Deep
  Transformer Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/eess.AS/paper_7.html">
      Deja-vu: Double Feature Presentation and Iterated Loss in Deep
  Transformer Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      Librispeechと大規模なビデオデータセットの両方で結果を示します。Librispeechの場合は10〜20％、ビデオの場合は3.2〜13％の相対的な改善が見られます。このモデルの中間出力仮説をトレーニングするために、特徴の再利用の直前に各層で目的関数を適用します。 
[要約]たとえば、音響モデルの複数の深さで入力フィーチャを供給することを提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-23">
        <br>2019-10-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Efficient And Scalable Neural Residual Waveform Coding With
  Collaborative Quantization -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/eess.AS/paper_8.html">
      Efficient And Scalable Neural Residual Waveform Coding With
  Collaborative Quantization
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      CQは、単純にLPCをニューラルネットワークに統合するだけでなく、高度なニューラルネットワークモデルと従来の、まだ効率的でドメイン固有のデジタル信号処理方法の計算能力を統合的に橋渡しします。モデルの複雑さをさらに抑えた9 kbpsの前身。また、CQがAMR-WBおよびOpusを上回る場合、最大24 kbpsに拡張できることも示しています。 
[要約] lpcの容量のコードブックと対応する残差.cqは、9 kbpsでその前身よりもはるかに高い品質を達成できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-13">
        <br>2020-02-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Evaluating Voice Conversion-based Privacy Protection against Informed
  Attackers -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/eess.AS/paper_9.html">
      Evaluating Voice Conversion-based Privacy Protection against Informed
  Attackers
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      従来の作業とは対照的に、匿名化スキームに関する攻撃者の知識に応じて、さまざまなリンケージ攻撃を設計できると主張します。変換された音声の有用性は、自動音声認識によって達成される単語誤り率によって測定され、プライバシー保護が評価されます3つの攻撃シナリオで、2つの周波数ワーピングベースの変換方法と深層学習ベースの方法を比較します。 
[ABSTRACT]これらの属性は、悪意のある目的で推測および悪用される可能性があります。これらには、音声クローニング、art、spoofing、またはxが含まれます。これらの属性は、最新の情報に使用されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-10">
        <br>2019-11-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Identifying Audio Adversarial Examples via Anomalous Pattern Detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/eess.AS/paper_10.html">
      Identifying Audio Adversarial Examples via Anomalous Pattern Detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ディープニューラルネットワークに基づく音声処理モデルは、敵の音声波形が良性サンプルと99.9％類似している場合でも、敵の攻撃を受けやすい。これらのモデルの活性化空間で異常パターン検出技術を適用することにより、オーディオ処理システムに対する現在の最先端の攻撃は、ノードの一部のサブセットで予想よりも高い活性化を体系的に引き起こし、良性サンプルのパフォーマンスを低下させることなく、最大0.98のAUCでこれらを検出できます。 。DNNベースの音声認識システムの幅広いアプリケーションを考えると、敵対的な例の存在を検出することは、実用的に重要です。 
[ABSTRACT]人間の活性化の敵対的な例の存在を検出します。これらは成功することが期待されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-13">
        <br>2020-02-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Comparison of user models based on GMM-UBM and i-vectors for speech,
  handwriting, and gait assessment of Parkinson's disease patients -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/eess.AS/paper_11.html">
      Comparison of user models based on GMM-UBM and i-vectors for speech,
  handwriting, and gait assessment of Parkinson's disease patients
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      結果は、患者の神経学的状態の評価における各種信号からの異なる特徴セットの重要性を示しています。パーキンソン病は、異なる運動障害の存在を特徴とする神経変性障害です。一方、ユーザーモデルベースガウス混合モデルについて-ユニバーサルバックグラウンドモデル（GMM-UBM）およびi-vectorは、特定のスピーカー特性をモデル化できるため、スピーカー検証などの生体認証アプリケーションの最先端と見なされます。 
[要旨]音声、手書き、歩行モデルからの情報は、パーキンソン病患者の神経学的状態を評価するために考慮されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-13">
        <br>2020-02-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: End-to-End Multi-speaker Speech Recognition with Transformer -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-14/eess.AS/paper_12.html">
      End-to-End Multi-speaker Speech Recognition with Transformer
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      次に、マルチチャネルの場合にニューラルビームフォーマのマスキングネットワークでトランスフォーマーを使用するために、計算を減らすためにシーケンス全体ではなくセグメントに制限されるように自己注意コンポーネントを変更します。音声認識モデルのRNNベースのエンコーダ/デコーダをTransformerアーキテクチャに置き換えます。最近、完全リカレントニューラルネットワーク（RNN）ベースのエンドツーエンドモデルが、両方のマルチスピーカー音声認識に効果的であることが証明されました。シングルチャネルおよびマルチチャネルのシナリオ。 
[概要]この作業では、2つの側面に焦点を当ててこれらのタスクにトランスモデルを使用する方法を検討します。最初に、計算を減らすためにシーケンス全体ではなくセグメントに制限されるように自己注意コンポーネントを変更します。 、自己制御コンポーネントを変更して、ニューラルビームフォーマのマスキングネットワークで使用します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-10">
        <br>2020-02-10
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="">
      
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本日更新された論文はありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="">
        <br>
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
