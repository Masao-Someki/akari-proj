<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
<title>Akari-2020-01-01の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
  <div class="header-logo">
    <a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
  </div>
</header>
<input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">

<div class="menubar">
<span class="bar"></span>
<span class="bar"></span>
<span class="bar"></span>
</div>

<ul>
<li><a id="home" href="../../index.html">Home</a></li>
<li><a id="about" href="../../teamAkariとは.html">About</a></li>
<li><a id="contact" href="../../contact.html">Contact</a></li>
<li><a id="contact" href="../../list/newest.html">New Papers</a></li>
<li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
</ul>

</label>
<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: nnAudio: An on-the-fly GPU Audio to Spectrogram Conversion Toolbox Using
  1D Convolution Neural Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-01/cs.SD/paper_0.html">
      nnAudio: An on-the-fly GPU Audio to Spectrogram Conversion Toolbox Using
  1D Convolution Neural Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      また、既存のCQTアルゴリズムをさらに最適化し、CQTスペクトログラムをエイリアシングなしではるかに高速な計算時間（$ 0.258 $秒からわずか$ 0.001 $秒まで）で取得できるようにします。 ）。このアプローチにより、波形からスペクトログラムへの変換レイヤーでの逆伝播も可能になります。これは、この変換プロセスをトレーニング可能にし、勾配降下法によってさらに最適化できることを意味します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-27">
        <br>2019-12-27
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: LayoutLM: Pre-training of Text and Layout for Document Image
  Understanding -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-01/cs.CL/paper_0.html">
      LayoutLM: Pre-training of Text and Layout for Document Image
  Understanding
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの知る限り、ドキュメントレベルの事前トレーニングのために単一のフレームワークでテキストとレイアウトが共同で学習されるのはこれが初めてです。これにより、ドキュメントイメージを理解するためのダウンストリームタスクのパフォーマンスが大幅に向上します。この記事では、\ textbf {LayoutLM}を使用して、スキャンされたドキュメントイメージ全体でテキストとレイアウト情報の相互作用をモデル化し、多数の実際のドキュメントに有益ですスキャンしたドキュメントからの情報抽出などの画像理解タスク。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-31">
        <br>2019-12-31
      </time>
    </span>
  </h3>
</article>
<!-- paper0: oLMpics -- On what Language Model Pre-training Captures -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-01/cs.CL/paper_1.html">
      oLMpics -- On what Language Model Pre-training Captures
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これに対処するために、ゼロショット評価（微調整なし）と、微調整LMの学習曲線を複数のコントロールの学習曲線と比較する評価プロトコルを提案します。 LM機能..基本的な課題は、タスクでのLMのパフォーマンスが、事前にトレーニングされた表現に起因するのか、タスクデータの微調整のプロセスに起因するのかを理解することです。 a）異なるLMは定性的に異なる推論能力を示します。たとえば、RoBERTaはBERTが完全に失敗する推論タスクに成功します。 （b）LMは抽象的な方法で推論せず、コンテキストに依存します。たとえば、RoBERTaは年齢を比較できますが、年齢が人間の年齢の典型的な範囲にある場合にのみそうすることができます。 （c）推論タスクの半分では、すべてのモデルが完全に失敗します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-31">
        <br>2019-12-31
      </time>
    </span>
  </h3>
</article>
<!-- paper0: OTEANN: Estimating the Transparency of Orthographies with an Artificial
  Neural Network -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-01/cs.CL/paper_2.html">
      OTEANN: Estimating the Transparency of Orthographies with an Artificial
  Neural Network
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      15の正書法で得られたスコアは、他の研究の推定値と一致しました。この研究では、人工ニューラルネットワーク（ANN）モデルを使用して、書かれた単語とその発音間の透明性を評価します。 ANN（OTEANN）..話された言語を書かれた媒体に転写するために、ほとんどのアルファベットは曖昧さのない音から文字への規則を可能にします。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-31">
        <br>2019-12-31
      </time>
    </span>
  </h3>
</article>
<!-- paper0: What Does My QA Model Know? Devising Controlled Probes using Expert
  Knowledge -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-01/cs.CL/paper_3.html">
      What Does My QA Model Know? Devising Controlled Probes using Expert
  Knowledge
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの評価は、トランスフォーマーベースのQAモデルが特定のタイプの構造語彙知識を認識する傾向があることを確認します。いくつかの基礎知識と推論の課題を伴うことが知られていますが、ベンチマークタスクで訓練されたときにモデルは実際にそのような知識を学習していますか？
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-31">
        <br>2019-12-31
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Essential Sentences for Navigating Stack Overflow Answers -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-01/cs.CL/paper_4.html">
      Essential Sentences for Navigating Stack Overflow Answers
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちは、2つの既存のアプローチを採用し、文のコンテキスト情報（たとえば、「ウィンドウを使用している」）が重要な文の識別に役立つという考えに基づいて2つの新しいアプローチを開発します。このペーパーでは、重要な文を識別するための4つの潜在的なアプローチを比較します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-31">
        <br>2019-12-31
      </time>
    </span>
  </h3>
</article>
<!-- paper0: TabFact: A Large-scale Dataset for Table-based Fact Verification -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-01/cs.CL/paper_5.html">
      TabFact: A Large-scale Dataset for Table-based Fact Verification
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      TabFactは、ソフト言語推論とハードシンボリック推論の両方を含むため、挑戦的です。データセットのデータとコードは、\ url {https://github.com/wenhuchen/Table-Fact-Checking}で提供されます。Table-BERT最先端の事前学習済み言語モデルを活用して、線形化されたテーブルとステートメントを検証用の連続ベクトルにエンコードします。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-05">
        <br>2019-09-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Revisiting Paraphrase Question Generator using Pairwise Discriminator -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-01/cs.CL/paper_6.html">
      Revisiting Paraphrase Question Generator using Pairwise Discriminator
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この論文では、文レベルの埋め込みを取得する方法を提案します。これを確実にする1つの方法は、真の言い換え埋め込みが近くなるように制約を追加し、関連性のない言い換え候補候補を遠くに追加することです。また、標準データセットでの言い換え生成およびセンチメント分析タスクで最先端のパフォーマンスを発揮します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-31">
        <br>2019-12-31
      </time>
    </span>
  </h3>
</article>
<!-- paper0: CASE: Context-Aware Semantic Expansion -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-01/cs.CL/paper_7.html">
      CASE: Context-Aware Semantic Expansion
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      以前の調査は、もしあれば、いくつかの同様のタスクのみを含み、評価には人間の注釈が必要です。この研究では、このタスクの注釈が既存のコーパスから完全に自動で収集できることを示します。コンテキストエンコーダーとアテンションスコアリング機能を適切に選択することで、競争力のある結果が得られることを示します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-31">
        <br>2019-12-31
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: nnAudio: An on-the-fly GPU Audio to Spectrogram Conversion Toolbox Using
  1D Convolution Neural Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-01/eess.AS/paper_0.html">
      nnAudio: An on-the-fly GPU Audio to Spectrogram Conversion Toolbox Using
  1D Convolution Neural Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      また、既存のCQTアルゴリズムをさらに最適化し、CQTスペクトログラムをエイリアシングなしではるかに高速な計算時間（$ 0.258 $秒からわずか$ 0.001 $秒まで）で取得できるようにします。通常、時間領域波形を周波数領域スペクトログラムに変換することはモデルのトレーニングの前に行われる事前のステップです。ディスクにスペクトログラムを保存する必要なく、オンザフライでスペクトログラムを生成できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-27">
        <br>2019-12-27
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Attention-based gated scaling adaptative acoustic model for ctc-based
  speech recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-01/eess.AS/paper_1.html">
      Attention-based gated scaling adaptative acoustic model for ctc-based
  speech recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの知る限り、この結果は、エンドツーエンドのフレームワークを使用してこのデータセットで達成された最高の認識精度です。AGSでは、メインネットワークの各隠れ層の出力は、抽出された補助ゲートマトリックスによってスケーリングされますこの論文では、注意ベースのゲートスケーリング（AGS）スキームを使用して、コネクショニスト時間分類（CTC）音響モデリングの深部特徴学習を改善する新しい適応手法を提案します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-31">
        <br>2019-12-31
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: What influences treatment response in animal models of non-alcoholic fatty liver disease? A meta-analysis with meta-regression. -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-01/biorxiv.physiology/paper_0.html">
      What influences treatment response in animal models of non-alcoholic fatty liver disease? A meta-analysis with meta-regression.
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ただし、出版バイアスを考慮した場合、これは-16％の差に減少しました。研究特性は、メタ回帰の変動のごく一部しか考慮できず、コホートの82％でバイアスのリスクが高いという以前の調査結果を再現しました。 ..古典的な医薬品開発パイプラインでは、ヒトの将来の有効性を測定するために、ヒト疾患の動物モデルを使用した研究が必要ですが、動物での成功からヒトへの変換率は比較的低いです。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-31">
        <br>2019-12-31
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
