<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-07-24の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Gated Recurrent Context: Softmax-free Attention for Online
  Encoder-Decoder Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.SD/paper_0.html">
      <font color="black">Gated Recurrent Context: Softmax-free Attention for Online
  Encoder-Decoder Speech Recognition</font>
    </a>
  </h2>
  <font color="black">多くのASR実験を通じて、提案されたオンラインアテンションテクニックのレイテンシとパフォーマンスのトレードオフが、テストフェーズでしきい値を調整するだけで制御できることを示しています。最近、アテンションベースのエンコーダーデコーダー（AED）モデルが示されています自動音声認識（ASR）における最先端のパフォーマンス。この問題に対処するために、オンラインでの注意のための新しいソフトマックスフリー注意方法とその修正された定式化を提案します。トレーニング段階。 
[要約] asrレイテンシハイパーパラダーを削減するための新しいシステムが開発されています。これらには、ハイパーパラダーを必要としないオンラインアテンションの新しいモデルが含まれています。提案された方法は、従来のグローバルアテンションとオンラインアテンションに対して競争力のあるパフォーマンスを示しました。 -エラー-レート（wers）</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-10">
        <br><font color="black">2020-07-10</font>
      </time>
    </span>
</section>
<!-- paper0: AP20-OLR Challenge: Three Tasks and Their Baselines -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.SD/paper_1.html">
      <font color="black">AP20-OLR Challenge: Three Tasks and Their Baselines</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、APSIPA Annual Summit and Conference（APSIPA ASC）とともに、言語認識システムのパフォーマンスを向上させることを目的とした5番目の東洋言語認識（OLR）チャレンジAP20-OLRを紹介します。KaldiおよびPytorchに基づいて、i-ベクトルおよびx-ベクトルシステムは、3つのタスクのベースラインとしても実施されます。3つのタスクのベースライン結果は、この課題のこれらのタスクが、より良いパフォーマンスを達成するためにより多くの努力を払う価値があることを示しています。 
[ABSTRACT]今年の課題は、依然として実用的でやりがいのある問題に焦点を当てています。3つのタスク：クロスチャネルのふた、言語認識、ap.theこれらのレシピはオンラインであり、レシピで利用可能になります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br><font color="black">2020-06-04</font>
      </time>
    </span>
</section>
<!-- paper0: Augmentation adversarial training for unsupervised speaker recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.SD/paper_2.html">
      <font color="black">Augmentation adversarial training for unsupervised speaker recognition</font>
    </a>
  </h2>
  <font color="black">オーグメンテーションは音響特性をシミュレートするため、オーグメンテーションに対して不変であるようにネットワークをトレーニングすると、ネットワークは一般にチャネル情報に対しても不変になります。しかし、発話内セグメントは同じ音響特性を共有するため、分離することは困難です。チャネル情報からの話者情報。この作業の目標は、話者ラベルなしで堅牢な話者認識モデルをトレーニングすることです。 
[ABSTRACT]作品は、対照的な学習に基づいています。この学習では、発話の埋め込みが似ていないことを奨励します。適用された拡張にバインドしながら、ネットワークをスピーカー情報に対して識別できるように訓練する拡張敵対的なトレーニング戦略が望まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Sound Field Translation and Mixed Source Model for Virtual Applications
  with Perceptual Validation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.SD/paper_3.html">
      <font color="black">Sound Field Translation and Mixed Source Model for Virtual Applications
  with Perceptual Validation</font>
    </a>
  </h2>
  <font color="black">結果として、このテクニックはリスナーのナビゲート可能な領域を制限するために残っています。映画フィルムのような非インタラクティブでリニアなエクスペリエンスは、高品質のサラウンドサウンドオーディオを提供して没入感を高めますが、リスナーのエクスペリエンスは通常、単一の音響的視点に固定されます。従来のサウンドフィールド変換技術は、記録を取り、それを仮想ソースの同等の環境に拡張します。 
[要約]提案された方法は、並進された位置での歪みに対する改善された音源定位と堅牢性の両方を提供</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Sound2Sight: Generating Visual Dynamics from Sound and Context -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.SD/paper_4.html">
      <font color="black">Sound2Sight: Generating Visual Dynamics from Sound and Context</font>
    </a>
  </h2>
  <font color="black">この埋め込みは、マルチヘッド注意ベースのオーディオビジュアルトランスフォーマーエンコーダーによって学習されます。学習された事前分布は、ビデオ予測モジュールをさらに調整して将来のフレームを生成するためにサンプリングされます。さらに、生成されたフレームの品質と一貫性を改善します。 、私たちは、合成と実際の視聴覚クリップを区別するマルチモーダル弁別器を提案します。 
[ABSTRACT]フレームごとの確率的事前学習は、オーディオと過去のフレームの同時埋め込みを条件としています。これをサンプリングして、ビデオ予測モジュールをさらに調整し、将来のフレームを生成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Sequential Routing Framework: Fully Capsule Network-based Speech
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.SD/paper_5.html">
      <font color="black">Sequential Routing Framework: Fully Capsule Network-based Speech
  Recognition</font>
    </a>
  </h2>
  <font color="black">これは、CNNベースのCTCネットワークよりも0.8％正確で、再帰型ニューラルネットワークトランスデューサー（RNN-Ts）のそれと同等です。さらに、この方法では、2つのアーキテクチャに比べて必要なパラメーターが半分未満です。 SRF、入力シーケンスはカプセル化され、ウィンドウサイズでスライスされます。 
[要約]提案された方法は82. 6％の音素認識率を達成します。2つのネットワークと比較して必要なパラメーターは半分未満です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: History Repeats Itself: Human Motion Prediction via Motion Attention -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_0.html">
      <font color="black">History Repeats Itself: Human Motion Prediction via Motion Attention</font>
    </a>
  </h2>
  <font color="black">関連する過去のモーションを集約し、グラフのたたみ込みネットワークで結果を処理することで、長期履歴のモーションパターンを効果的に活用して、将来のポーズを予測できます。Human3.6M、AMASS、および3DPWでの実験により、アプローチのメリットがわかります定期的アクションと非定期的アクションの両方に対応します。注意モデルのおかげで、3つのデータセットすべてに最新の結果が得られます。 
[ABSTRACT]人間の動きは、複雑なスポーツアクションやクッキングアクティビティでも繰り返し発生する傾向があります。しかし、既存の方法では、人間の動き自体が繰り返されているように見えるという観測をモデル化できません。現在の動きの類似性をキャプチャするために、動きの注意を抽出することを提案しますコンテキストと履歴モーションサブ-シーケンス</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Scratch that! An Evolution-based Adversarial Attack against Neural
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_1.html">
      <font color="black">Scratch that! An Evolution-based Adversarial Attack against Neural
  Networks</font>
    </a>
  </h2>
  <font color="black">MicrosoftのCognitive Services Image Captioning APIに対する攻撃を成功させ、さまざまな緩和戦略を提案します。スクラッチは、単色または複数色の直線または放物線のベジエ曲線などの多様な形状の下で効果的であることを示しています。引っかき傷が単色である極端な状況では、CIFAR-10で目標とする攻撃成功率は$ 66 \％$であり、同等の攻撃よりも桁違いに少ないクエリ数です。 
[要旨]敵が、5ドル未満をカバーするローカライズされた「敵のスクラッチ」を生成する可能性があります。画像のピクセルの％$。画像の場合、目標とする攻撃の成功率は66ドルのみです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br><font color="black">2019-12-05</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-end Learning of Compressible Features -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_2.html">
      <font color="black">End-to-end Learning of Compressible Features</font>
    </a>
  </h2>
  <font color="black">PCAに続いて量子化およびエントロピーコーディング）は、タスク固有の目的に合わせて調整されていないため、最適ではありません。機能を学習するためのタスク目的とともに圧縮率を共同で最適化する学習方法を提案します。プラグインの性質私たちの方法のおかげで、任意のターゲット目標と統合することが簡単になり、圧縮率とのトレードオフになります。 
[ABSTRACT]任意のターゲット目標と統合し、圧縮率とトレードオフするアルゴリズム</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Stochastic Super-Resolution for Downscaling Time-Evolving Atmospheric
  Fields with a Generative Adversarial Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_3.html">
      <font color="black">Stochastic Super-Resolution for Downscaling Time-Evolving Atmospheric
  Fields with a Generative Adversarial Network</font>
    </a>
  </h2>
  <font color="black">GANのソースコードは、https：//github.com/jleinonen/downscaling-rnn-ganで入手できます。GANは、両方のデータセットに対して現実的で時間的に一貫した超解像シーケンスを生成できることがわかります。GANをテストします2つのデータセットを使用します。1つはスイスからのレーダー測定降水量、もう1つは静止地球観測衛星16（GOES-16）から導出された雲の光学的厚さです。 
[ABSTRACT]条件付きガンが特定の入力に対してソリューションの集合を生成する能力は、確率的ダウンスケーリングに役立ちます。しかし、ガンの確率的性質は通常、超解像アプリケーションでは考慮されません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-20">
        <br><font color="black">2020-05-20</font>
      </time>
    </span>
</section>
<!-- paper0: Ground-truth resting-state signal provides data-driven estimation and
  correction for scanner distortion of fMRI time-series dynamics -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_4.html">
      <font color="black">Ground-truth resting-state signal provides data-driven estimation and
  correction for scanner distortion of fMRI time-series dynamics</font>
    </a>
  </h2>
  <font color="black">単一被験者レベルでのfMRI時系列ダイナミクスのスキャナー歪みを補正するために、測定対グラウンドトゥルースデータのペアのセットでたたみ込みニューラルネットワーク（CNN）をトレーニングしました。ダイナミックファントムを使用して、ボクセル単位のノイズを定量化しました。グラウンドトゥルースの時系列と測定されたfMRIデータを比較することにより、次のデータ品質メトリックを導き出しました。標準化された信号対ノイズ比（ST-SNR）とスキャナー間で直接比較できる動的忠実度。 
[ABSTRACT] fmriのファントムは、静止状態の脳と同等の動的信号を提供できます。センサーは、スキャナー全体で直接比較でき、ボクセル全体の8〜19％です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-14">
        <br><font color="black">2020-04-14</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic Analysis System of Calcaneus Radiograph: Rotation-Invariant
  Landmark Detection for Calcaneal Angle Measurement, Fracture Identification
  and Fracture Region Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_5.html">
      <font color="black">Automatic Analysis System of Calcaneus Radiograph: Rotation-Invariant
  Landmark Detection for Calcaneal Angle Measurement, Fracture Identification
  and Fracture Region Segmentation</font>
    </a>
  </h2>
  <font color="black">新しい正規化アプローチを実装することにより、RIRVメソッドは従来の回帰法と比較して明示的に回転不変性になります。このアプローチの利点は、分類とセグメンテーションを組み合わせたマルチタスク学習です。ランドマーク検出では、粗いサポートされているベクトル回帰（SVR）およびスケール不変特徴変換（SIFT）パッチ記述子に基づく-fine回転不変回帰投票（RIRV）ランドマーク検出方法。踵骨の気まぐれな回転の問題を解決します。 
[ABSTRACT]踵骨骨折は足根骨骨折で最も一般的なタイプです。骨骨骨折は骨で最も一般的であると考えられています。骨折診断を導き、骨折した踵骨の回復を促進することができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-10">
        <br><font color="black">2019-12-10</font>
      </time>
    </span>
</section>
<!-- paper0: SelfVIO: Self-Supervised Deep Monocular Visual-Inertial Odometry and
  Depth Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_6.html">
      <font color="black">SelfVIO: Self-Supervised Deep Monocular Visual-Inertial Odometry and
  Depth Estimation</font>
    </a>
  </h2>
  <font color="black">私たちは、KITTI、EuRoC、Cityscapesデータセットのパフォーマンスを最新のVIO、VO、視覚的同時ローカリゼーションおよびマッピング（VSLAM）アプローチと比較する提案されたフレームワークの包括的な定量的および定性的評価を提供します。この研究では、自己学習型ディープラーニングベースのVIOおよび深度マップリカバリアプローチ（SelfVIO）を紹介します。これには、敵対的トレーニングと自己適応型視覚慣性センサーフュージョンを使用します。 /またはIMUとカメラ間の外部キャリブレーション。 
[ABSTRACT] self-ib学習は、シーン内の幾何学的および測光の一貫性などの制約を利用して、有望な代替手段として浮上しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-22">
        <br><font color="black">2019-11-22</font>
      </time>
    </span>
</section>
<!-- paper0: Using modern motion estimation algorithms in existing video codecs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_7.html">
      <font color="black">Using modern motion estimation algorithms in existing video codecs</font>
    </a>
  </h2>
  <font color="black">新しい標準に対するこの比較のカバレッジを改善するためにすべきことはまだたくさんありますが、これらの最初の結果は、これらの調査に価値があることを示しています。このペーパーでは、さまざまなモーション推定法を使用したH.264とMP4圧縮の比較について説明します。新しい方法を使用してビデオの精度を向上させているため、選択した方法を使用して大幅な向上は見られません。これらの新しい技術は、しばらくの間ビデオ圧縮の主力であったブロックマッチングに基づいています。結果は、動きと圧縮の間の予想よりもはるかに複雑な相互関係を反映しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: A weakly supervised registration-based framework for prostate
  segmentation via the combination of statistical shape model and CNN -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_8.html">
      <font color="black">A weakly supervised registration-based framework for prostate
  segmentation via the combination of statistical shape model and CNN</font>
    </a>
  </h2>
  <font color="black">ターゲットの正確な決定は、前立腺生検、病変検出、標的療法などの前立腺インターベンションにおいて不可欠な手順です。検証結果は、9500ノードのSSMを使用したセグメンテーションフレームワークが0.904のダイスで最高のパフォーマンスを達成したことを示しています。 1.88 mmの平均表面距離。この問題に対処するため、畳み込みニューラルネットワーク（CNN）と統計的形状モデル（SSM）を組み合わせることにより、正確に前立腺をセグメンテーションするための弱く監視された登録ベースのフレームワークを提案しました。 
[要約]ニューヨークベースのニューラルネットワーク（ssm-net）を使用して、モデル変換、形状制御パラメーター、微調整センサーを予測し、前立腺境界を生成しました。結果は、2つのパブリックデータセットpromise12とnci-isbi 2013</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Show, Describe and Conclude: On Exploiting the Structure Information of
  Chest X-Ray Reports -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_9.html">
      <font color="black">Show, Describe and Conclude: On Exploiting the Structure Information of
  Chest X-Ray Reports</font>
    </a>
  </h2>
  <font color="black">最初に、FindingsとImpressionの関係を明示的にモデル化する2段階の戦略を提案します。この作業では、CXRイメージングレポートを生成するためのレポートセクション間およびレポートセクション内の構造情報を活用する新しいフレームワークを提案します。既存の研究ではめったに調査しませんこの基本的な構造情報を検討してください。 
[要約]提案されたアプローチは、構造情報を組み合わせることにより、高品質の医療レポートを生成することができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-26">
        <br><font color="black">2020-04-26</font>
      </time>
    </span>
</section>
<!-- paper0: Improving distribution and flexible quantization for DCT coefficients -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_10.html">
      <font color="black">Improving distribution and flexible quantization for DCT coefficients</font>
    </a>
  </h2>
  <font color="black">歪みだけを$ q $に最適化すると、大幅な改善がもたらされますが、より均一な分布によりエントロピーが増加します。JPEG画像圧縮のDCT-IIのようなフーリエ関連変換のAC係数は、ラプラス分布からのものであり、より一般的なEPD（指数べき分布）$ \ rho \ sim \ exp（-| x | ^ {\ kappa}）$ファミリがテストされ、最尤推定（MLE）$ \ kappa \ approxラプラス分布の代わりに0.5 $ $ \ kappa = 1 $-そのような置換により、$ \約0.1 $ビット/値の節約が得られます。特に、このような連続分布の場合、最適化された連続\ emph {量子化密度関数}による量子化アプローチについても説明されています$ q $、正則格子上の逆CDF（累積分布関数）$ Q $ $ \ {Q ^ {-1}（（i-1 / 2）/ N）：i = 1 \ ldots N \} $は量子化を行いますノード-最適化された（不均一な）量子化の柔軟で安価な選択を可能にします-レート歪みを伴うさまざまなサイズ$ N $の制御上。 
[ABSTRACT]奇数の量子化領域への標準の均一分割により、アグレッサーが大幅に低下します。標準の標準標準均一化は0 0 0.1パーセント-プラスプラスで、新しい標準レートを選択できます。標準の標準ツールより均一な速度のツールツールが役立つことがわかります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Illumination invariant hyperspectral image unmixing based on a digital
  surface model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_11.html">
      <font color="black">Illumination invariant hyperspectral image unmixing based on a digital
  surface model</font>
    </a>
  </h2>
  <font color="black">IISUは、放射輝度ハイパースペクトルデータとLiDAR派生デジタルサーフェスモデル（DSM）を使用して、アンミキシングフレームワークの可変照明とシャドウを物理的に説明する最初の試みを行います。提案されたモデルは、簡単な最適化手順によって効率的に解決されました。分離結果は、他の最先端の分離モデルが特にシェーディングされたピクセルでうまく機能しないことを示しました。 
[ABSTRACT]新しい論文は非混合モデルを提案しています。光追跡（デジタル）と呼ばれ、モデルはlightと呼ばれます。他のモデルがうまく機能しなかった方法を説明します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Regularization of Building Boundaries in Satellite Images using
  Adversarial and Regularized Losses -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_12.html">
      <font color="black">Regularization of Building Boundaries in Satellite Images using
  Adversarial and Regularized Losses</font>
    </a>
  </h2>
  <font color="black">純粋なマスクR-CNNモデルと比較して、全体的なアルゴリズムは、精度と完全性に関して同等のパフォーマンスを実現できます。ただし、不規則なフットプリントを生成するマスクR-CNNとは異なり、このフレームワークは、多くのアプリケーション..この論文では、敵対的損失と正則化損失の組み合わせで訓練された完全たたみ込みニューラルネットワークを使用して、衛星画像の境界改良と正則化を構築する方法を紹介します。 
[要約]完全に訓練されたシステムに加えて、ネットワーク全体で0.3％のスコアを達成できる</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Modeling and Enhancing Low-quality Retinal Fundus Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_13.html">
      <font color="black">Modeling and Enhancing Low-quality Retinal Fundus Images</font>
    </a>
  </h2>
  <font color="black">次に、分解モデルに基づいて、臨床指向の眼底強化ネットワーク〜（cofe-Net）〜が提案され、全体的な分解因子を抑制し、同時に臨床観察および分析のために解剖学的な網膜構造と病理学的特徴を保存します。また、眼底補正法が網膜血管セグメンテーションや視神経乳頭/カップ検出などの医療画像分析アプリケーションに役立つことも示しています。このホワイトペーパーでは、まず検眼鏡システムを分析し、主要な品質低下要因の信頼できる劣化をモデル化します。不均一な照明、ぼかし、アーティファクトを含みます。 
[要約]眼底強化ネットワークは、全体的な劣化要因を抑制し、同時に解剖学的な網膜画像を保存するために提案されています。眼底補正方法は、医療画像分析アプリケーションなどに役立ちます。 g、網膜血管のセグメンテーションと視神経乳頭/カップの検出</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-12">
        <br><font color="black">2020-05-12</font>
      </time>
    </span>
</section>
<!-- paper0: Stochastic Frequency Masking to Improve Super-Resolution and Denoising
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_14.html">
      <font color="black">Stochastic Frequency Masking to Improve Super-Resolution and Denoising
  Networks</font>
    </a>
  </h2>
  <font color="black">私たちの定式化に基づいて、ネットワークを正規化し、過剰適合問題に対処するためのトレーニングで使用される画像の確率的周波数マスキングを提案します。私たちの手法は、さまざまな合成カーネルを使用したブラインド超解像の最先端の方法を改善し、実際の超-解像度、ブラインドガウスのノイズ除去、および実画像のノイズ除去。超解像およびノイズ除去は、不適切でありながら基本的な画像復元タスクです。 
[ABSTRACT]周波数領域での劣化の分析-超解像でのカーネルの過剰適合を提示し、条件付き学習の視点を導入します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-16">
        <br><font color="black">2020-03-16</font>
      </time>
    </span>
</section>
<!-- paper0: Real-time CNN-based Segmentation Architecture for Ball Detection in a
  Single View Setup -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_15.html">
      <font color="black">Real-time CNN-based Segmentation Architecture for Ball Detection in a
  Single View Setup</font>
    </a>
  </h2>
  <font color="black">また、テスト時のデータ拡張により、検出精度が大幅に向上することも示しています。この推論モデルは、時間分析によって引き起こされる遅延なしにリアルタイムで実行できます。このペーパーでは、単一の視点からボールを検出するタスクについて考察します。ボールがプレーヤーと頻繁にやり取りしながら、背景とのコントラストが低いという、難しいが一般的な場合。 
[ABSTRACT]効率的なcnnアーキテクチャによって解決されるセグメンテーションタスクとして問題を提示することにより、新しいアプローチを提案します。システムは、時間分析によって引き起こされる遅延なしにリアルタイムで実行するように設計されています。追加の貢献として、この作業のベースとなっているデータセット</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: So you think you can DAS? A viewpoint on delay-and-sum beamforming -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_16.html">
      <font color="black">So you think you can DAS? A viewpoint on delay-and-sum beamforming</font>
    </a>
  </h2>
  <font color="black">この視点の記事では、DASビームフォーミングの基本について説明します。トランスデューサー要素の指向性からF値を決定し、遅延信号の位相分散から音速を決定することをお勧めします。特に、重要性について説明します。音のF値と速度の画質への影響、および物理的な観点からそれらの値を設定する1つの解決策を提案します。 
[ABSTRACT]システムはシンプルで、リアルタイムアプリケーションと互換性があります。トランスデューサー要素の指向性からf-数を決定できます。遅延信号の位相分散から音速</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: AttentionNAS: Spatiotemporal Attention Cell Search for Video
  Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_17.html">
      <font color="black">AttentionNAS: Spatiotemporal Attention Cell Search for Video
  Classification</font>
    </a>
  </h2>
  <font color="black">アテンションセルをI3D-R50に挿入すると、両方のデータセットで最先端のパフォーマンスが得られます。発見されたアテンションセルは、I3DやS3Dなどの既存のバックボーンネットワークにシームレスに挿入でき、ビデオ分類の精度が2以上向上します。 Kinetics-600とMiTの両方のデータセットの％。畳み込み演算には2つの制限があります。（1）すべての位置に同じフィルターが適用されるため、どこに焦点を合わせるかを明示的にモデル化しない、（2）長期依存関係のモデリングには不適切彼らは小さな近所でしか活動していないからです。 
[ABSTRACT]検索システムは、セル内のさまざまなデザインの選択肢を探索できます。これらのセルは、両方のビデオで非ローカルブロックよりも優れており、さまざまなモダリティ、バックボーン、およびデータセットにわたって強力な一般化を示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: xCos: An Explainable Cosine Metric for Face Verification Task -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_18.html">
      <font color="black">xCos: An Explainable Cosine Metric for Face Verification Task</font>
    </a>
  </h2>
  <font color="black">例外的なパフォーマンスを超えて、ディープフェース検証モデルは、それらが生成する結果を信頼できるように、より多くの解釈可能性を必要とします。LFWおよび提案された方法のさまざまな競合ベンチマークの有効性を実証し、顔検証のための新規で望ましいモデル解釈可能性を提供するだけではありません既存の顔認識モデルにプラグインすることで精度を確保することもできます。$ xCos $の助けを借りて、2つの入力顔のどの部分が類似しているか、モデルが注意を向けているか、局所的な類似性がどのように重み付けされているかを確認できます。出力$ xCos $スコアを形成します。 
[ABSTRACT]顔認証は最近の重要なタスクです。モバイルデバイスのアクセス制御、監視、自動個人ログオンなどのさまざまなアプリケーションに展開されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-11">
        <br><font color="black">2020-03-11</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Spatial-angular Regularization for Compressive Light Field
  Reconstruction over Coded Apertures -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_19.html">
      <font color="black">Deep Spatial-angular Regularization for Compressive Light Field
  Reconstruction over Coded Apertures</font>
    </a>
  </h2>
  <font color="black">具体的には、まず、圧縮LF再構成を陰的正則化項の逆問題として定式化します。次に、効率的な深い空間角畳み込みサブネットワークで正則化項を構築し、限定表現能力のない信号分布を包括的に調査します。この課題に取り組むために、我々は、学習されたコード化されたアパーチャを介した取得からの高品質LFの再構成のための新しい学習ベースのフレームワークを提案します。 
[要約]提案された方法は、深層学習フレームワークへの測定観測観測観測をエレガントに使用して、再構成のためにデータ駆動型の事前分布に完全に依存することを回避します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: TSIT: A Simple and Versatile Framework for Image-to-Image Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_20.html">
      <font color="black">TSIT: A Simple and Versatile Framework for Image-to-Image Translation</font>
    </a>
  </h2>
  <font color="black">任意のスタイル制御を備えたマルチモーダル画像合成が可能になります。体系的な研究により、提案された方法がいくつかの最先端のタスク固有のベースラインと比較され、知覚品質と定量評価の両方におけるその有効性が検証されます。正規化レイヤーの重要性。慎重に設計された2ストリーム生成モデルに、新しく提案された機能変換を粗から細に変換します。 
[要約]新たに提案された機能が融合された詳細な2ストリーム情報モデルを提供します。追加の制約は必要なく、非常にクリーンでシンプルな方法に貢献します。体系的な研究は、提案された方法をいくつかの状態-特定のベースラインと比較します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Guided Deep Decoder: Unsupervised Image Pair Fusion -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_21.html">
      <font color="black">Guided Deep Decoder: Unsupervised Image Pair Fusion</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、提案されたネットワークがさまざまな画像融合問題で最先端のパフォーマンスを達成できることを示しています。ガイダンス画像のマルチスケール機能をディープデコーダーネットワークに埋め込むための調整ユニット。 
[ABSTRACT]提案されたネットワークは、エンコーダー-デコーダーネットワークで構成されています。これにより、トレーニングデータなしで、監視されていない方法でネットワークパラメーターを最適化できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: PIPAL: a Large-Scale Image Quality Assessment Dataset for Perceptual
  Image Restoration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.IV/paper_22.html">
      <font color="black">PIPAL: a Large-Scale Image Quality Assessment Dataset for Perceptual
  Image Restoration</font>
    </a>
  </h2>
  <font color="black">実験は、提案された方法の有効性を示しています。最後に、アンチエイリアシングプーリングを導入することにより、GANベースの歪みでのIQAネットワークのパフォーマンスを改善します。PIPALに基づいて、IQAと超解像の両方の方法の新しいベンチマークを提示します。 
[要約]信頼性の高い「elo system」を使用して、1300万以上の人間の判断を収集し、pipal画像の主観的スコアを割り当てます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Web Similarity in Sets of Search Terms using Database Queries -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_0.html">
      <font color="black">Web Similarity in Sets of Search Terms using Database Queries</font>
    </a>
  </h2>
  <font color="black">NWDは、これらのデータベースからの知識を組み込んだコルモゴロフ複雑度理論に基づく新しいコンテキスト（異なるデータベース）学習アプローチを可能にします。NWDを2つのセットに制限すると、以前の正規化されたGoogle距離（NGD）が得られますが、NGDの組み合わせはありませんセット内のペアは、NWDがセットから抽出する情報を抽出できます。最後のペアは、健康被害間の新しい相関を提供します。 
[要約] nwdは、0（同一）から1（完全に異なる）までのスケールで検索語の共通の類似性を提供します。2つのセットへのnwdの制限により、以前の正規化されたGoogle距離（ngd）が得られます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2015-02-20">
        <br><font color="black">2015-02-20</font>
      </time>
    </span>
</section>
<!-- paper0: Label-similarity Curriculum Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_1.html">
      <font color="black">Label-similarity Curriculum Learning</font>
    </a>
  </h2>
  <font color="black">つまり、最初は、小さな間違いは大きな間違いよりも少なく修正され、微妙な違いを教える前に、まず広い概念を説明する教育プロセスに似ています。各クラスの確率は、ベクトル表現間のコサイン類似度の関数です。クラスと真のラベルの。クラスの類似性は、事前の知識に基づくことができます。 
[ABSTRACT]提案されたターゲット、カテゴリの類似性、ラベルの表現を標準的なものに変更します。コンセプトは類似性の事前知識とクラスのようなものに基づいています。これらの単語は標準的な単語の埋め込みを使用して空間に埋め込まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-15">
        <br><font color="black">2019-11-15</font>
      </time>
    </span>
</section>
<!-- paper0: History Repeats Itself: Human Motion Prediction via Motion Attention -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_2.html">
      <font color="black">History Repeats Itself: Human Motion Prediction via Motion Attention</font>
    </a>
  </h2>
  <font color="black">関連する過去のモーションを集約し、グラフのたたみ込みネットワークで結果を処理することで、長期履歴のモーションパターンを効果的に活用して将来のポーズを予測できます。人間のモーション予測は、過去のモーションが与えられた将来の人間のポーズを予測することを目的としています。 Human3.6M、AMASS、および3DPWの実験は、定期的および非定期的なアクションの両方に対する私たちのアプローチの利点を証明しています。 
[ABSTRACT]人間の動きは、複雑なスポーツアクションやクッキングアクティビティでも繰り返し発生する傾向があります。しかし、既存の方法では、人間の動き自体が繰り返されているように見えるという観測をモデル化できません。現在の動きの類似性をキャプチャするために、動きの注意を抽出することを提案しますコンテキストと履歴モーションサブ-シーケンス</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Funnel Activation for Visual Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_3.html">
      <font color="black">Funnel Activation for Visual Recognition</font>
    </a>
  </h2>
  <font color="black">ImageNet、COCO検出、およびセマンティックセグメンテーションタスクの実験を行い、視覚認識タスクにおけるFReLUの大幅な改善と堅牢性を示します。さらに、空間条件はピクセル単位のモデリング能力を簡単な方法で実現し、通常の畳み込み.. ReLUとPReLUの形式はそれぞれy = max（x、0）とy = max（x、px）ですが、FReLUはy = max（x、T（x））の形式です。ここで、T（x）は2D空間条件です。 
[ABSTRACT]視覚認識プロジェクトの実験を行っています。視覚認識タスクのfreluが改善されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Few-Shot Object Detection and Viewpoint Estimation for Objects in the
  Wild -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_4.html">
      <font color="black">Few-Shot Object Detection and Viewpoint Estimation for Objects in the
  Wild</font>
    </a>
  </h2>
  <font color="black">3Dデータを含め、両方のタスクに適用できるメタ学習フレームワークを提案します。ただし、サンプルが少ない新規オブジェクトカテゴリでは、パフォーマンスがまだ遅れています。この論文では、少数ショットオブジェクトの問題に取り組みます。検出および少数ショット視点推定。 
[ABSTRACT]オブジェクト検出と少数-非常に大きなベンチマークでショット視点の推定が優れた結果を達成しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: All at Once: Temporally Adaptive Multi-Frame Interpolation with Advanced
  Motion Modeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_5.html">
      <font color="black">All at Once: Temporally Adaptive Multi-Frame Interpolation with Advanced
  Motion Modeling</font>
    </a>
  </h2>
  <font color="black">最先端の方法は、一度に1フレームを補間する反復ソリューションです。時間領域でピラミッドスタイルのネットワークを利用して、ワンショットでマルチフレーム補間タスクを完了します。提案された方法は、簡単に拡張できます。ワンショットメカニズムにより効率を維持しながら、多数の新しいフレームを補間します。 
[要約]提案された方法は、ピラミッド型ネットワークを使用して、マルチフレーム補間タスクをワンショットで完了します。高速システムの需要に応じてフレームネットワークを使用します。この方法は、adobe240データセットに基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Smooth-AP: Smoothing the Path Towards Large-Scale Image Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_6.html">
      <font color="black">Smooth-AP: Smoothing the Path Towards Large-Scale Image Retrieval</font>
    </a>
  </h2>
  <font color="black">また、APのランキングベースのメトリックを直接最適化すると、他のディープメトリックの学習損失よりもメリットが大きい理由の分析も示します。Smooth-APを標準の検索ベンチマークであるStanford Online製品とVehicleIDに適用し、さらに大規模なデータセットで評価します。きめの細かいカテゴリの検索にはINaturalist、顔の検索にはVGGFace2とIJB-Cを使用します。AveragePrecision（AP）などのランク付けベースのメトリックの最適化は、微分できないため、悪名高いほど困難です。勾配降下法を使用して直接最適化することはできません。 
[ABSTRACT] smooth-apはスムーズ--apの代わりに最適化するシステムです。また、他の単純な学習損失よりもメリットがある理由の分析も提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Why Are Deep Representations Good Perceptual Quality Features? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_7.html">
      <font color="black">Why Are Deep Representations Good Perceptual Quality Features?</font>
    </a>
  </h2>
  <font color="black">この作業は、知覚品質の最適化における事前トレーニング済みの深いCNN機能の機能が、基本的な人間の視覚特性のキャプチャの成功と相関していることを示しています。特に、コントラストなどの人間の知覚の基本的な側面に分析を集中させます。感度と方向の選択性。高いスコアを受け取る事前トレーニング済みのCNN機能は、人間の品質判断の予測に優れていることを示しています。 
[ABSTRACT]これらの機能は、知覚品質を予測するのに優れています。ssimやpsnrなどの他の知覚メトリクスよりも、入力画像のより効率的な表現を提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-12-02">
        <br><font color="black">2018-12-02</font>
      </time>
    </span>
</section>
<!-- paper0: Polylidar3D -- Fast Polygon Extraction from 3D Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_8.html">
      <font color="black">Polylidar3D -- Fast Polygon Extraction from 3D Data</font>
    </a>
  </h2>
  <font color="black">結果は一貫して優れた速度と精度を示します。Polylidar3Dフロントエンドは、入力データをハーフエッジの三角形メッシュに変換します。この表現は、後続のバックエンド処理に共通レベルの入力データ抽象化を提供します。 
[要旨]グリプティックポリゴンは、障害物や穴を表す内部の切り欠きがある環境で平面を表します。polylidar3dは非常に高速であることが示されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-end Learning of Compressible Features -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_9.html">
      <font color="black">End-to-end Learning of Compressible Features</font>
    </a>
  </h2>
  <font color="black">残念ながら、生成された機能は高次元であり、保存に費用がかかります。たとえば、ビデオを処理する場合、例ごとに数十万の浮動小数点数。PCAに続いて量子化とエントロピーコーディング）は、タスク固有の目的に合わせて調整されていないため、最適ではありません。特徴を学習するためのタスク目標と共に圧縮率を共同で最適化する学習方法を提案します。 
[ABSTRACT]任意のターゲット目標と統合し、圧縮率とトレードオフするアルゴリズム</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Active Visual Information Gathering for Vision-Language Navigation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_10.html">
      <font color="black">Active Visual Information Gathering for Vision-Language Navigation</font>
    </a>
  </h2>
  <font color="black">VLNの主要な課題の1つは、あいまいな指示と環境の不十分な観察によって引き起こされる不確実性を軽減することによって、堅牢なナビゲーションをどのように実施するかです。すべてのステップ..この作業は、人間のナビゲーション動作からインスピレーションを引き出し、エージェントにアクティブな情報収集機能を提供して、よりインテリジェントな視覚言語ナビゲーションポリシーを実現します。 
[要約]探索ポリシーを学習するためのエンドツーエンド-エンドツーエンドのフレームワークを提案します。これには、あいまいな指示と環境の不十分な観察によって引き起こされる不確実性を軽減することにより、堅牢なナビゲーションを実行する方法が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: The Devil is in Classification: A Simple Framework for Long-tail
  Instance Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_11.html">
      <font color="black">The Devil is in Classification: A Simple Framework for Long-tail
  Instance Segmentation</font>
    </a>
  </h2>
  <font color="black">そのような観察に基づいて、最初に、実際にインスタンスセグメンテーション結果を向上させるロングテール分類パフォーマンスを改善するためのさまざまな手法を検討します。ベルとホイッスルがないと、最近のLVISデータセットとサンプルのテールクラスのインスタンスセグメンテーションのパフォーマンスが大幅に向上します。 COCO-LTデータセット..具体的には、最新のロングテールLVISデータセットに対する最新の2ステージインスタンスセグメンテーションモデルマスクR-CNNのパフォーマンス低下を体系的に調査し、主な原因が不正確であることを明らかにしますオブジェクト提案の分類。 
[ABSTRACT]これらのデータセットは、かなり長いテールでパフォーマンスが低下する傾向があります。これは、通常はロングテールである現実的なデータセットでパフォーマンスを向上できる傾向があるためです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Pixel-Pair Occlusion Relationship Map(P2ORM): Formulation, Inference &
  Application -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_12.html">
      <font color="black">Pixel-Pair Occlusion Relationship Map(P2ORM): Formulation, Inference &
  Application</font>
    </a>
  </h2>
  <font color="black">私たちのコードとデータは、http：//imagine.enpc.fr/~qiux/P2ORM/で入手できます。さまざまなデータセットでの実験は、このタスクで既存のデータよりもメソッドが優れていることを示しています。 、我々はまた、最先端の単眼深度推定法の性能を一貫して改善する新しい深度マップ改良法を提案します。 
[ABSTRACT]ニューヨークを拠点とする企業が、2D深度推定方法のパフォーマンスを改善するための新しい深度マップ改良方法を提案しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Implicit Latent Variable Model for Scene-Consistent Motion Forecasting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_13.html">
      <font color="black">Implicit Latent Variable Model for Scene-Consistent Motion Forecasting</font>
    </a>
  </h2>
  <font color="black">決定論的デコーダーと組み合わせて、交通参加者全体で一貫した軌道サンプルを取得し、モーション予測と相互作用の理解において最先端の結果を達成します。自動運転を計画するには、自律型車両がその環境を正確に認識する必要があります。特に、我々は、潜在的な潜在変数モデルを介して、将来の軌道上の共同分布を特徴付けることを提案します。 
[要約]このペーパーでは、センサーから直接、複雑な都市交通のシーン一貫性のあるモーション予測を学習することを目的としています。シーンを相互作用グラフとしてモデル化し、強力なグラフニューラルネットワークを使用して、シーンの分散潜在表現を学習します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Differentiable Augmentation for Data-Efficient GAN Training -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_14.html">
      <font color="black">Differentiable Augmentation for Data-Efficient GAN Training</font>
    </a>
  </h2>
  <font color="black">DiffAugmentを使用すると、ImageNet 128x128でISが100.8の最先端のFID 6.80を達成します。実験は、無条件生成とクラス条件生成の両方で、さまざまなGANアーキテクチャと損失関数に対して、メソッドの一貫した利益を示しています。 ..さらに、トレーニングデータが20％しかないため、CIFAR-10およびCIFAR-100の最高のパフォーマンスを実現できます。 
[ABSTRACT] diffaugmentにより、生成されたサンプルに微分可能な増大を採用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-18">
        <br><font color="black">2020-06-18</font>
      </time>
    </span>
</section>
<!-- paper0: Spatially Aware Multimodal Transformers for TextVQA -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_15.html">
      <font color="black">Spatially Aware Multimodal Transformers for TextVQA</font>
    </a>
  </h2>
  <font color="black">食料品の購入や公共交通機関の使用などの日常的な作業には、テキストによる手がかりが不可欠です。さらに、マルチヘッドセルフアテンションレイヤーの各ヘッドは、リレーションの異なるサブセットに焦点を当てています。既存のアプローチは、空間リレーションの使用に制限されており、シーンの空間構造を暗黙的に学習するための、完全に接続されたトランスフォーマのようなアーキテクチャ。 
[ABSTRACT]私たちのアプローチには2つの利点があります。各頭部は、すべての視覚エンティティに注意を分散させるのではなく、ローカルコンテキストを考慮します。ただし、st-vqaを含む他の方法は、絶対精度を4.2％向上させます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Anatomy-Aware Siamese Network: Exploiting Semantic Asymmetry for
  Accurate Pelvic Fracture Detection in X-ray Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_16.html">
      <font color="black">Anatomy-Aware Siamese Network: Exploiting Semantic Asymmetry for
  Accurate Pelvic Fracture Detection in X-ray Images</font>
    </a>
  </h2>
  <font color="black">限られた診断時間が救急医療で許可されている場合、視覚的に繊細でありながら病理学的に重要な骨折部位は、経験豊富な臨床医でも見逃される可能性があります。対称的な画像を全体的に分析するために空間変換層で強化されたシャムネットワークに構築する新しい骨折検出フレームワークを提案します。機能..通常の所見として左右対称の解剖学的構造を適用する視覚的な手がかりは、医療画像から微妙な異常を明確にするために臨床で広く使用されています。 
[ABSTRACT] CADメソッドでこのプラクティスを効果的にエミュレートするための不適切な研究が受けられました。このプラクティスを活用するために、これらのプラクティスを悪用することを目的としています。これらの研究は、ユニークな患者の2、359ピクセルに基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-03">
        <br><font color="black">2020-07-03</font>
      </time>
    </span>
</section>
<!-- paper0: WeightNet: Revisiting the Design Space of Weight Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_17.html">
      <font color="black">WeightNet: Revisiting the Design Space of Weight Networks</font>
    </a>
  </h2>
  <font color="black">完全に（グループ化された）完全に接続されたレイヤーで構成されるWeightNetを使用して、畳み込みウェイトを直接出力します。柔軟性のため、このメソッドはImageNetとCOCOの両方の検出タスクで既存のアプローチよりも優れており、精度-FLOPと精度-パラメータのトレードオフ.. WeightNetは、機能空間ではなくカーネル空間でトレーニングするのが簡単で、メモリを節約できます。 
[ABSTRACT]ウェイトネットは完全に（グループ化された）完全に接続されたレイヤーで構成されます。これは畳み込みウェイトを生成します。このメソッドは、imagenetとcocoの両方の検出タスクで既存のアプローチよりも優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning-based Vehicle Behaviour Prediction For Autonomous Driving
  Applications: A Review -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_18.html">
      <font color="black">Deep Learning-based Vehicle Behaviour Prediction For Autonomous Driving
  Applications: A Review</font>
    </a>
  </h2>
  <font color="black">自律車両の行動予測機能は、周辺環境の現在および過去の観測に基づいて、周辺車両の将来の状態を予測します。最初に、車両行動予測の一般的な問題の概要を示し、その課題について説明します。次に、分類と3つの基準に基づく最新のディープラーニングベースのソリューションのレビュー：入力表現、出力タイプ、および予測方法。このペーパーでは、いくつかの既知のソリューションのパフォーマンスについても説明し、文献の研究ギャップを特定し、潜在的な新しいソリューションの概要を示します。研究の方向性。 
[ABSTRACT]調査は最新のディープラーニングに基づいています-3つの基準に基づいています。車両の動作予測の課題について概説します。これには、最新の分類とレビューが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-25">
        <br><font color="black">2019-12-25</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic Analysis System of Calcaneus Radiograph: Rotation-Invariant
  Landmark Detection for Calcaneal Angle Measurement, Fracture Identification
  and Fracture Region Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_19.html">
      <font color="black">Automatic Analysis System of Calcaneus Radiograph: Rotation-Invariant
  Landmark Detection for Calcaneal Angle Measurement, Fracture Identification
  and Fracture Region Segmentation</font>
    </a>
  </h2>
  <font color="black">ランドマーク検出のために、サポートされているベクトル回帰（SVR）およびスケール不変特徴変換（SIFT）パッチ記述子に基づく粗から細かい回転不変回帰投票（RIRV）ランドマーク検出方法を提案し、浮気回転の問題を解決しました新規の正規化アプローチを実装することにより、RIRVメソッドは、従来の回帰法と比較して明示的に回転不変性になります。このアプローチの利点は、分類とセグメンテーションを組み合わせたマルチタスク学習です。 
[ABSTRACT]踵骨骨折は足根骨骨折で最も一般的なタイプです。骨骨骨折は骨で最も一般的であると考えられています。骨折診断を導き、骨折した踵骨の回復を促進することができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-10">
        <br><font color="black">2019-12-10</font>
      </time>
    </span>
</section>
<!-- paper0: Comprehensive Image Captioning via Scene Graph Decomposition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_20.html">
      <font color="black">Comprehensive Image Captioning via Scene Graph Decomposition</font>
    </a>
  </h2>
  <font color="black">包括的なキャプションモデルの利点を実証するための広範な実験を紹介します。このように、この方法では、正確で多様な固定された制御可能なキャプションを同時に考慮します。重要なサブグラフを選択し、それぞれをデコードする深いモデルを設計します選択したサブグラフを単一のターゲット文に。 
[ABSTRACT]この方法は、シーングラフの分解を一連のサブグラフに組み合わせます。モデルは、画像のさまざまな部分に対応できます。方法は、包括的なキャプションモデルに基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Bridging the Imitation Gap by Adaptive Insubordination -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_21.html">
      <font color="black">Bridging the Imitation Gap by Adaptive Insubordination</font>
    </a>
  </h2>
  <font color="black">これらのタスクにより適切に対処し、模倣のギャップを軽減するために、トレーニング中に模倣と報酬ベースの強化学習の損失を動的に再重み付けし、模倣と探索の切り替えを可能にする「適応不従順」（ADVISOR）を提案します。一連の挑戦的なタスクでは、 ADVISORは、純粋な模倣、純粋な強化学習、およびこれらのアプローチの順次の組み合わせよりも優れていることを示しています。探索スキルと記憶スキルを頻繁に切り替える必要があるタスクでは、多くの場合、うまくいくものの段階的な進行は失敗します。 
[ABSTRACT]ライター：エキスパートが使用した情報は学習したエージェントポリシーでは不十分であることを示しています。探索スキルと記憶スキルを頻繁に切り替える必要があるタスクでは、段階的な進行が失敗すると言います。ライター：模倣と強化学習は学習に役立ちます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: SelfVIO: Self-Supervised Deep Monocular Visual-Inertial Odometry and
  Depth Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_22.html">
      <font color="black">SelfVIO: Self-Supervised Deep Monocular Visual-Inertial Odometry and
  Depth Estimation</font>
    </a>
  </h2>
  <font color="black">過去10年間で、視覚慣性オドメトリ（VIO）と深度マップの推定のために、大量のラベル付きデータを必要とする教師ありディープラーニングアプローチが数多く提案されています。データ制限を克服するため、有望な代替手段として自己教師あり学習が登場しました。シーン内の幾何学的および測光の一貫性などの制約を利用します。この研究では、敵対的なトレーニングと自己適応型視覚慣性センサーの融合を使用した、新しい自己教師ありディープラーニングベースのVIOおよび深度マップ回復アプローチ（SelfVIO）を紹介します。 
[ABSTRACT] self-ib学習は、シーン内の幾何学的および測光の一貫性などの制約を利用して、有望な代替手段として浮上しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-22">
        <br><font color="black">2019-11-22</font>
      </time>
    </span>
</section>
<!-- paper0: MuCAN: Multi-Correspondence Aggregation Network for Video
  Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_23.html">
      <font color="black">MuCAN: Multi-Correspondence Aggregation Network for Video
  Super-Resolution</font>
    </a>
  </h2>
  <font color="black">ただし、既存のVSRメソッドにはいくつかの制限があります。ただし、フロー推定自体はエラーが発生しやすく、リカバリ結果に影響を与えます。これらの調査結果に基づいて、フレーム全体で同様のパッチを活用するための時間的マルチ対応集約戦略を提案します。スケールをまたがる画像の自己相似性を調査するためのクロススケール非ローカル対応集計方式。 
[ABSTRACT]マルチフレームからイントラフレームは、悪用の主要なソースです。自然画像には類似のパターンが存在します。これらのパターンは、vsrタスクではほとんど悪用されません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Structured Landmark Detection via Topology-Adapting Deep Graph Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_24.html">
      <font color="black">Structured Landmark Detection via Topology-Adapting Deep Graph Learning</font>
    </a>
  </h2>
  <font color="black">提案された方法は、ローカル画像機能とグローバル形状機能の両方を活用してグラフ信号を構築します。適応グラフトポロジは、2つのグラフ畳み込みネットワーク（GCN）でエンドツーエンドで学習されるタスク固有の構造を自然に探索し、着陸します。 3つの公開顔画像データセット（WFLW、300W、COFW-68）と3つの実際のX線医療データセット（頭部計測（公開）、手および骨盤）で行われます。 
[要約]提案された方法は、ローカル画像機能とグローバル形状機能の両方を活用してグラフ信号を構築します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-17">
        <br><font color="black">2020-04-17</font>
      </time>
    </span>
</section>
<!-- paper0: Autonomous Removal of Perspective Distortion based on Detection Results
  of Robotic Elevator Button Corner -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_25.html">
      <font color="black">Autonomous Removal of Perspective Distortion based on Detection Results
  of Robotic Elevator Button Corner</font>
    </a>
  </h2>
  <font color="black">この作業では、ボタンコーナーの検出結果に基づいて、エレベータパネル画像の遠近歪みを自動的に補正できる新しいアルゴリズムを提案します。このアルゴリズムは、従来の幾何学的アプローチよりも外れ値とノイズに対して従来の幾何学的アプローチよりもはるかにロバストです単一の画像を自律的に実行します。エレベーターボタンの認識は、エレベーターの自律運転を実現するための重要な機能です。 
[ABSTRACT]ボタン共有者は、ボタンボタンの機能を簡単に識別できます。アルゴリズムは、さまざまな画角を使用してボタンを識別します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: A weakly supervised registration-based framework for prostate
  segmentation via the combination of statistical shape model and CNN -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_26.html">
      <font color="black">A weakly supervised registration-based framework for prostate
  segmentation via the combination of statistical shape model and CNN</font>
    </a>
  </h2>
  <font color="black">ターゲットの正確な決定は、前立腺生検、病変検出、標的療法などの前立腺インターベンションに不可欠な手順です。この問題に対処するため、畳み込みニューラルネットワークを組み合わせることにより、正確に前立腺をセグメンテーションするための弱く監視された登録ベースのフレームワークを提案しました（CNN）と統計的形状モデル（SSM）。結果として、両方の要素が描写精度を改善し、ダイスはそれぞれ10％と7％増加しました。 
[要約]ニューヨークベースのニューラルネットワーク（ssm-net）を使用して、モデル変換、形状制御パラメーター、微調整センサーを予測し、前立腺境界を生成しました。結果は、2つのパブリックデータセットpromise12とnci-isbi 2013</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Show, Describe and Conclude: On Exploiting the Structure Information of
  Chest X-Ray Reports -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_27.html">
      <font color="black">Show, Describe and Conclude: On Exploiting the Structure Information of
  Chest X-Ray Reports</font>
    </a>
  </h2>
  <font color="black">最初に、FindingsとImpressionの関係を明示的にモデル化する2段階の戦略を提案します。この作業では、CXRイメージングレポートを生成するためのレポートセクション間およびレポートセクション内の構造情報を活用する新しいフレームワークを提案します。既存の研究ではめったに調査しませんこの基本的な構造情報を検討してください。 
[要約]提案されたアプローチは、構造情報を組み合わせることにより、高品質の医療レポートを生成することができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-26">
        <br><font color="black">2020-04-26</font>
      </time>
    </span>
</section>
<!-- paper0: Enhanced Transfer Learning for Autonomous Driving with Systematic
  Accident Simulation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_28.html">
      <font color="black">Enhanced Transfer Learning for Autonomous Driving with Systematic
  Accident Simulation</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、シミュレートされたデータでトレーニングされたモデルからの情報が、実世界のデータでトレーニングされたモデルに推測できることを示しており、現実世界のモデルのシミュレーションデータの潜在的な影響と異常な運転シナリオの処理における進歩を示しています。このデータを適用することにより自動運転モデルに対して、ランダムな初期化方法と比較して、シミュレートされたデータセットの転移学習がより良い一般化と衝突回避を提供することを示しています。シミュレーションデータは、以下のようなエッジケースをカバーするために、実際の運転データを拡張するために利用できます。車両事故。 
[要約]データ処理の重要性は、自動車事故を処理する際の高い社会的コストと、人間の運転者に対する潜在的な危険に見られます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Training Reduces Information and Improves Transferability -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_29.html">
      <font color="black">Adversarial Training Reduces Information and Improves Transferability</font>
    </a>
  </h2>
  <font color="black">いくつかのデータセットでCIFAR-10、CIFAR-100、ImageNetでトレーニングされた堅牢なネットワークを使用して結果を検証します。最近の結果は、堅牢であることに加えて、分類のために敵対的にトレーニングされたネットワークの機能が可逆性などの望ましいプロパティを有効にすることを示しています。 、我々は敵対的トレーニングが入力に関する表現とタスクに関する重みのフィッシャー情報を減らすことを示し、そして最小性の原則に違反することなく決定論的ネットワークの可逆性を説明する理論的議論を提供します。 
[ABSTRACT]敵対的なトレーニングにより、新しいタスクへの線形転送可能性が向上することを示します。これにより、表現の転送可能性とソースタスクの精度との間に新たなトレードオフが生じます。理論に反することなく、確定的ネットワークの可逆性を説明します。最小限の原則</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-22">
        <br><font color="black">2020-07-22</font>
      </time>
    </span>
</section>
<!-- paper0: Differentiable Hierarchical Graph Grouping for Multi-Person Pose
  Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_30.html">
      <font color="black">Differentiable Hierarchical Graph Grouping for Multi-Person Pose
  Estimation</font>
    </a>
  </h2>
  <font color="black">ただし、既存のボトムアップ方式では、キーポイントのグループ化は通常、キーポイントの検出とは独立して解決されるため、エンドツーエンドでトレーニングできず、パフォーマンスが最適化されません。COCOとOCHumanの両方のデータセットに対する広範な実験により、提案された方法が実証されていますボトムアップポーズ推定法のパフォーマンスを向上させます。特に、ボトムアップ複数人ポーズ推定タスクでのグラフのグループ化を学習するために、新しい微分可能な階層グラフグループ化（HGG）法を提案します。 
[ABSTRACT]以前のメソッドは2つのストリームに分割できます。ボトムアップメソッドはキーポイントを直接ローカライズしてから、さまざまな人物をクラスタ化します。これは一般にトップダウンメソッドよりも効率的です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-End Optimization of Scene Layout -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_31.html">
      <font color="black">End-to-End Optimization of Scene Layout</font>
    </a>
  </h2>
  <font color="black">この条件付き生成設計に加えて、シーンの2D投影のみを使用してレイアウトを改善できる微分可能レンダリングモジュールも統合します。実験により、このモデルは条件付きシーン合成でより高い精度と多様性を実現し、サンプルベースのシーン生成を可能にします。さまざまな入力フォーム..条件付きレイアウトシンセサイザーを使用して、入力例と同じ構造を共有するさまざまなレイアウトを生成できます。 
[ABSTRACT]シーングラフを抽象的で一般的な表現として使用します。異なる種類のフォームの異なるフォームを作成するために、異なるレイアウトを作成できます。差別化可能なレンダリングモジュールにより、ソースレイアウトを最適化して、特定の入力を分析に適合させることができます。 -総合ファッションファッションファッション</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Isotropy Maximization Loss and Entropic Score: Accurate, Fast,
  Efficient, Scalable, and Turnkey Neural Networks Out-of-Distribution
  Detection Based on The Principle of Maximum Entropy -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_32.html">
      <font color="black">Isotropy Maximization Loss and Entropic Score: Accurate, Fast,
  Efficient, Scalable, and Turnkey Neural Networks Out-of-Distribution
  Detection Based on The Principle of Maximum Entropy</font>
    </a>
  </h2>
  <font color="black">その結果、等エントロピー（距離ベース）であり、クロスエントロピーの最小化に依然依拠しているにもかかわらず、高いエントロピー（信頼性が低い）事後確率分布を生成する損失であるIsoMaxを提案します。敵対的なトレーニングや検証、データ拡張、アンサンブル法、生成手法、モデルのアーキテクチャの変更、メトリック学習、追加の分類子や回帰などの手法に依存せずに改善されました。IsoMax損失は、シームレスなSoftMax損失ドロップイン置換として機能し、全体的なソリューションは、正確、高速、効率的、スケーラブル、ターンキーです。 
[要約]ニューラルネットワークのフード検出パフォーマンスが低いのは、クロスシンプソン損失の異方性と極端な傾向が原因です。フード検出の迅速なエントロピースコアを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-15">
        <br><font color="black">2019-08-15</font>
      </time>
    </span>
</section>
<!-- paper0: Structured Compression and Sharing of Representational Space for
  Continual Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_33.html">
      <font color="black">Structured Compression and Sharing of Representational Space for
  Continual Learning</font>
    </a>
  </h2>
  <font color="black">P-MNIST、CIFAR-10およびCIFAR-100データセットでアルゴリズムを評価し、壊滅的な忘却の問題を克服しながら、最先端の方法と同等の精度を実現します。各タスクを学習した後、残差は次のように分析されますそれ自体と学習されたコア空間の両方の冗長性、および最小限の次元のセットがコア空間に追加されます。この現象を克服するための努力は、たとえば、古いデータまたはパラメトリック重要度スコアを保存する必要があるか、またはネットワークアーキテクチャの成長。 
[ABSTRACT]人工ニューラルネットワークは古いタスクについて学習した情報を上書きし、「壊滅的な忘却」を引き起こします。努力を使用して、継続的かつ効率的に学習するネットワークを作成できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-23">
        <br><font color="black">2020-01-23</font>
      </time>
    </span>
</section>
<!-- paper0: Neural Geometric Parser for Single Image Camera Calibration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_34.html">
      <font color="black">Neural Geometric Parser for Single Image Camera Calibration</font>
    </a>
  </h2>
  <font color="black">マンハッタンの世界の仮定に基づいて、カメラの回転と焦点距離を弱く監視された方法でさらに推定できます。2番目のネットワークは、人工シーンの事前知識が使用されている特定の画像と幾何学的キューに基づいて各候補を評価します画像の水平線と焦点距離からなるデータセットを監視することで、ネットワークをトレーニングして同じカメラパラメータを推定できます。 
[ABSTRACT]当社のネットワークは、同じカメラパラメータを推定するようにトレーニングできます。ニューラルアプローチのパフォーマンスは、既存のカメラキャリブレーションテクニックのパフォーマンスよりも大幅に高いと言えます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: PareCO: Pareto-aware Channel Optimization for Slimmable Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_35.html">
      <font color="black">PareCO: Pareto-aware Channel Optimization for Slimmable Neural Networks</font>
    </a>
  </h2>
  <font color="black">14のネットワークとデータセットの組み合わせで広範な経験的分析を実行し、過剰にパラメーター化されていないネットワークは、極端に過剰にパラメーター化されたネットワークよりもジョイントチャネルと重みの最適化から多くの利益を得ることを発見しました。単一のモデルと同じストレージコストの計算コスト（浮動小数点演算やFLOPの数など）が、モバイルデバイスなどのリソースに制約のある設定で最近提案されました。異なるレイヤー間で異種の幅乗数を可能にする、私たちは多目的最適化レンズからスリム化可能なネットワークを最適化する問題を定式化します。これは、サブネットワークの共有重みと幅乗数の両方を最適化するための新しいアルゴリズムにつながります。 
[ABSTRACT]スリミング可能なニューラルネットワークは、単一の幅を使用します-すべてのレイヤーの乗数が異なるパフォーマンスプロファイルのサブネットワークに到達します。これにより、レイヤーによってネットワークの予測精度への影響が異なり、フロップ要件も異なります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Semi-DerainGAN: A New Semi-supervised Single Image Deraining Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_36.html">
      <font color="black">Semi-DerainGAN: A New Semi-supervised Single Image Deraining Network</font>
    </a>
  </h2>
  <font color="black">より優れた排出結果を提供するために、実際のペアと偽のペアを区別するためのペアの弁別器を設計します。具体的には、両方のプロセスの同じパラメーターを共有するSSRMLと呼ばれる半教師付き雨ストリーク学習器が導出され、実際の画像がより多くの雨に寄与します。ストリーク情報..このホワイトペーパーでは、Semi-DerainGANと呼ばれる新しい半教師ありGANベースの排水ネットワークを提案します。 
[要約]排水ネットワークは合成データセットで印象的な結果を得ていますが、雨除去能力の一般化が弱いため、実際の画像では満足のいく結果が得られません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-23">
        <br><font color="black">2020-01-23</font>
      </time>
    </span>
</section>
<!-- paper0: PointTriNet: Learned Triangulation of 3D Point Sets -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_37.html">
      <font color="black">PointTriNet: Learned Triangulation of 3D Point Sets</font>
    </a>
  </h2>
  <font color="black">この作業では、幾何学的深層学習の新しいタスクである3D空間の一連の点の間に三角形分割を生成することを検討します。これらの学習問題はローカルの幾何データに作用するため、この方法は効率的でスケーラブルであり、見えない形状カテゴリに一般化されます。ネットワークは、点群として表された形状のコレクションから教師なしの方法でトレーニングされます。 
[ABSTRACT] pointtrinetは、3D学習パイプラインのシステムとして三角測量を予測する、区別可能でスケーラブルなネットワークです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-19">
        <br><font color="black">2020-04-19</font>
      </time>
    </span>
</section>
<!-- paper0: Illumination invariant hyperspectral image unmixing based on a digital
  surface model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_38.html">
      <font color="black">Illumination invariant hyperspectral image unmixing based on a digital
  surface model</font>
    </a>
  </h2>
  <font color="black">一方、提案されたモデルは、既存のモデルよりも正確な量と影の補正された反射率を推定しました。提案されたモデルは、簡単な最適化手順によって効率的に解かれました。非混合結果は、他の最先端の非混合であることを示しました。モデルは特にシェーディングされたピクセルではうまく機能しませんでした。 
[ABSTRACT]新しい論文は非混合モデルを提案しています。光追跡（デジタル）と呼ばれ、モデルはlightと呼ばれます。他のモデルがうまく機能しなかった方法を説明します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: CAD-Deform: Deformable Fitting of CAD Models to 3D Scans -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_39.html">
      <font color="black">CAD-Deform: Deformable Fitting of CAD Models to 3D Scans</font>
    </a>
  </h2>
  <font color="black">一連の徹底的な実験により、私たちの方法ではスキャンとCADのフィットが非常に厳密になり、合成されたCAD環境に存在する重要な幾何学的特徴を維持しながら、スキャンされた実世界環境のより正確なデジタルレプリカが可能になります。形状の取得と位置合わせは、 3Dスキャンを、モバイルやAR / VRゲームシナリオなどのコンテンツ作成に使用できる軽量のCAD表現に変えるための有望な手段です。この作業では、より正確なCADを取得する方法であるCAD-Deformを導入することで、この欠点に対処します。検索されたCADモデルを非厳密に変形することでフィットをスキャンします。 
[ABSTRACT] CADモデルは3D形状コレクションのモデルとして識別できる必要があります。CADモデルは3Dスキャンから簡単に取得でき、CADオブジェクトのクリーンで高品質な表面プロパティを維持できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Interpreting the Latent Space of GANs via Correlation Analysis for
  Controllable Concept Manipulation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_40.html">
      <font color="black">Interpreting the Latent Space of GANs via Correlation Analysis for
  Controllable Concept Manipulation</font>
    </a>
  </h2>
  <font color="black">潜在変数が生成結果の定量分析にどのように影響するか。提案された方法はデータセットFashion-MNISTとUT Zappos50Kで検証され、実験結果がその有効性を示しています。このホワイトペーパーでは、GANの固有のメカニズムをより深く理解するために、潜在変数と生成された画像の対応する意味内容の間の相関を分析することにより、GANの潜在空間を解釈する方法が提案されています。 
[要約]潜在的な変数と生成された画像の対応する意味内容との間の相関を分析することによりガンの潜在的な空間を解釈する方法が提案されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-23">
        <br><font color="black">2020-05-23</font>
      </time>
    </span>
</section>
<!-- paper0: Regularization of Building Boundaries in Satellite Images using
  Adversarial and Regularized Losses -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_41.html">
      <font color="black">Regularization of Building Boundaries in Satellite Images using
  Adversarial and Regularized Losses</font>
    </a>
  </h2>
  <font color="black">ただし、不規則なフットプリントを生成するマスクR-CNNとは異なり、私たちのフレームワークは、多くのアプリケーションで有益な、規則化された視覚的に楽しい建物の境界を生成します。この論文では、完全な畳み込みニューラルを使用して、衛星画像の境界の洗練と規則化を構築する方法を紹介します。敵対的損失と正則化損失の組み合わせでトレーニングされたネットワーク。純粋なマスクR-CNNモデルと比較して、全体的なアルゴリズムは、精度と完全性に関して同等のパフォーマンスを実現できます。 
[要約]完全に訓練されたシステムに加えて、ネットワーク全体で0.3％のスコアを達成できる</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Zero-Shot Recognition through Image-Guided Semantic Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_42.html">
      <font color="black">Zero-Shot Recognition through Image-Guided Semantic Classification</font>
    </a>
  </h2>
  <font color="black">したがって、セマンティック分類子は画像適応型であり、推論中に生成されます。IGSCは概念的にシンプルで、分類のための既存のディープアーキテクチャをわずかに拡張することで実現できます。しかし、それは効果的であり、標準的なベンチマークでの最先端の埋め込みベースの一般化されたZSLアプローチよりも優れています。マルチラベル分類のバイナリ関連性手法に動機付けられて、画像と意味分類子の間のマッピングを逆に学習することを提案します。 
[要約]提案された画像誘導の意味分類（igsc）メソッドは、ラベル分類子を作成します。これは、すべてのラベル埋め込みに適用され、ラベルが入力画像に属しているかどうかを判断します。ただし、効果的であり、最先端の埋め込みより優れています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Collaborative Video Object Segmentation by Foreground-Background
  Integration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_43.html">
      <font color="black">Collaborative Video Object Segmentation by Foreground-Background
  Integration</font>
    </a>
  </h2>
  <font color="black">DAVIS 2016、DAVIS 2017、YouTube-VOSの3つの人気のあるベンチマークで広範な実験を実施しています。コード：https://github.com/zx-yang/CFBI ..フォアグラウンドとバックグラウンドの両方から埋め込まれた機能により、 CFBIは、ピクセルレベルとインスタンスレベルの両方からの参照と予測シーケンス間のマッチングプロセスを実行し、CFBIをさまざまなオブジェクトスケールに対して堅牢にします。 
[要旨] cfbiは、フォアグラウンドオブジェクト（s）のピクセルを使用して埋め込みを検査します。フォアグラウンド-バックグラウンド統合（cfbi）アプローチによる協調的なビデオオブジェクトのセグメンテーションを提案します。当社のcfbiは、89.4％、81。9％のパフォーマンス（j $ f）を達成しました、81。4％、それぞれ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-18">
        <br><font color="black">2020-03-18</font>
      </time>
    </span>
</section>
<!-- paper0: PP-YOLO: An Effective and Efficient Implementation of Object Detector -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_44.html">
      <font color="black">PP-YOLO: An Effective and Efficient Implementation of Object Detector</font>
    </a>
  </h2>
  <font color="black">YOLOv3が実際に広く使用されていることを考慮して、YOLOv3に基づく新しいオブジェクト検出器を開発します。主に、モデルパラメーターとFLOPの数をほとんど増加させないさまざまな既存のトリックを組み合わせて、精度を向上させるという目標を達成しようとします。検出器は、速度がほとんど変わらないようにしながら、可能な限り検出します。オブジェクト検出は、コンピュータビジョンの最も重要な領域の1つであり、さまざまな実際のシナリオで重要な役割を果たします。 
[要約]このホワイトペーパーの目的は、新しいアプリケーションシナリオに直接適用できる、比較的バランスのとれた有効性と効率を備えたオブジェクト検出器を実装することです。主に、モデルパラメーターとフロップの数をほとんど増加させないさまざまな既存のトリックを組み合わせようとします。目標を達成するために.pp-yoloは、効率（45.％マップ）と効率（72.9 fps）のより良いバランスを達成でき、efficientdetやyolov4などの既存の最新の検出器を超えます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Modeling and Enhancing Low-quality Retinal Fundus Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_45.html">
      <font color="black">Modeling and Enhancing Low-quality Retinal Fundus Images</font>
    </a>
  </h2>
  <font color="black">次に、分解モデルに基づいて、臨床指向の眼底強化ネットワーク〜（cofe-Net）〜が提案され、全体的な分解因子を抑制し、同時に臨床観察および分析のために解剖学的な網膜構造と病理学的特性を維持します。両方の実験合成および実際の眼底画像は、私たちのアルゴリズムが網膜の詳細を失うことなく低品質の眼底画像を効果的に補正することを示しています。低品質の眼底画像は、臨床観察の不確実性を高め、誤診のリスクにつながります。 
[要約]眼底強化ネットワークは、全体的な劣化要因を抑制し、同時に解剖学的な網膜画像を保存するために提案されています。眼底補正方法は、医療画像分析アプリケーションなどに役立ちます。 g、網膜血管のセグメンテーションと視神経乳頭/カップの検出</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-12">
        <br><font color="black">2020-05-12</font>
      </time>
    </span>
</section>
<!-- paper0: Stochastic Frequency Masking to Improve Super-Resolution and Denoising
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_46.html">
      <font color="black">Stochastic Frequency Masking to Improve Super-Resolution and Denoising
  Networks</font>
    </a>
  </h2>
  <font color="black">私たちの定式化に基づいて、ネットワークを正則化し、過剰適合問題に対処するためのトレーニングで使用される画像の確率的周波数マスキングを提案します。私たちの手法は、さまざまな合成カーネルを使用したブラインド超解像度で最新の方法を改善し、実際のスーパー-解像度、ブラインドガウスのノイズ除去、および実画像のノイズ除去。これは、特に学習ベースの方法の場合、トレーニング中に見られる劣化に過剰に適合する傾向があるため、復元がさらに困難になります。 
[ABSTRACT]周波数領域での劣化の分析-超解像でのカーネルの過剰適合を提示し、条件付き学習の視点を導入します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-16">
        <br><font color="black">2020-03-16</font>
      </time>
    </span>
</section>
<!-- paper0: BSL-1K: Scaling up co-articulated sign language recognition using
  mouthing cues -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_47.html">
      <font color="black">BSL-1K: Scaling up co-articulated sign language recognition using
  mouthing cues</font>
    </a>
  </h2>
  <font color="black">この作業では、連続動画での標識認識のためのデータ収集への新しいスケーラブルなアプローチを紹介します。次の貢献をします。（1）署名者からの口頭の合図を使用して、ビデオデータから高品質の注釈を取得する方法を示します。結果は、BSL-1Kデータセット、前例のない規模のイギリス手話（BSL）標識のコレクションです。 （2）私たちは、BSL-1Kを使用してBSLで共同調印された標識の強力な標識認識モデルをトレーニングできること、およびこれらのモデルが他の手話言語およびベンチマークの優れた事前トレーニングをさらに形成することを示します-両方の最先端技術を超えていますMSASLおよびWLASLベンチマーク..最後に、（3）標識認識および標識スポッティングのタスクに新しい大規模評価セットを提案し、この領域の研究を刺激するのに役立つと期待されるベースラインを提供します。 
[要約]トレーニングの不足と適切なトレーニングデータの欠如は主要な障害です。サインスカラーのタスクの新しい評価を提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: A Study on Evaluation Standard for Automatic Crack Detection Regard the
  Random Fractal -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_48.html">
      <font color="black">A Study on Evaluation Standard for Automatic Crack Detection Regard the
  Random Fractal</font>
    </a>
  </h2>
  <font color="black">CovEvalでは、この問題に対してカバーボックスマッチングの考え方に基づく別のマッチングプロセスが採用されています。最適なモデルのリコール（XR）は、95.8で産業レベルを達成しています。物体検出は、自動産業検査の大きな可能性を秘めています。解決策として、亀裂検出の過小評価を修正するために、CovEvalというフラクタル利用可能な評価標準が提案されています。 
[ABSTRACT]ディープラーニングに基づく自動亀裂検出器は、広く使用されている平均平均精度（マップ）標準では過小評価されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: ProxyNCA++: Revisiting and Revitalizing Proxy Neighborhood Component
  Analysis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_49.html">
      <font color="black">ProxyNCA++: Revisiting and Revitalizing Proxy Neighborhood Component
  Analysis</font>
    </a>
  </h2>
  <font color="black">さらに、提案された高速移動プロキシは、プロキシの小さな勾配問題にも対処し、このコンポーネントは、低温スケーリングおよびグローバル最大プーリングと相乗効果を発揮します。ProxyNCA++と呼ばれる拡張モデルは、4つでRecall @ 1の22.9パーセントポイントの平均改善を達成します。オリジナルのProxyNCAアルゴリズムと比較して、さまざまなゼロショット検索データセット。さらに、CUB200、Cars196、Sop、およびInShopデータセットで最先端の結果を達成し、72.2、90.1、81.4のRecall @ 1スコアを達成します。それぞれ90.9。 
[ABSTRACT]また、グローバル平均プーリングと比較した場合、グローバルプーリングは一般的にうまく機能することがわかりました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br><font color="black">2020-04-02</font>
      </time>
    </span>
</section>
<!-- paper0: Harnessing spatial homogeneity of neuroimaging data: patch individual
  filter layers for CNNs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_50.html">
      <font color="black">Harnessing spatial homogeneity of neuroimaging data: patch individual
  filter layers for CNNs</font>
    </a>
  </h2>
  <font color="black">PIFレイヤーを使用するCNNは、特に低いサンプルサイズ設定でより高い精度が得られ、収束に必要なトレーニングエポックが少なくなることを示しています。私たちの知る限り、これはCNN学習に脳のMRIに関する事前知識を導入した最初の研究です.. 3つの異なるタスクとデータセット、つまりUK Biobankデータでの性別分類、ADNIデータでのアルツハイマー病の検出、および私立病院データでの多発性硬化症の検出について、PIFレイヤーを徹底的に評価しました。 
[ABSTRACT]新しいcnnアーキテクチャは、ニューロイメージングデータの空間的均一性に関する空間的概念と以前の概念を組み合わせたものです。より高い、より抽象的な層のパッチの個々のフィルター（pif）も導入します。付属データでの病気の検出と私立病院データでの多発性硬化症の検出</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Representation Sharing for Fast Object Detector Search and Beyond -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_51.html">
      <font color="black">Representation Sharing for Fast Object Detector Search and Beyond</font>
    </a>
  </h2>
  <font color="black">サーチスペースには、特にオブジェクト検出用に設計された多様な変換の豊富なセットが含まれています。設計されたサーチスペースに対処するために、表現共有（RepShare）と呼ばれる新しい検索アルゴリズムが提案され、定義された変換の最適な組み合わせを効果的に識別します。オブジェクト検出では、より困難なインスタンスセグメンテーションに関するFADの一般性をさらに示し、より多くのタスクに利益をもたらすことが期待されます。 
[ABSTRACT] rpnを持たない1ステージ検出器の場合、強力なサブネットワークを備えることがより要求されます。設計された検索スペースに対処するために、表現共有（repshare）と呼ばれる新しい検索アルゴリズムが提案され、最適な定義された変換の組み合わせ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Targeted Attack for Deep Hashing based Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_52.html">
      <font color="black">Targeted Attack for Deep Hashing based Retrieval</font>
    </a>
  </h2>
  <font color="black">次に、新しいコンポーネント投票方式を設計して、ターゲットラベルを持つオブジェクトのハッシュコードのセットの代表としてアンカーコードを取得します。その最適性保証も理論的に導き出されます。この論文では、ディープハッシュターゲット攻撃（DHTA）。このような検索に対するターゲット攻撃を研究します。広範な実験により、DHTAがディープハッシュベースの画像検索とビデオ検索の両方の攻撃に効果的であることが確認されています。 
[ABSTRACT]敵対的な例のハッシュコードは、ハッシュコードとターゲットラベルを持つ一連のオブジェクトのハッシュコード間の平均距離を最小化します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-15">
        <br><font color="black">2020-04-15</font>
      </time>
    </span>
</section>
<!-- paper0: Accurate RGB-D Salient Object Detection via Collaborative Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_53.html">
      <font color="black">Accurate RGB-D Salient Object Detection via Collaborative Learning</font>
    </a>
  </h2>
  <font color="black">この戦略により、ネットワークは追加の深さネットワークと深さ入力を使用せずに推論を行うことができます。明示的に抽出されたエッジ情報は顕著性と一緒になり、顕著領域とオブジェクト境界をより強調します。一方のプーリングとFCNでのアップサンプリング操作により、オブジェクトの境界がぼやける可能性があります。 
[ABSTRACT]深度と顕著性の学習は、相互に利益をもたらす方法で高レベルの機能学習プロセスに革新的に統合されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: A Generalization of Otsu's Method and Minimum Error Thresholding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_54.html">
      <font color="black">A Generalization of Otsu's Method and Minimum Error Thresholding</font>
    </a>
  </h2>
  <font color="black">GHTは、手書きのドキュメント画像の2値化（ピクセルごとの2値化を生成するようにトレーニングされたディープニューラルネットワークを含む）に関する最近の課題で、すべてのアルゴリズムのパフォーマンスを上回るか、またはそれに匹敵し、数十行のコードまたは簡単な変更として実装できることを示しています。大津の方法またはMETに。GHTは、適切な事前分布を持つガウス混合の近似最大事後推定を実行することによって機能します。GHTは、しきい値処理中にヒストグラムのビン幅を粗くする一般的な方法の明確な解釈も提供します。 
[ABSTRACT] ghtは、適切な事前分布を持つガウス分布の混合の正確な最大事後推定を実行することで機能します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Real-time CNN-based Segmentation Architecture for Ball Detection in a
  Single View Setup -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_55.html">
      <font color="black">Real-time CNN-based Segmentation Architecture for Ball Detection in a
  Single View Setup</font>
    </a>
  </h2>
  <font color="black">また、テスト時のデータ拡張により、検出精度が大幅に向上することも示しています。追加の貢献として、この作業の基礎となっているデータセットを公開します。推論モデルは、時間分析。 
[ABSTRACT]効率的なcnnアーキテクチャによって解決されるセグメンテーションタスクとして問題を提示することにより、新しいアプローチを提案します。システムは、時間分析によって引き起こされる遅延なしにリアルタイムで実行するように設計されています。追加の貢献として、この作業のベースとなっているデータセット</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Weakly Supervised 3D Object Detection from Lidar Point Cloud -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_56.html">
      <font color="black">Weakly Supervised 3D Object Detection from Lidar Point Cloud</font>
    </a>
  </h2>
  <font color="black">ステージ2は、いくつかの適切にラベル付けされたオブジェクトインスタンスを使用して、円筒形の提案を改良し、立方体と信頼スコアを取得する方法を学習します。わずか500の弱く注釈されたシーンと534の正確にラベル付けされた車両インスタンスを使用して、このメソッドは、現在のパフォーマンスの85〜95％を達成します最先端の完全に監視された検出器（3、712を網羅し、15、654のインスタンスを持つ正確に注釈が付けられたシーンが必要です）。これは、2段階のアーキテクチャ設計によって実現されます。 
[要約]私たちの方法は、500の弱い注釈付きシーンと534の正確にラベル付けされた車両インスタンスのみを使用します。これは、元のパフォーマンスの94％を超える3Dオブジェクト検出器をトレーニングするために使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Sound2Sight: Generating Visual Dynamics from Sound and Context -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_57.html">
      <font color="black">Sound2Sight: Generating Visual Dynamics from Sound and Context</font>
    </a>
  </h2>
  <font color="black">特に推論中にモダリティが欠落している可能性がある場合、モダリティ全体で関連付けを学習することは、堅牢なマルチモーダル推論にとって重要です。広範な実験により、Sound2Sightは、生成されたビデオ品質において最先端のパフォーマンスを大幅に上回り、多様なビデオコンテンツを生成することも実証しています。埋め込みは、マルチヘッド注意ベースのオーディオビジュアルトランスフォーマーエンコーダーによって学習されます。 
[ABSTRACT]フレームごとの確率的事前学習は、オーディオと過去のフレームの同時埋め込みを条件としています。これをサンプリングして、ビデオ予測モジュールをさらに調整し、将来のフレームを生成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: A Solution to Product detection in Densely Packed Scenes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_58.html">
      <font color="black">A Solution to Product detection in Densely Packed Scenes</font>
    </a>
  </h2>
  <font color="black">その結果、SKU-110kのテストセットで58.7 mAPを取得する方法を採用しました。この問題を解決するために、サンプリングレートと入力スケールの両方が通常のランダムクロップと比較して比較的十分であることを確認するランダムクロップ戦略を提案しました。この作業は、密にパックされたシーンデータセットSKU-110kのソリューションです。 
[ABSTRACT]メソッドにはトリックが詰め込まれ、hyper-captains.itが最適化されます。これは、さまざまなトリックがロードされたメソッドのメソッドの結果です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Through the Looking Glass: Neural 3D Reconstruction of Transparent
  Shapes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_59.html">
      <font color="black">Through the Looking Glass: Neural 3D Reconstruction of Transparent
  Shapes</font>
    </a>
  </h2>
  <font color="black">合成データセットをレンダリングして、モデルがさまざまなビューにわたる屈折光輸送を学習するように促します。コードとデータは公開されています。私たちの実験では、わずか5〜12の自然な形状を使用して、複雑な透明形状の高品質3Dジオメトリの復元が成功することを示しています画像。 
[ABSTRACT]屈折と反射によって引き起こされる複雑な光路により、従来のマルチビューステレオとディープマルチビューステレオの両方がこの課題を解決できなくなりました。当社の調査では、わずか5-12の自然画像を使用して、複雑な透明形状の高品質3Dジオメトリの復元に成功していることが示されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-22">
        <br><font color="black">2020-04-22</font>
      </time>
    </span>
</section>
<!-- paper0: AttentionNAS: Spatiotemporal Attention Cell Search for Video
  Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_60.html">
      <font color="black">AttentionNAS: Spatiotemporal Attention Cell Search for Video
  Classification</font>
    </a>
  </h2>
  <font color="black">アテンションセルをI3D-R50に挿入すると、両方のデータセットで最先端のパフォーマンスが得られます。発見されたアテンションセルは、I3DやS3Dなどの既存のバックボーンネットワークにシームレスに挿入でき、ビデオ分類の精度が2以上向上します。 Kinetics-600とMiTの両方のデータセットの％。畳み込み演算には2つの制限があります。（1）すべての位置に同じフィルターが適用されるため、どこに焦点を合わせるかを明示的にモデル化しない、（2）長期依存関係のモデリングには不適切彼らは小さな近所でしか活動していないからです。 
[ABSTRACT]検索システムは、セル内のさまざまなデザインの選択肢を探索できます。これらのセルは、両方のビデオで非ローカルブロックよりも優れており、さまざまなモダリティ、バックボーン、およびデータセットにわたって強力な一般化を示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: VisualCOMET: Reasoning about the Dynamic Context of a Still Image -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_61.html">
      <font color="black">VisualCOMET: Reasoning about the Dynamic Context of a Still Image</font>
    </a>
  </h2>
  <font color="black">このタスクの強力なベースラインパフォーマンスを確立し、視覚とテキストの常識推論の間の統合が鍵であり、非統合の代替案に勝つことを示します。以前発生した可能性のあるイベントを予測する視覚常識推論タスクの新しいフレームワークであるVisualCometを提案します、次に発生する可能性のあるイベント、および現在の人々の意図。視覚常識推論への研究をサポートするために、視覚常識推論の140万以上のテキスト記述からなる視覚常識グラフの最初の大規模リポジトリを紹介します60,000枚の画像のさまざまなセットに注釈を付け、それぞれを前後の短いビデオ要約と組み合わせます。 
[ABSTRACT]視覚的常識の推論に向けた研究をサポートするために、最初の大規模な視覚的常識イメージのキャッシュが作成されました。これには、水に浮かぶのに苦労している人のイメージと、その人の現時点での意図が留まることを含みます生きている、そして彼は近い将来助けを必要とするか、そうでなければ彼は洗い流されるでしょう</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-22">
        <br><font color="black">2020-04-22</font>
      </time>
    </span>
</section>
<!-- paper0: SBAT: Video Captioning with Sparse Boundary-Aware Transformer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_62.html">
      <font color="black">SBAT: Video Captioning with Sparse Boundary-Aware Transformer</font>
    </a>
  </h2>
  <font color="black">SBATは、マルチヘッドアテンションからのスコアに境界認識プール操作を採用し、さまざまなシナリオから多様な機能を選択します。2つのベンチマークデータセットの実験結果は、SBATがほとんどのメトリックの下で最先端の方法よりも優れていることを示しています。また、SBATまばらな操作によるローカル情報の損失を補償するローカル相関スキームが含まれています。 
[要約]バニラトランスフォーマーはユニモーダル言語生成用に提案されていますが、ビデオ表現の冗長性を減らすために提案されています。sbatはローカル相関スキームを使用してローカル操作を補正します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: xCos: An Explainable Cosine Metric for Face Verification Task -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_63.html">
      <font color="black">xCos: An Explainable Cosine Metric for Face Verification Task</font>
    </a>
  </h2>
  <font color="black">例外的なパフォーマンスを超えて、ディープフェイス検証モデルは、生成する結果を信頼できるように、より多くの解釈が必要です。データの量が増えると、ディープたたみ込みニューラルネットワークは、フェイス検証タスクの非常に高い精度を実現できます。 xCos $を使用すると、2つの入力面のどの部分が類似しているか、モデルが注意を向けているか、局所的な類似性がどのように重み付けされて出力$ xCos $スコアが形成されているかを確認できます。 
[ABSTRACT]顔認証は最近の重要なタスクです。モバイルデバイスのアクセス制御、監視、自動個人ログオンなどのさまざまなアプリケーションに展開されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-11">
        <br><font color="black">2020-03-11</font>
      </time>
    </span>
</section>
<!-- paper0: Garment Design with Generative Adversarial Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_64.html">
      <font color="black">Garment Design with Generative Adversarial Networks</font>
    </a>
  </h2>
  <font color="black">具体的には、属性GAN（AttGAN）---人間の顔の属性編集に成功したことが証明された生成モデル---は、衣服の視覚的属性の自動編集に利用され、大規模なファッションデータセットでテストされています。デザインコンセプトの属性レベルの編集にGANを使用し、将来の作業で取り組むべきいくつかの重要な制限と研究の問題を強調します。深い生成モデルの最近の進歩により、自動生成やデザイナーによる認知障害を克服する新しい可能性が生まれました。設計コンセプトの編集。 
[ABSTRACT]デザイナーは視覚的な編集のための新しいコンセプトを作成しました。彼らは自動化された属性を開発したと言います-デザインコンセプトのレベル編集</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Right for the Right Reason: Making Image Classification Robust -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_65.html">
      <font color="black">Right for the Right Reason: Making Image Classification Robust</font>
    </a>
  </h2>
  <font color="black">この論文では、モデルが適切な画像部分にどの程度焦点を当てているかを自動的に定量化するための新しいスコアを提案します。スコアは、CNNモデルに説明者を適用することにより、最も決定的な画像領域の度合いを考慮して計算されます。 -分類されるオブジェクトのシルエットと重なる。最近、分類器が最も焦点を合わせる入力画像の部分を決定することによって、そのような説明を提供する試みが行われている。 
[ABSTRACT]調査では、多くのモデルが正しい分類を生成しているが、誤った理由により、画像の無関係な部分に基づいていることを示しています。スコアは、最も決定的な画像領域がオブジェクトのシルエットと重複する度合いを考慮して計算されます分類される</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: CrossTransformers: spatially-aware few-shot transfer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_66.html">
      <font color="black">CrossTransformers: spatially-aware few-shot transfer</font>
    </a>
  </h2>
  <font color="black">次に、CrossTransformersと呼ばれる新しいトランスフォーマーベースのニューラルネットワークアーキテクチャを提案します。これは、少数のラベル付けされた画像とラベル付けされていないクエリを取り、クエリとラベル付けされた画像の間の粗い空間対応を見つけ、空間的距離を計算することによってクラスメンバーシップを推測します。対応する機能..分類問題の新しいクラスや入力のドメインシフトなど、データがほとんどない新しいタスクを考えると、最新のビジョンシステムのパフォーマンスは著しく急速に低下します。結果は、より堅牢な分類器になります。 ImageNetから他の多くのビジョンデータセットへの転送を評価するための最近のデータセットであるMeta-Datasetの最先端のパフォーマンスを介して示すタスクおよびドメインシフト。 
[ABSTRACT]新しい研究は、ニューラルネットワーク表現が現代のビジョンシステムを支える方法が監督崩壊の対象となる方法を示しています。私たちは、自己管理学習を使用して、より優れた転送を行う小さな機能を奨励します。これらは、タスクとドメインフォームにより堅牢な新しい分類子を含みます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-22">
        <br><font color="black">2020-07-22</font>
      </time>
    </span>
</section>
<!-- paper0: Whole-Body Human Pose Estimation in the Wild -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_67.html">
      <font color="black">Whole-Body Human Pose Estimation in the Wild</font>
    </a>
  </h2>
  <font color="black">大規模な実験により、COCO-WholeBodyを使用して、全身ポーズ推定のためにゼロから深いモデルをトレーニングするだけでなく、顔のランドマーク検出や手のキーポイント推定などのさまざまなタスクの強力な事前トレーニングデータセットとしても使用できます。 ZoomNetという名前の単一ネットワークモデルは、人体全体の階層構造を考慮して、同じ人のさまざまな身体部分のスケール変動を解決するように考案されています。この空白を埋めるために、COCO-WholeBodyを導入します。 COCOデータセットを全身注釈で拡張します。 
[ABSTRACT]最初のベンチマークは、人体全体に手動で注釈を付けています。これには、顔に68、手に42、体と足に23の133の高密度ランドマークが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: DPDist : Comparing Point Clouds Using Deep Point Cloud Distance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_68.html">
      <font color="black">DPDist : Comparing Point Clouds Using Deep Point Cloud Distance</font>
    </a>
  </h2>
  <font color="black">類似オブジェクトの比較や登録などの難しいタスクで提案された距離をテストし、面取り距離、地球の移動距離などの一般的に使用される距離よりも大幅に改善されることを示しています。DeepPoint Cloud Distance（ DPDist）は、1つのクラウド内のポイントと、他のポイントクラウドがサンプリングされた推定表面との間の距離を測定します。表面は、3D修正済みフィッシャーベクトル表現を使用して、ローカルで効率的に推定されます。 
[ABSTRACT]深い点群の距離は、1つの雲の中の点と、他の点群がサンプリングされる推定面との間の距離を測定します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-24">
        <br><font color="black">2020-04-24</font>
      </time>
    </span>
</section>
<!-- paper0: Distortion Robust Image Classification using Deep Convolutional Neural
  Network with Discrete Cosine Transform -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_69.html">
      <font color="black">Distortion Robust Image Classification using Deep Convolutional Neural
  Network with Discrete Cosine Transform</font>
    </a>
  </h2>
  <font color="black">DCT-Netは「盲目的に」一度だけ訓練され、さらなる再訓練なしに一般的な状況で適用されます。ガウスぼかし、モーションブラー、ソルトアンドペッパーノイズ、ガウスノイズ、CIFAR-10 / 100に追加されたスペックルノイズに対して提案された方法を評価します。実験結果は、訓練されると、DCT-Netはさまざまな目に見えない画像の歪みに対して一般化するだけでなく、文献の他の方法よりも優れていることを示しています。 
[ABSTRACT]提案されたdct-vgg16の上に構築されたネットワーク内のネットワークは、画質の低下に対して脆弱であることがわかります。これは、これらの作業が初めて適用された可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-11-14">
        <br><font color="black">2018-11-14</font>
      </time>
    </span>
</section>
<!-- paper0: ReLaB: Reliable Label Bootstrapping for Semi-Supervised Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_70.html">
      <font color="black">ReLaB: Reliable Label Bootstrapping for Semi-Supervised Learning</font>
    </a>
  </h2>
  <font color="black">ラベル付きサンプルが少ないデータセットを想定して、まず教師なし学習アルゴリズムを利用して教師なし潜在的特徴を学習し、次にこれらの特徴にラベル伝播アルゴリズムを適用し、ラベルノイズ検出アルゴリズムを使用して正しくラベル付けされたサンプルのみを選択します。パフォーマンスの低下のないニューラルネットワークは、人間の注釈の労力を効果的に削減するための鍵です。これにより、ReLaBは、最初のいくつかのラベル付きサンプルから信頼できる拡張ラベル付きセットを作成し、半教師あり学習に使用できます。ネットワークアーキテクチャと自己-supervisedメソッドは、ラベルの伝播を成功させるために重要であり、ReLaBがCIFAR-10、CIFAR-100、およびmini-ImageNetの非常に制限された監視のシナリオで半監視学習を大幅に改善することを示します。 
[ABSTRACT]私たちは、信頼できるラベルブートストラップ（relab）を提案します。これは、半教師付き学習ソリューションへの道を開く、教師なし予見アルゴリズムです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: HITNet: Hierarchical Iterative Tile Refinement Network for Real-time
  Stereo Matching -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_71.html">
      <font color="black">HITNet: Hierarchical Iterative Tile Refinement Network for Real-time
  Stereo Matching</font>
    </a>
  </h2>
  <font color="black">高レベルの精度を達成するために、私たちのネットワークは、幾何学的に視差に関する理由だけでなく、傾斜した平面の仮説も推論して、幾何学的ワーピングおよびアップサンプリング操作をより正確に実行できるようにします。私たちのアーキテクチャは本質的にマルチ解像度であり、さまざまなレベルでの情報の伝播を可能にします。このペーパーでは、リアルタイムステレオマッチングのための新しいニューラルネットワークアーキテクチャであるHITNetについて説明します。 
[ABSTRACT]ヒットネットは、2ビューステレオのeth3d Webサイトで公開されているすべてのメトリックで1位から3位にランク付けされています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Match Distributions for Domain Adaptation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_72.html">
      <font color="black">Learning to Match Distributions for Domain Adaptation</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、マッチング損失（L2M）を自動的に学習することで、マッチング損失に関する手作りの事前分布に依存せずに自動的に学習することを提案します。パブリックデータセットでの実験は、SOTAメソッドに対するL2Mの優位性を実証します。代わりに、L2Mは、メタネットワークを使用してデータドリブンの方法で分布マッチング損失を学習することによる誘導バイアス。 
[ABSTRACT]クロスドメインのジョイント分布を直接一致させることは困難です。既存の方法では、mmdや敵対などの事前定義された距離を使用して、限界分布または条件付き分布の相違を減らす傾向があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-17">
        <br><font color="black">2020-07-17</font>
      </time>
    </span>
</section>
<!-- paper0: TSIT: A Simple and Versatile Framework for Image-to-Image Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_73.html">
      <font color="black">TSIT: A Simple and Versatile Framework for Image-to-Image Translation</font>
    </a>
  </h2>
  <font color="black">正規化レイヤーの重要性を発掘し、慎重に設計された2ストリーム生成モデルに、新しく提案された特徴変換を粗から細に変換します。体系的な研究は、提案された方法をいくつかの最先端のタスクと比較します固有のベースライン。知覚品質と定量評価の両方でその有効性を検証します。画像から画像への変換のためのシンプルで用途の広いフレームワークを紹介します。 
[要約]新たに提案された機能が融合された詳細な2ストリーム情報モデルを提供します。追加の制約は必要なく、非常にクリーンでシンプルな方法に貢献します。体系的な研究は、提案された方法をいくつかの状態-特定のベースラインと比較します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Guided Deep Decoder: Unsupervised Image Pair Fusion -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_74.html">
      <font color="black">Guided Deep Decoder: Unsupervised Image Pair Fusion</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、提案されたネットワークがさまざまな画像融合問題で最先端のパフォーマンスを達成できることを示しています。2つのネットワークは、機能改善ユニットによって接続され、ガイダンス画像のマルチスケール機能をディープデコーダーネットワークに埋め込みます。提案されているネットワークでは、トレーニングデータなしで、監視されていない方法でネットワークパラメータを最適化できます。 
[ABSTRACT]提案されたネットワークは、エンコーダー-デコーダーネットワークで構成されています。これにより、トレーニングデータなしで、監視されていない方法でネットワークパラメーターを最適化できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Deep learning for image segmentation: veritable or overhyped? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_75.html">
      <font color="black">Deep learning for image segmentation: veritable or overhyped?</font>
    </a>
  </h2>
  <font color="black">ディープラーニングは、オブジェクトの特徴を非線形関数にマッピングすることにより、従来の画像セグメンテーション手法と同じピクセルレベルの精度を実現できますか？国際的なビジョンの課題の限られたカテゴリを分類するディープラーニングによって達成された高い精度と比較して、同じ課題のディープラーニングによって達成された画像セグメンテーションの精度は、約80パーセントにすぎません。 95％に近いです。 
[要約]ディープラーニングは、ピクセルレベルの画像セグメンテーションのための最も強力なツールです。国際的な生物医学の課題で達成された結果は、95％近くです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-16">
        <br><font color="black">2019-04-16</font>
      </time>
    </span>
</section>
<!-- paper0: PIPAL: a Large-Scale Image Quality Assessment Dataset for Perceptual
  Image Restoration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_76.html">
      <font color="black">PIPAL: a Large-Scale Image Quality Assessment Dataset for Perceptual
  Image Restoration</font>
    </a>
  </h2>
  <font color="black">最後に、アンチエイリアシングプーリングを導入することにより、GANベースの歪みでのIQAネットワークのパフォーマンスを改善します。適切な評価方法を使用することは重要ですが、IRAアルゴリズムの開発とともにIQA方法も更新する必要があります。提案された方法の。 
[要約]信頼性の高い「elo system」を使用して、1300万以上の人間の判断を収集し、pipal画像の主観的スコアを割り当てます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Online Invariance Selection for Local Feature Descriptors -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_77.html">
      <font color="black">Online Invariance Selection for Local Feature Descriptors</font>
    </a>
  </h2>
  <font color="black">画像全体でのこれらのメタ記述子の類似性は、ローカル記述子を照合するときに正しい不変性を選択するために使用されます..不変であるか、不変でないか：これは、ローカル記述子に関するこの作業で定式化された質問です。記述子の実行時の不変性の選択（LISRD）は、不変性が不要な場合に識別性を維持しながら、記述子を画像の不利な変化に適応させることができます。 
[ABSTRACT]ディスクリプターは、不変性が必要ない場合に識別性を維持しながら、画像の不利な変化に適応できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-17">
        <br><font color="black">2020-07-17</font>
      </time>
    </span>
</section>
<!-- paper0: Leveraging Bottom-Up and Top-Down Attention for Few-Shot Object
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_78.html">
      <font color="black">Leveraging Bottom-Up and Top-Down Attention for Few-Shot Object
  Detection</font>
    </a>
  </h2>
  <font color="black">コードはhttps://github.com/chenxy99/AttFDNet。で入手できます。少数ショットオブジェクト検出器のパフォーマンスと解釈性を向上させるために、両方の利点を活用した注意深い少数ショットオブジェクト検出ネットワーク（AttFDNet）を提案します。 -downとボトムアップアテンション..タスクにとらわれないボトムアップアテンションは、自然に顕著なオブジェクトを検出してローカライズするのに役立つ事前機能として機能します。 
[ABSTRACT]最近の研究では、物体検出やその他の視覚タスクにおける自己学習トップダウン注意メカニズムの有効性が示されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: CurveLane-NAS: Unifying Lane-Sensitive Architecture Search and Adaptive
  Point Blending -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CV/paper_79.html">
      <font color="black">CurveLane-NAS: Unifying Lane-Sensitive Architecture Search and Adaptive
  Point Blending</font>
    </a>
  </h2>
  <font color="black">私たちは、従来の車線検出よりも現実的な課題をもたらす曲線車線検出の問題に対処し、最新の支援/自動運転システムを容易にします。CULaneなどの従来の車線ベンチマークに関する広範な実験も、CurveLane-NASの優位性を実証しています。現在の手作業で設計されたレーン検出方法は、長距離のコンテキスト情報と詳細な曲線軌跡の両方をモデル化していないため、曲線レーン、特にリモートパーツをキャプチャするには十分に堅牢ではありません。 
[ABSTRACT]現在の手動設計のレーン検出方法は、カーブレーン、特にリモートパーツをキャプチャするほど堅牢ではありません。ただし、長距離のコンテキスト情報と詳細なカーブの軌跡の両方をモデル化できないため、敏感です。新しいベンチマークカーブレーンにより、新しいsota 74を達成します。8％f1-culaneのスコア</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Web Similarity in Sets of Search Terms using Database Queries -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CL/paper_0.html">
      <font color="black">Web Similarity in Sets of Search Terms using Database Queries</font>
    </a>
  </h2>
  <font color="black">NWDは、これらのデータベースからの知識を組み込んだコルモゴロフ複雑度理論に基づく新しいコンテキスト（異なるデータベース）学習アプローチを可能にします。NWDを2つのセットに制限すると、以前の正規化されたGoogle距離（NGD）が得られますが、NGDの組み合わせはありませんセット内のペアは、NWDがセットから抽出する情報を抽出できます。正規化されたWeb距離（NWD）は、World Wide Webまたは別の大規模な電子データベース（Wikipediaなど）、および検索エンジンに基づく類似性または正規化されたセマンティック距離です。信頼できる集計ページ数を返します。 
[要約] nwdは、0（同一）から1（完全に異なる）までのスケールで検索語の共通の類似性を提供します。2つのセットへのnwdの制限により、以前の正規化されたGoogle距離（ngd）が得られます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2015-02-20">
        <br><font color="black">2015-02-20</font>
      </time>
    </span>
</section>
<!-- paper0: Revealing semantic and emotional structure of suicide notes with
  cognitive network science -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CL/paper_1.html">
      <font color="black">Revealing semantic and emotional structure of suicide notes with
  cognitive network science</font>
    </a>
  </h2>
  <font color="black">重要な肯定的な概念は「愛」です。これは、自殺のノート全体で意味的に目立つ方法で自己を他の人に関連付ける情報を統合します。私たちの結果は、正と負の値の用語を結びつける感情的な構文がある程度の構造化を引き起こすことを示しています感情構造が無作為化されたnullモデルよりもはるかに高いバランス。自殺する人の認知的および感情的な認識を理解することは、最も敏感な科学的課題の1つです。 
[要約]この作品は、自殺ノートの言語的内容に構造を与えます。私たちの認知ネットワーク表現は、自殺ノートの言語の定量分析を可能にします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: NITS-Hinglish-SentiMix at SemEval-2020 Task 9: Sentiment Analysis For
  Code-Mixed Social Media Text -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CL/paper_2.html">
      <font color="black">NITS-Hinglish-SentiMix at SemEval-2020 Task 9: Sentiment Analysis For
  Code-Mixed Social Media Text</font>
    </a>
  </h2>
  <font color="black">この作業は、このようなコードが混合されたHinglishテキストの感情分析を実行可能に完了するためのNITS-Hinglish-SentiMixというシステムを提案します。これらのテキストの概念を適切に理解する能力は本当に必要です。私たちのチーム、rns2020は、 SemEval2020は、コードが混在するソーシャルメディアテキストの感情分析を実行するシステムを設計する予定です。 
[要約]提案されたシステムは、テストデータでf-スコア0. 617を記録しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: COVID-19 Twitter Dataset with Latent Topics, Sentiments and Emotions
  Attributes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CL/paper_3.html">
      <font color="black">COVID-19 Twitter Dataset with Latent Topics, Sentiments and Emotions
  Attributes</font>
    </a>
  </h2>
  <font color="black">データセットの使用方法を説明するために、トピック、感情、感情の属性とそれらの時間的分布に関する記述統計を提示し、コミュニケーション、心理学、公衆衛生、経済学、疫学での可能なアプリケーションについて説明します。このリソースペーパーでは、大規模なデータセットについて説明します。 2020年1月28日から7月1日までの間に1,300万人を超えるユニークユーザーからの6,300万件を超えるコロナウイルス関連のTwitter投稿をカバーしています。ツイートには強い懸念と感情が表れているため、自然言語処理技術と機械学習を使用してツイートの内容を分析しましたベースのアルゴリズム、および各ツイートに関連付けられた推定された17の潜在的意味属性（1）10の検出されたトピックに対するツイートの関連性を示す10の属性、2）価数（すなわち、不快/快感）および感情の強さの程度を示す5つの量的属性恐怖、怒り、悲しみと喜びの4つの主要な感情にまたがる強度、および3）感情カテゴリと最も支配的な感情カテゴリをそれぞれ示す2つの定性的属性。 
[要約]自然言語処理技術と機械学習ベースのアルゴリズムを使用してツイートの内容を分析しました。各ツイートに関連付けられた17の潜在的意味属性を推定しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Product Title Generation for Conversational Systems using BERT -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CL/paper_4.html">
      <font color="black">Product Title Generation for Conversational Systems using BERT</font>
    </a>
  </h2>
  <font color="black">BERTを使用して、シーケンスからシーケンスへのアプローチを提案し、入力されたWebタイトルから短く自然な話し言葉のタイトルを生成します。画像や詳細な製品情報をユーザーに提示できるディスプレイデバイスと比較して、製品の短いタイトルが必要です。 eコマース企業は通常、簡潔さを必要とする場合、人間が作成したものかアルゴリズムによって生成された短い製品タイトルをWebページに表示しますが、これらのタイトルは自然な話し言葉とは異なります。 
[ABSTRACT] e-コマース会社はウェブページに短い製品タイトルを表示します。製品の短いタイトルは自然な話し言葉とは異なります。e-コマース会社は短いタイトルを使用して音声アシスタントと通信します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Modelling Hierarchical Structure between Dialogue Policy and Natural
  Language Generator with Option Framework for Task-oriented Dialogue System -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CL/paper_5.html">
      <font color="black">Modelling Hierarchical Structure between Dialogue Policy and Natural
  Language Generator with Option Framework for Task-oriented Dialogue System</font>
    </a>
  </h2>
  <font color="black">私たちの作業では、（1）HDNOと呼ばれるオプションフレームワークを使用して、対話ポリシーと自然言語ジェネレーター（NLG）の間の階層構造をモデル化することを提案します。 （2）階層的強化学習（HRL）を使用してHDNOをトレーニングし、架空の演劇に触発されたHRL中に対話ポリシーとNLGの間で交互の更新を提案し、ユーザーの要求を満たすと同時に、生成されたシステム発話の包括性を維持します。 （3）言語モデルでモデル化された弁別子を使用して、さらに理解しやすくするための追加の報酬として提案します。マルチドメインダイアログのデータセットであるMultiWoz 2.0およびMultiWoz 2.1でHDNOをテストし、トレーニングされた単語レベルのE2Eモデルと比較します。 RL、LaRL、およびHDSAを使用すると、自動メトリックで評価された全体的なパフォーマンスが大幅に向上しました。それにもかかわらず、RLを使用した既存の作業では、ユーザーの要求を満たすパフォーマンスを向上させると、生成されたシステム発話のわかりやすさが損なわれる可能性があります。 
[ABSTRACT]多くの以前の作品は、監督されたインスピレーションを得てe2gユーザーをトレーニングしました。ただし、不快なシステムの発話の偏りはボトルネックのままです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-11">
        <br><font color="black">2020-06-11</font>
      </time>
    </span>
</section>
<!-- paper0: Clustering of Social Media Messages for Humanitarian Aid Response during
  Crisis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CL/paper_6.html">
      <font color="black">Clustering of Social Media Messages for Humanitarian Aid Response during
  Crisis</font>
    </a>
  </h2>
  <font color="black">この研究では、ディープラーニングと自然言語処理の最近の進歩が、情報提供を分類するタスクの以前のアプローチよりも優れており、研究や展開にそれらを採用するようにフィールドを奨励していることを示しています。ソーシャルメディアは急速に不可欠なツールに成長しました危機発生時にニーズを伝え、表現するための人々。危機管理のためのソーシャルメディアデータの分析におけるこれまでの研究は、主に、実行可能な（または有益な）危機関連メッセージを自動的に識別することに焦点を当ててきました。 
[ABSTRACT]危機管理のためにソーシャルメディアデータを分析する前の作業は、学習に焦点を当てています。これらの方法を、情報提供の2つのサブタスクに拡張します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Distilling BERT into Simple Neural Networks with Unlabeled Transfer Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CL/paper_7.html">
      <font color="black">Distilling BERT into Simple Neural Networks with Unlabeled Transfer Data</font>
    </a>
  </h2>
  <font color="black">さらに、XtremeDistilを使用したこの作業の多言語拡張（MukherjeeとHassan Awadallah、2020）のために、パラメーター圧縮の点で最大35倍、バッチ推論のレイテンシスピードアップの点で最大51倍の多言語BERTのような教師モデルの大規模な蒸留を示します41言語を超えるNERのF1スコアの95％を保持します。この作業では、限られた量のラベル付きトレーニングインスタンスに加えて、ドメイン内のラベルなし転送データを大量に利用して、BERTを蒸留するためにこのギャップを埋めます。自己監督による大量のテキストの巨大なモデルの事前トレーニングの進歩により、さまざまな自然言語処理タスクで最先端の結果が得られました。 
[ABSTRACT]これらの巨大で高価なモデルは、下流のタスクで実際に使用するのは困難です。しかし、大規模な教師のパフォーマンスと比較して、小規模な学生モデルのパフォーマンスにはギャップがあることがわかります。学生モデルは、巨大な教師を26倍になりながら、教師のパフォーマンスと一致するか、わずかに超える</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-04">
        <br><font color="black">2019-10-04</font>
      </time>
    </span>
</section>
<!-- paper0: IIT Gandhinagar at SemEval-2020 Task 9: Code-Mixed Sentiment
  Classification Using Candidate Sentence Generation and Selection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CL/paper_8.html">
      <font color="black">IIT Gandhinagar at SemEval-2020 Task 9: Code-Mixed Sentiment
  Classification Using Candidate Sentence Generation and Selection</font>
    </a>
  </h2>
  <font color="black">提案されたアプローチは、Bi-LSTMベースの神経分類器と比較してシステムパフォーマンスの改善を示しています。これは、ソーシャルメディアサイト、オンラインゲーム、製品レビューなどのさまざまなプラットフォームで頻繁に使用される通信パターンです。結果ユーモアの検出、意図の分類など、テキストデータでのコード混合のさまざまなその他のニュアンスを理解する機会を提供します。
[要約]コード-ミキシングは、テキストの感情分析の課題に追加されます。提案されたアプローチは、システムパフォーマンスの向上</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-25">
        <br><font color="black">2020-06-25</font>
      </time>
    </span>
</section>
<!-- paper0: HCMS at SemEval-2020 Task 9: A Neural Approach to Sentiment Analysis for
  Code-Mixed Texts -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CL/paper_9.html">
      <font color="black">HCMS at SemEval-2020 Task 9: A Neural Approach to Sentiment Analysis for
  Code-Mixed Texts</font>
    </a>
  </h2>
  <font color="black">コード混合言語に関連する問題は、多くの場合、リソースの欠如と高度な転移学習を実行するための資料の欠如に悩まされています。この論文では、コード混合テキストの感情分類を含むSentimixヒンディー語英語タスクへの提出について説明します。また、F1スコアが67.1％であることから、単純な畳み込みと注意が妥当な結果を生み出す可能性があることを示しています。 
[ABSTRACT] 67。1％のf1スコア、単純な畳み込みと注意が妥当な結果を生成する可能性があることを示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Show, Describe and Conclude: On Exploiting the Structure Information of
  Chest X-Ray Reports -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CL/paper_10.html">
      <font color="black">Show, Describe and Conclude: On Exploiting the Structure Information of
  Chest X-Ray Reports</font>
    </a>
  </h2>
  <font color="black">この作業では、CXRイメージングレポートを生成するためのレポートセクション間およびレポートセクション内の構造情報を活用する新しいフレームワークを提案します。最初に、所見と印象の関係を明示的にモデル化する2段階の戦略を提案します。既存の研究ではめったに調査しませんこの基本的な構造情報を検討してください。 
[要約]提案されたアプローチは、構造情報を組み合わせることにより、高品質の医療レポートを生成することができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-26">
        <br><font color="black">2020-04-26</font>
      </time>
    </span>
</section>
<!-- paper0: Word Representation for Rhythms -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CL/paper_11.html">
      <font color="black">Word Representation for Rhythms</font>
    </a>
  </h2>
  <font color="black">BERTモデルは、リズムワードの構文上の可能性を探索するために作成されます。1034個のノッティンガムデータセットを使用して、サイズが450のリズムワード辞書（制御トークンなし）が生成されます。この論文では、リズムパターンのワード表現戦略を提案します。 
[ABSTRACT]サイズが450のリズム単語辞書（制御トークンなし）を作成</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning based, end-to-end metaphor detection in Greek language
  with Recurrent and Convolutional Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CL/paper_12.html">
      <font color="black">Deep Learning based, end-to-end metaphor detection in Greek language
  with Recurrent and Convolutional Neural Networks</font>
    </a>
  </h2>
  <font color="black">モデルは、トレーニングタプル、文、およびそれらのラベルに基づいてのみトレーニングおよび評価されます。提示されたモデルは、非常に高い精度スコアを達成し、すでに精度0.82を達成していた以前の最先端の結果を大幅に改善します。同等の結果精度0.91とF1スコア0.91は、双方向ゲーテッドリカレントユニット（GRU）とたたみ込みリカレントニューラルネット（CRNN）でも実現されます。 
[ABSTRACT]結果は、限られたラベル付きリソースでトレーニングされた、メタファー検出モデルの最先端のコレクションです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: AP20-OLR Challenge: Three Tasks and Their Baselines -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CL/paper_13.html">
      <font color="black">AP20-OLR Challenge: Three Tasks and Their Baselines</font>
    </a>
  </h2>
  <font color="black">KaldiとPytorchに基づいて、i-vectorシステムとx-vectorシステムのレシピも3つのタスクのベースラインとして実施されます。今年の課題は、3つのタスクを含む実際的で挑戦的な問題に焦点を当てています。 、（2）方言の識別、（3）ノイズの多いLID .. 3つのタスクのベースライン結果は、この課題のこれらのタスクが、より良いパフォーマンスを達成するためにより多くの努力を払う価値があることを示しています。 
[ABSTRACT]今年の課題は、依然として実用的でやりがいのある問題に焦点を当てています。3つのタスク：クロスチャネルのふた、言語認識、ap.theこれらのレシピはオンラインであり、レシピで利用可能になります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br><font color="black">2020-06-04</font>
      </time>
    </span>
</section>
<!-- paper0: Health, Psychosocial, and Social issues emanating from COVID-19 pandemic
  based on Social Media Comments using Natural Language Processing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CL/paper_14.html">
      <font color="black">Health, Psychosocial, and Social issues emanating from COVID-19 pandemic
  based on Social Media Comments using Natural Language Processing</font>
    </a>
  </h2>
  <font color="black">COVID-19のパンデミックは、人間の生活の多くの側面に影響を与える世界的な健康危機を引き起こしました。さらに、20の肯定的なテーマが結果から浮上しました。 
[要約]ソーシャルメディアデータは、コロナウイルスの蔓延を制御するために使用されています。これらの対策は、ワクチンや抗ウイルス薬がない場合に実施されています。これらには、物理的な距離や政策の取り組みなどのステップが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: AI4D -- African Language Dataset Challenge -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CL/paper_15.html">
      <font color="black">AI4D -- African Language Dataset Challenge</font>
    </a>
  </h2>
  <font color="black">言語および音声技術がより高度になるにつれて、データ、スペルチェッカー、品詞タガーなどのアフリカ言語の基本的なデジタルリソースの欠如は、これらの言語と他の言語との間のデジタルの格差が拡大し続けていることを意味します。タスク固有の教師付き機械学習モデルのトレーニングに使用できる注釈付きデータセットの例です。この作品は、AI4D-アフリカ言語データセットチャレンジの編成について詳しく説明します。これは、競合課題を通じてアフリカ言語データセットの作成、編成、発見を奨励する取り組みです。 。 
[ABSTRACT] ai4d-アフリカ言語のデータセットの課題は、データセットの作成、整理、発見を奨励することを目的としています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Applying GPGPU to Recurrent Neural Network Language Model based Fast
  Network Search in the Real-Time LVCSR -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CL/paper_16.html">
      <font color="black">Applying GPGPU to Recurrent Neural Network Language Model based Fast
  Network Search in the Real-Time LVCSR</font>
    </a>
  </h2>
  <font color="black">リカレントニューラルネットワーク言語モデル（RNNLM）は、その卓越したパフォーマンスにより、音声認識のさまざまな分野で使用され始めています。実験により、提案されたアプローチは、ワードエラーレート（WER）を維持しながら、さまざまな状況でリアルタイム速度を実現することが示されています。このドキュメントでは、GPNPUをRNNLMベースのグラフトラバーサルに適用する新しい方法を提案します。 
[要約]提案されたアプローチは、WSJコーパスと社内データの両方で評価されました。音声認識のさまざまな分野で提案されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: VisualCOMET: Reasoning about the Dynamic Context of a Still Image -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CL/paper_17.html">
      <font color="black">VisualCOMET: Reasoning about the Dynamic Context of a Still Image</font>
    </a>
  </h2>
  <font color="black">VisualCometを提案します。これは、以前に発生した可能性のあるイベント、次に発生する可能性のあるイベント、および現在の人々の意図を予測するための視覚的常識推論タスクの新しいフレームワークです。視覚的常識推論への研究をサポートするために、最初の視覚的常識の推論の140万以上のテキストによる説明で構成される視覚的常識グラフの大規模なリポジトリ。60,000の画像の多様なセットに注意深く注釈が付けられ、それぞれ前後の短いビデオ要約とペアになっています。つまり、画像に表示されている人々とテキストの常識の説明で言及されている人々との間の相互参照リンク）により、画像とテキストの緊密な統合が可能になります。 
[ABSTRACT]視覚的常識の推論に向けた研究をサポートするために、最初の大規模な視覚的常識イメージのキャッシュが作成されました。これには、水に浮かぶのに苦労している人のイメージと、その人の現時点での意図が留まることを含みます生きている、そして彼は近い将来助けを必要とするか、そうでなければ彼は洗い流されるでしょう</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-22">
        <br><font color="black">2020-04-22</font>
      </time>
    </span>
</section>
<!-- paper0: SBAT: Video Captioning with Sparse Boundary-Aware Transformer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/cs.CL/paper_18.html">
      <font color="black">SBAT: Video Captioning with Sparse Boundary-Aware Transformer</font>
    </a>
  </h2>
  <font color="black">SBATに基づいて、マルチモーダル相互作用を後押しするためにアラインメントされたクロスモーダルエンコードスキームをさらに提案します。 SBATは、ほとんどのメトリックで最新の方法よりも優れています。 
[要約]バニラトランスフォーマーはユニモーダル言語生成用に提案されていますが、ビデオ表現の冗長性を減らすために提案されています。sbatはローカル相関スキームを使用してローカル操作を補正します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: Gated Recurrent Context: Softmax-free Attention for Online
  Encoder-Decoder Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.AS/paper_0.html">
      <font color="black">Gated Recurrent Context: Softmax-free Attention for Online
  Encoder-Decoder Speech Recognition</font>
    </a>
  </h2>
  <font color="black">多くのASR実験を通じて、提案されたオンラインアテンションテクニックのレイテンシとパフォーマンスのトレードオフが、テストフェーズでしきい値を調整するだけで制御できることを示しています。最近、アテンションベースのエンコーダーデコーダー（AED）モデルが示されています自動音声認識（ASR）における最先端のパフォーマンス。この問題に対処するために、オンラインでの注意のための新しいソフトマックスフリー注意方法とその修正された定式化を提案します。トレーニング段階。 
[要約] asrレイテンシハイパーパラダーを削減するための新しいシステムが開発されています。これらには、ハイパーパラダーを必要としないオンラインアテンションの新しいモデルが含まれています。提案された方法は、従来のグローバルアテンションとオンラインアテンションに対して競争力のあるパフォーマンスを示しました。 -エラー-レート（wers）</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-10">
        <br><font color="black">2020-07-10</font>
      </time>
    </span>
</section>
<!-- paper0: AP20-OLR Challenge: Three Tasks and Their Baselines -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.AS/paper_1.html">
      <font color="black">AP20-OLR Challenge: Three Tasks and Their Baselines</font>
    </a>
  </h2>
  <font color="black">3つのタスクのベースライン結果は、この課題のこれらのタスクがより良いパフォーマンスを達成するためにより多くの努力を払う価値があることを示しています。KaldiおよびPytorchに基づいて、i-vectorおよびx-vectorシステムのレシピも3つのタスクのベースラインとして実施されます..このホワイトペーパーでは、APSIPA Annual Summit and Conference（APSIPA ASC）とともに、言語認識システムのパフォーマンスを向上させることを目的とした5番目の東洋言語認識（OLR）チャレンジAP20-OLRを紹介します。 
[ABSTRACT]今年の課題は、依然として実用的でやりがいのある問題に焦点を当てています。3つのタスク：クロスチャネルのふた、言語認識、ap.theこれらのレシピはオンラインであり、レシピで利用可能になります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br><font color="black">2020-06-04</font>
      </time>
    </span>
</section>
<!-- paper0: Augmentation adversarial training for unsupervised speaker recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.AS/paper_2.html">
      <font color="black">Augmentation adversarial training for unsupervised speaker recognition</font>
    </a>
  </h2>
  <font color="black">オーグメンテーションは音響特性をシミュレートするため、オーグメンテーションに対して不変であるようにネットワークをトレーニングすると、ネットワークがチャネル情報に対して一般的に不変になることも奨励されます。また、自己監視モデルのパフォーマンスは人間のパフォーマンスをはるかに超えています。しかし、発話内セグメントは同じ音響特性を共有しているため、チャネル情報から話者情報を分離することは困難です。 
[ABSTRACT]作品は、対照的な学習に基づいています。この学習では、発話の埋め込みが似ていないことを奨励します。適用された拡張にバインドしながら、ネットワークをスピーカー情報に対して識別できるように訓練する拡張敵対的なトレーニング戦略が望まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Sound Field Translation and Mixed Source Model for Virtual Applications
  with Perceptual Validation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.AS/paper_3.html">
      <font color="black">Sound Field Translation and Mixed Source Model for Virtual Applications
  with Perceptual Validation</font>
    </a>
  </h2>
  <font color="black">シネマフィルムのような非インタラクティブでリニアなエクスペリエンスは、没入感を高める高品質のサラウンドサウンドオーディオを提供しますが、通常、リスナーのエクスペリエンスは単一の音響の視点に固定されます。実験..数値シミュレーションとの比較調査により、スパース展開が固有のスイートスポット制約を緩和し、スパース環境のローカライザビリティが向上することが実証されました。 
[要約]提案された方法は、並進された位置での歪みに対する改善された音源定位と堅牢性の両方を提供</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Sound2Sight: Generating Visual Dynamics from Sound and Context -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.AS/paper_4.html">
      <font color="black">Sound2Sight: Generating Visual Dynamics from Sound and Context</font>
    </a>
  </h2>
  <font color="black">この埋め込みは、マルチヘッドアテンションベースのオーディオビジュアルトランスフォーマーエンコーダーによって学習されます。確率的事前確率により、モデルは、提供されたオーディオおよび過去のコンテキストと一致する複数のもっともらしい未来をサンプリングできます。さらに、品質と生成されたフレームの一貫性を考慮して、合成クリップと実際の視聴覚クリップを区別するマルチモーダル弁別器を提案します。 
[ABSTRACT]フレームごとの確率的事前学習は、オーディオと過去のフレームの同時埋め込みを条件としています。これをサンプリングして、ビデオ予測モジュールをさらに調整し、将来のフレームを生成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Version Control of Speaker Recognition Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.AS/paper_5.html">
      <font color="black">Version Control of Speaker Recognition Systems</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、実稼働環境での展開方法に応じて、さまざまな種類の話者認識システムのさまざまなバージョン管理戦略について説明します。一般的な話者認識システムは、2つの段階で構成されます。登録段階では、プロファイルが生成されます。ユーザー提供の登録音声;ランタイムステージでは、ランタイムオーディオの音声IDが保存されているプロファイルと比較されます。このホワイトペーパーでは、話者認識システムで最も困難な実用的なエンジニアリング問題の1つであるモデルとユーザープロファイルのバージョン管理について説明します。 
[ABSTRACT]システムは2つの段階で構成されています。登録段階では、ユーザーが提供した登録音声からプロファイルが生成されます。ランタイムステージ。結果として、システムを更新する必要はありませんが、より正確にする必要があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Sequential Routing Framework: Fully Capsule Network-based Speech
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-24/eess.AS/paper_6.html">
      <font color="black">Sequential Routing Framework: Fully Capsule Network-based Speech
  Recognition</font>
    </a>
  </h2>
  <font color="black">これは、CNNベースのCTCネットワークよりも0.8％正確で、リカレントニューラルネットワークトランスデューサー（RNN-T）の精度と同等です。ルーティング中、2種類の情報、学習可能な重み、反復出力がスライス間で共有されます。 SRFでは、入力シーケンスはカプセル化され、ウィンドウサイズでスライスされます。 
[要約]提案された方法は82. 6％の音素認識率を達成します。2つのネットワークと比較して必要なパラメーターは半分未満です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
