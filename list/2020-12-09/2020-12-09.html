<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-12-09の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: A Geometric Framework for Pitch Estimation on Acoustic Musical Signals -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.SD/paper_0.html">
      <font color="black">A Geometric Framework for Pitch Estimation on Acoustic Musical Signals</font>
    </a>
  </h2>
  <font color="black">現在の多くの技術は、信じられないほど効果的ですが、音響音楽信号によって示される複雑な音楽パターンを支える基礎となる数学的構造を引き出すことを目的としていません。この論文で提示されたフレームワークは、PE問題に取り組むためのまったく新しい方法を開きます。従来の分析アプローチと、現在文献を支配している新しい機械学習（ML）手法の両方で使用されている可能性があります。理論的および実験的観点の両方からアプローチに取り組み、さらなる作業の基礎となる新しいフレームワークを提示します。その分野で、そして（最先端ではないが）相対的な有効性を示す結果。 
[ABSTRACT]モノピッチ推定とマルチピッチ推定は、数学的にも概念的にも困難であることが証明されています。この方法は、この分野でのさらなる研究の基礎となる新しいフレームワークと、（最先端ではありませんが）十分に実証された結果とともに提示されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: SAR-Net: A End-to-End Deep Speech Accent Recognition Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.SD/paper_1.html">
      <font color="black">SAR-Net: A End-to-End Deep Speech Accent Recognition Network</font>
    </a>
  </h2>
  <font color="black">私たちの深いフレームワークはマルチタスク学習メカニズムを採用しており、主に3つのモジュールで構成されています。共有CNNおよびRNNベースのフロントエンドエンコーダー、コアアクセント認識ブランチ、および音声スペクトログラムを入力として受け取る補助音声認識ブランチです。アクセントはスピーキング関連の音色であるため、音声認識ブランチを追加すると、トレーニング中のアクセント認識のオーバーフィット現象が効果的に抑制されます。より具体的には、共有エンコーダから学習したシーケンシャル記述子を使用して、アクセント認識ブランチは最初にすべての記述子を凝縮します。次に、埋め込みベクトルに変換し、顔認識ドメインで人気のあるさまざまな識別損失関数を調べて、埋め込み識別を強化します。 
[ABSTRACT]アクセント認識は、同じアクセントを持つスピーカーのコンパクトなグループレベルの機能を取得する上でより難しい問題です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-25">
        <br><font color="black">2020-11-25</font>
      </time>
    </span>
</section>
<!-- paper0: Environment Sound Classification using Multiple Feature Channels and
  Attention based Deep Convolutional Neural Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.SD/paper_2.html">
      <font color="black">Environment Sound Classification using Multiple Feature Channels and
  Attention based Deep Convolutional Neural Network</font>
    </a>
  </h2>
  <font color="black">私たちの知る限り、単一環境の音声分類モデルが3つのデータセットすべてで最先端の結果を達成できるのはこれが初めてです。このような複数の機能は、信号または音声処理にこれまで使用されたことはありません。 ..この論文の目新しさは、Mel-Frequency Cepstral Coefficients（MFCC）、Gammatone Frequency Cepstral Coefficients（GFCC）、Constant Q-transform（CQT）、およびChromagramで構成される複数の機能チャネルを使用することにあります。 
[概要]複数の機能チャネルには、mel-周波数ケプストラム度（mfcc）、ガンマトーン周波数ケプストラムエントリ（gfcc）、定数q-変換（cqt）、およびクロマトグラムが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-28">
        <br><font color="black">2019-08-28</font>
      </time>
    </span>
</section>
<!-- paper0: I'm Sorry for Your Loss: Spectrally-Based Audio Distances Are Bad at
  Pitch -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.SD/paper_3.html">
      <font color="black">I'm Sorry for Your Loss: Spectrally-Based Audio Distances Are Bad at
  Pitch</font>
    </a>
  </h2>
  <font color="black">私たちのタスクは人間にとっては些細なことですが、これらの音声距離では困難であり、電流損失を改善することで自己教師あり音声学習を大幅に進歩させることができることを示唆しています。ピッチ距離を測定する合成ベンチマークで、一般的に使用される音声間損失を比較します。 2つの静止した正弦波の間..結果は驚くべきものです。多くの場合、ピッチ方向の感覚が不十分です。 
[ABSTRACT]合成ベンチマークは、2つの静止正弦波間のピッチ距離を測定します。結果は、合成ベンチマークのデータに基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Spectral Response Function Guided Deep Optimization-driven Network for
  Spectral Super-resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.IV/paper_0.html">
      <font color="black">Spectral Response Function Guided Deep Optimization-driven Network for
  Spectral Super-resolution</font>
    </a>
  </h2>
  <font color="black">完全なデータ駆動型CNNとは異なり、補助スペクトル応答関数（SRF）を使用して、CNNをガイドし、スペクトル関連性のあるバンドをグループ化します。スペクトル超解像（SSR）は、高空間分解能（HR）ハイパースペクトル画像を取得するために使用される方法です。 HRマルチスペクトル画像から..さらに、チャネルアテンションモジュール（CAM）と再定式化されたスペクトル角度マッパー損失関数を適用して、効果的な再構成モデルを実現します。 
[ABSTRACT]ハイパースペクトル画像は、ハイパースペクトル画像を作成するために使用される方法です。この方法は、高空間解像度（hr）のハイパースペクトル画像を取得するために使用されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Blockchain-Federated-Learning and Deep Learning Models for COVID-19
  detection using CT Imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.IV/paper_1.html">
      <font color="black">Blockchain-Federated-Learning and Deep Learning Models for COVID-19
  detection using CT Imaging</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、さまざまなソース（さまざまな病院）から少量のデータを収集し、ブロックチェーンベースのフェデレーションラーニングを使用してグローバルディープラーニングモデルをトレーニングするフレームワークを提案します。ブロックチェーンテクノロジーはデータを認証し、フェデレーションラーニングはモデルのプライバシーを保護しながらグローバルにトレーニングします。組織..コラボレーションモデルの構築とプライバシーの保護は、グローバルな深層学習モデルをトレーニングするための主要な関心事です。 
[概要]病院は、covidの陽性症例を特定するのが困難に直面しています-19人の患者。これは、検査キットの不足と信頼性によるものです。これは、ウイルスの症例を特定できないことを意味し、検査の不足につながる可能性があります。これらのテストツールを使用して、グローバルな深層学習モデルを作成できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-10">
        <br><font color="black">2020-07-10</font>
      </time>
    </span>
</section>
<!-- paper0: CoShaRP: A Convex Program for Single-shot Tomographic Shape Sensing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.IV/paper_2.html">
      <font color="black">CoShaRP: A Convex Program for Single-shot Tomographic Shape Sensing</font>
    </a>
  </h2>
  <font color="black">したがって、形状事前は、線形の不適切な画像推定問題を、形状の回転並進を推定する非線形問題に変換します。さらに、十分な数の投影角度が形成される従来の断層撮影よりも困難です。測定により、単純な反転プロセスが可能になります。このペーパーでは、形状の可能な回転変換の辞書を使用して、非線形性を回避します。 
[要約]論文では、可能なロトの辞書を使用して非線形性を回避しています-translations.cosharpはシンプレックス-タイプの制約に依存しており、primal-デュアルアルゴリズムを使用してすばやく解決できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: NightVision: Generating Nighttime Satellite Imagery from Infra-Red
  Observations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.IV/paper_3.html">
      <font color="black">NightVision: Generating Nighttime Satellite Imagery from Infra-Red
  Observations</font>
    </a>
  </h2>
  <font color="black">利用可能な赤外線観測を使用して可視画像を生成することで、ギャップを埋めることができます。衛星画像への機械学習のアプリケーションの最近の爆発は、可視画像に依存することが多く、したがって夜間のデータ不足に悩まされています。提案された方法有望な結果を示し、独立したテストセットで最大86 \％の構造類似性指数（SSIM）を達成し、赤外線観測から生成された視覚的に説得力のある出力画像を提供します。 
[概要]提案された方法は、赤外線画像を学習することによって作成できます。これらには、衛星画像を使用して可視画像を生成することが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-13">
        <br><font color="black">2020-11-13</font>
      </time>
    </span>
</section>
<!-- paper0: Comparison of Image Quality Models for Optimization of Image Processing
  Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.IV/paper_4.html">
      <font color="black">Comparison of Image Quality Models for Optimization of Image Processing
  Systems</font>
    </a>
  </h2>
  <font color="black">ここでは、画像処理アルゴリズムの最適化の目的としての使用に関してIQAモデルの大規模な比較を実行します。最適化された画像の主観的テストにより、知覚パフォーマンスの観点から競合モデルをランク付けし、それらを解明することができます。これらのタスクの相対的な長所と短所、および将来のIQAモデルに組み込むための一連の望ましいプロパティを提案します。この目的のために収集された知覚データセットは、IQAメソッドを改善するための有用なベンチマークを提供しましたが、それらを頻繁に使用すると、過剰適合のリスクが生じます。 
[概要] iqaモデルはiqaモデルを改善するために使用されていますが、頻繁に使用すると過剰適合のリスクが生じます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-04">
        <br><font color="black">2020-05-04</font>
      </time>
    </span>
</section>
<!-- paper0: Depth estimation from 4D light field videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.IV/paper_5.html">
      <font color="black">Depth estimation from 4D light field videos</font>
    </a>
  </h2>
  <font color="black">合成および実世界の4DLFビデオを使用した実験結果は、時間情報がノイズの多い領域での深度推定精度の向上に寄与することを示しています。データセットとコードはhttps://mediaeng-lfv.github.io/LFV_Disparity_Estimationで入手できます。この論文では、4DLFビデオから深度を推定するためのエンドツーエンドのニューラルネットワークアーキテクチャを提案します。 
[概要]ほとんどの研究は、静的な4d lf画像からの深度推定に焦点を当てています。この研究は、中規模の合成4dlfビデオデータセットも構築します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-05">
        <br><font color="black">2020-12-05</font>
      </time>
    </span>
</section>
<!-- paper0: Two-stage multi-scale breast mass segmentation for full mammogram
  analysis without user intervention -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.IV/paper_6.html">
      <font color="black">Two-stage multi-scale breast mass segmentation for full mammogram
  analysis without user intervention</font>
    </a>
  </h2>
  <font color="black">したがって、統合されたコンピュータ支援診断システムは、自動で正確な胸のしこりの描写のために臨床医を支援するために必要です。ただし、ネイティブマンモグラムから胸のしこりを手動でセグメント化することは時間がかかり、エラーが発生しやすいです。 -高解像度のフルマンモグラムから正確な質量輪郭を提供するステージマルチスケールパイプライン。 
[ABSTRACT] x-光線マンモグラム分析は、主に疑わしい関心領域のローカリゼーションとそれに続くセグメンテーションを指し、良性と悪性の病変分類をさらに進めます。その中には、畳み込みエンコーダー-ネストされた高密度スキップ接続を使用したデコーダーネットワークがあります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-27">
        <br><font color="black">2020-02-27</font>
      </time>
    </span>
</section>
<!-- paper0: Process of image super-resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.IV/paper_7.html">
      <font color="black">Process of image super-resolution</font>
    </a>
  </h2>
  <font color="black">ニューロンのネットワークと画像または画像ライブラリに基づいて学習する超解像再構成のモデルを使用して、低解像度画像を高解像度画像で再構成する方法も知られています。この技術的進歩は、フランス特許FR1855485（https://patents.google.com/patent/FR3082980A1、HALリファレンスhttps://hal.archives-ouvertes.fr/hal-01875898v1を参照）の要求に応じて2018年。そのカテゴリ。ゾーンは、画像のすっきりを高めるために、前述のゾーンにピクセルを追加するために使用される補間のタイプを決定します。 
[概要]超解像再構成を使用して高解像度画像を取得する方法が知られています。このプロセスでは、1つまたは複数の低解像度画像を削除します。この論文の目的は、人間の顔を再構成できることを実証することです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-17">
        <br><font color="black">2019-04-17</font>
      </time>
    </span>
</section>
<!-- paper0: Digital Gimbal: End-to-end Deep Image Stabilization with Learnable
  Exposure Times -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.IV/paper_8.html">
      <font color="black">Digital Gimbal: End-to-end Deep Image Stabilization with Learnable
  Exposure Times</font>
    </a>
  </h2>
  <font color="black">単一の画像のぼけを除去したり、固定露出バーストのノイズを除去したりする従来のアプローチに対するこの方法の利点を示します。さらに、バーストの露出時間をエンドツーエンドで学習し、フレーム全体のノイズとブラーのバランスをとることをお勧めします。この作業では、高速で安定化されていないカメラの入力から、機械的に安定化されたシステムをデジタルでエミュレートすることを提案します。 
[概要]未知の動きに関連する、ノイズの多い短い露出フレームのバーストを集約することにより、シャープな高SN比画像を推定するCNNをトレーニングします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Interpretable deep learning regression for breast density estimation on
  MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.IV/paper_9.html">
      <font color="black">Interpretable deep learning regression for breast density estimation on
  MRI</font>
    </a>
  </h2>
  <font color="black">350人の患者を使用してCNNをトレーニングし、75人を検証に、81人を独立したテストに使用しました。回帰畳み込みニューラルネットワーク（CNN）を使用して506人の乳がん患者の乳房密度を評価しました。CNNの入力は乳房のスライスでした。 128 x 128ボクセルのMRIであり、出力は0（脂肪の多い乳房）と1（高密度の乳房）の間の連続密度値でした。 
[概要]テストでcnnによって予測された密度は、有意に相関していました。cnnが予測した場合、fgtと脂肪組織に基づいていました。これは、密度の予測が構造に基づいていることを意味します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: A software decoder implementation for H.266/VVC video coding standard -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.IV/paper_10.html">
      <font color="black">A software decoder implementation for H.266/VVC video coding standard</font>
    </a>
  </h2>
  <font color="black">Versatile Video Coding Standard（H.266 / VVC）は、2020年7月にITU-TおよびISO / IECのJoint Video Expert Team（JVET）によって完成されました。SIMD命令拡張とデータおよび追加の並列処理を使用した最適化されたデコーダーの実装x86ベースのCPUで4K60fps VVCビットストリームのリアルタイムデコードを実現できるタスクレベルの並列処理が提示されます。この新しいITU推奨/国際標準は、よく知られているH.265 / HEVCビデオコーディング標準の後継です。圧縮効率は2倍になりますが、計算が複雑になるという犠牲も伴います。 
[概要]新しいitu推奨/国際規格はhの後継です。 265 / hevcビデオコーディングstandard.h。 265またはhevcビデオ放送規格は圧縮効率を約2倍にしました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-04">
        <br><font color="black">2020-12-04</font>
      </time>
    </span>
</section>
<!-- paper0: GMM-Based Generative Adversarial Encoder Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.IV/paper_11.html">
      <font color="black">GMM-Based Generative Adversarial Encoder Learning</font>
    </a>
  </h2>
  <font color="black">私たちのフレームワークは一般的であり、どのGAN戦略にも簡単に組み込むことができます。特に、VanillaGANとWassersteinGANの両方でそれを示し、ISスコアとFIDスコアの両方の点で生成された画像の改善につながります。 .. GANは画像を生成するための強力なモデルですが、潜在空間を推測できないため、エンコーダーを必要とするアプリケーションでの使用が直接制限されます。 
[概要]エンコーダーを使用して、gmmを使用してエンコーダーを作成できます。両方で、isスコアとfidスコアの両方の点で生成された画像の改善につながります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Overcomplete Representations Against Adversarial Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.IV/paper_12.html">
      <font color="black">Overcomplete Representations Against Adversarial Videos</font>
    </a>
  </h2>
  <font color="black">このアプローチは、グローバルな情報を収集するための大きな受容野を持っているが、ローカルの詳細を見落としている不完全な表現を学習します。実験結果は、画像に焦点を当てた防御がビデオに対して効果がない可能性があることを示しています。攻撃、物理的に実現可能な攻撃への乗法攻撃。ただし、攻撃されたビデオから防御するために開発された防御方法はほんの一握りです。 
[概要]ほとんどの復元ネットワークはエンコーダー-デコーダーアーキテクチャを採用しています。最初に空間次元を縮小し、次に拡大します。過剰な表現には反対の特性があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Harnessing Multi-View Perspective of Light Fields for Low-Light Imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.IV/paper_13.html">
      <font color="black">Harnessing Multi-View Perspective of Light Fields for Low-Light Imaging</font>
    </a>
  </h2>
  <font color="black">したがって、L3Fnetと呼ばれる低照度ライトフィールド（L3F）復元用のディープニューラルネットワークを提案します。提案されたL3Fnetの有効性は、このデータセットの視覚的比較と数値的比較の両方によってサポートされます。シングルフレームDSLR画像をL3Fnetに適した形式に変換します。これを疑似LFと呼びます。 
[概要]提案されたl3fnetは、各lfビューの必要な視覚的強化を実行します。また、ビュー全体でエピポーラジョエルを保持し、cnnのkat kinectを記述します。さまざまなシーンのデータセット</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-05">
        <br><font color="black">2020-03-05</font>
      </time>
    </span>
</section>
<!-- paper0: Bayesian Image Reconstruction using Deep Generative Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.IV/paper_14.html">
      <font color="black">Bayesian Image Reconstruction using Deep Generative Models</font>
    </a>
  </h2>
  <font color="black">生成モデルによるベイズ再構成（BRGM）と呼ばれる私たちの方法は、事前にトレーニングされた単一のジェネレータモデルを使用して、さまざまな前方破損モデルと組み合わせることにより、さまざまな画像復元タスク、つまり超解像とインペインティングを解決します。強力な事前情報を構築できる3つの大規模で多様なデータセットのBRGM：（i）Flick Faces HighQualityデータセットからの60,000枚の画像\ cite {karras2019style}（ii）MIMIC IIIからの240,000枚の胸部X線および（iii）a 5つの脳MRIデータセットと7,329スキャンを組み合わせたコレクション。コードと事前トレーニング済みモデルをオンラインで利用できるようにします。 
[概要]コードと事前トレーニング済みモデルはオンラインで入手できます。これらは、強力な事前分布を構築できる3つの大きなデータセットでbrgmを示しました。これらには、画像のペアでトレーニングするトレーニングシステムが含まれています。また、状態を使用しています。同じ手法を使用した後期美術モデルモデル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Hierarchical Residual Attention Network for Single Image
  Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.IV/paper_15.html">
      <font color="black">Hierarchical Residual Attention Network for Single Image
  Super-Resolution</font>
    </a>
  </h2>
  <font color="black">並行して、軽量の階層的アテンションメカニズムは、ネットワークからアテンションバンクに最も関連性の高い機能を抽出して、最終出力を改善し、ネットワーク内の連続操作による情報損失を防ぎます。したがって、処理は2つの独立した計算パスに分割されます。これを同時に実行できるため、低解像度の画像から高解像度の画像の細部を再構築するための非常に効率的で効果的なモデルが得られます。残りの特徴を効率的に利用するために、これらは階層的に特徴に集約されます。ネットワーク出力での事後使用のためのバンク。 
[概要]この論文では、新しい軽量の超解像モデルを紹介します。並行して、軽量の空間的注意メカニズムが、ネットワークから注意バンクに最も関連性の高いものを抽出します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Channel Pruning via Multi-Criteria based on Weight Dependency -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.IV/paper_16.html">
      <font color="black">Channel Pruning via Multi-Criteria based on Weight Dependency</font>
    </a>
  </h2>
  <font color="black">さらに、多くの剪定方法は、評価に1つの基準のみを使用し、試行錯誤の方法で剪定構造と精度のスイートスポットを見つけます。これには時間がかかる可能性があります。機能マップの重要性は、関連する重み値、計算コスト、およびパラメーター量を含む3つの側面。重みの依存関係を無視します。 
[ABSTRACT]チャネルプルーニングアルゴリズムは、さまざまなモデルを効率的に圧縮できます。重み依存の現象を使用して、関連するフィルターと次のレイヤーの対応する部分重みを評価することで重要性を取得します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-06">
        <br><font color="black">2020-11-06</font>
      </time>
    </span>
</section>
<!-- paper0: R3Det: Refined Single-Stage Detector with Feature Refinement for
  Rotating Object -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.IV/paper_17.html">
      <font color="black">R3Det: Refined Single-Stage Detector with Feature Refinement for
  Rotating Object</font>
    </a>
  </h2>
  <font color="black">特徴改良モジュールの重要なアイデアは、現在の改良された境界ボックスの位置情報を、ピクセル単位の特徴補間によって対応する特徴点に再エンコードして、特徴の再構築と位置合わせを実現することです。3つの人気のあるリモートセンシングパブリックデータセットDOTA、 HRSC2016、UCAS-AOD、および1つのシーンテキストデータセットICDAR2015は、私たちのアプローチの有効性を示しています。回転検出は、マルチアングルオブジェクトの位置を特定し、それらを背景から効果的に分離することが難しいため、困難な作業です。 
[ABSTRACT]より正確な特徴を取得することで検出パフォーマンスを向上させる新しい特徴改良モジュール。特徴改良により、より詳細な特徴を取得できます。より正確な回転情報のために、正確なスキュー損失が提案され、スキューの計算が導出できない</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-15">
        <br><font color="black">2019-08-15</font>
      </time>
    </span>
</section>
<!-- paper0: Raw Image Deblurring -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.IV/paper_18.html">
      <font color="black">Raw Image Deblurring</font>
    </a>
  </h2>
  <font color="black">最終的には、考案された新しいrawベースのぼけ除去方法とまったく新しいDeblur-RAWデータセットに基づいて、さらなる機会の新しい場所を示します。したがって、RAW画像と処理されたsRGB画像の両方を含む新しいデータセットを構築し、新しいRAW画像の独自の特性を利用するモデル。RAW画像のみからトレーニングされた提案されたブレ除去モデルは、最先端のパフォーマンスを実現し、処理されたsRGB画像でトレーニングされたモデルを上回ります。 
[概要]提案されたブレ除去カーネルは、生の画像からのみ機能します。ただし、私たちの知る限り、利用可能な新しいブレ除去ネットワークはありません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Spectral Response Function Guided Deep Optimization-driven Network for
  Spectral Super-resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_0.html">
      <font color="black">Spectral Response Function Guided Deep Optimization-driven Network for
  Spectral Super-resolution</font>
    </a>
  </h2>
  <font color="black">最後に、自然とリモートセンシング画像を含む2種類のデータセットでの実験は、提案された方法のスペクトル増強効果を示しています。完全にデータ駆動型のCNNとは異なり、補助スペクトル応答関数（SRF）を使用してCNNをグループ化しますスペクトル関連性のあるバンド..従来のSSR手法には、モデル駆動型アルゴリズムと深層学習が含まれます。 
[ABSTRACT]ハイパースペクトル画像は、ハイパースペクトル画像を作成するために使用される方法です。この方法は、高空間解像度（hr）のハイパースペクトル画像を取得するために使用されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Generalized iterated-sums signatures -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_1.html">
      <font color="black">Generalized iterated-sums signatures</font>
    </a>
  </h2>
  <font color="black">機械学習アプリケーションに精神的に近い、反復和署名の3つの非線形変換を紹介し、それらのプロパティの一部を示します。以前の作業に触発された反復和署名の一般化バージョンの代数的プロパティを調査します。 F.〜Kir \ &#39;alyとH.〜Oberhauser ..特に、テンソル代数上の単語の変形された準シャッフル積を検討することにより、テンソル代数上の関連する線形マップの文字プロパティを復元する方法を示します。 
[概要]関連する線形マップの文字プロパティを復元する方法を示します。これらには、変形された準-後者の単語のシャッフル積が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: 3DIoUMatch: Leveraging IoU Prediction for Semi-Supervised 3D Object
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_2.html">
      <font color="black">3DIoUMatch: Leveraging IoU Prediction for Semi-Supervised 3D Object
  Detection</font>
    </a>
  </h2>
  <font color="black">そのために、信頼性ベースのフィルタリングメカニズムを導入します。人気のあるポイントクラウドベースのオブジェクト検出器であるVoteNetをバックボーンとして採用し、教師と生徒の相互学習フレームワークを活用して、ラベル付きのトレインセットからラベルなしのトレインセットに情報を伝播します。疑似ラベルの形式で..3Dオブジェクト検出は、取得が困難な3D注釈に大きく依存する、重要でありながら要求の厳しいタスクです。 
[概要]私たちのアプローチの鍵は、新しい微分可能な3d iou推定モジュールです。たとえば、3dioumatchは、テスト時間の強化を通じてローカリゼーションを改善するために使用されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: BanglaWriting: A multi-purpose offline Bangla handwriting dataset -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_3.html">
      <font color="black">BanglaWriting: A multi-purpose offline Bangla handwriting dataset</font>
    </a>
  </h2>
  <font color="black">通常の単語とは別に、データセットは261のわかりやすい上書きと450の手書きのストライキとミスで構成されます。すべてのバウンディングボックスと単語ラベルは手動で生成されます。データセットは、複雑な光学文字/単語認識、ライターの識別に使用できます。 、手書きの単語セグメンテーション、および単語生成。 
[概要]各ページには境界が含まれています-各単語を境界付けるボックス。バングラ語彙の234個の一意の単語が含まれています。このデータセットには5,470個以上の一意の単語が含まれています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-15">
        <br><font color="black">2020-11-15</font>
      </time>
    </span>
</section>
<!-- paper0: StacMR: Scene-Text Aware Cross-Modal Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_4.html">
      <font color="black">StacMR: Scene-Text Aware Cross-Modal Retrieval</font>
    </a>
  </h2>
  <font color="black">次に、このデータセットを使用して、キャプションからのテキストとビジュアルシーンからのテキストに特殊な表現を使用し、それらを共通に調整する、より優れたシーンテキスト対応のクロスモーダル検索方法など、シーンテキストを活用するいくつかのアプローチについて説明します。埋め込みスペース..クロスモーダル検索の最近のモデルは、いくつか言及すると、シーングラフとオブジェクトの相互作用によって提供される視覚シーンのますます豊富な理解から恩恵を受けています。データセットとコードはhttp://europe.naverlabs.comで入手できます。 / stacmr 
[ABSTRACT]これにより、画像の視覚的表現とそのキャプションの文学的表現との間のマッチングが改善されました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Two-Phase Learning for Overcoming Noisy Labels -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_5.html">
      <font color="black">Two-Phase Learning for Overcoming Noisy Labels</font>
    </a>
  </h2>
  <font color="black">最初のフェーズでは、MORPHは、遷移点の前にすべてのトレーニングサンプルのネットワークの更新を開始します。その後、MORPHは、最大のセーフセットに対してのみネットワークのトレーニングを再開します。これにより、ほぼ確実に真のラベルが付けられたサンプルの収集が維持されます。各エポック..その2フェーズ学習により、MORPHは、実用的なあらゆるタイプのラベルノイズに対するノイズのないトレーニングを実現します。 
[ABSTRACT] morphは、ネットワークが誤ったラベルの付いたサンプルを急速に記憶し始めた時点で、学習フェーズを自動的に切り替えます。監視なしで、学習フェーズは、推定された最良の遷移ポイントに基づいて次のフェーズに変換されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Blockchain-Federated-Learning and Deep Learning Models for COVID-19
  detection using CT Imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_6.html">
      <font color="black">Blockchain-Federated-Learning and Deep Learning Models for COVID-19
  detection using CT Imaging</font>
    </a>
  </h2>
  <font color="black">この論文では、さまざまなソース（さまざまな病院）から少量のデータを収集し、ブロックチェーンベースのフェデレーション学習を使用してグローバルディープラーニングモデルをトレーニングするフレームワークを提案します。まず、データの不均一性を処理するデータ正規化手法を提案します。データは、さまざまな種類のCTスキャナーを備えたさまざまな病院から収集されます。ブロックチェーンテクノロジーがデータを認証し、組織のプライバシーを保護しながら、フェデレーションラーニングがモデルをグローバルにトレーニングします。 
[概要]病院は、covidの陽性症例を特定するのが困難に直面しています-19人の患者。これは、検査キットの不足と信頼性によるものです。これは、ウイルスの症例を特定できないことを意味し、検査の不足につながる可能性があります。これらのテストツールを使用して、グローバルな深層学習モデルを作成できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-10">
        <br><font color="black">2020-07-10</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Objective Interpolation Training for Robustness to Label Noise -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_7.html">
      <font color="black">Multi-Objective Interpolation Training for Robustness to Label Noise</font>
    </a>
  </h2>
  <font color="black">ハイパーパラメータとアブレーションの研究により、私たちの方法の重要な要素が検証されます。逆に、ラベルノイズの下での教師あり対照学習の動作を調査して、これらのシナリオで画像分類を改善する方法を理解します。標準の対照学習は、ノイズにラベルを付け、この動作を軽減するための補間トレーニング戦略を提案します。 
[概要]このシステムは、統合のための新しいモデルを開発するために使用できます。また、新しいテストシステムのツールとしても使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: CoShaRP: A Convex Program for Single-shot Tomographic Shape Sensing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_8.html">
      <font color="black">CoShaRP: A Convex Program for Single-shot Tomographic Shape Sensing</font>
    </a>
  </h2>
  <font color="black">数値実験は、CoShaRPが適度にノイズの多い測定から安定して形状を回復することを示しています。シングルコーンビーム投影測定からターゲット画像を推定することを目的としたシングルショットX線トモグラフィーを紹介します。したがって、形状は事前に線形病気を変換します。 -形状の回転変換を推定する非線形問題に画像推定問題を提起しました。 
[要約]論文では、可能なロトの辞書を使用して非線形性を回避しています-translations.cosharpはシンプレックス-タイプの制約に依存しており、primal-デュアルアルゴリズムを使用してすばやく解決できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Texture Transform Attention for Realistic Image Inpainting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_9.html">
      <font color="black">Texture Transform Attention for Realistic Image Inpainting</font>
    </a>
  </h2>
  <font color="black">テクスチャ変換アテンションネットワーク（TTA-Net）は、細かいディテールで欠落領域の修復をより適切に生成します。これらの観察に動機付けられて、パッチベースの方法を提案します。テクスチャ変換アテンションは、細かいテクスチャを使用して新しい再構成されたテクスチャマップを作成するために使用されます。結果としてテクスチャ情報を効率的に転送できる粗いセマンティクス。 
[概要]パッチベースのパッチベースのパッチベースのネットワークは、パッチベースのパッチベースのシステムに基づいています。パッチベースのパッチベースの接続を使用して、「パッチベース」のソリューションを作成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Portrait Style Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_10.html">
      <font color="black">Learning Portrait Style Representations</font>
    </a>
  </h2>
  <font color="black">この作業を容易にするために、計算分析用に準備された肖像画の最初の大規模データセットも提示します。これらの専門家の人間の知識、統計、および写真のリアリズムの優先順位が美術史研究とスタイル表現に与える影響を調整し、これらの表現を使用してアーティストのゼロショット分類を実行します。コンピュータービジョンでのアートワークのスタイル分析は、主に、ブラシストロークなどの低レベルのスタイル特性の理解を最適化することにより、ターゲット画像生成の結果を達成することに焦点を当てています。 
[概要]スタイルの類似性の監視として美術史家によって注釈が付けられたトリプレットを組み込むことで、学習したスタイルの特徴に変化が見られます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: NightVision: Generating Nighttime Satellite Imagery from Infra-Red
  Observations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_11.html">
      <font color="black">NightVision: Generating Nighttime Satellite Imagery from Infra-Red
  Observations</font>
    </a>
  </h2>
  <font color="black">利用可能な赤外線観測を使用して可視画像を生成することで、ギャップを埋めることができます。衛星画像への機械学習のアプリケーションの最近の爆発は、可視画像に依存することが多く、したがって夜間のデータ不足に悩まされています。提案された方法有望な結果を示し、独立したテストセットで最大86 \％の構造類似性指数（SSIM）を達成し、赤外線観測から生成された視覚的に説得力のある出力画像を提供します。 
[概要]提案された方法は、赤外線画像を学習することによって作成できます。これらには、衛星画像を使用して可視画像を生成することが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-13">
        <br><font color="black">2020-11-13</font>
      </time>
    </span>
</section>
<!-- paper0: Vid2CAD: CAD Model Alignment using Multi-View Constraints from Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_12.html">
      <font color="black">Vid2CAD: CAD Model Alignment using Multi-View Constraints from Videos</font>
    </a>
  </h2>
  <font color="black">マルチビュー制約を活用することで、このメソッドはオクルージョンを解決し、個々のフレームで表示されていないオブジェクトを処理して、すべてのオブジェクトをシーンの単一のグローバルに一貫したCAD表現に再構築します。私たちが構築しているアートシングルフレームメソッドMask2CADは、Scan2CADを大幅に改善します（クラス平均精度11.6％から30.2％）。この統合プロセスにより、フレームごとの予測におけるスケールと深度のあいまいさが解決され、一般にすべてのポーズパラメータの推定。 
[ABSTRACT]私たちの方法は、任意のビデオを処理し、そこに表示される各オブジェクトの9 dofポーズを完全に回復することができます。この統合プロセスは、フレームごとの予測でスケールとネットワークのあいまいさを解決し、一般的にすべてのポーズの推定を厳密に改善します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Comparison of Image Quality Models for Optimization of Image Processing
  Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_13.html">
      <font color="black">Comparison of Image Quality Models for Optimization of Image Processing
  Systems</font>
    </a>
  </h2>
  <font color="black">最適化された画像の主観的テストにより、知覚パフォーマンスの観点から競合モデルをランク付けし、これらのタスクにおける相対的な長所と短所を解明し、将来のIQAモデルに組み込むための一連の望ましいプロパティを提案できます。このために収集された知覚データセット目的はIQA手法を改善するための有用なベンチマークを提供しましたが、それらを多用すると過剰適合のリスクが生じます。具体的には、11のフルリファレンスIQAモデルを使用して、4つの低レベルビジョンタスク（ノイズ除去、ブレ除去、スーパー解像度、および圧縮。 
[概要] iqaモデルはiqaモデルを改善するために使用されていますが、頻繁に使用すると過剰適合のリスクが生じます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-04">
        <br><font color="black">2020-05-04</font>
      </time>
    </span>
</section>
<!-- paper0: Depth estimation from 4D light field videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_14.html">
      <font color="black">Depth estimation from 4D light field videos</font>
    </a>
  </h2>
  <font color="black">合成および実世界の4DLFビデオを使用した実験結果は、時間情報がノイズの多い領域での深度推定精度の向上に寄与することを示しています。この研究では、深層学習のトレーニングに使用できる中規模の合成4DLFビデオデータセットも構築します。ベースの方法..この論文は、4DLFビデオからの深度推定のためのエンドツーエンドのニューラルネットワークアーキテクチャを提案します。 
[概要]ほとんどの研究は、静的な4d lf画像からの深度推定に焦点を当てています。この研究は、中規模の合成4dlfビデオデータセットも構築します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-05">
        <br><font color="black">2020-12-05</font>
      </time>
    </span>
</section>
<!-- paper0: Two-stage multi-scale breast mass segmentation for full mammogram
  analysis without user intervention -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_15.html">
      <font color="black">Two-stage multi-scale breast mass segmentation for full mammogram
  analysis without user intervention</font>
    </a>
  </h2>
  <font color="black">したがって、統合されたコンピュータ支援診断システムは、自動で正確な乳房の腫瘤の描写のために臨床医を支援するために必要です。さまざまなタイプの乳房の異常の中で、腫瘤は乳がんの最も重要な臨床所見です。この作業では、2つを提示します。 -高解像度のフルマンモグラムから正確な質量輪郭を提供するステージマルチスケールパイプライン。 
[ABSTRACT] x-光線マンモグラム分析は、主に疑わしい関心領域のローカリゼーションとそれに続くセグメンテーションを指し、良性と悪性の病変分類をさらに進めます。その中には、畳み込みエンコーダー-ネストされた高密度スキップ接続を使用したデコーダーネットワークがあります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-27">
        <br><font color="black">2020-02-27</font>
      </time>
    </span>
</section>
<!-- paper0: Process of image super-resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_16.html">
      <font color="black">Process of image super-resolution</font>
    </a>
  </h2>
  <font color="black">ニューロンのネットワークと画像または画像ライブラリに基づいて学習する超解像再構成のモデルを使用して、低解像度画像を高解像度画像に再構成する方法も知られています。そのゾーンのカテゴリによって、画像のすっきりを高めるために、前述のゾーンにピクセルを追加するために使用される補間のタイプ。数千のトレーニング画像の大規模なデータベース（arXiv：2003を参照）。 
[概要]超解像再構成を使用して高解像度画像を取得する方法が知られています。このプロセスでは、1つまたは複数の低解像度画像を削除します。この論文の目的は、人間の顔を再構成できることを実証することです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-17">
        <br><font color="black">2019-04-17</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Generate Content-Aware Dynamic Detectors -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_17.html">
      <font color="black">Learning to Generate Content-Aware Dynamic Detectors</font>
    </a>
  </h2>
  <font color="black">これらにより、優れたパフォーマンスを維持しながら、より高い計算効率を実現します。提案された方法は、コンテンツ認識動的検出器（CADDet）と呼ばれます。私たちの知る限り、CADDetは、オブジェクトに動的ルーティングメカニズムを導入した最初の作業です。検出。 
[概要]提案された方法は、コンテンツ認識動的検出器（キャデット）と呼ばれます。これは、オブジェクト検出に合わせたコースを適用します。これは、オブジェクト検出に動的ルーティングメカニズムを導入した最初の作業です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Modeling human visual search: A combined Bayesian searcher and saliency
  map approach for eye movement guidance in natural scenes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_18.html">
      <font color="black">Modeling human visual search: A combined Bayesian searcher and saliency
  map approach for eye movement guidance in natural scenes</font>
    </a>
  </h2>
  <font color="black">このアプローチは、固視ランクとスキャンパスの類似性の関数として検出されたターゲットのパーセンテージの両方で、スキャンパス全体で人間と非常によく似た動作をもたらし、眼球運動のシーケンス全体を再現します。ベイジアンオブザーバーモデルが提案されています。このタスクは、アクティブなサンプリングプロセスとして視覚的検索を表すためです。それにもかかわらず、それらは主に人工画像で評価され、自然画像にどのように適応するかはほとんど未踏のままです。 
[要約]顕著性モデルは凝視位置を予測するのに役立ちましたが、凝視の時間-シーケンスに関する情報を提供しません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-17">
        <br><font color="black">2020-09-17</font>
      </time>
    </span>
</section>
<!-- paper0: Human Motion Tracking by Registering an Articulated Surface to 3-D
  Points and Normals -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_19.html">
      <font color="black">Human Motion Tracking by Registering an Articulated Surface to 3-D
  Points and Normals</font>
    </a>
  </h2>
  <font color="black">一方の側で観測点と法線の間に新しいメトリックを導入し、もう一方の側でパラメータ化されたサーフェスを導入します。後者は、楕円体のセットのブレンドとして定義されます。このメトリックは、どちらかを扱う場合に適していると主張します。視覚的船体または視覚的形状の観察..不完全なシルエットから収集されたまばらな視覚的形状データ（3D表面点および法線）を使用して人間の動きを追跡することにより、この方法を説明します。 
[概要]システムは、人体表現からのデータデータデータに基づいています。人体部分または外れ値クラスターのデータを予測するために使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: DeepWriteSYN: On-Line Handwriting Synthesis via Deep Short-Term
  Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_20.html">
      <font color="black">DeepWriteSYN: On-Line Handwriting Synthesis via Deep Short-Term
  Representations</font>
    </a>
  </h2>
  <font color="black">これは、完全に合成された、または与えられた手書きサンプルの自然なバリエーションとして、長期的に現実的な手書き生成に向けたモジュールとして非常に役立ちます。この研究では、深い短期表現による新しいオンライン手書き合成アプローチであるDeepWriteSYNを提案します。提案されたアプローチの主な利点は、合成が短時間のセグメント（文字の一部から完全な文字まで実行できる）で実行されることと、VAEを構成可能な手書きデータセットでトレーニングできることです。 
[ABSTRACT] deepwritesynは、リアルな手書きのバリエーションを生成できます。これら2つのプロパティにより、シンセサイザーに多くの柔軟性がもたらされます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-14">
        <br><font color="black">2020-09-14</font>
      </time>
    </span>
</section>
<!-- paper0: Semantic Image Synthesis via Efficient Class-Adaptive Normalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_21.html">
      <font color="black">Semantic Image Synthesis via Efficient Class-Adaptive Normalization</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、投資収益率の観点から、この空間適応型正規化の有効性を詳細に分析し、その変調パラメータが、特に空間適応性よりも意味認識の恩恵を受けていることを確認します。高解像度入力マスクの場合..この観察に触発されて、セマンティッククラスにのみ適応する軽量で同等に効果的なバリアントであるクラス適応正規化（CLADE）を提案します。空間適応性をさらに向上させるために、 CLADEの正規化パラメーターを変調し、CLADEの真に空間適応型のバリアントであるCLADE-ICPEを提案するために、セマンティックレイアウトから計算されたクラス内位置マップエンコーディング。 
[ABSTRACT]適応型正規化（セマンティック）は、spatial.projectにのみ適応する軽量で同等に効果的なバリアントであり、大量のデータとジョブベースのリスクを削減することを目的としています。プロジェクトを使用して、不要な計算の数</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Adaptive confidence thresholding for monocular depth estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_22.html">
      <font color="black">Adaptive confidence thresholding for monocular depth estimation</font>
    </a>
  </h2>
  <font color="black">しきい値処理された信頼マップによって除外された疑似深度ラベルは、単眼深度ネットワークを監視するために使用されます。さらに、ピクセル適応畳み込み（PAC）による不確実性マップの助けを借りて、単眼深度マップを改良する確率的フレームワークを提案します。層..実験結果は、最先端の単眼深度推定法よりも優れた性能を示しています。 
[要約]この論文では、事前に訓練されたステレオマッチング法によって生成されたステレオ画像の疑似グラウンドトゥルース深度マップを活用する新しいアプローチを提案します。提案されたしきい値学習は、既存の信頼度推定アプローチのパフォーマンスを改善するためにも使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-27">
        <br><font color="black">2020-09-27</font>
      </time>
    </span>
</section>
<!-- paper0: Using Feature Alignment can Improve Clean Average Precision and
  Adversarial Robustness in Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_23.html">
      <font color="black">Using Feature Alignment can Improve Clean Average Precision and
  Adversarial Robustness in Object Detection</font>
    </a>
  </h2>
  <font color="black">敵対的訓練に基づいて、2つの特徴整列方法、すなわち知識蒸留特徴整列（KDFA）と自己教師あり特徴整列（SSFA）を提案します。クリーンな画像での2Dオブジェクト検出はよく研究されているトピックですが、敵対的な攻撃に対する脆弱性は依然として懸念されています。ネットワークの中間層の機能を調整することで、検出器のクリーンなAPと堅牢性を向上させることができます。 
[ABSTRACT]敵対的なトレーニングにより、オブジェクト検出器の堅牢性が向上しました。しかし同時に、クリーンな画像の平均精度が大幅に低下します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Planet cartography with neural learned regularization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_24.html">
      <font color="black">Planet cartography with neural learned regularization</font>
    </a>
  </h2>
  <font color="black">ディープラーニングの可能性を活用し、モックサーフェスから正則化を学習するエキソアースのマッピング手法を提案します。このアプローチで信頼性の高いマッピングを実行できることを示し、使用している場合でも非常にコンパクトな大陸を生成します。単一通過帯観測..さらに重要なことに、地球のように外惑星が部分的に曇っている場合、表面の同じ位置（地形と海面温度に関連する）で常に発生する永続的な雲の分布を一緒にマッピングできる可能性があることを示します表面を横切って移動する非永続的な雲で。 
[概要]太陽系外惑星での生命の兆候を見つけることは、惑星大気のバルク組成を決定することによって最初に達成される可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Rotation-Invariant Autoencoders for Signals on Spheres -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_25.html">
      <font color="black">Rotation-Invariant Autoencoders for Signals on Spheres</font>
    </a>
  </h2>
  <font color="black">特に、$ S ^ 2 $と$ SO（3）$の畳み込み層で構成されるオートエンコーダアーキテクチャを注意深く設計します。この論文では、球対称表現の回転不変表現の教師なし学習の問題について考察します。3Dとして回転はしばしば厄介な要因であり、潜在空間はこれらの入力変換に対して正確に不変になるように制約されます。 
[概要]研究者は最近、球面画像の分類により適した深層学習法を開発しました。これらには、3Dで処理される$ so（3）$畳み込みが含まれます。これは、2D回転がしばしば厄介な要因であるためです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Efficient Estimation of Influence of a Training Instance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_26.html">
      <font color="black">Efficient Estimation of Influence of a Training Instance</font>
    </a>
  </h2>
  <font color="black">ドロップアウトマスクを切り替えることにより、各トレーニングインスタンスを学習した、または学習しなかったサブネットワークを使用して、その影響を推定できます。この論文では、影響を推定するための効率的な方法を提案します。分類に関するBERTとVGGNetの実験を通じてデータセットでは、提案された方法がトレーニングの影響をキャプチャし、エラー予測の解釈可能性を高め、一般化を改善するためにトレーニングデータセットをクレンジングできることを示します。 
[ABSTRACT]私たちの方法はドロップアウトに触発されています。ドロップアウトは、サブネットワークをゼロマスクし、サブネットワークが各トレーニングインスタンスを学習するのを防ぎます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Neural Image Compression and Explanation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_27.html">
      <font color="black">Neural Image Compression and Explanation</font>
    </a>
  </h2>
  <font color="black">私たちのコードはhttps://github.com/lxuniverse/NICEで入手できます。生成されたマスクは、CNNの最終予測への影響によって測定された各ピクセルの顕著性をキャプチャできます。また、重要なピクセルが元の高解像度を維持し、重要でない背景ピクセルが低解像度にサブサンプリングされる混合解像度画像の生成にも使用できます。具体的には、NICEは確率的バイナリゲートを接続することにより、入力画像上にスパースマスクを生成します。画像の各ピクセルに、そのパラメータは説明されるCNN分類器との相互作用を通じて学習されます。 
[ABSTRACT]畳み込みニューラルネットワーク（cnns）の予測を説明するための新しい優れたフレームワークが開発されています。生成されたマスクは、最終的な相互作用への影響によって測定された各ピクセルの顕著性をキャプチャできます。これも使用できます。重要なピクセルが元の高解像度を維持する混合解像度の画像を作成する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-09">
        <br><font color="black">2019-08-09</font>
      </time>
    </span>
</section>
<!-- paper0: Permuted AdaIN: Reducing the Bias Towards Global Statistics in Image
  Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_28.html">
      <font color="black">Permuted AdaIN: Reducing the Bias Towards Global Statistics in Image
  Classification</font>
    </a>
  </h2>
  <font color="black">グローバル画像統計が歪んでいるため、このスワッピング手順により、ネットワークは形状やテクスチャなどの手がかりに依存します。確率$ p $のランダム順列を選択し、それ以外の場合はID順列を選択することで、効果の強さを制御できます。ドメイン適応とドメイン一般化の設定では、私たちの方法は、GTAVからCityscapesへの転送学習タスクとPACSベンチマークで最先端の結果を達成します。 
[ABSTRACT] convokeと呼ばれる私たちの方法は、サンプルの分析分析に基づいています。これにより、形状とローカル画像の手がかりの間に類似しているが異なる違いが生じます。この方法は、複数のアーキテクチャのimagenetとcifar-100-cでも改善されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-09">
        <br><font color="black">2020-10-09</font>
      </time>
    </span>
</section>
<!-- paper0: Digital Gimbal: End-to-end Deep Image Stabilization with Learnable
  Exposure Times -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_29.html">
      <font color="black">Digital Gimbal: End-to-end Deep Image Stabilization with Learnable
  Exposure Times</font>
    </a>
  </h2>
  <font color="black">単一画像のブレ除去や固定露出バーストのノイズ除去という従来のアプローチに対するこの方法の利点を示します。作動ジンバルを使用した機械的画像安定化により、カメラの動きによるぼやけに悩まされることなく、長時間露光ショットをキャプチャできます。バーストの露出時間をエンドツーエンドで行うことで、フレーム全体でノイズとブラーのバランスを取ります。 
[概要]未知の動きに関連する、ノイズの多い短い露出フレームのバーストを集約することにより、シャープな高SN比画像を推定するCNNをトレーニングします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: CASTing Your Model: Learning to Localize Improves Self-Supervised
  Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_30.html">
      <font color="black">CASTing Your Model: Learning to Localize Improves Self-Supervised
  Representations</font>
    </a>
  </h2>
  <font color="black">これらの制限を克服するために、対照的注意監視チューニング（CAST）を提案します。現在のSSLメソッドは、象徴的な画像で最高のパフォーマンスを発揮し、多くのオブジェクトを含む複雑なシーン画像で苦労します。CASTは、教師なし顕著性マップを使用して作物をインテリジェントにサンプリングします。 Grad-CAMの注意喪失を介して接地監視を提供します。 
[概要]ヴェルダーブレーメンのsslメソッドは、シーン画像でトレーニングすると、不十分な接地を示し、不十分な監督上の苦労を受け取ります。キャストは、教師なし顕著性マップを使用して作物をインテリジェントにサンプリングし、ギャップを介して接地監視を提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: TAP: Text-Aware Pre-training for Text-VQA and Text-Caption -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_31.html">
      <font color="black">TAP: Text-Aware Pre-training for Text-VQA and Text-Caption</font>
    </a>
  </h2>
  <font color="black">シーンテキストのキャプチャに失敗する従来の視覚言語の事前トレーニングと、視覚およびテキストのモダリティとの関係とは対照的に、TAPは事前トレーニングにシーンテキスト（OCRエンジンから生成）を明示的に組み込みます。 Text-VQAタスクとText-CaptionタスクのText-AwarePre-training（TAP）。これら2つのタスクは、それぞれ質問回答と画像キャプション生成のために画像内のシーンテキストを読み取って理解することを目的としています。 
[概要]これらの2つのタスクは、画像内のシーンテキストを読み取って理解することを目的としています。パフォーマンスをさらに向上させるために、概念的なキャプションデータセットに基づいて大規模なセットを構築します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Interpretable deep learning regression for breast density estimation on
  MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_32.html">
      <font color="black">Interpretable deep learning regression for breast density estimation on
  MRI</font>
    </a>
  </h2>
  <font color="black">350人の患者を使用してCNNをトレーニングし、75人を検証に、81人を独立したテストに使用しました。CNNの予測に基づいて調べたところ、FGTのボクセルは一般に正のSHAP値を示し、脂肪組織のボクセルは一般に負の値を示しました。 SHAP値、および非乳房組織のボクセルは、一般にゼロに近いSHAP値を持っていました。テストセットでCNNによって予測された密度は、グラウンドトゥルース密度と有意に相関していました（N = 81人の患者、スピアマンのrho = 0.86、P &lt;0.001）。 
[概要]テストでcnnによって予測された密度は、有意に相関していました。cnnが予測した場合、fgtと脂肪組織に基づいていました。これは、密度の予測が構造に基づいていることを意味します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: SSCNav: Confidence-Aware Semantic Scene Completion for Visual Semantic
  Navigation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_33.html">
      <font color="black">SSCNav: Confidence-Aware Semantic Scene Completion for Visual Semantic
  Navigation</font>
    </a>
  </h2>
  <font color="black">SSCNavは、環境の部分的な観察を前提として、最初に、観察されていないシーンのセマンティックラベルと、独自の予測に関連付けられた信頼マップを使用して完全なシーン表現を推測します。このペーパーでは、アクティブなアクションを生成するタスクである視覚的なセマンティックナビゲーションに焦点を当てます。エージェントは、未知の環境で指定されたターゲットオブジェクトカテゴリに移動します。次に、ポリシーネットワークは、シーンの完了結果と信頼マップからアクションを推測します。 
[要約]アルゴリズムは、カテゴリの例を見つけてナビゲートできる必要があります。これは、信頼性を意識したセマンティックシーン完了モジュールに基づいています。ポリシーネットワークがあり、シーン完了結果と信頼マップからアクションを推測します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: GMM-Based Generative Adversarial Encoder Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_34.html">
      <font color="black">GMM-Based Generative Adversarial Encoder Learning</font>
    </a>
  </h2>
  <font color="black">私たちのフレームワークは一般的であり、どのGAN戦略にも簡単に組み込むことができます。特に、VanillaGANとWassersteinGANの両方でそれを示し、ISスコアとFIDスコアの両方の点で生成された画像の改善につながります。 .. GANは画像を生成するための強力なモデルですが、潜在空間を推測できないため、エンコーダーを必要とするアプリケーションでの使用が直接制限されます。 
[概要]エンコーダーを使用して、gmmを使用してエンコーダーを作成できます。両方で、isスコアとfidスコアの両方の点で生成された画像の改善につながります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Independent Instance Maps for Crowd Localization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_35.html">
      <font color="black">Learning Independent Instance Maps for Crowd Localization</font>
    </a>
  </h2>
  <font color="black">この目的のために、インディペンデントインスタンスマップセグメンテーション（IIM）という名前の群集ローカリゼーションのエンドツーエンドで直接的なフレームワークを提案します。BMはローカリゼーションモデルに2つの利点をもたらします。より正確にインスタンス; 2）バイナリ予測とラベルの損失を使用してモデルを直接トレーニングします。密度マップとボックス回帰とは異なり、IIMの各インスタンスは重複していません。 
[ABSTRACT]ワイヤレスベースの方法は、非常に密度の高いシーンと大規模な範囲スケールのバリエーションの群集を処理できます。セグメンテーション位置の位置を改善するために、構造化されたインスタンスマップを出力するための微分可能な2値化モジュールを提示します。提案された方法は効果的であり、 5つの人気のある収集データセットのアートメソッド</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Data Instance Prior for Transfer Learning in GANs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_36.html">
      <font color="black">Data Instance Prior for Transfer Learning in GANs</font>
    </a>
  </h2>
  <font color="black">多様なソースドメインでトレーニングされた自己監視/教師あり事前トレーニングネットワークから事前に得られた有益なデータを活用することにより、限られたデータドメインのGANの新しい転送学習方法を提案します。高品質の画像の生成の進歩..限られたデータレジームでは、トレーニングは通常発散するため、生成されるサンプルは低品質で多様性に欠けます。 
[概要]これまでの研究では、転送学習とデータ派生手法を活用して、低データ設定でのトレーニングに取り組んできました。提案された方法は、画質と多様性の点で、既存の最先端技術を上回り、ターゲット画像が少ないドメインに知識を効果的に転送します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Budget-Aware Adapters for Multi-Domain Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_37.html">
      <font color="black">Budget-Aware Adapters for Multi-Domain Learning</font>
    </a>
  </h2>
  <font color="black">私たちの直感では、実際のアプリケーションではドメインとタスクの数が非常に多くなる可能性があるため、効果的なMDLアプローチは、精度だけでなく、パラメーターをできるだけ少なくすることにも焦点を当てる必要があります。実験的に、私たちのアプローチが認識精度は、最先端のアプローチと競合しますが、ストレージと計算の両方の点ではるかに軽いネットワークを備えています。マルチドメイン学習（MDL）は、一般的なディープアーキテクチャから派生した一連のモデルを学習する問題を指します。それぞれが特定のドメイン（写真、スケッチ、絵画など）でタスクを実行することに特化しています。 
[ABSTRACT] mdlは、ネットワークパラメータの数と望遠鏡の複雑さの観点から、調整可能な予算で実際のモデルを取得することに特に関心があります。これらのタイプには、周囲に適応するために使用されるこれらのタイプのディープモデルが含まれます。この方法は調整することです。モデルのモデルモデルモデルのサイズ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-05-15">
        <br><font color="black">2019-05-15</font>
      </time>
    </span>
</section>
<!-- paper0: NeRD: Neural Reflectance Decomposition from Image Collections -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_38.html">
      <font color="black">NeRD: Neural Reflectance Decomposition from Image Collections</font>
    </a>
  </h2>
  <font color="black">困難な非ランバート反射率、複雑なジオメトリ、および未知の照明でさえ、高品質のモデルに分解できます。さらに、これらの放射輝度フィールドの評価は、リソースと時間がかかります。この問題は、照明が単一でない場合、本質的により困難です。実験室条件下の光源ですが、代わりに制約のない環境照明です。 
[ABSTRACT] nerdは、物理ベースのレンダリングを神経放射輝度フィールドに導入することでこの分解を実現する方法です。データセットとコードは、実際のプロジェクトページで入手できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-07">
        <br><font color="black">2020-12-07</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-View Adaptive Fusion Network for 3D Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_39.html">
      <font color="black">Multi-View Adaptive Fusion Network for 3D Object Detection</font>
    </a>
  </h2>
  <font color="black">KITTI 3Dオブジェクト検出データセットで実施された評価は、提案されたAPFおよびAPWモジュールが大幅なパフォーマンスの向上を提供することを示しています。この問題を解決するために、LiDAR鳥瞰図LiDAR範囲を取得するシングルステージマルチビューフュージョンフレームワークを提案します。 3Dオブジェクト検出の入力としてのビューおよびカメラビュー画像。さらに、注意深いポイントワイズ重み付け（APW）モジュールは、ネットワークが構造情報とポイントフィーチャの重要性を学習するのに役立つように設計されており、2つの追加タスク、つまり、前景分類と中心回帰、および予測された前景確率は、ポイントフィーチャを再重み付けするために使用されます。 
[概要]注意深いポイントワイズフュージョンモジュールを提案し、マルチビュー機能のアダプティブフュージョンをポイントワイズで実現できるアテンションメカニズムを備えた3つのソースの重要性を推定します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-02">
        <br><font color="black">2020-11-02</font>
      </time>
    </span>
</section>
<!-- paper0: SPU-Net: Self-Supervised Point Cloud Upsampling by Coarse-to-Fine
  Reconstruction with Self-Projection Optimization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_40.html">
      <font color="black">SPU-Net: Self-Supervised Point Cloud Upsampling by Coarse-to-Fine
  Reconstruction with Self-Projection Optimization</font>
    </a>
  </h2>
  <font color="black">合成データセットと実際にスキャンされたデータセットの両方でさまざまな実験を行い、その結果は、最先端の教師あり手法と同等のパフォーマンスを達成していることを示しています。さらに、生成されたポイントセットのノイズの多いポイントをさらに最適化するために、自己教師あり点雲のアップサンプリングを容易にするために、共同損失として、均一項と再構成項に関連付けられた新しい自己射影最適化を提案します。ただし、トレーニング用の大規模なペアの疎密点セットを取得するのは費用がかかり、面倒です。実際にスキャンされたスパースデータ。 
[概要] spu--netという名前の自己監視ポイントクラウドアップサンプリングネットワークは、下にあるオブジェクト表面にあるポイントの固有のアップサンプリングパターンをキャプチャすることを目的としています。具体的には、監視情報としてグラウンドトゥルースデンスポイントセットを使用します。合成ペアトレーニングデータでのみトレーニング済み</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Overcomplete Representations Against Adversarial Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_41.html">
      <font color="black">Overcomplete Representations Against Adversarial Videos</font>
    </a>
  </h2>
  <font color="black">実験結果は、画像に焦点を当てた防御はビデオには効果がない可能性があることを示していますが、OUDefendは、加法攻撃、乗法攻撃から物理的に実現可能な攻撃に至るまで、さまざまなタイプの敵対的なビデオに対する堅牢性を強化します。 -敵対的なビデオに対する防御のための完全な復元ネットワーク（OUDefend）の下で。したがって、OUDefendは、これら2つの表現を学習することにより、ローカル機能とグローバル機能のバランスをとるように設計されています。 
[概要]ほとんどの復元ネットワークはエンコーダー-デコーダーアーキテクチャを採用しています。最初に空間次元を縮小し、次に拡大します。過剰な表現には反対の特性があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: iGibson, a Simulation Environment for Interactive Tasks in Large
  Realistic Scenes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_42.html">
      <font color="black">iGibson, a Simulation Environment for Interactive Tasks in Large
  Realistic Scenes</font>
    </a>
  </h2>
  <font color="black">iGibsonは、インタラクティブなタスクの研究を容易にするためにいくつかの重要な機能を統合します：i）高品質の視覚仮想センサー信号（RGB、深度、セグメンテーション、LiDAR、フローなど）の生成、ii）オブジェクトの材料を変更するためのドメインランダム化（視覚的なテクスチャとダイナミクスの両方）および/またはそれらの形状、iii）ロボットのベースとアームの衝突のない軌道を生成するための統合されたサンプリングベースのモーションプランナー、およびiv）人間のデモンストレーションの効率的な収集を可能にする直感的な人間-iGibsonインターフェイス。シーンは、3Dスキャンされた実世界の家のレプリカであり、オブジェクトの分布とレイアウトを実世界のものに合わせます。また、iGibson機能により、ナビゲーションエージェントの一般化が可能になり、人間とiGibsonのインターフェイスと統合されたモーションが示されます。プランナーは、人間が示した単純な行動の効率的な模倣学習を促進します。 
[ABSTRACT] igibsonは、インタラクティブなタスクの研究を容易にするためにいくつかの重要な機能を統合しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-05">
        <br><font color="black">2020-12-05</font>
      </time>
    </span>
</section>
<!-- paper0: FLIC: Fast Lidar Image Clustering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_43.html">
      <font color="black">FLIC: Fast Lidar Image Clustering</font>
    </a>
  </h2>
  <font color="black">公開データの詳細な評価と確立された方法との比較を通じて、これらの側面が単一のCPUコアで最先端のパフォーマンスとランタイムをどのように実現するかを示します。私たちの方法がユークリッド距離の特性を活用して3つを保持する方法を示します-次元の測定情報、高速計算のために2次元表現に絞り込まれます。ただし、スキャンごとに大量のLidarポイントが存在する可能性があるため、オブジェクトを識別するために調整されたアルゴリズムが必要です（
[ABSTRACT]システムは重要です。自律走行を実用化するためのビルディングブロック。これらには、さまざまなドライバー支援システムのタグ付けなどの追跡システムが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-01">
        <br><font color="black">2020-03-01</font>
      </time>
    </span>
</section>
<!-- paper0: Reinforcement Based Learning on Classification Task Could Yield Better
  Generalization and Adversarial Accuracy -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_44.html">
      <font color="black">Reinforcement Based Learning on Classification Task Could Yield Better
  Generalization and Adversarial Accuracy</font>
    </a>
  </h2>
  <font color="black">従来のクロスエントロピー損失の代わりに、強化学習で使用されるバニラポリシー勾配法と同様の報酬ベースの最適化関数を使用してモデルをトレーニングしました。この作業では、深層学習モデルをトレーニングする新しい方法を提案しました。画像分類タスク..人間はそのような摂動に対して非常に頑強です。考えられる理由の1つは、人間が「ターゲットラベル」と「予測ラベル」の間のエラーに基づいて分類することを学習しないが、おそらく予測で受けた強化のためである可能性があります。 
[概要]これらのディープニューラルネットワークは、敵対的な例に対して脆弱です。敵対的な例に対して非常に脆弱であると彼らは言います。これらは、何人の人々であるかを知るのが困難です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Do Adversarially Robust ImageNet Models Transfer Better? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_45.html">
      <font color="black">Do Adversarially Robust ImageNet Models Transfer Better?</font>
    </a>
  </h2>
  <font color="black">さらなる分析により、転送学習のコンテキストでロバストモデルと標準モデルの違いがさらに明らかになります。具体的には、敵対的にロバストなImageNet分類器に焦点を当て、ダウンストリーム分類タスクの標準スイートで精度が向上することを示します。コードとモデルは次のとおりです。 https://github.com/Microsoft/robust-models-transferで入手できます。 
[ABSTRACT]広く訓練されたモデルはスキルで自分自身を識別することができます。これは、初期精度が転移学習の重要な側面であることを示唆しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-16">
        <br><font color="black">2020-07-16</font>
      </time>
    </span>
</section>
<!-- paper0: Weakly-Supervised Cross-Domain Adaptation for Endoscopic Lesions
  Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_46.html">
      <font color="black">Weakly-Supervised Cross-Domain Adaptation for Endoscopic Lesions
  Segmentation</font>
    </a>
  </h2>
  <font color="black">さらに、新しい自己監視型疑似ラベルジェネレーターは、転送が困難なターゲットサンプルと転送が容易なターゲットサンプルの両方に自信を持って疑似ピクセルラベルを提供するように設計されています。無関係なセマンティック特性。その後、動的に検索された特徴のセントロイドは、カテゴリごとの分布シフトを狭めるように調整されます。 
[概要]ニューヨークベースの転送可能性フレームワークが開発され、転送可能な幅広いコンテキスト依存関係が強調されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-modal Visual Tracking: Review and Experimental Comparison -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_47.html">
      <font color="black">Multi-modal Visual Tracking: Review and Experimental Comparison</font>
    </a>
  </h2>
  <font color="black">さらに、PTB、VOT19-RGBD、GTOT、RGBT234、VOT19-RGBTの5つのデータセットでトラッカーの有効性を分析するための広範な実験を実施します。マルチモーダルトラッキングの徹底的なレビューを提供するために、マルチモーダルトラッキングを要約します。モーダルトラッキングアルゴリズム、特にさまざまな側面からの統一された分類法における可視深度（RGB-D）トラッキングと可視熱（RGB-T）トラッキング。コンピュータビジョンの基本的なタスクとして、ビジュアルオブジェクトトラッキングは、近年。 
[ABSTRACT]研究者は、特定のシーンを処理するために複数のモダリティからの情報を導入しました。目的は、トラッカーをより広い範囲のアプリケーションに拡張することです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: A Number Sense as an Emergent Property of the Manipulating Brain -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_48.html">
      <font color="black">A Number Sense as an Emergent Property of the Manipulating Brain</font>
    </a>
  </h2>
  <font color="black">その結果、私たちのモデルは、シーン内のオブジェクトの数を推定する機能と、{\ emsubitization}を取得します。数と量のある施設の重要な側面は、教師の明示的な監督なしに学習できると結論付けます。これらには、ゼロと最初のいくつかの自然数の個別のカテゴリ、順序の概念、および数と相関する信号が含まれます。 
[概要]数と量のある施設のモデルは、教師の監督なしで学習することができます。理解するために、数と量を予見する規則性を示す画像表現が出現しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Roses Are Red, Violets Are Blue... but Should Vqa Expect Them To? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_49.html">
      <font color="black">Roses Are Red, Violets Are Blue... but Should Vqa Expect Them To?</font>
    </a>
  </h2>
  <font color="black">質問と概念が不均等に分散しているため、微妙なトレーニングセット統計を利用するモデルを好む傾向があります。VQAに関連する質問と概念の大きくて不均衡な多様性と高水準の注釈付きデータの欠如は、モデルが「理由」を学習するのを妨げる傾向があります。 、代わりに「知識に基づいた推測」を実行するように導き、特定のトレーニングセット統計に依存します。これは、実際のシナリオに一般化するのに役立ちません。これらの懸念を克服するように設計されたGQA-OODベンチマークを提案します。まれで頻繁な質問と回答のペアの両方で、前者は推論能力の評価に適していると主張します。これは、多かれ少なかれ悪用バイアスに訓練されたモデルで実験的に検証します。 
[要約]質問と概念は不均等に分散されており、微妙なトレーニングセット統計を利用するモデルを好む傾向があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-09">
        <br><font color="black">2020-06-09</font>
      </time>
    </span>
</section>
<!-- paper0: HourNAS: Extremely Fast Neural Architecture Search Through an Hourglass
  Lens -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_50.html">
      <font color="black">HourNAS: Extremely Fast Neural Architecture Search Through an Hourglass
  Lens</font>
    </a>
  </h2>
  <font color="black">これらの非バイタルブロックの検索スペースは、計算リソースの制約の下で手頃な価格の候補のみをカバーするようにさらに縮小されます。砂時計の狭い首のように機能し、入力から出力までの保証されたパスのバイタルブロックディープニューラルネットワークは、情報の流れを制限し、ネットワークの精度に影響を与えます。高精度を維持しながら非常に高速なNASを実現するために、重要なブロックを特定し、アーキテクチャ検索で優先することを提案します。 
[概要]砂時計に触発されたアプローチは、アーキテクチャの効果によって提案されます。他のブロックはネットワークの大部分を占め、ネットワーク全体の複雑さを決定します。これらの重要でないブロックの検索スペースは、カバーするだけにさらに縮小されます。手頃な価格の候補者</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-29">
        <br><font color="black">2020-05-29</font>
      </time>
    </span>
</section>
<!-- paper0: Globetrotter: Unsupervised Multilingual Translation from Visual
  Alignment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_51.html">
      <font color="black">Globetrotter: Unsupervised Multilingual Translation from Visual
  Alignment</font>
    </a>
  </h2>
  <font color="black">私たちの言語表現は、単一のステージで1つのモデルで共同でトレーニングされます。代わりに、視覚モダリティを使用して複数の言語を整列させ、それらの間のブリッジとして画像を使用するフレームワークを導入します。52の言語での実験は、私たちの方法が優れていることを示しています。検索を使用した教師なし単語レベルおよび文レベルの翻訳のベースライン。 
[ABSTRACT]既存の教師なしメソッドは通常、言語表現のブリッジプロパティに依存します。言語と画像の間のクロスモーダルアライメントを推定し、この推定を使用してクロスリンガル表現の学習をガイドします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Augmentation and Evaluation Schemes for Semantic Image
  Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_52.html">
      <font color="black">Improving Augmentation and Evaluation Schemes for Semantic Image
  Synthesis</font>
    </a>
  </h2>
  <font color="black">最後に、3つの異なるデータセットにわたる最先端のセマンティック画像合成モデルを使用して、両方のクラス分割で、拡張スキームで得られた強力な定量的および定性的改善を示します。拡張GANモデルをバニラの対応物に対してベンチマークしながら、以前のセマンティック画像合成研究で報告された定量化メトリックは、外部の事前トレーニング済みセグメンテーションネットワークを介して導出されるため、特定のセマンティッククラスに強く偏っていることを発見します。したがって、個別に分析することにより、確立されたセマンティック画像合成評価スキームを改善することを提案します。特定のセマンクションネットワークのバイアスクラスとバイアスクラスで生成された画像のパフォーマンス。 
[概要]ガンベースのセマンティック画像研究のために特別に作成された新しいセマンティック画像合成スキーム。この目的のために、超解像度モデルのために特別に設計された新しい拡張スキームを紹介します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-25">
        <br><font color="black">2020-11-25</font>
      </time>
    </span>
</section>
<!-- paper0: Perceptual Robust Hashing for Color Images with Canonical Correlation
  Analysis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_53.html">
      <font color="black">Perceptual Robust Hashing for Color Images with Canonical Correlation
  Analysis</font>
    </a>
  </h2>
  <font color="black">リングリボンの外側の境界上の重要なコーナーポイントのローカルカラー特徴は、カラーベクトル角度（CVA）によって抽出され、カラー低次モーメント（CLM）は、グローバルカラー特徴を抽出するために利用されます。最後に、2種類の特徴ベクトルがあります。正準相関分析（CCA）を介して融合され、スクランブル後に最終ハッシュが生成されます。直接連結と比較して、CCA特徴融合法は分類パフォーマンスを向上させ、2セットの特徴ベクトル間の全体的な相関をより適切に反映します。 
[概要]二次画像は一連のリングに分割されます-異なる半径と同じピクセル数のリボン</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Uncovering the Intrinsic Data Structures for Unsupervised Domain
  Adaptation using Structurally Regularized Deep Clustering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_54.html">
      <font color="black">Towards Uncovering the Intrinsic Data Structures for Unsupervised Domain
  Adaptation using Structurally Regularized Deep Clustering</font>
    </a>
  </h2>
  <font color="black">構造的類似性の仮定を強化することにより、セマンティックセグメンテーションのピクセルレベルのUDAタスク用にSRDC ++を拡張できます。印象的な結果が得られましたが、これらの方法には、ターゲット識別の固有のデータ構造を損傷する潜在的なリスクがあります。特に帰納的設定でのUDAタスクの一般化の問題..技術的には、ターゲットデータの正規化された識別クラスタリングを生成的なものと統合する構造的に正規化されたディープクラスタリングのハイブリッドモデルを提案するため、この方法をSRDC ++と呼びます。 
[概要]私たちのハイブリッドモデルは、カルバックを最小限に抑えるディープクラスタリングフレームワークに基づいています-ネットワーク予測の分布と補助的な予測の分布の間のより大きな相違。ターゲット分類とセマンティックセグメンテーションの7つのudaベンチマークで広範な実験を行います</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Parameter Efficient Multimodal Transformers for Video Representation
  Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_55.html">
      <font color="black">Parameter Efficient Multimodal Transformers for Video Representation
  Learning</font>
    </a>
  </h2>
  <font color="black">私たちのアプローチを示すために、Kinetics-700からの30秒のクリップでモデルを事前トレーニングし、それを視聴覚分類タスクに転送します。私たちのアプローチがパラメーターを最大80 $ \％$削減し、モデルをトレーニングできることを示します。エンドツーエンドでゼロから..また、モデルがトランスフォーマーで学習するCNN埋め込みスペースで測定されたインスタンスの類似性に基づくネガティブサンプリングアプローチを提案します。 
[概要]適応するためにモデルをモダリティに分解します。また、モデルがトランスフォーマーで学習する新しいモデルを提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Harnessing Multi-View Perspective of Light Fields for Low-Light Imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_56.html">
      <font color="black">Harnessing Multi-View Perspective of Light Fields for Low-Light Imaging</font>
    </a>
  </h2>
  <font color="black">したがって、L3Fnetと呼ばれる低照度ライトフィールド（L3F）復元用のディープニューラルネットワークを提案します。これは、単一フレームのデジタル一眼レフ画像をL3Fnetに適した形式に変換することによって行います。疑似LF ..低照度のLFを復元するには、さまざまなLFビューに存在する幾何学的な手がかりを利用する必要があります。これは、単一フレームの低照度の強調手法では不可能です。 
[概要]提案されたl3fnetは、各lfビューの必要な視覚的強化を実行します。また、ビュー全体でエピポーラジョエルを保持し、cnnのkat kinectを記述します。さまざまなシーンのデータセット</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-05">
        <br><font color="black">2020-03-05</font>
      </time>
    </span>
</section>
<!-- paper0: Confidence-aware Non-repetitive Multimodal Transformers for TextCaps -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_57.html">
      <font color="black">Confidence-aware Non-repetitive Multimodal Transformers for TextCaps</font>
    </a>
  </h2>
  <font color="black">この目的のために、上記の課題に取り組むために、信頼性を意識した非反復マルチモーダルトランスフォーマー（CNMT）を提案します。キャプションの単語の冗長性の問題に対処するために、生成モジュールには、キャプションの繰り返し単語の予測を回避する反復マスクが含まれています。 。画像を説明する場合、重要な情報を理解するには、視覚的なシーンでテキストを読むことが重要です。 
[概要]新しいアプローチでは、読み取り能力が低いため、正確な説明を生成できません。抽出されたすべてのocrトークンから重要な単語を選択できないことが含まれます。私たちのモデルは、textcapsデータセットの最新モデルよりも優れています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-07">
        <br><font color="black">2020-12-07</font>
      </time>
    </span>
</section>
<!-- paper0: Cost Sensitive Optimization of Deepfake Detector -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_58.html">
      <font color="black">Cost Sensitive Optimization of Deepfake Detector</font>
    </a>
  </h2>
  <font color="black">アップロードされたビデオのごく一部だけがディープフェイクであることが明らかであるため、検出パフォーマンスはコストに敏感な方法で測定する必要があります。映画の発明以来、操作されたビデオが存在していました。ディープジェネレーティブモデリング、信頼できる見た目の偽のビデオの生成が現実のものになりました。 
[概要]視聴者を騙すことができる動画を作成することは、長い道のりでした。ただし、これは動画を生成するための推定方法です。現在の作業では、ソースが顔を向ける、いわゆるディープフェイク動画に焦点を当てています。ターゲットと交換されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Smooth Variational Graph Embeddings for Efficient Neural Architecture
  Search -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_59.html">
      <font color="black">Smooth Variational Graph Embeddings for Efficient Neural Architecture
  Search</font>
    </a>
  </h2>
  <font color="black">実装は\ url {https://github.com/automl/SVGe}で提供します。提案されたアプローチは、トレーニング時にアーキテクチャの構造サブセットを評価するスムーズな変分ニューラルアーキテクチャ埋め込みスペースを構築することにより、両側からの利点を活用します。推論時にこの部分空間から推定できるようにしながら、予測されたパフォーマンスを使用する時間。2つの一般的な検索空間、ENASアプローチとNAS-Bench-101検索空間によって定義されたグラフ構造のコンテキストで提案されたアプローチを評価します。両方の最新技術を改善します。 
[要約]提案されたアプローチは、2つの一般的な検索スペースのコンテキストで提案されます。検索スペースに関する詳細情報を取得するために使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-09">
        <br><font color="black">2020-10-09</font>
      </time>
    </span>
</section>
<!-- paper0: Dynamic Object Removal and Spatio-Temporal RGB-D Inpainting via
  Geometry-Aware Adversarial Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_60.html">
      <font color="black">Dynamic Object Removal and Spatio-Temporal RGB-D Inpainting via
  Geometry-Aware Adversarial Learning</font>
    </a>
  </h2>
  <font color="black">修復の問題を画像から画像への変換タスクとしてキャストし、モデルは、影や反射など、シーン内の動的オブジェクトの存在と相関する領域も修正します。粗い後の新しいジオメトリ対応のDynaFillアーキテクチャを提案します。 -to-fineトポロジと、ゲートされた反復フィードバックメカニズムを組み込んで、以前のタイムステップからの情報を適応的に融合します。RGB-D画像、セマンティックセグメンテーションラベル、カメラポーズ、およびグラウンドトゥルースRGB-D情報を含む大規模な超現実的なデータセットを紹介します。閉塞領域。 
[概要]私たちのアーキテクチャは、敵対的なトレーニングを使用して、細かいリアルなテクスチャを合成します。これらにより、注意に頼ることなく、空間的および時間的に一貫した方法で、閉塞領域の色と深さの構造をオンラインで幻覚化することができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-12">
        <br><font color="black">2020-08-12</font>
      </time>
    </span>
</section>
<!-- paper0: Structure-Consistent Weakly Supervised Salient Object Detection with
  Local Saliency Coherence -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_61.html">
      <font color="black">Structure-Consistent Weakly Supervised Salient Object Detection with
  Local Saliency Coherence</font>
    </a>
  </h2>
  <font color="black">一貫性のある顕著性マップが入力と同じ画像のさまざまなスケールで予測されるように、顕著性構造の一貫性損失を自己監視として設計します。これは、モデルの一般化能力を強化するための正則化手法と見なすことができます。ソースコードはhttpで入手できます。 ：//github.com/siyueyu/SCWSSOD ..広範な実験により、私たちの方法が6つのベンチマークで新しい最先端のパフォーマンスを達成することが示されています（例：
[ABSTRACT]十分に監視されていない顕著なマップと完全に監視されている顕著なマップ間のパフォーマンスのギャップは非常に大きい。これまでのほとんどの著名な作品は、多くの鐘と笛を伴う複雑なトレーニング方法を採用しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Active Visual Localization in Partially Calibrated Environments -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_62.html">
      <font color="black">Active Visual Localization in Partially Calibrated Environments</font>
    </a>
  </h2>
  <font color="black">ローカリゼーションの手作りベースラインに対してアルゴリズムをベンチマークし、ローカリゼーションの成功率でアプローチが大幅に優れていることを示します。私たちの主な貢献は、部分観測マルコフ決定過程としてアクティブな視覚的ローカリゼーション問題を定式化し、深層強化に基づくアルゴリズムフレームワークを提案することです。それを解決することを学ぶ..さらに、合成データと実際のデータの両方で構成され、アクティブな視覚的ローカリゼーションの困難なシナリオをシミュレートする屋内シーンデータセットACR-6を提案します。 
[ABSTRACT]課題に対処するために、画像を使用してインテリジェントモーションを生成するポリシーを検索し、部分的に調整された環境で視覚情報を与えられたエージェントをアクティブにローカライズする方法を検討します。さらに、屋内シーンデータセットacr-6を提案します。合成データと実際のデータの両方</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: An explainable deep vision system for animal classification and
  detection in trail-camera images with automatic post-deployment retraining -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_63.html">
      <font color="black">An explainable deep vision system for animal classification and
  detection in trail-camera images with automatic post-deployment retraining</font>
    </a>
  </h2>
  <font color="black">このシステムは、自動再トレーニングアルゴリズムを利用して、データドリフトを検出し、システムを更新します。パイプライン全体で、人間のラベラーの平均30秒に対して、0.5秒未満で画像を処理します。動物分類システムは、動物画像を全体として分類します。 93％の感度と96％の特異性。 
[概要]鳥の検出システムは、93％未満の感度、92％の特異性、および68％の平均交差率を達成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Machine Learning Based Analysis of Finnish World War II Photographers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_64.html">
      <font color="black">Machine Learning Based Analysis of Finnish World War II Photographers</font>
    </a>
  </h2>
  <font color="black">さらに、写真家分類器ネットワークから抽出された特徴を使用して、写真家間の類似点と相違点を分析しました。さらに、いくつかの写真から写真家を正常に認識できるニューラルネットワークをトレーニングすることができました。これは、そのような写真が確かに特徴的であることを示しています。特定の写真家向け..具体的には、公に利用可能なフィンランド戦時写真アーカイブで多数の写真を撮影した著名なフィンランド第二次世界大戦の写真家を分析します。このアーカイブには、1939-1945年のフィンランドの冬、継続、ラップランド戦争の写真が16万枚含まれています。 。 
[概要]公開されているフィンランドの戦時中の写真アーカイブで多数の写真を撮影した、フィンランドの著名な第二次世界大戦の写真家を分析します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-22">
        <br><font color="black">2019-04-22</font>
      </time>
    </span>
</section>
<!-- paper0: Bayesian Image Reconstruction using Deep Generative Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_65.html">
      <font color="black">Bayesian Image Reconstruction using Deep Generative Models</font>
    </a>
  </h2>
  <font color="black">生成モデルによるベイズ再構成（BRGM）と呼ばれる私たちの方法は、単一の事前トレーニング済みジェネレータモデルを使用して、さまざまな前方破損モデルと組み合わせることにより、さまざまな画像復元タスク、つまり超解像とインペインティングを解決します。古典的な例（低解像度、高解像度）画像のペアでトレーニングする最近の超解像手法が含まれます。3つのデータセットすべてにわたって、データセット固有のハイパーパラメータ調整なしで、私たちのアプローチは超解像で最先端のパフォーマンスを生み出します。各再構築タスクに固有の最先端の方法と比較した、特に低解像度レベルでの解像度、および修復。 
[概要]コードと事前トレーニング済みモデルはオンラインで入手できます。これらは、強力な事前分布を構築できる3つの大きなデータセットでbrgmを示しました。これらには、画像のペアでトレーニングするトレーニングシステムが含まれています。また、状態を使用しています。同じ手法を使用した後期美術モデルモデル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_66.html">
      <font color="black">CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions</font>
    </a>
  </h2>
  <font color="black">さらに、人間の認知心理学の分野からの力のダイナミクスの理論に触発されて、原因、有効化、および防止の概念を通じてオブジェクトの意図を理解することを含む新しい質問カテゴリを紹介します。人工知能と深層学習の最近の進歩は人間と機械の推論能力のギャップを研究することへの関心を復活させました。これには、10の異なる仮想環境からの3Kビデオから生成された38Kビデオと質問のペアが含まれ、互いに相互作用するさまざまな数の動いているオブジェクトが含まれています。 
[ABSTRACT]クラフトは、物理的な力とオブジェクトの相互作用についての因果関係を必要とするデータに答える新しい視覚的な質問です。これは、プロジェクトに関する一連の研究の最新のものです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: VAE-Info-cGAN: Generating Synthetic Images by Combining Pixel-level and
  Feature-level Geospatial Conditional Inputs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_67.html">
      <font color="black">VAE-Info-cGAN: Generating Synthetic Images by Combining Pixel-level and
  Feature-level Geospatial Conditional Inputs</font>
    </a>
  </h2>
  <font color="black">VAE-Info-cGANの主な目的のアプリケーションは、地理空間分析とリモートセンシングに関連する問題のコンピュータービジョンベースのモデリングのためのターゲットデータ拡張のための合成データ（およびラベル）生成です。逆に、多くのアプリケーションに十分なトレーニングデータを取得することは特にアプリケーションがまれなイベントや極端なイベントのモデリングを伴う場合、経済的に法外な、または実行不可能な可能性があります。GPS軌道データセットでの実験は、提案されたモデルが、ラスターのみを条件として、さまざまな地理的位置にわたってさまざまな形式の時空間集計を正確に生成できることを示しています。道路網の表現。 
[概要]提案されたモデルは、変分オートエンコーダー（vae）と条件付き情報を最大化する敵対的生成ネットワーク（infogan）を組み合わせたものです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: KNN-enhanced Deep Learning Against Noisy Labels -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_68.html">
      <font color="black">KNN-enhanced Deep Learning Against Noisy Labels</font>
    </a>
  </h2>
  <font color="black">ニューラルネットワークを繰り返しトレーニングし、ラベルを更新して、ラベルの回復率を高め、分類パフォーマンスを向上させます。ディープニューラルネットワーク（DNN）での教師あり学習は、データを大量に消費します。K-NearestNeighbors（KNN）の堅牢性に触発されています。データノイズ、この作業では、ラベルのクリーンアップにディープKNNを適用することを提案します。 
[概要]ノイズの多いラベルが存在する場合のdnnのパフォーマンスの最適化が最も重要になっています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Structured Attention Graphs for Understanding Deep Image Classifications -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_69.html">
      <font color="black">Structured Attention Graphs for Understanding Deep Image Classifications</font>
    </a>
  </h2>
  <font color="black">画像分類に関する反事実的な質問に答えるために、SAGの使用を従来のアテンションマップと比較するユーザー調査を実施します。SAGを計算するアプローチとSAGの視覚化を提案し、分類器の決定についてより深い洞察を得ることができるようにします。ベースラインと比較して、SAGに基づく比較反事実的質問に回答するときにユーザーがより正確であることを示します。 
[要約]関心のある画像ごとに、単一のアテンションマップが作成され、分類に対する重要性に基づいてオブジェクトが決定されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-13">
        <br><font color="black">2020-11-13</font>
      </time>
    </span>
</section>
<!-- paper0: Hierarchical Residual Attention Network for Single Image
  Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_70.html">
      <font color="black">Hierarchical Residual Attention Network for Single Image
  Super-Resolution</font>
    </a>
  </h2>
  <font color="black">したがって、処理は同時に実行できる2つの独立した計算パスに分割され、低解像度の画像から高解像度の画像の細部を再構築するための非常に効率的で効果的なモデルが得られます。並行して、軽量の階層アテンションメカニズムは、ネットワークからアテンションバンクに最も関連性の高い機能を抽出して、最終出力を改善し、ネットワーク内の連続操作による情報損失を防ぎます。残りの機能を効率的に使用するために、これらは階層的に機能に集約されます。ネットワーク出力での事後使用のためのバンク。 
[概要]この論文では、新しい軽量の超解像モデルを紹介します。並行して、軽量の空間的注意メカニズムが、ネットワークから注意バンクに最も関連性の高いものを抽出します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Identifying and interpreting tuning dimensions in deep networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_71.html">
      <font color="black">Identifying and interpreting tuning dimensions in deep networks</font>
    </a>
  </h2>
  <font color="black">私たちの方法は、合成ガボールフィルターバンクの調整次元とImageNetでトレーニングされたInceptionV1の最初の2つの層の調整次元を正しく識別します。研究者はディープニューラルネットワークでこれらの調整次元の類似物を手動で識別しようとしましたが、私たちは気づいていません。それらを発見する自動方法..神経科学では、調整次元は、ニューロンのグループの活性化分散の多くを説明する刺激属性です。 
[要約]深いネットワークの「第1層」の特定と解釈を支援します。調査結果は、レベルを特定して調整するための新しい研究活動の一部です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-05">
        <br><font color="black">2020-11-05</font>
      </time>
    </span>
</section>
<!-- paper0: Continual Adaptation of Visual Representations via Domain Randomization
  and Meta-learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_72.html">
      <font color="black">Continual Adaptation of Visual Representations via Domain Randomization
  and Meta-learning</font>
    </a>
  </h2>
  <font color="black">このようなメタドメインは、ランダム化された画像操作によっても生成されます。このコンテキストでは、忘却に対して本質的により堅牢なモデルを学習する1つの方法は、ドメインのランダム化です。視覚タスクの場合、重い画像操作で現在のドメインの分布をランダム化します。 ..分類からセマンティックセグメンテーションに至るまで、さまざまな実験で経験的に実証しました。このアプローチでは、新しいドメインに移管したときに壊滅的な忘却が発生しにくいモデルが得られます。 
[概要]モデルが異なる視覚ドメインから連続して学習する場合、最新のものを優先して過去の視覚ドメインを忘れがちです。これは、正則化が現在のドメインから別の「補助」へのモデルの転送に関連する大きな損失に明示的にペナルティを課す場合です。メタ-ドメイン。このような、私たちのアプローチは、新しいドメインに転送されたときに壊滅的な忘却の傾向が少ないモデルをもたらすことを実証しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Accurate 3D Object Detection using Energy-Based Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_73.html">
      <font color="black">Accurate 3D Object Detection using Energy-Based Models</font>
    </a>
  </h2>
  <font color="black">したがって、この作業では、EBMネットワークのコアモジュールとして機能する3Dバウンディングボックスの微分可能なプーリング演算子を設計します。KITTIデータセットでは、提案されたアプローチは、すべての3DODメトリックでSA-SSDベースラインを一貫して上回り、可能性を示しています。非常に正確な3DODのためのEBMベースの回帰の分析..確率的回帰のための条件付きエネルギーベースモデル（EBM）の最近の進歩を調査することにより、このタスクに対処します。 
[ABSTRACT]スランプのebmsは、画像内の2Dオブジェクト検出で優れたパフォーマンスを示していますが、これらの手法は3Dバウンディングボックスとは直接関係ありません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Performance Analysis of Keypoint Detectors and Binary Descriptors Under
  Varying Degrees of Photometric and Geometric Transformations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_74.html">
      <font color="black">Performance Analysis of Keypoint Detectors and Binary Descriptors Under
  Varying Degrees of Photometric and Geometric Transformations</font>
    </a>
  </h2>
  <font color="black">標準データセットで実験を行い、さまざまな画像変換の下での各メソッドのパフォーマンスの比較を分析しました。（1）FAST、AGAST、ORB検出器がより高速で、より多くのキーポイントを検出した、（2）AKAZEおよびKAZE検出器ORBは幾何学的変化に対してよりロバストであり、ORBは幾何学的変化に対してより堅牢であり、（3）一般に、記述子はKAZEおよびAKAZE検出器と組み合わせた場合に優れたパフォーマンスを示し、（4）BRIEF、LUCID、ORB記述子は比較的高速でした。（5）いずれの記述子も幾何学的変換では特にうまく機能せず、BRISK、FREAK、およびAKAZEのみが妥当な復元力を示しました。関心点検出器を分析するために検出フェーズと記述フェーズを分離し、異なる検出器のペアワイズ組み合わせのパフォーマンスを評価しました。記述子。 
[概要]関心点検出器を分析するために、検出フェーズと説明フェーズを分離しました。高速、agast、orb検出器の方が高速で、より多くのキーポイントを検出しました。一般に、記述子は、kazeおよびakaze検出器と組み合わせるとパフォーマンスが向上します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Self-EMD: Self-Supervised Object Detection without ImageNet -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_75.html">
      <font color="black">Self-EMD: Self-Supervised Object Detection without ImageNet</font>
    </a>
  </h2>
  <font color="black">さらに重要なことに、より多くのラベルなし画像で40.4％mAPにさらに改善でき、より簡単に取得できるラベルなしデータを活用する大きな可能性を示しています。空間構造を維持し、Earth Moverの距離を採用するために画像埋め込みとして畳み込み特徴マップを保持します（ EMD）2つの埋め込み間の類似性を計算します。コードが利用可能になります。 
[ABSTRACT]一般的に使用されている画像ネットのようなオブジェクトデータセットではなく、cocoのようなラベルのない非アイコン画像データセットで直接トレーニングされたメソッド</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-27">
        <br><font color="black">2020-11-27</font>
      </time>
    </span>
</section>
<!-- paper0: MOTChallenge: A Benchmark for Single-Camera Multiple Target Tracking -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_76.html">
      <font color="black">MOTChallenge: A Benchmark for Single-Camera Multiple Target Tracking</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、ベンチマークの最初の3つのリリースを収集します。（i）MOT15と、過去数年間に提出された多数の最先端の結果、（ii）新しい挑戦的なビデオを含むMOT16、および（iii） MOT16シーケンスをより正確なラベルで拡張し、3つの異なるオブジェクト検出器での追跡パフォーマンスを評価するMOT17。最後に、最先端のトラッカーの分類と幅広いエラー分析を提供します。これは、新規参入者が関連作業を理解するのに役立ちます。 MOTコミュニティの研究動向、そしてうまくいけば、将来の研究の方向性に光を当てることができます。 
[概要]歩行者はmotコミュニティで最も研究されているオブジェクトであるため、ベンチマークは複数の人の追跡に焦点を当てています。ラベル付きボックスの数が大幅に増加するだけでなく、歩行者以外の複数のオブジェクトクラスのラベルも提供します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-15">
        <br><font color="black">2020-10-15</font>
      </time>
    </span>
</section>
<!-- paper0: Why Adversarial Interaction Creates Non-Homogeneous Patterns: A
  Pseudo-Reaction-Diffusion Model for Turing Instability -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_77.html">
      <font color="black">Why Adversarial Interaction Creates Non-Homogeneous Patterns: A
  Pseudo-Reaction-Diffusion Model for Turing Instability</font>
    </a>
  </h2>
  <font color="black">理論的および経験的研究により、これらの現象の根底にあるメカニズムを説明するための疑似反応拡散モデルを提示します。興味深いことに、敵対的相互作用を持つニューロンのシステムでチューリングのようなパターンを観察します。この研究では、そのようなパターンを作成するためのチューリング不安定性の関与。 
[ABSTRACT]チューリングモデルは単純化と理想化ですが、パターンを説明するための最もよく知られている理論モデルの1つです。この研究では、このようなパターンを作成するためのチューリング不安定性の関与を確立します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Variational Interaction Information Maximization for Cross-domain
  Disentanglement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_78.html">
      <font color="black">Variational Interaction Information Maximization for Cross-domain
  Disentanglement</font>
    </a>
  </h2>
  <font color="black">私たちの実装は、https：//github.com/gr8joo/IIAEで公開されています。画像から画像への変換およびクロスドメイン検索タスクにおけるモデルの有効性を示します。目的の扱いやすい境界を導き出し、Interaction Information Auto-Encoder（IIAE）という名前の生成モデルを提案します。 
[概要]私たちのアプローチは、クロスドメインのもつれを解くための望ましい表現と、変分オートエンコーダへの接続に関する洞察を明らかにします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Semi-supervised learning with an open augmenting unknown class for
  cost-effective training and reliable classifications -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_79.html">
      <font color="black">Semi-supervised learning with an open augmenting unknown class for
  cost-effective training and reliable classifications</font>
    </a>
  </h2>
  <font color="black">さらに、結果として得られる分類器が、半教師ありのラベルなしトレーニングセットまたはいわゆるオープンセットからのものであるかどうかにかかわらず、絶対的な新規クラス検出が可能であることを保証します。この手法は、ラベル付けに関するトレーニングコストを大幅に削減すると同時に、分類の信頼性..（a）部分的にラベル付けされたデータセットをトレーニングし、（b）結果のネットワークが対象のドメイン外のデータを分離することを保証する機能は、ニューラルネットワーク分類器の実用的で費用効果の高い適用性を大幅に拡張します。 
[概要]分類器は、実用的でコストを節約する半教師あり基準をトレーニングするアプリに基づいています。結果は、関心のある状態であり、この種の最初のものです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-04">
        <br><font color="black">2020-02-04</font>
      </time>
    </span>
</section>
<!-- paper0: Channel Pruning via Multi-Criteria based on Weight Dependency -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_80.html">
      <font color="black">Channel Pruning via Multi-Criteria based on Weight Dependency</font>
    </a>
  </h2>
  <font color="black">次に、グローバル正規化を使用して、クロスレイヤー比較を実現します。関連する重み値、計算コスト、パラメーター量など、3つの側面でフィーチャマップの重要性を設計します。広範な実験により、CPMCが他のモデルよりも大幅に優れていることが示されています。 
[ABSTRACT]チャネルプルーニングアルゴリズムは、さまざまなモデルを効率的に圧縮できます。重み依存の現象を使用して、関連するフィルターと次のレイヤーの対応する部分重みを評価することで重要性を取得します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-06">
        <br><font color="black">2020-11-06</font>
      </time>
    </span>
</section>
<!-- paper0: What makes for good views for contrastive learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_81.html">
      <font color="black">What makes for good views for contrastive learning</font>
    </a>
  </h2>
  <font color="black">データの複数のビュー間の対照学習は、最近、自己教師あり表現学習の分野で最先端のパフォーマンスを達成しました。この論文では、経験的分析を使用して、ビュー選択の重要性をよりよく理解し、削減する必要があると主張します。タスク関連の情報をそのまま維持しながら、ビュー間の相互情報（MI）..副産物として、ImageNet分類の教師なし事前トレーニングで新しい最先端の精度を実現します（$ 73 \％$ top -ResNet-50を使用した1つの線形読み取り。 
[概要]たとえば、さまざまなビューの選択の影響はあまり研究されていません。この理論を検証するために、miを減らすことを目的として、効果的なビューを学習する教師なしおよび半教師ありフレームワークを考案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-20">
        <br><font color="black">2020-05-20</font>
      </time>
    </span>
</section>
<!-- paper0: A Unifying Framework for Formal Theories of Novelty:Framework, Examples
  and Discussion -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_82.html">
      <font color="black">A Unifying Framework for Formal Theories of Novelty:Framework, Examples
  and Discussion</font>
    </a>
  </h2>
  <font color="black">これらの分野で重要な研究が行われていますが、問題の領域を超えた新規性の正式な定義がないことに顕著なギャップがあります。私たちのフレームワークは、シンボリックAIから強化学習まで、幅広い領域に適用できます。したがって、私たちは、ノベルティの正式な理論のための最初の統一されたフレームワークを提示し、そのフレームワークを使用して、ノベルティタイプのファミリーを正式に定義します。 
[ABSTRACT]問題には、通常の入力の新しい摂動に耐性があることが含まれます。ただし、新しいインポートに適応し、新しいインポートに適応しています。これは、複数の研究グループと異なるドメインにまたがる研究者のチームです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: R3Det: Refined Single-Stage Detector with Feature Refinement for
  Rotating Object -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_83.html">
      <font color="black">R3Det: Refined Single-Stage Detector with Feature Refinement for
  Rotating Object</font>
    </a>
  </h2>
  <font color="black">3つの人気のあるリモートセンシングパブリックデータセットDOTA、HRSC2016、UCAS-AOD、および1つのシーンテキストデータセットICDAR2015での実験は、私たちのアプローチの有効性を示しています。TensorflowとPytorchのバージョンコードはhttps://github.com/Thinklab-SJTUで入手できます。 / R3Det_Tensorflowとhttps://github.com/SJTU-Thinklab-Det/r3det-on-mmdetection、およびR3Detもオープンソースの回転検出ベンチマークに統合されています：https：//github.com/yangxue0827/RotationDetection .. Forより正確な回転推定では、SkewIoUの計算が導き出せないという問題を解決するために、おおよそのSkewIoU損失が提案されます。 
[ABSTRACT]より正確な特徴を取得することで検出パフォーマンスを向上させる新しい特徴改良モジュール。特徴改良により、より詳細な特徴を取得できます。より正確な回転情報のために、正確なスキュー損失が提案され、スキューの計算が導出できない</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-15">
        <br><font color="black">2019-08-15</font>
      </time>
    </span>
</section>
<!-- paper0: Scale Aware Adaptation for Land-Cover Classification in Remote Sensing
  Imagery -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_84.html">
      <font color="black">Scale Aware Adaptation for Land-Cover Classification in Remote Sensing
  Imagery</font>
    </a>
  </h2>
  <font color="black">フレームワークには、標準機能ディスクリミネーターと新しいスケールディスクリミネーターを備えたデュアルディスクリミネーターアーキテクチャがあります。モデルの一般化を改善するためにドメイン適応が提案されていますが、これらのアプローチは、リモート間で一般的に見られるスケール変動を処理するには効果的ではありません。画像コレクションのセンシング..また、スケールが強化された機能を生成するスケールアテンションモジュールも紹介します。 
[概要]以前は、土地被覆分類は完全に接続されたニューラルネットワークの開発の恩恵を受けていました。これにより、センシングモデルが他のデータセットに一般化する能力が制限されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: UnrealPerson: An Adaptive Pipeline towards Costless Person
  Re-identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_85.html">
      <font color="black">UnrealPerson: An Adaptive Pipeline towards Costless Person
  Re-identification</font>
    </a>
  </h2>
  <font color="black">個人再識別（ReID）の主な難しさは、注釈付きデータを収集し、モデルを異なるドメイン間で転送することにあります。その基本的な部分は、高品質の合成画像を制御可能な分布から生成できるシステムです。これは優れた機能を提供します。教師なしドメイン適応の基礎。事前にトレーニングされたモデルは、より高い精度に向けて最先端のアルゴリズムに簡単にプラグインできます。 
[ABSTRACT] unrealpersonは、非現実的な画像データを最大限に活用して、トレーニング段階と展開段階の両方でコストを削減する新しいパイプラインです。この方法は、msmt17に直接転送すると、38.5％のランク-1の精度を達成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Context-Aware Graph Convolution Network for Target Re-identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_86.html">
      <font color="black">Context-Aware Graph Convolution Network for Target Re-identification</font>
    </a>
  </h2>
  <font color="black">このように、ハードサンプルはコンテキスト情報で対処できます-グラフの推論中の簡単なサンプルの中でも特に情報の流れ-ソニング..具体的には、効果的なハードギャラリーサンプラーを採用して、面積に見合ったグラフサイズを維持しながらポジティブサンプルの高いリコールを取得します、これはまた、計算の複雑さが低いトレーニングプロセスの不均衡な問題を弱めることができます。実験は、提案された方法が、限られたオーバーヘッドでプラグアンドプレイ方式で人と車両の両方の再識別データセットで最先端のパフォーマンスを達成することを示しています。 
[概要]これらのほとんどは、コンテンツの類似性に個別に基づいており、クエリセットとギャラリーセットのコンテキスト情報を使用しません。これらには、適度なグラフサイズを維持しながら、陽性サンプルの高い再現率を取得するためのハードギャラリーサンプルが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: The Lottery Ticket Hypothesis for Object Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_87.html">
      <font color="black">The Lottery Ticket Hypothesis for Object Recognition</font>
    </a>
  </h2>
  <font color="black">したがって、このようなモデルでは、ストレージ要件と計算量を減らすことが最も重要です。パフォーマンスを低下させることなく、さまざまなサブタスクで全体のスパース性が最大80％の宝くじを見つける方法についてのガイダンスを提供します。最後に、オブジェクトのサイズ、頻度、検出の難しさなど、さまざまなタスク属性に関して、トレーニングされたチケットの動作を分析します。 
[概要]最近の調査によると、パフォーマンスの低下を招くことなく、さまざまなサブタスクで全体のスパース性が最大80％の宝くじが示されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Raw Image Deblurring -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_88.html">
      <font color="black">Raw Image Deblurring</font>
    </a>
  </h2>
  <font color="black">最終的には、考案された新しいrawベースのぼけ除去方法とまったく新しいDeblur-RAWデータセットに基づいて、さらなる機会の新しい場所を示します。RAW画像のみからトレーニングされた、提案されたぼけ除去モデルは、最先端のパフォーマンスを実現します。さらに、処理されたsRGB画像でトレーニングされたモデルを上回ります。さらに、微調整により、新しいデータセットでトレーニングされた提案されたモデルを他のセンサーに一般化できます。 
[概要]提案されたブレ除去カーネルは、生の画像からのみ機能します。ただし、私たちの知る限り、利用可能な新しいブレ除去ネットワークはありません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Physics-Guided Recurrent Graph Networks for Predicting Flow and
  Temperature in River Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_89.html">
      <font color="black">Physics-Guided Recurrent Graph Networks for Predicting Flow and
  Temperature in River Networks</font>
    </a>
  </h2>
  <font color="black">デラウェア川流域のサブセットにおける温度と河川流量の予測における提案手法の有効性を示します。また、さまざまな河川セグメントのパフォーマンスのバランスをとる新しい損失関数を提案します。特に、提案手法がもたらすことを示します。温度/ストリームフローにおいて、最先端の物理ベースのモデルより33 \％/ 14 \％向上し、従来の機械学習モデル（たとえば、長期メモリニューラルネットワーク）より24 \％/ 14 \％向上しました。トレーニングに非常にまばらな（0.1 \％）観測データを使用した予測。 
[概要]河川ネットワーク内の複数のセグメント間の相互作用をキャプチャするために、再帰グラフネットワークモデルが提案されています。提案された方法は、33の学習機械ベースの物理ベースの物理ベースの物理ベースの物理物理ベースの物理ベースの予測をもたらします。また、さまざまな河川セグメントでパフォーマンスのバランスをとる新しい損失関数も提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-26">
        <br><font color="black">2020-09-26</font>
      </time>
    </span>
</section>
<!-- paper0: Odyssey: Creation, Analysis and Detection of Trojan Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_90.html">
      <font color="black">Odyssey: Creation, Analysis and Detection of Trojan Models</font>
    </a>
  </h2>
  <font color="black">これらの2つの要素を利用して、攻撃の知識がなくても動作し、既存の方法を大幅に上回る効率的なトロイの木馬検出器を提案します。固有のDNNプロパティの分析に基づく検出器を提案します。トロイの木馬プロセスの影響を受けます。包括的な分析のために、3,000を超えるクリーンなトロイの木馬モデルを備えたこれまでで最も多様なデータセットであるOdysseusを開発します。 
[概要]攻撃者は、一部のトレーニングサンプルにトリガーを挿入することでトレーニングパイプラインを妨害し、トリガーを含むサンプルに対してのみ悪意を持って動作するようにモデルをトレーニングします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-16">
        <br><font color="black">2020-07-16</font>
      </time>
    </span>
</section>
<!-- paper0: Meta-Learning with Adaptive Hyperparameters -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_91.html">
      <font color="black">Meta-Learning with Adaptive Hyperparameters</font>
    </a>
  </h2>
  <font color="black">具体的には、ステップごとのハイパーパラメータを適応的に生成できる小さなメタネットワークを導入します。学習率と重み減衰係数。したがって、高速適応プロセスを大幅に強化する新しい重み更新ルールを提案します。実験結果は、高速適応のためのハイパーパラメータの適応学習（ALFA）は、最近の数ショット学習アプローチではしばしば無視されていた同様に重要な要素です。 
[ABSTRACT]アルファを使用したランダム初期化からの高速適応は改善できます。高速初期化からの高速トレーニングタスクはmamlを調整できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-31">
        <br><font color="black">2020-10-31</font>
      </time>
    </span>
</section>
<!-- paper0: Cross-Modal Collaborative Representation Learning and a Large-Scale RGBT
  Benchmark for Crowd Counting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_92.html">
      <font color="black">Cross-Modal Collaborative Representation Learning and a Large-Scale RGBT
  Benchmark for Crowd Counting</font>
    </a>
  </h2>
  <font color="black">さらに、提案されたアプローチはマルチモーダル群集カウントに普遍的であり、ShanghaiTechRGBDデータセットで優れたパフォーマンスを達成することもできます。この作業では、光学的および熱的情報を組み込むことが歩行者の認識に大いに役立つことがわかります。 RGBT-CCベンチマークは、RGBT群集カウントのためのフレームワークの有効性を示しています。 
[概要]大規模なrgbt群集カウントベンチマークには、2,000ペアのrgb-138、389人の注釈付きの熱画像が含まれています。提案されたアプローチは、マルチモーダル群集カウントに共通です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Dynamic Anchor Learning for Arbitrary-Oriented Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_93.html">
      <font color="black">Dynamic Anchor Learning for Arbitrary-Oriented Object Detection</font>
    </a>
  </h2>
  <font color="black">このように、検出器は高品質のアンカーを動的に選択して正確なオブジェクト検出を実現し、分類と回帰の相違を軽減します。新しく導入されたDALにより、任意の方向のオブジェクトに対して優れた検出パフォーマンスを実現します。いくつかの水平プリセットアンカー..これは、IoUを介したアンカーの品質評価が適切ではないことを示しており、これにより、分類の信頼性とローカリゼーションの精度の間に不整合が生じます。 
[概要]この方法では、高品質のアンカーを簡単に選択して、正確なオブジェクト検出を実現できます。これは、トレーニングの正と負の候補を識別するために使用されています。これにより、分類の信頼性とローカリゼーションの精度に矛盾が生じます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: MANGO: A Mask Attention Guided One-Stage Scene Text Spotter -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CV/paper_94.html">
      <font color="black">MANGO: A Mask Attention Guided One-Stage Scene Text Spotter</font>
    </a>
  </h2>
  <font color="black">実験結果は、提案された方法が、規則的および不規則なテキストスポッティングベンチマーク、すなわち、ICDAR 2013、ICDAR 2015、Total-Text、およびSCUT-CTW1500の両方で競争力のある最新のパフォーマンスを達成することを示しています。テキストの輪郭のコンパクトさ）。MANGOは本質的に任意の形状のテキストスポッティングに適応し、粗い位置情報のみでエンドツーエンドでトレーニングできることに注意してください（\ emph {eg 
[ABSTRACT]ほとんどの方法は検出を連結しようとします部分とシーケンス認識部分を2段階のテキストスポッティングフレームワークに</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Relevance Transformer: Generating Concise Code Snippets with Relevance
  Feedback -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_0.html">
      <font color="black">Relevance Transformer: Generating Concise Code Snippets with Relevance
  Feedback</font>
    </a>
  </h2>
  <font color="black">Django、Hearthstone、CoNaLaなど、コード生成用の複数の標準ベンチマークデータセットで実験を行います。さらに、疑似関連性フィードバックを使用して外部知識を組み込んだRelevanceTransformerと呼ばれる新しいモデルを提案します。RelevanceTransformerはデコードプロセスにバイアスをかけます。多様性を強化しながら、既存の取得されたコードと同様です。 
[概要]新世代のコード駆動型ツールが多くのアイデアに組み込まれています。結果は、ブルー評価を使用するためのアプリケーションのモデルを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-06">
        <br><font color="black">2020-07-06</font>
      </time>
    </span>
</section>
<!-- paper0: Knowledge-Driven Distractor Generation for Cloze-style Multiple Choice
  Questions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_1.html">
      <font color="black">Knowledge-Driven Distractor Generation for Cloze-style Multiple Choice
  Questions</font>
    </a>
  </h2>
  <font color="black">4つのドメインにわたるデータセットの実験結果は、フレームワークが以前の方法よりももっともらしく信頼性の高いディストラクタを生成することを示しています。この論文では、オープンドメインのクローズスタイルの多肢選択問題の気を散らす選択肢を自動的に生成する新しい構成可能なフレームワークを提案します。は、小さなディストラクタ候補セットを効果的に作成するための汎用知識ベースと、もっともらしく信頼できるディストラクタを選択するための機能豊富なランク付け学習モデルを組み込んでいます。このデータセットは、のベンチマークとしても使用できます。将来の気晴らしの生成。 
[要約]調査によると、私たちはテキストメッセージに気を取られる可能性が高いです。気を散らすものは以前の方法よりも可能性が高い</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br><font color="black">2020-04-21</font>
      </time>
    </span>
</section>
<!-- paper0: Multilingual Speech Translation with Efficient Finetuning of Pretrained
  Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_2.html">
      <font color="black">Multilingual Speech Translation with Efficient Finetuning of Pretrained
  Models</font>
    </a>
  </h2>
  <font color="black">これにより、低トレーニングコストで大規模な事前トレーニング済みモデルを効果的に活用できます。私たちのアプローチは、多対多多言語モデルでのゼロショットクロスリンガル転送とゼロショットペアワイズ翻訳の両方で強力なパフォーマンスを示し、達成するための魅力的なアプローチになります。トレーニングコストが低く、データ効率が向上した高品質の音声翻訳。音響モデリングにwav2vec 2.0を使用し、多言語テキスト生成にmBARTを使用することで、私たちのアプローチは32の翻訳方向に新しい最先端技術を進歩させました（カスケードSTを超えました）。多言語STベンチマークCoVoST2（13のEn-X方向で平均+4.4 BLEU、19のX-En方向で平均+4.3 BLEU）。 
[概要]最小限のlna（レイヤーノルムと注意）の微調整により、言語間およびモダリティ間の転送機能を実現できます。私たちのアプローチは、stベンチマークcovost2で32の翻訳方向の新しい最先端技術を進歩させました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-24">
        <br><font color="black">2020-10-24</font>
      </time>
    </span>
</section>
<!-- paper0: CTRLsum: Towards Generic Controllable Text Summarization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_3.html">
      <font color="black">CTRLsum: Towards Generic Controllable Text Summarization</font>
    </a>
  </h2>
  <font color="black">私たちのアプローチにより、ユーザーは、キーワードのセットまたは説明プロンプトの形式でテキスト入力を介して要約システムと対話することにより、生成された要約の複数の側面を制御できます。さらに、標準の制御されていない要約設定で使用すると、CTRLsumは状態を実現します。 -CNN / DailyMailデータセットの最新の結果..要約データセットの3つのドメインと5つの制御の側面に対するアプローチの有効性を定量的に示します：1）エンティティ中心および2）長さ制御可能な要約、3）貢献の要約科学論文、4）特許出願に関する発明目的の要約、および5）読解環境でのニュース記事に関する質問に基づく要約。 
[ABSTRACT]要約により、ユーザーはシンプルでシンプルなシステムを作成できます。これにより、幅広い要約情報を実現できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Efficient Estimation of Influence of a Training Instance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_4.html">
      <font color="black">Efficient Estimation of Influence of a Training Instance</font>
    </a>
  </h2>
  <font color="black">ドロップアウトマスクを切り替えることで、各トレーニングインスタンスを学習した、または学習しなかったサブネットワークを使用して、その影響を推定できます。分類データセットでのBERTとVGGNetの実験を通じて、提案された方法がトレーニングの影響をキャプチャし、強化できることを示します。エラー予測の解釈可能性、および一般化を改善するためのトレーニングデータセットのクレンジング。ニューラルネットワークモデルに対するトレーニングインスタンスの影響を理解することは、解釈可能性の改善につながります。 
[ABSTRACT]私たちの方法はドロップアウトに触発されています。ドロップアウトは、サブネットワークをゼロマスクし、サブネットワークが各トレーニングインスタンスを学習するのを防ぎます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Document Graph for Neural Machine Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_5.html">
      <font color="black">Document Graph for Neural Machine Translation</font>
    </a>
  </h2>
  <font color="black">ドキュメントグラフを作成するために、隣接関係、構文依存関係、語彙の一貫性、共参照など、いくつかのタイプの関係を採用しています。IWSLT英語-フランス語、中国語-英語、WMT英語-ドイツ語、Opensubtitle英語などのさまざまなNMTベンチマークでの実験-ロシア語、ドキュメントグラフを使用すると、翻訳の品質が大幅に向上することを示しています。ただし、既存のドキュメントレベルのNMTメソッドのほとんどは、前の文のいくつかのセットを超えるコンテキストを活用できませんでした。 
[概要]ほとんどの既存のドキュメント-レベルwtメソッドは期待に挑戦できませんでした。これらのツールを使用する代わりに、関連するコンテキストを接続するパターンを開発しました。目的はこの問題に対処することであり、ドキュメントは次のように表すことができると仮定します。グラフ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-07">
        <br><font color="black">2020-12-07</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Represent Programs with Heterogeneous Graphs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_6.html">
      <font color="black">Learning to Represent Programs with Heterogeneous Graphs</font>
    </a>
  </h2>
  <font color="black">どちらのタスクでも、完全なコードスニペットのセマンティクスについて推論する必要があります。ASTでは、ノードごとに変数や制御フローなどのさまざまな種類の情報が含まれ、ノードとそのすべての子の間の関係も異なる場合があります。ASDL文法を使用します。プログラムグラフのノードおよびエッジタイプを定義するためのプログラミング言語の使用。 
[概要]作品は、追加の制御またはデータフロー情報をastsに提供しますが、さまざまなタイプのノードやエッジなどの構造情報の重要なコンポーネントを無視しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Cross-lingual Approach to Abstractive Summarization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_7.html">
      <font color="black">Cross-lingual Approach to Abstractive Summarization</font>
    </a>
  </h2>
  <font color="black">私たちの仕事では、ディープニューラルネットワークとシーケンス間アーキテクチャに基づく事前トレーニング済みの英語要約モデルを使用して、スロベニアのニュース記事を要約しました。要約については、再利用できないデコーダーのため、このような言語間のモデル転送はこれまで試みられていませんでした。ニューラルモデルの側面..微調整のために、ターゲット言語データの比率が異なるいくつかのモデルを開発しました。 
[概要]結果は、大規模なトレーニングセットが存在しない用語に調整されました。ターゲット言語の評価に追加の言語の概要を使用しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: From Bag of Sentences to Document: Distantly Supervised Relation
  Extraction via Machine Reading Comprehension -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_8.html">
      <font color="black">From Bag of Sentences to Document: Distantly Supervised Relation
  Extraction via Machine Reading Comprehension</font>
    </a>
  </h2>
  <font color="black">実験は、私たちの方法が新しい最先端のDSパフォーマンスを達成することを示しています。エンティティに関するすべての文をドキュメントとして再編成し、リレーション固有の質問でドキュメントをクエリしてリレーションを抽出することにより、ドキュメントベースのDSパラダイムは次のことができます。すべてのセンテンスレベル、インターセンテンスレベル、およびエンティティレベルの証拠を同時にエンコードして活用します。ただし、バッグベースのパラダイムでは、関係抽出にインターセンテンスレベルとエンティティレベルの証拠を活用できません。それらのノイズ除去アルゴリズムは、多くの場合、特殊で複雑です。 
[ABSTRACT]遠隔監視は、mrcモデルを効果的にトレーニングできる新しいdsシステムです。dsloss（遠隔監視損失）と呼ばれ、新しいモデルのトレーニングに使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: On the Importance of Word Order Information in Cross-lingual Sequence
  Labeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_9.html">
      <font color="black">On the Importance of Word Order Information in Cross-lingual Sequence
  Labeling</font>
    </a>
  </h2>
  <font color="black">さらに、提案された方法は、強力な言語間ベースラインに適用して、パフォーマンスを向上させることもできます。この仮説を検証するために、モデルをソース言語の語順に鈍感にすることで、ターゲット言語の適応パフォーマンスを向上できるかどうかを調査します。さらに、この仮説に基づいて、下流の言語間シーケンスラベリングタスクで多言語BERTを微調整するための新しい方法を提案します。 
[概要]この論文では、ソース言語の語順に適合するクロスリンガルモデルが強力なターゲットを処理できない可能性があると仮定します。そうするために、シーケンスエンコーダーに適合する情報を減らし、パフォーマンスの変化を観察します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-30">
        <br><font color="black">2020-01-30</font>
      </time>
    </span>
</section>
<!-- paper0: Revisiting Iterative Back-Translation from the Perspective of
  Compositional Generalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_10.html">
      <font color="black">Revisiting Iterative Back-Translation from the Perspective of
  Compositional Generalization</font>
    </a>
  </h2>
  <font color="black">（3）このメカニズムをさらに促進するために、カリキュラムの反復逆翻訳を提案します。これにより、疑似並列データの品質が向上し、パフォーマンスがさらに向上します。（2）反復逆翻訳が役立つ理由を理解するために、慎重に検討します。パフォーマンスの向上を調べて、反復逆変換が疑似パラレルデータのエラーをますます修正できることを確認します。この作業では：（1）最初に、反復逆変換が構成一般化ベンチマーク（CFQおよびSCAN）のパフォーマンスを大幅に向上させることを経験的に示します。 ）。 
[概要]パフォーマンスを改善できるかどうか、またどのように改善できるかを調査するために、反復的な逆翻訳、シンプルでありながら効果的な半教師あり方法を再検討します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Discovering key topics from short, real-world medical inquiries via
  natural language processing and unsupervised learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_11.html">
      <font color="black">Discovering key topics from short, real-world medical inquiries via
  natural language processing and unsupervised learning</font>
    </a>
  </h2>
  <font color="black">このアプローチは、オントロジーや注釈を必要としません。ただし、問い合わせの量が多く、専門的な性質があるため、タイムリーで、繰り返し、包括的な分析を実行することは困難です。発見されたトピックは、によって判断されるように、意味があり、医学的に関連しています。医療情報の専門家は、したがって、一方的な医療の問い合わせが貴重な顧客の洞察の源であることを示しています。 
[概要]自然言語処理と教師なし学習に基づく機械学習アプローチを提案し、顧客からの現実世界の医療問い合わせの重要なトピックを自動的に発見します。発見されたトピックは、医療情報の専門家によって判断されるように、意味があり、医学的に関連性があります。未承諾の医療問い合わせは、貴重な顧客洞察の源です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: The Role of Interpretable Patterns in Deep Learning for Morphology -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_12.html">
      <font color="black">The Role of Interpretable Patterns in Deep Learning for Morphology</font>
    </a>
  </h2>
  <font color="black">エンコーダーがパターンマッチングネットワークである、標準のシーケンス間モデルの修正バージョンを使用します。形態素解析、レンマ化、コピーの3つのタスクで文字パターンの役割を調べます。ソースは同じですがターゲットが異なるため、さまざまなタスクにとって重要なサブワードと、それらが互いにどのように関連しているかを比較できます。 
[概要]標準のシーケンスからシーケンスモデルへの修正バージョンを使用します。エンコーダーはパターンマッチングネットワークです。この方法では、出力の生成に重要なサブワードを学習できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: CrossNER: Evaluating Cross-Domain Named Entity Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_13.html">
      <font color="black">CrossNER: Evaluating Cross-Domain Named Entity Recognition</font>
    </a>
  </h2>
  <font color="black">それでも、実験はこのクロスドメインNERタスクの課題も示しています。コードとデータはhttps://github.com/zliucr/CrossNERで入手できます。次に、さまざまなレベルのレバレッジの有効性を調査するための包括的な実験を実施します。クロスドメインタスクのドメイン適応型事前トレーニングを実行するためのドメインコーパスと事前トレーニング戦略。 
[概要]ほとんどのnerベンチマークには、ドメイン（特殊なエンティティタイプ）がないか、特定のドメインに焦点を当てていません。これは、クロスドメインテストがnerドメインの適応に有益であることを意味します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Globetrotter: Unsupervised Multilingual Translation from Visual
  Alignment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_14.html">
      <font color="black">Globetrotter: Unsupervised Multilingual Translation from Visual
  Alignment</font>
    </a>
  </h2>
  <font color="black">私たちの言語表現は、単一のステージで1つのモデルで共同でトレーニングされます。52の言語での実験は、私たちの方法が、検索を使用した監視されていない単語レベルおよび文レベルの翻訳のベースラインよりも優れていることを示しています。画像をそれらの間の架け橋として使用して、複数の言語を整列させるモダリティ。 
[ABSTRACT]既存の教師なしメソッドは通常、言語表現のブリッジプロパティに依存します。言語と画像の間のクロスモーダルアライメントを推定し、この推定を使用してクロスリンガル表現の学習をガイドします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Discourse Parsing of Contentious, Non-Convergent Online Discussions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_15.html">
      <font color="black">Discourse Parsing of Contentious, Non-Convergent Online Discussions</font>
    </a>
  </h2>
  <font color="black">ロジスティック回帰からBERTまでの一連の分類モデルを検討します。最後に、論争のある非収束オンラインディスカッションの最初のラベル付きデータセットを共有します。成功したディスカッションの尺度を再定義し、新しい談話注釈スキーマを開発します。談話戦略の階層を反映しています。 
[ABSTRACT]新しいスキーマを使用して、制御されていないディスカッションを分析できます。新しいスキーマを使用すると、プロジェクトのツールとして使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: CAiRE-COVID: A Question Answering and Query-focused Multi-Document
  Summarization System for COVID-19 Scholarly Information Management -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_16.html">
      <font color="black">CAiRE-COVID: A Question Answering and Query-focused Multi-Document
  Summarization System for COVID-19 Scholarly Information Management</font>
    </a>
  </h2>
  <font color="black">また、質問に関連するより関連性の高い情報を提供するために、クエリに焦点を当てた抽象的および抽出的なマルチドキュメント要約方法を提案します。さらに、各モジュールのさまざまなメトリックの一貫した改善を示す定量的実験を実施します。情報抽出と状態を組み合わせます。最先端のQAとクエリに焦点を当てたマルチドキュメント要約手法。クエリが与えられた既存の文献から証拠スニペットを選択して強調表示します。 
[概要]私たちのシステムは、マイニングの最近の課題に取り組むことを目的としています。コミュニティからの優先度の高い質問に答え、重要な質問を要約します-関連情報</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-04">
        <br><font color="black">2020-05-04</font>
      </time>
    </span>
</section>
<!-- paper0: Facts2Story: Controlling Text Generation by Key Facts -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_17.html">
      <font color="black">Facts2Story: Controlling Text Generation by Key Facts</font>
    </a>
  </h2>
  <font color="black">自然言語で表現された一連の事実をより長い物語に拡張することに基づく制御された生成タスクを提案します。私たちは、遵守しながら競争力のある流暢さを生み出す計画とクローズのモデル（微調整されたXLNetを使用）を提案します。要求されたコンテンツ..事前にトレーニングされたモデルの微調整に基づいて、このタスクで3つの方法を評価します。 
[概要]人間が制御するタスクと、大規模なトレーニングデータセットを導出する方法を紹介します。gpt2などの自己回帰の一方向言語モデルは流暢さを向上させますが、要求された事実を順守するのに苦労していることを示します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: TNT-KID: Transformer-based Neural Tagger for Keyword Identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_18.html">
      <font color="black">TNT-KID: Transformer-based Neural Tagger for Keyword Identification</font>
    </a>
  </h2>
  <font color="black">トランスフォーマーアーキテクチャを目前の特定のタスクに適合させ、ドメイン固有のコーパスで言語モデルの事前トレーニングを活用することにより、モデルは、競争力のある堅牢なキーワード抽出を提供することにより、教師ありと教師なしの両方の最先端のアプローチの欠陥を克服できます。さまざまな異なるデータセットでのパフォーマンス。最高のパフォーマンスを発揮するシステムに必要な手動でラベル付けされたデータのごく一部しか必要としません。この調査では、モデルの内部動作に関する貴重な洞察を備えた徹底的なエラー分析と、特定の影響を測定するアブレーション調査も提供します。全体的なパフォーマンスに関するキーワード識別ワークフローのコンポーネント。利用可能なテキストデータの量が増えるにつれ、これらのデータの自動分析、分類、および要約が可能なアルゴリズムの開発が必要になりました。 
[ABSTRACT]キーワード識別用のトランスフォーマーベースのニューラルタガー（tnt-kid）はkickstarterで利用できます。これは米国中の研究者が利用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-20">
        <br><font color="black">2020-03-20</font>
      </time>
    </span>
</section>
<!-- paper0: Distilling Knowledge from Reader to Retriever for Question Answering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_19.html">
      <font color="black">Distilling Knowledge from Reader to Retriever for Question Answering</font>
    </a>
  </h2>
  <font color="black">情報検索のタスクは、オープンドメインの質問応答など、多くの自然言語処理システムの重要なコンポーネントです。このような方法を使用する場合の課題は、クエリとサポートドキュメントのペアに対応する、検索データを取得して検索モデルをトレーニングすることです。 。私たちのアプローチは、検索されたドキュメントに基づいてタスクを解決するために使用されるリーダーモデルの注意スコアを活用して、検索者の合成ラベルを取得します。 
[ABSTRACT]新しい論文は、ダウンストリームタスクのレトリーバーモデルを学習する手法を提案しています。注釈付きの検索とドキュメントのペアは必要ありません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Confidence-aware Non-repetitive Multimodal Transformers for TextCaps -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_20.html">
      <font color="black">Confidence-aware Non-repetitive Multimodal Transformers for TextCaps</font>
    </a>
  </h2>
  <font color="black">キャプションの単語の冗長性の問題に対処するために、生成モジュールには、キャプションの繰り返し単語の予測を回避するための繰り返しマスクが含まれています。この目的のために、上記の課題に取り組むための信頼性を意識した非繰り返しマルチモーダルトランスフォーマー（CNMT）を提案します。 。私たちのCNMTは、読み取り、推論、生成モジュールで構成されています。読み取りモジュールは、より優れたOCRシステムを使用してテキストの読み取り能力を強化し、最も注目に値するトークンを選択するための信頼性を埋め込みます。 
[概要]新しいアプローチでは、読み取り能力が低いため、正確な説明を生成できません。抽出されたすべてのocrトークンから重要な単語を選択できないことが含まれます。私たちのモデルは、textcapsデータセットの最新モデルよりも優れています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-07">
        <br><font color="black">2020-12-07</font>
      </time>
    </span>
</section>
<!-- paper0: A Topological Method for Comparing Document Semantics -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_21.html">
      <font color="black">A Topological Method for Comparing Document Semantics</font>
    </a>
  </h2>
  <font color="black">2つのドキュメント間のセマンティクスの類似性を比較するためのトポロジカルな永続性に基づく新しいアルゴリズムが提案されています。ドキュメントのセマンティクスの比較は、自然言語処理と情報取得の両方で最も困難なタスクの1つです。比較のために。 
[概要]この論文では、別の音を出すことを望んでいます。実験は、人間の裁判官の結果を含むドキュメントデータセットで実行されます。結果は、私たちのアルゴリズムが非常に人間的な一貫した結果を生成できることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Combining Machine Learning and Human Experts to Predict Match Outcomes
  in Football: A Baseline Model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_22.html">
      <font color="black">Combining Machine Learning and Human Experts to Predict Match Outcomes
  in Football: A Baseline Model</font>
    </a>
  </h2>
  <font color="black">私たちのデータセットは、英国プレミアリーグの6シーズンにわたる代表的な期間に焦点を当てており、ガーディアンの新聞の試合プレビューが含まれています。このペーパーで提示されたモデルは、63.18％の精度を達成し、従来の統計を6.9％向上させています。メソッド..このペーパーでは、新しいアプリケーションに焦点を当てたベンチマークデータセットと、フットボール（サッカー）の試合結果を予測するための一連のベースライン自然言語処理および機械学習モデルの結果を示します。 
[概要]ゲームの予測精度のベースラインを示します。このホワイトペーパーで紹介するモデルは、63.18％の精度を達成しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_23.html">
      <font color="black">CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions</font>
    </a>
  </h2>
  <font color="black">CRAFTの2つの質問カテゴリには、以前に研究された記述的質問と反事実的質問が含まれます。人工知能と深層学習の最近の進歩により、人間と機械の推論能力のギャップを研究することへの関心が復活しました。人間の認知心理学の分野では、原因、有効化、防止の概念を通じてオブジェクトの意図を理解することを含む新しい質問カテゴリを紹介します。 
[ABSTRACT]クラフトは、物理的な力とオブジェクトの相互作用についての因果関係を必要とするデータに答える新しい視覚的な質問です。これは、プロジェクトに関する一連の研究の最新のものです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Human-Labeled Data through Dynamic Automatic Conflict
  Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_24.html">
      <font color="black">Improving Human-Labeled Data through Dynamic Automatic Conflict
  Resolution</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、（a）一般的なクラウドソーシングセマンティックアノテーションタスクによって生成されるラベルのノイズを推定し、（b）他のタスクと比較してラベリングプロセスの結果として生じるエラーを最大20〜30％削減するためのスケーラブルな方法論を開発および実装します。一般的なラベリング戦略..これにより、DACRがより正確になるだけでなく、幅広いラベリングタスクで利用できるようになります。重要なことに、動的自動競合解決（DACR）と呼ばれる、ラベリングプロセスへのこの新しいアプローチには根拠が必要ありません。真実のデータセットであり、代わりにプロジェクト間の注釈の不整合に基づいています。 
[概要]ラベリングプロセスへのこの新しいアプローチは、グラウンドトゥルースデータセットを必要としません。代わりに、dacrはプロジェクト間のアノテーションの不整合に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Early Detection of Fake News by Utilizing the Credibility of News,
  Publishers, and Users Based on Weakly Supervised Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_25.html">
      <font color="black">Early Detection of Fake News by Utilizing the Credibility of News,
  Publishers, and Users Based on Weakly Supervised Learning</font>
    </a>
  </h2>
  <font color="black">3つの実世界のデータセットで実験を行った結果、SMANは4時間で91％を超える精度で偽のニュースを検出できることがわかりました。これは、最先端のモデルよりもはるかに高速です。信頼できる信頼できるソースからのニュース、または評判の良い多くのユーザーによって共有されているニュースは、他のニュースよりも信頼性が高くなります。以前の弱く監視された情報としてのパブリッシャーとユーザーの信頼性を使用して、大量のニュースで偽のニュースをすばやく見つけ、普及の初期段階。 
[概要]大量のニュースから偽のニュースを簡単に見つけて、配布の初期段階で検出できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Extractive Opinion Summarization in Quantized Transformer Spaces -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_26.html">
      <font color="black">Extractive Opinion Summarization in Quantized Transformer Spaces</font>
    </a>
  </h2>
  <font color="black">QTは、人気主導の要約のために再利用するベクトル量子化変分オートエンコーダーに触発されています。また、50のホテルの一般的およびアスペクト固有の要約で構成される意見要約の大規模な評価ベンチマークであるSPACEを公開しています。量子化された空間のクラスタリング解釈と新しい抽出アルゴリズムを使用して、数百のレビューの中から人気のある意見を発見します。これは、実用的な範囲の意見の要約に向けた重要なステップです。 
[ABSTRACT] qtは、人気のために再利用するオートエンコーダーに触発されています。これにより、トレーニングを行わなくても制御可能な要約が可能になります。qtは、量子化された空間のプロパティを使用して、アスペクト固有の要約を抽出します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-End Chinese Parsing Exploiting Lexicons -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_27.html">
      <font color="black">End-to-End Chinese Parsing Exploiting Lexicons</font>
    </a>
  </h2>
  <font color="black">中国語の構文解析は、従来、単語のセグメンテーション、品詞タグ付け、依存関係の構文解析モジュールを含む3つのパイプラインシステムによって解決されてきました。特に、私たちの構文解析モデルは、単語文字グラフの注意ネットワークに依存しており、外部で文字入力を充実させることができます。単語の知識..3つの中国語構文解析ベンチマークデータセットでの実験は、モデルの有効性を示し、エンドツーエンドの中国語構文解析で最先端の結果を達成しています。 
[ABSTRACT] 3つの中国語解析ベンチマークデータセットでの実験は、モデルの有効性を示しています。これらのモデルには、単語ネットワーク、品詞、依存構造の出力を学習する文字ニューロンに基づくパラチオン終了モデルが含まれています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Label Refinement Improves Dataless Text Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/cs.CL/paper_28.html">
      <font color="black">Unsupervised Label Refinement Improves Dataless Text Classification</font>
    </a>
  </h2>
  <font color="black">私たちの主なソリューションは、クラスタリングベースのアプローチです。2つの独立したエンコーダーでテキストカテゴリのペアをエンコードするものと、単一のジョイントエンコーダーでテキストカテゴリのペアをエンコードするものの、広く使用されている2つの分類器アーキテクチャのパフォーマンスを改善することで、アプローチの幅広い適用性を示します。信頼性により、データレス分類器はラベルの説明の選択に非常に敏感になり、実際のデータレス分類の幅広い適用を妨げます。 
[概要]データレステキスト分類を改善する場合、私たちのアプローチはkを使用して予測を洗練します-クラスタリングを意味します。新しいアプローチは、異なるデータセット間でデータレス分類を一貫して改善し、分類器をラベル説明の選択に対してより堅牢にします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: A Geometric Framework for Pitch Estimation on Acoustic Musical Signals -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.AS/paper_0.html">
      <font color="black">A Geometric Framework for Pitch Estimation on Acoustic Musical Signals</font>
    </a>
  </h2>
  <font color="black">現在の多くの技術は、信じられないほど効果的ですが、音響音楽信号によって示される複雑な音楽パターンを支える基礎となる数学的構造を引き出すことを目的としていません。この論文で提示されたフレームワークは、PE問題に取り組むためのまったく新しい方法を開きます。従来の分析アプローチと、現在文献を支配している新しい機械学習（ML）手法の両方で使用されている可能性があります。理論的および実験的観点の両方からアプローチに取り組み、さらなる作業の基礎となる新しいフレームワークを提示します。その分野で、そして（最先端ではないが）相対的な有効性を示す結果。 
[ABSTRACT]モノピッチ推定とマルチピッチ推定は、数学的にも概念的にも困難であることが証明されています。この方法は、この分野でのさらなる研究の基礎となる新しいフレームワークと、（最先端ではありませんが）十分に実証された結果とともに提示されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Bayesian Learning of LF-MMI Trained Time Delay Neural Networks for
  Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.AS/paper_1.html">
      <font color="black">Bayesian Learning of LF-MMI Trained Time Delay Neural Networks for
  Speech Recognition</font>
    </a>
  </h2>
  <font color="black">少数の単一パラメーターサンプルを使用する効率的な変分推論アプローチにより、トレーニング時間と評価時間の両方で、ベースラインTDNNシステムの計算コストに匹敵する計算コストが保証されます。統計的に有意なワードエラー率（WER）の絶対値0.4％〜1.8％の削減（5 ％-11％相対）は、F平滑化、L2ノルムペナルティ、自然勾配、モデル平均化、およびモデル平均化を含む複数の正規化方法を使用して、最先端の900時間速度摂動配電盤コーパストレーニングベースラインLF-MMIファクタリングTDNNシステムで取得されました。ドロップアウト、i-Vectorに加えて、隠れユニット貢献（LHUC）ベースのスピーカー適応とRNNLM再スコアリングの学習..1000時間のLibriSpeechデータトレーニングシステムを小さなDementiaBank高齢者音声コーパスに迅速に移植する必要がある3番目のクロスドメイン適応タスクでは、提案されたベイジアンTDNNLF-MMIシステムは、直接重量微調整を使用してベースラインシステムを最大2.5 \％絶対WER削減で上回りました。 
[ABSTRACT]識別ベースのtdnnバリアントシステムは、重み条件と隠れた活性化関数の選択、または隠れた層の出力に関する不確実性をモデル化するために提案されています。これらのシステムは、直接重みを微調整してベースラインシステムを上回りました。 &#39;</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Disentanglement of Speaker Representation for
  Attribute-Driven Privacy Preservation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.AS/paper_2.html">
      <font color="black">Adversarial Disentanglement of Speaker Representation for
  Attribute-Driven Privacy Preservation</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、認証コンポーネントの1つまたはいくつかの個人的な側面を隠すことができる属性駆動型プライバシー保護の概念を紹介します。VoxCelebデータセットを使用して実行された実験では、定義されたモデルが操作を可能にすることが示されています（つまり、最初の解決策として、特定の話者属性をその神経表現から解きほぐす敵対的な自動エンコード方法を定義します。
[要約]これらには、年齢、性別、民族性または年齢、年齢が含まれます。これらは、人々がより可能性が高いという事実に関連しています。これは、敵対的な自動エンコード方式を定義するための最初のステップです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Environment Sound Classification using Multiple Feature Channels and
  Attention based Deep Convolutional Neural Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.AS/paper_3.html">
      <font color="black">Environment Sound Classification using Multiple Feature Channels and
  Attention based Deep Convolutional Neural Network</font>
    </a>
  </h2>
  <font color="black">このような複数の機能は、これまで信号やオーディオの処理に使用されたことはありません。また、以前のモデルと比較して、時間と機能ドメインで別々に機能する空間的に分離可能な畳み込みで構成される、より深いCNN（DCNN）を採用しています。チャネルと空間的注意を一緒に実行します。 
[概要]複数の機能チャネルには、mel-周波数ケプストラム度（mfcc）、ガンマトーン周波数ケプストラムエントリ（gfcc）、定数q-変換（cqt）、およびクロマトグラムが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-28">
        <br><font color="black">2019-08-28</font>
      </time>
    </span>
</section>
<!-- paper0: I'm Sorry for Your Loss: Spectrally-Based Audio Distances Are Bad at
  Pitch -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-09/eess.AS/paper_4.html">
      <font color="black">I'm Sorry for Your Loss: Spectrally-Based Audio Distances Are Bad at
  Pitch</font>
    </a>
  </h2>
  <font color="black">結果は驚くべきものです。多くの人は音感がよくありません。これらの欠点は、単純なランクの仮定を使用して明らかになります。私たちのタスクは人間にとっては些細なことですが、これらの音声距離では困難であり、自己教師あり音声学習で大きな進歩を遂げることができることを示唆しています。現在の損失を改善します。 
[ABSTRACT]合成ベンチマークは、2つの静止正弦波間のピッチ距離を測定します。結果は、合成ベンチマークのデータに基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
