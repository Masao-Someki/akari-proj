<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-08-14の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Large-scale Transfer Learning for Low-resource Spoken Language
  Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.SD/paper_0.html">
      <font color="black">Large-scale Transfer Learning for Low-resource Spoken Language
  Understanding</font>
    </a>
  </h2>
  <font color="black">これは、標準のトランスフォーマアーキテクチャに依存する大量の自動音声認識アノテーション付きデータでエンコーダコンポーネントを事前トレーニングし、少量のターゲットラベル付きデータでSLUモデルを微調整することによって実装されます。FluentAIデータセットでの実験では、言語間転移学習およびマルチタスク戦略は、ベースラインと比較して、それぞれ最大で4：52％および3：89％改善されています。2番目の戦略はマルチタスク学習戦略を採用しており、SLUモデルは、基礎となる同じエンコーダを共有し、堅牢性と汎化能力を向上させます。 
[ABSTRACT] sluモデルモデルモデルモデルは、過剰適合のリスクを排除するように設計されています。sluモデルモデルには、モデルの構築に使用できるモデルがあります。sluモデルは、slu sluタスクでも使用できますが、モデルは終わり</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: MLNET: An Adaptive Multiple Receptive-field Attention Neural Network for
  Voice Activity Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.SD/paper_1.html">
      <font color="black">MLNET: An Adaptive Multiple Receptive-field Attention Neural Network for
  Voice Activity Detection</font>
    </a>
  </h2>
  <font color="black">最近、ディープニューラルネットワーク（DNN）ベースのVADは、従来の信号処理方法よりも優れたパフォーマンスを達成しました。この問題を解決するために、本稿では、MLNETと呼ばれる適応型複数受容野注意ニューラルネットワークを提案して、VADタスクを完了します。 。既存のDNNベースのモデルは、常に固定ウィンドウを手作りして、コンテキスト音声情報を利用してVADのパフォーマンスを向上させていました。 
[ABSTRACT]ディープニューラルネットワーク（dnn）-ベースのVADは、より優れたコンテキスト音声を実現しました。コンテキスト音声情報の固定ウィンドウは、予測できないさまざまなノイズ環境を処理できません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Sum-Product Networks for Robust Automatic Speaker Identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.SD/paper_2.html">
      <font color="black">Sum-Product Networks for Robust Automatic Speaker Identification</font>
    </a>
  </h2>
  <font color="black">SPNスピーカーモデルは、複数の信号対雑音比（SNR）レベルで実際の非定常および色付きノイズソースで評価されます。さらに、この作業は、SPNがロバストな自動音声認識などの関連タスクで潜在的であることを示しています。 （ASR）と自動話者検証（ASV）。音声を確実に表すスペクトル特徴の周辺確率密度関数（PDF）を使用することで、SPNが堅牢性を維持できることを示しています。 
[要約]私たちは、spnが将来の堅牢な音声処理のための有用なツールになる可能性があることを示すことを目指しています。2つの最近の畳み込みニューラルネットワークよりも堅牢であると識別されたasiシステム</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-26">
        <br><font color="black">2019-10-26</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic Speech Recognition Benchmark for Air-Traffic Communications -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.SD/paper_3.html">
      <font color="black">Automatic Speech Recognition Benchmark for Air-Traffic Communications</font>
    </a>
  </h2>
  <font color="black">このペーパーは、170時間を超えるATCo音声データでトレーニングされたいくつかの最先端のASRモデルの探索的ベンチマークを伝えています。ATCo環境のASRシステムは、英語以外のスピーカー、コックピットノイズ、話者に依存するバイアス、およびトレーニング用のドメイン内の小さなATCデータベース。開発されたASRシステムは、4つのデータベースにわたって7.75％の平均単語誤り率（WER）を達成します。 
[要旨]航空管制官は、パイロットと航空管制官（atco）との間の唯一の接触方法です。前者は最も広く使用されており、後者は海洋メッセージに必須であり、一部の国内問題では制限されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-18">
        <br><font color="black">2020-06-18</font>
      </time>
    </span>
</section>
<!-- paper0: Enhancing Speech Intelligibility in Text-To-Speech Synthesis using
  Speaking Style Conversion -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.SD/paper_4.html">
      <font color="black">Enhancing Speech Intelligibility in Text-To-Speech Synthesis using
  Speaking Style Conversion</font>
    </a>
  </h2>
  <font color="black">提案された音声システムは、（a）ロンバードのスピーキングスタイルデータと（b）スペクトルシェーピングおよびダイナミックレンジ圧縮（SSDRC）の2つの修正戦略を利用しており、時間周波数領域で信号エネルギーを再分配することにより、明瞭度が向上することが示されています。 。この拡張をLombard-SSDRC TTSシステムと呼びます。Intelligibilityin Bits（SIIB-Gauss）メジャーによって定量化された理解度の向上は、提案されたLombard-SSDRC TTSシステムが音声で110％と130％の間の大幅な相対的改善を示すことを示しています。 -シェイプドノイズ（SSN）、および最新のTTSアプローチに対する競合スピーカーノイズ（CSN）で47％〜140％。 
[ABSTRACT] lombard-ssdrc ttsが音声csnを正常に増加させました。これには、中央値のキーワード修正率でssnが455％、csnが104％含まれていました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic Quality Assessment for Audio-Visual Verification Systems. The
  LOVe submission to NIST SRE Challenge 2019 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.SD/paper_5.html">
      <font color="black">Automatic Quality Assessment for Audio-Visual Verification Systems. The
  LOVe submission to NIST SRE Challenge 2019</font>
    </a>
  </h2>
  <font color="black">最近のNIST SRE19オーディオビジュアルチャレンジデータセットでこの品質依存の融合によってもたらされた改善を示します。このために、顔と話者の両方のモダリティの自動品質評価のためにトレーニングできるユニバーサルモデルを提案します。スコアは、独立した単峰型のパーツで構成されるマルチモーダル生体認証システムの基礎です。 
[要約]このモデルは、単峰型システムによって生成された表現の品質を推定します。これらのモデルは、スコアを強化するために使用されます-話者のレベル融合-顔認証モジュール</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Prosody Learning Mechanism for Speech Synthesis System Without Text
  Length Limit -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.SD/paper_6.html">
      <font color="black">Prosody Learning Mechanism for Speech Synthesis System Without Text
  Length Limit</font>
    </a>
  </h2>
  <font color="black">この論文では、TTSシステムに基づいて音声の韻律をモデル化するための韻律学習メカニズムが提案されており、音声の韻律情報が韻律学習者によってメルスペクトラムから抽出され、音素シーケンスと組み合わせてメルスペクトラムを再構築します。一方、韻律予測結果を改善するために、事前トレーニング済みの言語モデルからのテキストのセマンティック機能が導入されています。英語と北京語での実験により、より満足な韻律を持つ音声が私たちのモデルで得られたことが示されています。 
[要約] ttsシステムに基づいて音声の韻律をモデル化するために、新しい学習形式が提案されます。音声の韻律情報は、韻律学習者によってメルスペクトラムから抽出され、音素シーケンスと組み合わせて、メル-スペクトルを再構築します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Cross attentive pooling for speaker verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.SD/paper_7.html">
      <font color="black">Cross attentive pooling for speaker verification</font>
    </a>
  </h2>
  <font color="black">実験は、VoxCelebデータセットに対して実行されます。このデータセットでは、同等のプーリング戦略を上回っています。このホワイトペーパーでは、参照クエリペア全体のコンテキスト情報を利用して、最も多く含まれる発話レベルの埋め込みを生成するクロスアテンティブプーリング（CAP）を提案します。ペアワイズマッチング問題の識別情報。このホワイトペーパーの目的は、テキストに依存しない話者検証であり、発話は「野生の」ビデオからのものであり、無関係な信号が含まれている可能性があります。 
[要約]スピーカーの埋め込みを生成する既存の方法は疑問です。テストはボクセレブデータセットのデータに基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Speaking Speed Control of End-to-End Speech Synthesis using
  Sentence-Level Conditioning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.SD/paper_8.html">
      <font color="black">Speaking Speed Control of End-to-End Speech Synthesis using
  Sentence-Level Conditioning</font>
    </a>
  </h2>
  <font color="black">この論文では、追加の入力として文レベルの発話速度値を使用して、合成音声の発話速度（速度制御可能なTTS; SCTTS）を制御する制御可能なエンドツーエンドのテキスト音声変換（TTS）システムを提案します。 、提案されたSCTTSシステムは、グローバルスタイルのトークンベースのスタイルエンコーダーを採用することにより、ピッチなどの他のスピーチ属性を維持しながら、発声速度を制御できます。さらに、高速、通常、低速のリスニングテストスピーチは、特に低速のスピーチの場合、SCTTSが文全体で同じレートで継続時間を増減する他の音素継続時間制御アプローチよりも自然なスピーチを生成できることを示しました。 
[ABSTRACT]提案されたscttsは、追加の十分にトレーニングされたモデルまたは外部音声データベースを必要としません。エンドツーエンドの方法でトレーニングできます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-30">
        <br><font color="black">2020-07-30</font>
      </time>
    </span>
</section>
<!-- paper0: Channel-wise Subband Input for Better Voice and Accompaniment Separation
  on High Resolution Music -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.SD/paper_9.html">
      <font color="black">Channel-wise Subband Input for Better Voice and Accompaniment Separation
  on High Resolution Music</font>
    </a>
  </h2>
  <font color="black">比較の目的で、異なるスケール、アーキテクチャ、CWS設定のモデルで音声と伴奏の分離（VAS）を実行します。このペーパーでは、畳み込みニューラルネットワーク（CNN）の新しい入力フォーマットであるチャネル単位のサブバンド入力（CWS）を紹介します。周波数領域の音楽ベースの分離（MSS）モデルに基づいています。パラメータが少なく、トレーニングデータが少なく、トレーニング時間が短いにもかかわらず、8バンドCWS入力を備えたMDenseNetは、元のMMDenseNetを大幅に上回っています。 
[要約]私たちはcnnベースの高解像度mssモデルの主要な問題に対処することを目指しています。提案されたアプローチは、各サブバンドでの効果的な重み共有を可能にし、チャネル間の柔軟性を高めます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-12">
        <br><font color="black">2020-08-12</font>
      </time>
    </span>
</section>
<!-- paper0: Conv-Transformer Transducer: Low Latency, Low Frame Rate, Streamable
  End-to-End Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.SD/paper_10.html">
      <font color="black">Conv-Transformer Transducer: Low Latency, Low Frame Rate, Streamable
  End-to-End Speech Recognition</font>
    </a>
  </h2>
  <font color="black">インターリーブされた畳み込み層は、パフォーマンスにとって重要な将来のコンテキストのモデリングに使用されます。アラインメントを学習するために注意メカニズムに依存し、入力オーディオを双方向にエンコードします。さらに、一定に保つために、自己注意の履歴コンテキストの長さを制限します。各デコードステップの計算コスト。 
[ABSTRACT]元のトランスフォーマーは、エンコーダー/デコーダーアーキテクチャーを備え、オフラインasrにのみ適しています。このアーキテクチャーはconv-transformerトランスデューサーと呼ばれ、外部言語モデルなしで競争力のあるパフォーマンスを実現します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Evolutionary Algorithm Enhanced Neural Architecture Search for
  Text-Independent Speaker Verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.SD/paper_11.html">
      <font color="black">Evolutionary Algorithm Enhanced Neural Architecture Search for
  Text-Independent Speaker Verification</font>
    </a>
  </h2>
  <font color="black">実験結果は、NASベースのモデルが最先端のスピーカー検証モデルよりも優れていることを示しています。最先端のスピーカー検証モデルは、ディープラーニング技術に基づいており、専門家またはエンジニア.. NASは深いネットワーク構造を自動的に学習できるため、NASの概念を有名なx-ベクトルネットワークに導入します。 
[ABSTRACT]話者確認タスクにニューラルアーキテクチャ検索（nas）のアイデアを取り入れます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Towards Modality Transferable Visual Information Representation with
  Optimal Model Compression -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.IV/paper_0.html">
      <font color="black">Towards Modality Transferable Visual Information Representation with
  Optimal Model Compression</font>
    </a>
  </h2>
  <font color="black">このように、組み込まれた新しいモダリティを最適化することで、全体的なパフォーマンスをさらに保証できます。このホワイトペーパーでは、転送可能なモダリティの哲学を活用する視覚信号表現の新しい方式を提案します。視覚信号をコンパクトに表現することは、さまざまな画像/ビデオ中心のアプリケーション。 
[要約]提案されたフレームワークは、最先端のビデオコーディング標準で実装されるように設計されています。広範な評価に基づいて、表現能力が大幅に向上しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning to Quantify Pulmonary Edema in Chest Radiographs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.IV/paper_1.html">
      <font color="black">Deep Learning to Quantify Pulmonary Edema in Chest Radiographs</font>
    </a>
  </h2>
  <font color="black">最先端の機械学習技術を適用すると、最も古く、最も広く利用されているイメージング手法の1つから、新しい定量的イメージングバイオマーカーを作成できます。胸部X線写真の肺水腫の等級付けは、よく知られた放射線所見に基づいています。目的：私たちは、肺水腫の重症度を評価し、基礎となるデータとコードの両方をリリースして、マシンビジョンの将来のアルゴリズム開発のベンチマークとして機能する臨床機械学習タスクを開発します。 
[要約]肺水腫の等級付けは、よく知られている放射線所見に基づく</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Massively Parallel Amplitude-Only Fourier Neural Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.IV/paper_2.html">
      <font color="black">Massively Parallel Amplitude-Only Fourier Neural Network</font>
    </a>
  </h2>
  <font color="black">必要な実ドメインからフーリエドメインへの変換は、ゼロ静的電力で光学レンズによって受動的に実行されます。興味深いことに、振幅のみのCNNは、位相ベースのパラダイムとは対照的に、コヒーレンスノイズに対して本質的に堅牢で、2桁以上の大きさを備えています。液晶ベースのシステムよりも遅延が少ない。ただし、ワイヤーの容量性充電などの基本的な物理学による制限に加え、データの保存と処理のシステムアーキテクチャにより、その需要は根本的な電子技術を上回っており、どちらも最近のプロセッサの傾向を促進異質性。 
[ABSTRACT]データを保存および処理するシステムアーキテクチャは、プロセッサの不均一性に向けた最近の傾向を促進</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Mask Self-Supervised Learning for Physics-Guided Neural Networks
  in Highly Accelerated MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.IV/paper_3.html">
      <font color="black">Multi-Mask Self-Supervised Learning for Physics-Guided Neural Networks
  in Highly Accelerated MRI</font>
    </a>
  </h2>
  <font color="black">脳MRIの結果は、マルチマスクSSDUがSSDUおよびCG-SENSEと比較してより良い再構成品質を達成することを示しています。結論：提案されたマルチマスクSSDUアプローチは、効率的な有効化により、完全にサンプリングされたデータなしで、物理誘導ニューラルネットワークのトレーニングの改善を可能にします。複数のマスクでアンダーサンプリングされたデータを使用します。結果：膝MRIの結果は、提案されたマルチマスクSSDUがSSDUよりも優れており、教師付きDL-MRIと緊密に動作する一方で、CG-SENSEよりも大幅に優れていることを示しています。 
[ABSTRACT]物理の自己監視学習-取得されたアンダーサンプリングされたデータを誘導再構成ネットワークが2つの互いに素なセットに分割します。1つは展開されたネットワークのデータ整合性（dc）に使用され、もう1つはトレーニング損失を定義するために使用されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: AdaIN-Switchable CycleGAN for Efficient Unsupervised Low-Dose CT
  Denoising -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.IV/paper_4.html">
      <font color="black">AdaIN-Switchable CycleGAN for Efficient Unsupervised Low-Dose CT
  Denoising</font>
    </a>
  </h2>
  <font color="black">残念ながら、cycleGANアプローチの主な制限の1つは、推論フェーズで使用されるのは1つだけですが、トレーニングフェーズで2つのディープニューラルネットワークジェネレーターが必要になることです。共有ベースラインネットワークのおかげで、追加のメモリ要件と重量の増加が最小限に抑えられ、トレーニングデータが小さい場合でもトレーニングをより安定して行うことができます。特に、ベースラインジェネレーターが低線量CT画像を変換するように、適応型インスタンス正規化（AdaIN）レイヤーを使用して単一のジェネレーターが実装されます。 AdaINコードを変更するだけで、ルーチン線量のCT画像を高線量から低線量に変換するジェネレーターに切り替えることができます。 
[要約] cycleganは、対応する高線量参照データを必要とせずに低線量ct画像品質を向上させる強力な教師なし学習方式として実証されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: CycleMorph: Cycle Consistent Unsupervised Deformable Image Registration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.IV/paper_5.html">
      <font color="black">CycleMorph: Cycle Consistent Unsupervised Deformable Image Registration</font>
    </a>
  </h2>
  <font color="black">提案された方法は非常に柔軟性があり、さまざまなアプリケーションの2Dおよび3D登録問題の両方に適用でき、大規模な登録でのメモリの問題に対処するためにマルチスケール実装に簡単に拡張できます。メソッドは、超高速な計算時間にもかかわらず、その優れたパフォーマンスのために広範囲にわたって調査されてきました。変形場の定性的および定量的評価は、提案された方法のサイクル一貫性の有効性も検証します。 
[要約]提案された方法は非常に柔軟性が高いため、2Dと3Dの両方の登録問題に適用できます。大規模な登録でのメモリの問題に対処するために、マルチスケール実装に簡単に拡張できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Efficient Adaptation of Neural Network Filter for Video Compression -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.IV/paper_6.html">
      <font color="black">Efficient Adaptation of Neural Network Filter for Video Compression</font>
    </a>
  </h2>
  <font color="black">ビデオコーディングパイプラインの後処理アーティファクト削除ステップとして適用されるニューラルネットワークフィルターの効率的な微調整方法論を提示します。 7つのテストシーケンスでのArt Versatile Video Coding（VVC）標準コーデック。提案された方法は、従来の微調整アプローチよりもはるかに速く収束を実現し、実用的なアプリケーションに適しています。 
[要約]提案された方法は、微調整のビットを作成するために使用できます。微調整の代替として使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-28">
        <br><font color="black">2020-07-28</font>
      </time>
    </span>
</section>
<!-- paper0: Improving distribution and flexible quantization for DCT coefficients -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.IV/paper_7.html">
      <font color="black">Improving distribution and flexible quantization for DCT coefficients</font>
    </a>
  </h2>
  <font color="black">フーリエ関連変換のAC係数（JPEG画像圧縮のDCT-IIなど）はラプラス分布からのものであることが一般的な知識ですが、より一般的なEPD（指数べき分布）$ \ rho \ sim \ exp（-| x | ^ {\ kappa}）$ファミリ、最尤推定（MLE）につながる$ \ kappa \約0.5 $ラプラス分布の代わりに$ \ kappa = 1 $-このような置換により、平均で$ \約0.1 $ビット/値の節約（グレースケールの場合はピクセルあたり、RGBの場合は$ 3 \ times $まで）。歪みのみに対して$ q $を最適化すると、大幅に改善されますが、より均一な分布のためにエントロピーが増加します。特に、このような連続分布の場合、最適化された連続\ emph {量子化密度関数} $ q $による量子化アプローチについても説明します。これは、正則格子上の逆CDF（累積分布関数）$ Q $ $ \ {Q ^ {-1}（（i-1 / 2） / N）：i = 1 \ ldots N \} $は量子化ノードを提供します-最適化された（不均一な）quaの柔軟で安価な選択を可能にしますntization-レート-ディストーションコントロールを備えた、さまざまなサイズ$ N $の。 
[ABSTRACT] 110万ドルはより広範な分布にリンクしている可能性があると推定されています。また、すでにデコードされたACファームの幅の予測についても説明します。これは、「通常の」data.plusに費やされた金額に基づいています。 、これがどれだけ可能かを予測することが可能です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Bone Segmentation in Contrast Enhanced Whole-Body Computed Tomography -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.IV/paper_8.html">
      <font color="black">Bone Segmentation in Contrast Enhanced Whole-Body Computed Tomography</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、トレーニングデータのウィンドウ処理とシグモイド活性化しきい値の選択の変更に基づいて、新しい前処理技術を備えたUネットアーキテクチャの概要を説明し、低線量造影の全身CTスキャンから骨髄領域を正常にセグメント化します。メソッドは、2つの内部データセットと1つの外部テストデータセットでそれぞれ平均ダイス係数0.979、0.965、および0.934を達成しました。適切な前処理が骨と造影剤を区別するために重要であり、限られたデータで優れた結果を達成できることを実証しました。 
[要約]提案された方法は、2つの内部データセットと1つの外部テストデータセットで0. 979、0. 965、および0. 934の平均ダイスエントリを達成しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-12">
        <br><font color="black">2020-08-12</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Modality Pathology Segmentation Framework: Application to Cardiac
  Magnetic Resonance Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.IV/paper_9.html">
      <font color="black">Multi-Modality Pathology Segmentation Framework: Application to Cardiac
  Magnetic Resonance Images</font>
    </a>
  </h2>
  <font color="black">さらに、ノイズ除去オートエンコーダー（DAE）をASSNに統合して、もっともらしい形状のセグメンテーション結果を生成します。具体的には、ASSNは、病変が存在する可能性のある解剖学的構造をセグメント化することを目的としています。病理学的領域セグメンテーション..主に2つのニューラルネットワークで構成されます：解剖学的構造セグメンテーションネットワーク（ASSN）と病理学的領域セグメンテーションネットワーク（PRSN）。 
[要約] myops20チャレンジデータセットの研究者は、私たちのフレームワークが心筋の瘢痕と浮腫のセグメンテーションに対して有望なパフォーマンスを達成できることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: MINI-Net: Multiple Instance Ranking Network for Video Highlight
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.IV/paper_10.html">
      <font color="black">MINI-Net: Multiple Instance Ranking Network for Video Highlight
  Detection</font>
    </a>
  </h2>
  <font color="black">特に、max-maxランキング損失を形成して、最も可能性の高いポジティブセグメントインスタンスと最も難しいネガティブセグメントインスタンスとの間の信頼性の高い相対比較を取得します。この作業では、特定の特定のイベントに対して弱監視ビデオハイライト検出モデリングをキャストすることを提案します。複数インスタンスランキングネットワーク（MINI-Net）学習として。3つの挑戦的な公開ベンチマークに関する広範な実験結果により、問題を解決するための複数インスタンスランキングアプローチの有効性が明確に検証されます。 
[ABSTRACT]日常生活の動画には、スキーやサーフィンなど、複数のイベントタイプのハイライトセグメントが含まれている可能性があります。各動画はセグメントのバッグと見なされるため、ミニネットは、特定のイベントのハイライトセグメントを含むポジティブバッグ。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-20">
        <br><font color="black">2020-07-20</font>
      </time>
    </span>
</section>
<!-- paper0: Localizing the Common Action Among a Few Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.IV/paper_11.html">
      <font color="black">Localizing the Common Action Among a Few Videos</font>
    </a>
  </h2>
  <font color="black">トリミングされていない長いビデオのアクションの開始と終了は、共通のクラスラベルを知らなくても、同じアクションを含むトリミングされたビデオのサンプルのほんの一部に基づいて決定されます。ネットワークには以下が含まれます：（\ textit {i}）aいくつかのトリミングされたサポートビデオとトリミングされていないクエリビデオの表現を同時に補完する相互強化モジュール。 （\ textit {ii}）サポートビデオをクエリブランチに繰り返し融合するプログレッシブアラインメントモジュール。 （\ textit {iii}）ペアワイズマッチングモジュールを使用して、さまざまなサポートビデオの重要性を比較検討します。このペーパーでは、トリミングされていない長いビデオのアクションの時間的な範囲を特定するよう努めています。 
[ABSTRACT]新しい3Dたたみ込みネットワークアーキテクチャは、サポート動画の表現を検索動画セグメントに合わせるように設計されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: LGNN: a Context-aware Line Segment Detector -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.IV/paper_12.html">
      <font color="black">LGNN: a Context-aware Line Segment Detector</font>
    </a>
  </h2>
  <font color="black">最先端技術と比較して、LGNNは精度を損なうことなくほぼリアルタイムのパフォーマンスを実現します。3D点群にアクセスできる場合、環境の3Dワイヤーフレームをロバストに抽出するためのマルチモーダルラインセグメント分類手法を紹介します。具体的には、LGNNは各ラインセグメントの新しい四重表現を活用し、GNNモジュールは予測された候補を頂点として受け取り、スパースグラフを作成して構造コンテキストを適用します。 
[ABSTRACT]これは、連携する新しいネットワークの最新の例です。これらには、各ラインの新しい四つ組表現が含まれます。lgnnは、時間追跡とも呼ばれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Revisiting Temporal Modeling for Video Super-resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.IV/paper_13.html">
      <font color="black">Revisiting Temporal Modeling for Video Super-resolution</font>
    </a>
  </h2>
  <font color="black">広範な実験により、提案されたRRNは非常に計算効率が高く、他の時間モデリング方法よりも詳細な時間整合性のあるVSR結果を生成することが示されています。この作業では、3つの時間モデリング方法（2D CNNと早期融合、3D CNNビデオ超解像のための遅い融合とリカレントニューラルネットワーク）。さらに、提案された方法は、いくつかの広く使用されているベンチマークで最先端の結果を達成します。 
[要約]提案された方法は、広く使用されているいくつかのベンチマークで状態および現在の残余ネットワーク結果を達成しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Powers of layers for image-to-image translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.IV/paper_14.html">
      <font color="black">Powers of layers for image-to-image translation</font>
    </a>
  </h2>
  <font color="black">テスト時には、いくつかの利点があります。重みパラメータの数が制限されており、構成の設計により、変換の強さを反復回数で変調できます。これは、たとえば、ノイズのタイプまたは量が抑制する方法は事前にわかりません。反復の累乗効果を軽減するには、特定のトレーニングスケジュールが必要です。 
[ABSTRACT]重みを固定した画像オートエンコーダアーキテクチャから始めます。モデルのパフォーマンスは、cycleganと同等かそれ以上です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Neural collaborative filtering for unsupervised mitral valve
  segmentation in echocardiography -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.IV/paper_15.html">
      <font color="black">Neural collaborative filtering for unsupervised mitral valve
  segmentation in echocardiography</font>
    </a>
  </h2>
  <font color="black">ニューラルネットワークの協調フィルタリングを使用した心エコー検査ビデオの低次元埋め込みに基づく僧帽弁セグメンテーションのための自動化された監視されていない方法を提案します。これは、最先端の\ emph {監視されていない}および\ emph {監視されている}メソッドより優れています低品質のビデオまたはスパースアノテーションの場合..僧帽弁輪とリーフレットのセグメンテーションは、複数のタスクを実行する医師をサポートできる機械学習パイプラインを確立するための重要な最初のステップを指定します。たとえば、僧帽弁疾患の診断、手術計画、および術中処置。 
[要約]この方法は、さまざまな僧帽弁疾患の患者のエコーオペラ動画のコレクションで評価されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Predicting Visual Overlap of Images Through Interpretable Non-Metric Box
  Embeddings -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_0.html">
      <font color="black">Predicting Visual Overlap of Images Through Interpretable Non-Metric Box
  Embeddings</font>
    </a>
  </h2>
  <font color="black">その後、局所特徴はそのスケールでのみ検出される必要があります。私たちのアプローチは2つの画像間の非対称関係を測定します。幾何学的検証の必要性を取り除きませんが、スケール空間での検索を削減する解釈可能な画像埋め込みを提案します本質的にルックアップに。 
[ABSTRACT]これが既知のシーンである場合、答えは通常、スケールスペース全体の高額な検索を必要とし、ローカルフィーチャの大規模なセットのマッチングと幾何学的検証を行います。その結果、たとえば、どのテスト画像がクローズ-別のバージョンのアップ-どのスケールファクターか</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: What leads to generalization of object proposals? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_1.html">
      <font color="black">What leads to generalization of object proposals?</font>
    </a>
  </h2>
  <font color="black">Open Images V4データセットでは、このような典型的なセットを形成するために選択できるのはクラスの25％のみであることを示しています。データセットのプロパティ-視覚的な多様性とラベルスペースの粒度-優れた一般化に必要なものを体系的に調査します。典型的なクラスのアイデアを導入します。より効率的な方法で一般化された提案を取得するために検出モデルをトレーニングするために必要な十分で必要なクラスのセット。 
[ABSTRACT]優れた提案モデルは、目に見えないクラスに一般化する提案を提供できます。たとえば、クラスの25％だけがそのようなプロトタイプの数を形成できます。このモデルを使用して、より透明にすることができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Rethinking Curriculum Learning with Incremental Labels and Adaptive
  Compensation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_2.html">
      <font color="black">Rethinking Curriculum Learning with Incremental Labels and Adaptive
  Compensation</font>
    </a>
  </h2>
  <font color="black">トレーニングプロセス全体を通して、すべてのラベルがモデルに認識されるまで、目に見えないグラウンドトゥルースラベルを一定の増分で再帰的に明らかにします。さらに、モデルへの新しいラベルの導入のペースを調整することの重要性と、スムーズなターゲットベクトル。最初の段階であるインクリメンタルラベルの概要では、データを相互に排他的なサブセットに分割します。1つはグラウンドトゥルースラベルのサブセットを含み、もう1つは疑似ラベルに添付された残りのデータを含みます。 
[ABSTRACT]これには、インクリメンタルラベルと適応型補正（ライラック）を使用した学習が含まれます。このメソッドは、トレーニング全体を通じてデータセット全体を一貫して使用しながら、サンプルの難易度ではなく、一意の出力ラベルの数をインクリメンタルに増やします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-13">
        <br><font color="black">2020-01-13</font>
      </time>
    </span>
</section>
<!-- paper0: Mish: A Self Regularized Non-Monotonic Activation Function -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_3.html">
      <font color="black">Mish: A Self Regularized Non-Monotonic Activation Function</font>
    </a>
  </h2>
  <font color="black">さらに、Mishの数学的な定式化を関数のSwishファミリーとの関連で検討し、一次微分動作がディープラーニューラルネットワークの最適化に役立つ正則化因子としてどのように機能しているかについての直感的な理解を提案します。複数のアーキテクチャにわたるImageNet-1kやMS-COCOなどのベンチマークに好影響を与えます。たとえば、MishはYOLOv4でLeaky ReLUよりも平均精度が高い（$ AP_ {50} ^ {val} $でCSP-DarkNet-53バックボーンを使用した）MS-COCOオブジェクト検出で2.1 $ \％$、ImageNet-1kでResNet-50でReLUを実行すると、他のすべてのネットワークパラメーターとハイパーパラメーターを一定に保ちながら、トップ1の精度で$ \約$ 1 $ \％$になります。 
[ABSTRACT]アーキテクチャとアクティベーション関数の最適な組み合わせに対するいくつかのよく知られたベンチマークで実験的に検証しました。これらには、アーキテクチャ、アクティベーション関数、およびアーキテクチャまたはアクティベーション関数の最適な組み合わせの有用な例のリストが含まれています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-23">
        <br><font color="black">2019-08-23</font>
      </time>
    </span>
</section>
<!-- paper0: The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution
  Generalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_4.html">
      <font color="black">The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution
  Generalization</font>
    </a>
  </h2>
  <font color="black">したがって、評価された方法で一貫してロバストネスが改善されることはありません。これを動機として、最新のデータを拡張し、1000倍以上のラベル付きデータで事前トレーニングされたモデルよりも優れた新しいデータ拡張方法を導入します。ベンチマークを使用して、以前の配布外の堅牢性に関する仮説を提案し、それらをテストに投入しました。 
[ABSTRACT]以前に提案された低血圧の在庫を取り、それらをテストに投入します。また、新しいデータ拡張方法を導入します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-29">
        <br><font color="black">2020-06-29</font>
      </time>
    </span>
</section>
<!-- paper0: What Should Not Be Contrastive in Contrastive Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_5.html">
      <font color="black">What Should Not Be Contrastive in Contrastive Learning</font>
    </a>
  </h2>
  <font color="black">さらに、不変空間と変動空間の連結は、粗視化、細粒化、少数ショットのダウンストリーム分類タスク、さまざまなデータ破損など、調査したすべてのタスクで最適に機能することがわかりました。マルチヘッドネットワークを使用しています。各拡張全体で情報をキャプチャし、単独でダウンストリームタスクのすべてのベースラインを上回る共有バックボーンを使用します。このモデルは、それぞれが1つの拡張を除いてすべて不変である個別の埋め込みスペースを構築することにより、視覚的表現のさまざまな不変要素をキャプチャすることを学習します。 
[ABSTRACT]私たちのモデルは、視覚的表現のさまざまな応答要素をキャプチャすることを学習します。これらには、1つの拡張を除いてすべて異なる埋め込みスペースが含まれます。ネットワークだけで、調査するすべてのタスクで最高のパフォーマンスを発揮します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: BioMetricNet: deep unconstrained face verification through learning of
  metrics regularized onto Gaussian distributions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_6.html">
      <font color="black">BioMetricNet: deep unconstrained face verification through learning of
  metrics regularized onto Gaussian distributions</font>
    </a>
  </h2>
  <font color="black">この選択により、調整可能な単純な線形決定境界を使用できるようになり、誤警報と真の受け入れ率の間の望ましいトレードオフを達成でき、閉形式で記述できる損失関数につながります。このホワイトペーパーでは、この一般的なフレームワークは、顔認証用にこの種の最初のものであり、ガウス分布に合わせて調整します。野生のラベル付き顔（LFW）、YouTube顔（YTF）、正面顔の有名人などの公開されているデータセットに対する広範な分析と実験ワイルド（CFP）、およびクロスエイジLFW（CALFW）、クロスポーズLFW（CPLFW）、インザワイルドエイジデータセット（AgeDB）などの挑戦的なデータセットは、大幅なパフォーマンスの向上を示し、BioMetricNetの有効性と優位性を確認します既存の最先端のメソッドを超えて。 
[要約]提案されたアプローチは、顔の特徴に特定のメトリックを課しません。代わりに、一致するペアと一致しないペアが明確に分離され、適切に動作するターゲット分布にマッピングされる潜在表現を学習することで、決定空間を形成します。顔認証用にその種の最高のものを提示し、顔認識に合わせて調整した論文</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: DFEW: A Large-Scale Database for Recognizing Dynamic Facial Expressions
  in the Wild -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_7.html">
      <font color="black">DFEW: A Large-Scale Database for Recognizing Dynamic Facial Expressions
  in the Wild</font>
    </a>
  </h2>
  <font color="black">第二に、野生の動的FERを処理するために、式クラスタ化時空間特徴学習（EC-STFL）フレームワークと呼ばれる新しい方法を提案します。変更点。3番目に、提案されたEC-STFLだけでなく、多くの時空間ディープフィーチャー学習方法を使用して、DFEWで広範なベンチマーク実験を行います。 
[要旨]ビデオクリップには、実際のシナリオでさまざまな困難な干渉が含まれています。これらは、多くの時空間ディープフィーチャー学習方法の使用を提案しています。結果は、dfewデータベースに表示されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Look, Listen, and Attend: Co-Attention Network for Self-Supervised
  Audio-Visual Representation Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_8.html">
      <font color="black">Look, Listen, and Attend: Co-Attention Network for Self-Supervised
  Audio-Visual Representation Learning</font>
    </a>
  </h2>
  <font color="black">実験により、既存の方法と比較してパラメーターが少ない一方で、私たちのモデルが口実タスクで最先端のパフォーマンスを実現することが示されています。それらの間の相互作用..私たちのアプローチの一般化可能性と転送可能性をさらに評価するために、2つの下流タスク、つまり音源定位とアクション認識に事前トレーニング済みモデルを適用します。 
[要約]音声イベントと視覚イベントの間に根本的な相関関係があります。これらは無料の監視あり情報として使用でき、音声の口実タスクを解くことによってニューラルネットワークをトレーニングできます-視覚的同期</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Attention-based Fully Gated Conventional Recurrent Neural Network for
  Russian Handwritten Text -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_9.html">
      <font color="black">Attention-based Fully Gated Conventional Recurrent Neural Network for
  Russian Handwritten Text</font>
    </a>
  </h2>
  <font color="black">また、Tahnからの複数の出力機能と入力機能を利用して完全にゲート化されたレイヤーを提案します。この提案された作業により、より良い結果が得られ、手書きカザフ語とロシア語データベース（HKR）でモデルを実験しました。最初にHKRデータセットで作業し、他のほとんどの既存のモデルに対して最先端の結果を示します。この研究は、カザフ語とロシア語でトレーニングされた注意エンコーダーデコーダーネットワークで手書きテキストのタスクに取り組みます。 
[要旨]私たちは、完全にゲートされたcnnに基づいた新しいディープニューラルネットワークモデルを開発します。これは、複数の双方向GRUと機能を操作する注意メカニズムによってサポートされています。当社の研究は、HKRデータセットの最初の作業です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-12">
        <br><font color="black">2020-08-12</font>
      </time>
    </span>
</section>
<!-- paper0: NAS-Count: Counting-by-Density with Neural Architecture Search -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_10.html">
      <font color="black">NAS-Count: Counting-by-Density with Neural Architecture Search</font>
    </a>
  </h2>
  <font color="black">AMSNetのエンコーダーとデコーダーはマイクロレベルの検索から発見されたさまざまなセルで構成され、マルチパスアーキテクチャはマクロレベルの検索を通じて調査されます。MSE損失のピクセルレベルの分離問題を解決するために、AMSNetはマルチスケール構造情報を監視する自動検索スケールピラミッドプーリング損失（SPPLoss）。具体的には、カウント固有の2レベルの検索スペースを利用します。 
[ABSTRACT]この作業では、ニューラルアーキテクチャ検索（nas）を使用してカウントモデルの設計を自動化します。amsnetは、エンドツーデコーダアーキテクチャ、自動マルチスケールネットワーク（amsnet）を開発しました。amsnetのエンコーダとデコーダは構成されていますマイクロレベルの検索から発見されたさまざまなセルの集合、マルチパスアーキテクチャはマイクロレベルの検索を通じて探索されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-29">
        <br><font color="black">2020-02-29</font>
      </time>
    </span>
</section>
<!-- paper0: Deep k-NN Defense against Clean-label Data Poisoning Attacks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_11.html">
      <font color="black">Deep k-NN Defense against Clean-label Data Poisoning Attacks</font>
    </a>
  </h2>
  <font color="black">提案された戦略は、両方の攻撃で99％以上の有毒な例を検出し、モデルのパフォーマンスを損なうことなくそれらを削除できることを示しています。さらに、アブレーション研究を通じて、kの値の選択と実装の簡単なガイドラインを発見しました。クラス不均衡のある現実世界のデータセットに対するディープk-NN防御。この作業では、CIFAR-10データセットに対する機能の衝突と凸状ポリトープのクリーンラベル攻撃の両方に対するシンプルでありながら非常に効果的なディープk-NN防御を提案します。 。 
[要約]提案された防御策は、現在のクリーンラベル中毒攻撃戦略を無効にできることを示しています。ただし、攻撃の効果と現実的なアプリケーションにもかかわらず、信頼できる防御策は示されていません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-29">
        <br><font color="black">2019-09-29</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Modality Transferable Visual Information Representation with
  Optimal Model Compression -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_12.html">
      <font color="black">Towards Modality Transferable Visual Information Representation with
  Optimal Model Compression</font>
    </a>
  </h2>
  <font color="black">提案されたフレームワークは、最先端のビデオコーディング標準（つまり、多用途のビデオコーディング）に実装されており、広範な評価に基づいて大幅に優れた表現機能が確認されています。オンライントレーニングで入力シーンの統計を吸収し、レートユーティリティの最適化という意味で効率的に表すことができ、ビットストリームのエンハンスメントレイヤーとして機能します。したがって、組み込まれた新しいモダリティを最適化することにより、全体的なパフォーマンスをさらに保証できます。 。 
[要約]提案されたフレームワークは、最先端のビデオコーディング標準で実装されるように設計されています。広範な評価に基づいて、表現能力が大幅に向上しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Sparse Coding Driven Deep Decision Tree Ensembles for Nuclear
  Segmentation in Digital Pathology Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_13.html">
      <font color="black">Sparse Coding Driven Deep Decision Tree Ensembles for Nuclear
  Segmentation in Digital Pathology Images</font>
    </a>
  </h2>
  <font color="black">このようにして、一連の決定木集団を学習することにより、より豊富な画像外観モデルとより多くのコンテキスト情報が統合されます。ディープニューラルネットワークと比較して、提案されたScD2TEは逆伝播計算を必要とせず、ハイパーパラメータの依存度が低くなります。前のすべてのレイヤーの外観と高レベルのコンテキスト機能は、それらを入力としてフィードフォワードに連結することによってシームレスに結合されます。これにより、後続のレイヤーの出力がより正確になり、モデル全体のトレーニングが効率的になります。 
[要約]表現学習の新しい方法を高速に連結します。高密度コーディング駆動型深層学習と呼ばれるこの方法は、表現学習に新しい視点を提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Visual Localization for Autonomous Driving: Mapping the Accurate
  Location in the City Maze -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_14.html">
      <font color="black">Visual Localization for Autonomous Driving: Mapping the Accurate
  Location in the City Maze</font>
    </a>
  </h2>
  <font color="black">広範なフィールドテスト結果は、私たちのアプローチが挑戦的な都心の設定でも位置をロバストに予測できることを示しています。ロケーション予測のロバスト性を向上させます。都市設定のローカリゼーションの課題に対処するために、視覚的なローカリゼーションのための新しい機能投票手法を提案します。 
[ABSTRACT]ミシガン大学の研究者は、視覚的位置確認のための新しい機能投票手法を提案しています。この方法は、最先端の視覚的視覚視覚ネットワークでテストする必要があります。自律型車両が正確な位置情報を見つけるのに役立ちます。望ましい時間制限内の都市の迷路</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Black Magic in Deep Learning: How Human Skill Impacts Network Training -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_15.html">
      <font color="black">Black Magic in Deep Learning: How Human Skill Impacts Network Training</font>
    </a>
  </h2>
  <font color="black">結果は、参加者の経験と最終的なパフォーマンスの間に強い正の相関があることを示しています。彼らのタスクは、特定のディープラーニングアーキテクチャのハイパーパラメータ最適化を実行することです。さらに、経験豊富な参加者は、平均して少ないリソースを使用してより良いソリューションを見つけることを示しています。 
[要約]異なるレベルの経験を持つ31人の参加者に基づく初期調査。結果は、参加者の経験と最終的なパフォーマンスの間に強い正の相関があることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning to Quantify Pulmonary Edema in Chest Radiographs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_16.html">
      <font color="black">Deep Learning to Quantify Pulmonary Edema in Chest Radiographs</font>
    </a>
  </h2>
  <font color="black">結果：半教師ありモデルと事前トレーニング済みモデルから、受信者動作特性曲線（AUROC）の下の面積を測定しました。最新の機械学習手法を適用すると、1つから新しい定量的イメージングバイオマーカーを生成できます。背景：急性代償不全のCHF患者の臨床管理の決定は、単なる欠如や存在ではなく、肺水腫の重症度に基づいて行われることがよくあります。 
[要約]肺水腫の等級付けは、よく知られている放射線所見に基づく</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: ExplAIn: Explanatory Artificial Intelligence for Diabetic Retinopathy
  Diagnosis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_17.html">
      <font color="black">ExplAIn: Explanatory Artificial Intelligence for Diabetic Retinopathy
  Diagnosis</font>
    </a>
  </h2>
  <font color="black">この説明フレームワークの目新しさは、ブラックボックスAIアルゴリズムと同じように、画像監視のみでエンドツーエンドでトレーニングされることです。病変と病変カテゴリの概念はそれ自体で出現します。この新しいフレームワークは、 AIの展開を容易にするための高い分類パフォーマンスと説明可能性。病変の位置特定を改善するために、前景/背景の分離は、前景のピクセルを遮蔽することで入力画像を健康的な画像に変換するように、自己監視によってトレーニングされます。 
[ABSTRACT]「ブラックボックス」はai aiの作品ですが、「ブラックボックス」の性質はまだ保留されています。「説明」と呼ばれるシステムは、画像内の病変をセグメント化して分類することを学習します。病変の位置特定の改善、前景ピクセル入力画像を健康に見える画像に変換する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Full-Body Awareness from Partial Observations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_18.html">
      <font color="black">Full-Body Awareness from Partial Observations</font>
    </a>
  </h2>
  <font color="black">人間の3Dメッシュ復元は大きく進歩し、消費者のビデオデータから世界について学ぶことに大きな関心が寄せられています。この問題を調査し、それに対処するために多くの貢献をします。（i）シンプルだが非常に効果的なセルフ人間の3Dメッシュ復元システムを消費者のビデオに適合させ、最近の2つのシステムへの適用を実証するトレーニングフレームワーク。 （ii）私たちは、画像外キーポイントの評価を含む、このタスクを研究するための4つの消費者ビデオデータセット全体の13Kフレームの評価プロトコルとキーポイントアノテーションを導入します。 （iii）私たちの方法は、ベースラインと比較して、トレーニングされたデータセットからのテストビデオと、さらに適応することなく他の3つのデータセットの両方で、PCKと人間の被験者の判断を大幅に改善することを示しています。プロジェクトのウェブサイト：https： //crockwell.github.io/partial_humans 
[ABSTRACT]ヒューマンメッシュリカバリテクニックは、消費者のビデオデータではうまく機能しません。これは、多くの異常な写真と積極的な切り捨てが原因です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: An Efficient Data Retrieval Parallel Reeb Graph Algorithm -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_19.html">
      <font color="black">An Efficient Data Retrieval Parallel Reeb Graph Algorithm</font>
    </a>
  </h2>
  <font color="black">過去10年間で大きなデータセットを計算する要求が高まり、トポロジ計算の並列化が検討されています。つまり、リーブグラフの並列アルゴリズムに加えて、リーブから元の多様体データを抽出する方法についても説明します。グラフ構造..我々は、境界のある場合とない場合の三角メッシュで並列拡張リーブグラフアルゴリズムを提案します。 
[ABSTRACT]リーブグラフは、幾何学的処理、画像処理、コンピュータグラフィックス、およびアルゴリズムで非常に重要であることが示されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-10-18">
        <br><font color="black">2018-10-18</font>
      </time>
    </span>
</section>
<!-- paper0: DF-GAN: Deep Fusion Generative Adversarial Networks for Text-to-Image
  Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_20.html">
      <font color="black">DF-GAN: Deep Fusion Generative Adversarial Networks for Text-to-Image
  Synthesis</font>
    </a>
  </h2>
  <font color="black">2）セマンティックの一貫性のために、既存のモデルは追加のネットワークを使用してセマンティックの一貫性を保証し、トレーニングの複雑さを増やし、追加の計算コストをもたらします。テキストの説明から高解像度のリアルな画像を合成することは困難な作業です。既存のテキストから画像モデルには3つの問題があります。1）バックボーンには、異なるスケールの画像を生成するために複数のジェネレーターとディスクリミネーターが積み重ねられており、トレーニングプロセスが遅く、非効率です。 
[ABSTRACT]ほぼすべての新しいテキスト-画像に基づく階層化された敵対的ネットワークをバックボーンとして使用します。これらには、クロスモーダル注意メカニズムを使用して、テキストと画像の機能を融合します。トレーニングの複雑さと追加コストをもたらす</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Shift Equivariance in Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_21.html">
      <font color="black">Shift Equivariance in Object Detection</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、これらの方法のいずれも完全なシフト等分散を提供できないことを示しています。これがオブジェクトの検出にどの程度影響を与える可能性があるかは、主に2つのアーキテクチャの違いと最新の検出器の予測空間の次元が原因です。 
[ABSTRACT]たとえば、cnnベースの分類はシフトフリーではありません。しかし、最近の研究では、cnnベースのオブジェクト検出検出はシフトフリーではないことが示されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Understanding Dynamic Scenes using Graph Convolution Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_22.html">
      <font color="black">Understanding Dynamic Scenes using Graph Convolution Networks</font>
    </a>
  </h2>
  <font color="black">この提案された明示的なエンコーディングと中間時空間相互作用グラフの使用法は、時間的に順序付けられた空間関係のセットでエンドエンドを直接学習するタスクに適していることを示します。新しい多関係グラフ畳み込みネットワークを提示します。 （MRGCN）移動単眼カメラによって取得された一連の時間的に順序付けられたフレームから路上車両の動作をモデル化するフレームワーク。シーンで条件付けされたMRGCNの注意メカニズムを提案し、異なる相互作用からの情報の重要性を動的にスコアリングタイプ。 
[ABSTRACT]車両フレームに頼ることなく、学習を複数のデータセットにシームレスに転送することも示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-09">
        <br><font color="black">2020-05-09</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Knowledge Transfer from Unlabeled Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_23.html">
      <font color="black">Adversarial Knowledge Transfer from Unlabeled Data</font>
    </a>
  </h2>
  <font color="black">私たちの方法の重要な新しい側面は、ラベルなしソースデータがラベル付きターゲットデータのクラスとは異なるクラスである可能性があり、一部の既存のアプローチとは異なり、個別の口実タスクを定義する必要がないことです。このホワイトペーパーでは、インターネットスケールのラベルなしデータから知識を転送して、特定の視覚認識タスクでの分類器のパフォーマンスを向上させるための新しい敵対知識転送（AKT）フレームワーク。視覚認識への機械学習アプローチは大きな期待を提供しますが、既存の方法のほとんどは、大量のラベル付きトレーニングデータの可用性に大きく依存します。 
[ABSTRACT]提案されたラベルなし学習フレームワークは、ラベルなしソースデータの特徴空間をラベル付きターゲットデータと組み合わせます。ターゲット分類子を使用して、ソースデータの疑似ラベルを予測できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Contextual Diversity for Active Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_24.html">
      <font color="black">Contextual Diversity for Active Learning</font>
    </a>
  </h2>
  <font color="black">グラウンドトゥルースラベルがない場合、コンテキストを評価することは難しいため、空間的に共起するクラスに関連する混乱を捉えるコンテキストの多様性の概念を導入します。大きな注釈付きデータセットの要件は、深い畳み込みニューラルネットワークの使用を制限します（ CNN）は、多くの実用的なアプリケーションに使用されます。コンテキストの多様性（CD）は、CNNによって関心領域に対して予測された確率ベクトルには、通常、より大きな受容野からの情報が含まれているという重要な観察に依存しています。 
[ABSTRACT]最新のcnnアーキテクチャは、空間コンテキストを多用して非常に正確な予測を実現します。調査に加えて、アクティブラーニング（al）手法を使用して、微調整時に最大の精度が得られるデータのサブセットを選択します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Mask Self-Supervised Learning for Physics-Guided Neural Networks
  in Highly Accelerated MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_25.html">
      <font color="black">Multi-Mask Self-Supervised Learning for Physics-Guided Neural Networks
  in Highly Accelerated MRI</font>
    </a>
  </h2>
  <font color="black">結論：提案されたマルチマスクSSDUアプローチは、複数のマスクでアンダーサンプリングされたデータを効率的に使用できるようにすることで、完全にサンプリングされたデータなしで物理誘導ニューラルネットワークのトレーニングを改善できるようにします。脳MRIの結果は、マルチマスクSSDUがより良い再構成を達成することを示しています。 SSDUおよびCG-SENSEと比較した品質。方法：現在、物理ガイド付き再構築ネットワークの自己教師あり学習は、取得したアンダーサンプリングデータを2つの互いに素なセットに分割し、1つは展開されたネットワークのデータ整合性（DC）に使用され、もう1つはトレーニング損失を定義します。 
[ABSTRACT]物理の自己監視学習-取得されたアンダーサンプリングされたデータを誘導再構成ネットワークが2つの互いに素なセットに分割します。1つは展開されたネットワークのデータ整合性（dc）に使用され、もう1つはトレーニング損失を定義するために使用されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: AdaIN-Switchable CycleGAN for Efficient Unsupervised Low-Dose CT
  Denoising -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_26.html">
      <font color="black">AdaIN-Switchable CycleGAN for Efficient Unsupervised Low-Dose CT
  Denoising</font>
    </a>
  </h2>
  <font color="black">共有ベースラインネットワークのおかげで、追加のメモリ要件と重みの増加が最小限に抑えられ、トレーニングデータが小さい場合でもトレーニングをより安定して行うことができます。特に、単一のジェネレーターは、アダプティブインスタンス正規化（AdaIN）レイヤーを使用して実装されるため、低線量CT画像を通常線量CT画像に変換するベースラインジェネレーターは、AdaINコードを変更するだけで高線量から低線量に変換するジェネレーターに切り替えることができます。実験結果は、提案された方法が以前のサイクルGANよりも優れていることを示しています。パラメータの半分だけを使用しながらアプローチします。 
[要約] cycleganは、対応する高線量参照データを必要とせずに低線量ct画像品質を向上させる強力な教師なし学習方式として実証されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: SkeletonNet: A Topology-Preserving Solution for Learning Mesh
  Reconstruction of Object Surfaces from RGB Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_27.html">
      <font color="black">SkeletonNet: A Topology-Preserving Solution for Learning Mesh
  Reconstruction of Object Surfaces from RGB Images</font>
    </a>
  </h2>
  <font color="black">提案されたSkeletonNetの有効性を検証する徹底的な実験を行います。技術的には、骨格ポイントセットのブリッジドラーニングを介して骨格のボリューム表現を学習するNovellSkeletonNetdesignを提案します。 1D骨格曲線と2D骨格シート、および洗練された高解像度の骨格ボリュームのためのグローバルにガイドされたサブボリューム合成の効率的なモジュール。 SkeletonNetをエンドツーエンドでトレーニングできるようにするdifferentiablePoint2Voxellayerを提示します。SkeGCNNとSkeDISNoutは既存のメソッドも実行します。また、異なるメトリックで測定すると、独自のメリットがあります。 
[要約]スケルトンベースのグラフ畳み込みニューラルネットワーク（skegcnn）とスケルトン、正則化されたディープインプリシットサーフェスネットワーク（skedisn）。これらは、既存の明示的なメッシュ変形のフレームワークと、下流のサーフェス再構築タスクのインプリシットフィールド学習に基づいて構築および改善されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Forest R-CNN: Large-Vocabulary Long-Tailed Object Detection and Instance
  Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_28.html">
      <font color="black">Forest R-CNN: Large-Vocabulary Long-Tailed Object Detection and Instance
  Segmentation</font>
    </a>
  </h2>
  <font color="black">マスクR-CNNベースラインと比較して、フォレストR-CNNはパフォーマンスを大幅に向上させ、レアカテゴリと全体的なカテゴリでそれぞれ11.5％と3.9％APを改善します。親クラスを作成する方法は一意ではないため、複数のツリーをさらに構築して分類フォレストを形成し、各ツリーがきめの細かい分類への投票に貢献します。オブジェクト分析の以前の成功にもかかわらず、ロングテールデータ分布で多数のオブジェクトカテゴリを検出およびセグメント化することは依然として困難です問題とあまり調査されていません。 
[ABSTRACT]大規模な語彙の分類器の場合、ノイズの多いロジットが得られる可能性がはるかに高く、誤認識につながる可能性があります。親クラスノードの数が大幅に少ないため、ロジットのノイズが少なくなり、大きなクラスのノードに存在する誤ったまたはノイズの多いロジットを抑制します。問題を軽減するために、シンプルで効果的なリサンプリング方法を提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: CycleMorph: Cycle Consistent Unsupervised Deformable Image Registration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_29.html">
      <font color="black">CycleMorph: Cycle Consistent Unsupervised Deformable Image Registration</font>
    </a>
  </h2>
  <font color="black">サイクルの一貫性は、変形中にトポロジーを維持する暗黙の正則化を提供することにより、画像のレジストレーションパフォーマンスを向上させます。大量登録でのメモリの問題に対処するための実装。変形フィールドの定性的および定量的評価は、提案された方法のサイクル整合性の有効性も検証します。 
[要約]提案された方法は非常に柔軟性が高いため、2Dと3Dの両方の登録問題に適用できます。大規模な登録でのメモリの問題に対処するために、マルチスケール実装に簡単に拡張できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Self-supervised Video Representation Learning by Pace Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_30.html">
      <font color="black">Self-supervised Video Representation Learning by Pace Prediction</font>
    </a>
  </h2>
  <font color="black">さらに、類似のビデオコンテンツに関する合意を最大化することにより、異なるペースの識別に向けてモデルをプッシュする対照学習をさらに導入します。提案された方法の有効性を検証するために、いくつかの代替ネットワークでアクション認識およびビデオ検索タスクに関する広範な実験を行いますアーキテクチャ..このペーパーでは、ビデオペース予測による、新しい視点からの自己監視ビデオ表現学習の問題に対処します。 
[要約]アイデアは、人間の視覚システムがビデオのペース、たとえば、映画制作で広く使用されている手法であるスローモーションに敏感であるという事実に基づいています。これは、ネットワークがそのようなペースの推論タスクでのみ成功できるという信念に基づいていますそれは根本的なビデオコンテンツを理解しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Condensing Two-stage Detection with Automatic Object Key Part Discovery -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_31.html">
      <font color="black">Condensing Two-stage Detection with Automatic Object Key Part Discovery</font>
    </a>
  </h2>
  <font color="black">キーパーツモデリングは、検出されたキーパーツの細かい機能をエンコードし、グローバルモデリングは、ラフで全体的なオブジェクト特性をエンコードします。これらの検出されたキーパーツを使用して、オブジェクトの外観モデリングをキーパーツモデリングプロセスとグローバルモデリングプロセスに分解します。検出..コードはhttps://github.com/zhechen/Condensing2stageDetectionでリリースされています。 
[ABSTRACT]新しい方法により、50％の検出ヘッドのモデルパラメータを大幅に削減できます。これにより、オブジェクトの外観モデリングを主要なパーツモデリングプロセスと検出用のグローバルモデリングプロセスに分解できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-10">
        <br><font color="black">2020-06-10</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Accuracy-Fairness Paradox: Adversarial Example-based Data
  Augmentation for Visual Debiasing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_32.html">
      <font color="black">Towards Accuracy-Fairness Paradox: Adversarial Example-based Data
  Augmentation for Visual Debiasing</font>
    </a>
  </h2>
  <font color="black">ターゲットタスクに対処するときの特定の保護されたグループまたは敏感なグループへのバイアスに関する機械学習の公平性の懸念。生成された敵対的な例は、オンラインの方法でバイアス変数の分布のバランスをとることによってターゲットタスクのトレーニングデータセットを補足します。顔のデータ分析属性認識は、（1）不均衡なトレーニングデータ分布からのモデルバイアスの属性、および（2）データ分布のバランスをとる際の敵対的な例の可能性を示します。 
[要約]このペーパーでは、画像分類タスクのコンテキストでバイアス除去の問題を研究します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-27">
        <br><font color="black">2020-07-27</font>
      </time>
    </span>
</section>
<!-- paper0: Modeling Caricature Expressions by 3D Blendshape and Dynamic Texture -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_33.html">
      <font color="black">Modeling Caricature Expressions by 3D Blendshape and Dynamic Texture</font>
    </a>
  </h2>
  <font color="black">形状とテクスチャの両方のコンポーネントの組み合わせにより、人気の3DMM表現の拡張によって似顔絵の非自明な表現を効果的に定義できるため、似顔絵を柔軟に任意の表現に変形して、形と色空間の両方で視覚的に良好な結果を得ることができます。 。実験は、提案された方法の有効性を実証します。.私たちのソリューションの鍵は、似顔絵の表現をモデル化する新しい方法で、従来の3DMM表現を似顔絵の領域に拡張します。 
[ABSTRACT]メソッドは、似顔絵の形状モデリングとテクスチャモデルで構成されます。これは、問題の解決策を提供し、目的の表現を作成する機能の強化に重点を置きます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Feature Binding with Category-Dependant MixUp for Semantic Segmentation
  and Adversarial Robustness -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_34.html">
      <font color="black">Feature Binding with Category-Dependant MixUp for Semantic Segmentation
  and Adversarial Robustness</font>
    </a>
  </h2>
  <font color="black">私たちの仕事では、クラスラベルに基づいて画像をブレンドし、ブレンドされた画像を同時にセグメント化して分離するフィーチャバインディングネットワークをトレーニングすることにより、これは高密度の画像ラベリングのタスクで達成されます。このプロセスを通じて、一般的なメカニズムを明らかにします。 、従来の方法とは異なり、ベースセグメンテーションネットワークのパフォーマンスを向上させると同時に、敵対的攻撃に対する堅牢性を向上させます。この前提は、機能バインディングの概念に基づいています。これは、アクティベーションが空間やレイヤーに広がるプロセスとして定義されています。ネットワーク内の統合に成功し、正しい推論の決定に到達します。 
[要約]前提は、機能バインディングの概念に基づいています。これは、ネットワーク内のスペースとレイヤーにまたがるアクティベーションが正常に統合されるプロセスとして定義されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: DeepC-MVS: Deep Confidence Prediction for Multi-View Stereo
  Reconstruction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_35.html">
      <font color="black">DeepC-MVS: Deep Confidence Prediction for Multi-View Stereo
  Reconstruction</font>
    </a>
  </h2>
  <font color="black">私たちは、公に利用可能な現実のMVSデータセットからの（半）密なグラウンドトゥルースデプスマップに基づいて信頼性予測ネットワークをトレーニングします。特に、マルチビューステレオ（MVS）用に明示的に調整された信頼予測ネットワークを提案し、パイプライン内の深度マップ外れ値フィルタリングと深度マップ改良の両方に使用して、品質を向上させます。最終的な3D再構成。 
[ABSTRACT] 3d再構成のコンテキストでのdnnsの使用は、メモリと計算の制約のため、未解決の課題です。マルチビューステレオ（mvs）用に特別に調整された信頼予測ネットワークを提案し、両方の深度マップに使用します外れ値フィルタリングと深度マップの改良</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-01">
        <br><font color="black">2019-12-01</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic Speech Recognition Benchmark for Air-Traffic Communications -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_36.html">
      <font color="black">Automatic Speech Recognition Benchmark for Air-Traffic Communications</font>
    </a>
  </h2>
  <font color="black">このペーパーは、170時間を超えるATCo音声データでトレーニングされたいくつかの最新のASRモデルの探索的ベンチマークを伝えています。スピーカーのアクセントによるクロスアクセントの欠陥は、これにより、ATC環境でシステムを実現可能にします。これにより、CleanSky EC-H2020 ATCO2を紹介します。これは、空域からATCo音声データを収集、整理、および自動的に前処理するASRベースのプラットフォームの開発を目的とするプロジェクトです。 
[要旨]航空管制官は、パイロットと航空管制官（atco）との間の唯一の接触方法です。前者は最も広く使用されており、後者は海洋メッセージに必須であり、一部の国内問題では制限されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-18">
        <br><font color="black">2020-06-18</font>
      </time>
    </span>
</section>
<!-- paper0: Strategies for Robust Image Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_37.html">
      <font color="black">Strategies for Robust Image Classification</font>
    </a>
  </h2>
  <font color="black">私たちの貢献は、これらの変更に対するロバスト性を一般化および改善するモデルの機能を強化するさまざまなトレーニングテクニックを提示します。一貫性のある正確な結果を生成する画像分類モデルの機能に悪影響を与える要因を探求します。モデルの分類機能は悪影響を受けますデジタル異常または物理的環境の変化の結果としての画像の変更によって。 
[要約]画像分類モデルが一貫した正確な結果を生成する能力に影響を与える要因を調査します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-26">
        <br><font color="black">2020-03-26</font>
      </time>
    </span>
</section>
<!-- paper0: Efficient Adaptation of Neural Network Filter for Video Compression -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_38.html">
      <font color="black">Efficient Adaptation of Neural Network Filter for Video Compression</font>
    </a>
  </h2>
  <font color="black">私たちの方法は、7つのテストシーケンスで最新の多用途ビデオコーディング（VVC）標準コーデックと比較した場合、最大9.7％の平均BDレートゲインを達成することを示しています。生成されたビデオビットストリームに重み更新を含めることができます。既存のビデオコーデックによって。提案された方法は、従来の微調整アプローチよりもはるかに速く収束を実現し、実用的なアプリケーションに適しています。 
[要約]提案された方法は、微調整のビットを作成するために使用できます。微調整の代替として使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-28">
        <br><font color="black">2020-07-28</font>
      </time>
    </span>
</section>
<!-- paper0: Pose Estimation for Vehicle-mounted Cameras via Horizontal and Vertical
  Planes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_39.html">
      <font color="black">Pose Estimation for Vehicle-mounted Cameras via Horizontal and Vertical
  Planes</font>
    </a>
  </h2>
  <font color="black">2番目のクラスの場合、平面は地面に対して垂直であり、法線が不明です。たとえば、建物のファサードです。新規の方法は、従来のアルゴリズムよりも正確または同等であり、最先端のロバスト推定器に含めると高速になります。提案された方法によって、最小のケースと過剰決定のケースの両方を解決できます。 
[ABSTRACT]初めて、求められた平面はカメラ軸の1つに平行であることが期待されます。この方法は、グラウンドゼロの線形システムによって解決されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Branched Multi-Task Networks: Deciding What Layers To Share -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_40.html">
      <font color="black">Branched Multi-Task Networks: Deciding What Layers To Share</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、これらの制限を超えて、採用されたタスクのアフィニティを活用することにより、分岐されたマルチタスクネットワークを自動的に構築するアプローチを提案します。これまでの研究は、次善の方法であるレイヤー共有のレベルを決定するためにアドホックな方法に依存しているか、またはかなり高価なネットワーク設計を確立するためにニューラルアーキテクチャ検索技術を利用していました。 
[ABSTRACT]これらの影響を受けるネットワークは、通常、いくつかの共有レイヤーで始まります。その後、さまざまなタスクが独自のレイヤーのシーケンスに分岐します。以前の作業では、レイヤー共有のレベルを決定するためにアドホックメソッドに依存していました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-05">
        <br><font color="black">2019-04-05</font>
      </time>
    </span>
</section>
<!-- paper0: Recurrent Deconvolutional Generative Adversarial Networks with
  Application to Text Guided Video Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_41.html">
      <font color="black">Recurrent Deconvolutional Generative Adversarial Networks with
  Application to Text Guided Video Generation</font>
    </a>
  </h2>
  <font color="black">これらの問題に対処するために、ジェネレーターとしてリカレントデコンボリューションネットワーク（RDN）とディスクリミネーターとして3Dコンボリューショナルニューラルネットワーク（3D-CNN）を含むリカレントデコンボリューション生成敵対ネットワーク（RD-GAN）を提案します。提案されたモデルRDNを押して現実的なビデオを生成し、3D-CNNが実際のビデオと区別できないようにすることで、共同トレーニングを行うことができます。RDNは、従来のリカレントニューラルネットワークのデコンボリューションバージョンであり、生成されたビデオフレームと条件付き情報を有効活用します。 
[要約]提案されたrdnは、ネットワークのデコンボリューションバージョンです。生成されたビデオフレームの長期的な時間依存性をモデル化できます。ビデオに関する現実的な情報を生成するために使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: On failures of RGB cameras and their effects in autonomous driving
  applications -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_42.html">
      <font color="black">On failures of RGB cameras and their effects in autonomous driving
  applications</font>
    </a>
  </h2>
  <font color="black">さらに、対応する失敗した画像を生成するためのソフトウェアライブラリを構築し、それらを自動運転シミュレーターの訓練されたエージェントにフィードします。訓練されたエージェントの誤動作により、障害の影響、特に結果として生じる安全リスクをよりよく理解できます。 。車載カメラの障害が自動運転タスクを危険にさらし、その後、駆動システムによって処理される画像が変更されたときに安全でない動作を引き起こす可能性があることは否定できません。安全で堅牢な車両アーキテクチャとインテリジェントシステムの定義をサポートするには、このペーパーでは、効果と既知の緩和策の分析とともに、車両カメラの故障モデルを定義します。 
[ABSTRACT]カメラは自動運転タスクを危険にさらす可能性があり、駆動システムによって処理される画像が変更されると、安全でない動作を引き起こす可能性があります。訓練されたエージェントの誤動作により、障害の影響、特に結果として生じる安全リスクをよりよく理解できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: DSDNet: Deep Structured self-Driving Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_43.html">
      <font color="black">DSDNet: Deep Structured self-Driving Network</font>
    </a>
  </h2>
  <font color="black">多数の大規模な自動運転データセットの実験により、モデルが最先端の技術を大幅に上回っていることを示しています。サンプルベースの定式化により、連続確率変数の確率論的推論の難しさを克服できます。さらに、DSDNetアクターの予測される将来の分布を明示的に活用して、構造化された計画コストを使用して安全な操縦を計画します。 
[ABSTRACT]エネルギーベースのエネルギーベースのモデルのネットワークが開発されました。これにより、ランダムデータを予測する困難を克服できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Motion Similarity Modeling -- A State of the Art Report -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_44.html">
      <font color="black">Motion Similarity Modeling -- A State of the Art Report</font>
    </a>
  </h2>
  <font color="black">人間の動きの分析は、現実的なトレーニングシミュレーションやロボット工学やアニメーションでの本格的な動きなど、幅広い可能性を切り開きます。調査では、さまざまな類似性の側面と動きの特徴を要約し、2つのアクション間の類似性を測定する方法について説明します。モーション分析はアプリケーションに依存しているため、特定のユースケースに適したモーション類似性メソッドを見つけることが不可欠です。 
[要約]最先端のレポートは、人間の動作分析とさまざまな類似性モデリング方法の概要を提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Objective Neural Architecture Search Based on Diverse Structures
  and Adaptive Recommendation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_45.html">
      <font color="black">Multi-Objective Neural Architecture Search Based on Diverse Structures
  and Adaptive Recommendation</font>
    </a>
  </h2>
  <font color="black">発見された高性能セルを使用してネットワークアーキテクチャを構築します。より軽量なアーキテクチャを実現するには、より柔軟で多様なニューラルアーキテクチャが求められており、より効率的な方法をより大きな検索空間向けに設計する必要があります。これにより、MoARRを提案します。アルゴリズム。既存の研究結果と履歴情報を利用して、軽量で正確なアーキテクチャをすばやく見つけます。 
[ABSTRACT]モアアルゴリズムは既存の研究結果と履歴情報を使用して、軽量で正確なアーキテクチャをすばやく見つけます。この方法は、ネットワークアーキテクチャの多様性を高めながら、セル構造評価の検索スペースを削減します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-06">
        <br><font color="black">2020-07-06</font>
      </time>
    </span>
</section>
<!-- paper0: Fast Fourier Color Constancy -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_46.html">
      <font color="black">Fast Fourier Color Constancy</font>
    </a>
  </h2>
  <font color="black">私たちのFFCCの実装は、モバイルデバイス上で毎秒約700フレームで実行され、正確でリアルタイムの時間的にコヒーレントな自動ホワイトバランスアルゴリズムとして使用できるようにします。光源推定、FFCCは、より優れたトレーニング手法、効果的な時間平滑化手法、およびエラー分析のためのより豊富な方法を可能にします。高速フーリエ色恒常性（FFCC）を提示します。トーラス。 
[ABSTRACT] ffccは、以前の最新の状態よりも13〜20％低いエラーを生成します。これは、以前の最新の状態と比較して、250〜3000倍高速です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2016-11-23">
        <br><font color="black">2016-11-23</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Caricature via Semantic Shape Transform -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_47.html">
      <font color="black">Learning to Caricature via Semantic Shape Transform</font>
    </a>
  </h2>
  <font color="black">大規模な写真似顔絵ベンチマークデータセットに対するアプローチの有効性を、最新の手法と比較して示します。提案されたフレームワークは、顔の構造を維持しながら、視覚的に楽しい形状の誇張をレンダリングできることを示しています。さらに、このモデルにより、ユーザーはセマンティックマップを介して形状を操作できます。 
[ABSTRACT]シンプルなアルゴリズムは、人物の描画の詳細な詳細マップに基づいています。これを使用して、ユーザーが形状を操作できるアルゴリズムを作成できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-12">
        <br><font color="black">2020-08-12</font>
      </time>
    </span>
</section>
<!-- paper0: Perceive, Predict, and Plan: Safe Motion Planning Through Interpretable
  Semantic Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_48.html">
      <font color="black">Perceive, Predict, and Plan: Safe Motion Planning Through Interpretable
  Semantic Representations</font>
    </a>
  </h2>
  <font color="black">大規模な手動運転のデータセットと閉ループシミュレーションでの実験は、提案されたモデルが、はるかに安全な軌道を生成しながら、人間の行動を模倣する際に最先端のプランナーよりも大幅に優れていることを示しています。自動運転車両の知覚、予測、モーションプランニングを共同で実行し、解釈可能な中間表現を生成するエンドツーエンドの学習可能なネットワーク。既存のニューラルモーションプランナーとは異なり、モーションプランニングコストは、知覚および予測の見積もりと一致しています。 
[ABSTRACT]私たちのシステムは学習終了です-人間のデモから計画されました。私たちのネットワークは人間のデモから学習されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-end Contextual Perception and Prediction with Interaction
  Transformer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_49.html">
      <font color="black">End-to-end Contextual Perception and Prediction with Interaction
  Transformer</font>
    </a>
  </h2>
  <font color="black">特に、推定される将来の軌跡間の社会的コンプライアンスを大幅に改善し、予測されるアクター間の衝突を大幅に減らします。2つの挑戦的な実世界のデータセット、ATG4DとnuScenesでのアプローチを検証します。このアプローチが優れていることを示しています。両方のデータセットの最先端。 
[ABSTRACT]モデルはエンドツーネットワークでトレーニングでき、リアルタイムで実行できます。私たちのアプローチは、両方のデータセットで最先端のパフォーマンスを発揮できることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo
  Collections -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_50.html">
      <font color="black">NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo
  Collections</font>
    </a>
  </h2>
  <font color="black">NeRF-Wをダビングしたシステムを有名なランドマークのインターネット写真コレクションに適用することで、未知で交絡する要因にもかかわらず、写真のように空間的に一貫したシーン表現を生成し、最先端の技術を大幅に改善します。多層パーセプトロンの重みを使用してシーンの体積密度と色を暗黙的にモデル化する放射輝度フィールド（NeRF）。この作業では、これらの問題に対処するためにNeRFに一連の拡張機能を導入し、それにより、インターネットから取得した非構造化画像コレクション。 
[ABSTRACT]私たちは、多層パーセプトロンの重みを使用してシーンの体積密度と色をシミュレートする神経放射輝度フィールド（neunding）に基づいて構築します。インターネットから取得した非構造化画像コレクションから正確な再構成を可能にします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-05">
        <br><font color="black">2020-08-05</font>
      </time>
    </span>
</section>
<!-- paper0: Compression of Deep Learning Models for Text: A Survey -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_51.html">
      <font color="black">Compression of Deep Learning Models for Text: A Survey</font>
    </a>
  </h2>
  <font color="black">しかし、これらのモデルはサイズが非常に大きくなります。一方、実際のアプリケーションでは、小さなモデルサイズ、短い応答時間、低い計算電力ワット数が要求されます。この調査では、6種類の方法（剪定、量子化、知識蒸留）について説明します。 、パラメータ共有、テンソル分解、および線形変換に基づく方法）を使用して、このようなモデルを圧縮し、実際の業界のNLPプロジェクトでの展開を可能にします。 
[ABSTRACT]これらのモデルを圧縮するための6種類の方法について説明します。これらのモデルは、プロジェクトの一部として再作成する必要があります。これらには、米国および米国からのデータデータの認識が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-12">
        <br><font color="black">2020-08-12</font>
      </time>
    </span>
</section>
<!-- paper0: Occluded Prohibited Items Detection: an X-ray Security Inspection
  Benchmark and De-occlusion Attention Module -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_52.html">
      <font color="black">Occluded Prohibited Items Detection: an X-ray Security Inspection
  Benchmark and De-occlusion Attention Module</font>
    </a>
  </h2>
  <font color="black">テストセットは、検出器のパフォーマンスをよりよく理解するために、3つのオクルージョンレベルにさらに分割されています。OPIXrayベンチマークとモデルは、https：//github.com/OPIXray-author/OPIXrayでリリースされています。荷物が重なった手荷物やスーツケースの場合、X線画像で禁止されているアイテムを検出するためのパフォーマンスが不十分になります。 
[ABSTRACT] X線画像検出でオブジェクトを検出する機能が重要な問題です。jimboulden：簡単に挿入できるプラグアンドプレイモジュールである閉塞防止注意モジュール（doam）を提案します最も人気のある検出器を宣伝する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-18">
        <br><font color="black">2020-04-18</font>
      </time>
    </span>
</section>
<!-- paper0: Attention-Guided Discriminative Region Localization and Label
  Distribution Learning for Bone Age Assessment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_53.html">
      <font color="black">Attention-Guided Discriminative Region Localization and Label
  Distribution Learning for Bone Age Assessment</font>
    </a>
  </h2>
  <font color="black">具体的には、最初に分類モデルをトレーニングして、判別領域の注意マップを学習し、手領域、最も判別領域（手根骨）、次に次に判別領域（中手骨）を見つけます。BAAをとる代わりに年齢ラベルスペースのラベルのあいまいさの問題のために最適ではない一般的な回帰タスクとして、異なる年齢の手の画像間の序数関係を利用し、さらに多くをもたらす、共同年齢分布学習と期待回帰の使用を提案します。堅牢な年齢推定..トレーニングアノテーションを使用しないため、手動アノテーションが必要な既存の最先端の半自動ディープラーニングベースのメソッドと比較して、このメソッドは競争力のある結果を実現します。 
[要旨]この論文は、注意-注釈なしでbaaの識別領域を自動的にローカライズするガイド付きアプローチを提案します。詳細な研究は、rsna小児骨年齢データによって行われました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-30">
        <br><font color="black">2020-05-30</font>
      </time>
    </span>
</section>
<!-- paper0: Jointly Cross- and Self-Modal Graph Attention Network for Query-Based
  Moment Localization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_54.html">
      <font color="black">Jointly Cross- and Self-Modal Graph Attention Network for Query-Based
  Moment Localization</font>
    </a>
  </h2>
  <font color="black">パラメトリックメッセージパッシングを通じて、CMGはビデオとセンテンス全体で関連するインスタンスを強調表示し、SMGはフレーム（単語）相関のために各モダリティ内のペアワイズ関係をモデル化します。フレームと単語がノードとして表されるモーダルリレーショングラフ（SMG）、およびクロスモーダルノードとセルフモーダルノードのペア間の関係は、注意メカニズムによって記述されます。このような共同グラフの複数のレイヤーにより、CSMGANは2つのモダリティ間の高次相互作用を効果的にキャプチャし、より正確なローカリゼーションを可能にします。 
[ABSTRACT]ジョイントグラフは、クロスモーダル相互作用グラフ（cmg）とセルフモーダル関係グラフ（smg）で構成されます。クロスペアとセルフペアラルノードのペア間の接続は、注意メカニズムによって記述されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-04">
        <br><font color="black">2020-08-04</font>
      </time>
    </span>
</section>
<!-- paper0: Robust Image Matching By Dynamic Feature Selection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_55.html">
      <font color="black">Robust Image Matching By Dynamic Feature Selection</font>
    </a>
  </h2>
  <font color="black">この問題に対処するために、さまざまな縮尺で機能を動的に選択することにより、堅牢な機能を生成します。ただし、高レベルの機能マップは空間解像度が低く、対応一致のクラス内バリエーションを区別するための正確で細かい機能を提供するには不十分です。 ..個々のアクションが新しい機能を必要とするか、またはマッチングスコアを参照して選択エピソードを終了する、画像マッチングのRL環境を定義します。 
[ABSTRACT]これは、一連の作品が機能ベースのシステムを作成したのは初めてです。動的に機能を選択して堅牢な機能を生成すると言います。画像マッチングのrl環境を定義し、個々のアクションに新しい機能が必要になるか、終了するようにします一致するスコアを参照することによる選択戦略</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Reliability of Decision Support in Cross-spectral Biometric-enabled
  Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_56.html">
      <font color="black">Reliability of Decision Support in Cross-spectral Biometric-enabled
  Systems</font>
    </a>
  </h2>
  <font color="black">関連するアプリケーションには、個人やチーム、および状況認識システムにおける人間の行動監視やストレス検出が含まれます。このペーパーでは、顔や顔の表情のバイオメトリクスを利用する意思決定支援システムのパフォーマンスの評価について説明します。クロスの利用可能なデータベースの使用顔と顔の表情のスペクトルビデオを使用して、人間と機械のシステムのパフォーマンスの評価基準に影響を与える生体認証のバイアス現象を実証する一連の実験を行いました。 
[ABSTRACT]評価基準には、エラーのリスクと関連する決定の信頼性、および決定の決定に対する認識されたシステムの信頼の変化への寄与が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_57.html">
      <font color="black">Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution</font>
    </a>
  </h2>
  <font color="black">効率的な3Dモデルの範囲を探索するには、まずSPVConvに基づいて柔軟なアーキテクチャ設計スペースを定義し、次に3Dニューラルアーキテクチャ検索（3D-NAS）を提示して、この多様な設計スペースで最適かつ効率的なネットワークアーキテクチャを効率的かつ効果的に検索します。この目的のために、バニラスパースコンボリューションに高解像度のポイントベースブランチを装備した軽量3Dモジュールであるスパースポイントボクセルコンボリューション（SPVConv）を提案します。ハードウェアリソースが限られているため、既存の3D知覚モデルでは、低解像度のボクセル化と積極的なダウンサンプリングにより、小さなインスタンス（歩行者、自転車など）を非常によく認識します。 
[ABSTRACT]新しい3D知覚モデルは小さなインスタンスを認識できません。このポイント-ベースのブランチは、大きな屋外シーンからでも細かいディテールを維持できます。最新のミンコウスキーネットより3.3％優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-31">
        <br><font color="black">2020-07-31</font>
      </time>
    </span>
</section>
<!-- paper0: Few shot clustering for indoor occupancy detection with extremely
  low-quality images from battery free cameras -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_58.html">
      <font color="black">Few shot clustering for indoor occupancy detection with extremely
  low-quality images from battery free cameras</font>
    </a>
  </h2>
  <font color="black">屋内環境での人間の占有率の信頼できる検出は、さまざまなエネルギー効率、セキュリティ、および安全性のアプリケーションにとって重要です。ベンチマークデータセットでアルゴリズムを検証および比較する以外に、小説を使用して実際の家から収集されたストリーミング画像でのアルゴリズムのパフォーマンスも示しますバッテリー不要のカメラハードウェア。私たちは、低電力イメージセンサーからの非常に低品質でプライバシーを保護する画像を使用して、占有検出のこの課題を検討します。 
[ABSTRACT]非常に低品質のプライバシーを使用した占有カメラ技術のこの課題を考慮します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Visually Explaining Similarity Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_59.html">
      <font color="black">Towards Visually Explaining Similarity Models</font>
    </a>
  </h2>
  <font color="black">学習した機能の埋め込みのみに依存することにより、我々のアプローチがあらゆる種類のCNNベースの類似性アーキテクチャに適用できることを示します。これは、一般的な視覚的説明可能性に向けた重要なステップです。結果として得られる類似性モデルが機能し、視覚的に説明できることを示します。 、説明の制約なしにトレーニングされた対応するベースラインモデルよりも優れています。結果として得られる視覚的な説明は、単なる解釈以上のものであることを示しています。それらは、類似性の説明に基づく新しいトレーニング可能な制約を使用して、モデル学習プロセス自体に注入できます。 
[ABSTRACT]結果として得られる視覚的な説明は、単なる解釈可能性以上のものであることを示しています。これらは、新しいトレーニング可能な制約を使用して、モデル学習プロセス自体に注ぎ込むことができます。3つの異なる種類のタスクに関する広範な実験を使用して、アプローチを実証しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: A Sensitivity Analysis Approach for Evaluating a Radar Simulation for
  Virtual Testing of Autonomous Driving Functions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_60.html">
      <font color="black">A Sensitivity Analysis Approach for Evaluating a Radar Simulation for
  Virtual Testing of Autonomous Driving Functions</font>
    </a>
  </h2>
  <font color="black">実際のテストドライブに代わるものとして有望ですが、仮想テストはレーダーシステム全体を詳細にシミュレーションし、電磁波の伝播を概算するために計算集約型のシミュレーション技術を使用するため、時間がかかります。シミュレーションベースのテストは自動運転機能の検証作業を大幅に削減する有望なアプローチ。特にレーダーは、伝統的にモデル化が最も難しいセンサーの1つでした。 
[ABSTRACT]レーダーは、モデル化するのが最も難しいセンサーの1つです。カメラやレーダーなどのセンサーは、このテスト作業で重要な役割を果たします。システムは現在、日本でテストされています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Modality Pathology Segmentation Framework: Application to Cardiac
  Magnetic Resonance Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_61.html">
      <font color="black">Multi-Modality Pathology Segmentation Framework: Application to Cardiac
  Magnetic Resonance Images</font>
    </a>
  </h2>
  <font color="black">さらに、ノイズ除去オートエンコーダー（DAE）をASSNに統合して、もっともらしい形状のセグメンテーション結果を生成します。具体的には、ASSNは、病変が存在する可能性のある解剖学的構造をセグメント化することを目的としています。病理学的領域のセグメンテーション.. PRSNは、ASSNの結果に基づいて病理学的領域をセグメント化するように設計されています。チャネル注意に基づく融合ブロックが、マルチモダリティCMR画像からマルチモダリティ情報をよりよく集約するために提案されています。 
[要約] myops20チャレンジデータセットの研究者は、私たちのフレームワークが心筋の瘢痕と浮腫のセグメンテーションに対して有望なパフォーマンスを達成できることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Image segmentation via Cellular Automata -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_62.html">
      <font color="black">Image segmentation via Cellular Automata</font>
    </a>
  </h2>
  <font color="black">ピクセルグリッドに密に生息しているコロニーを検討します。すべてのセルは、現在の状態、色、および$ 3 \ times 3 $近傍の状態を使用するランダムな更新によって管理されます。実用的な観点から、このアプローチでは非常に効率的なモデルを構築します-最小のオートマトンは、10,000未満のパラメーターを使用して複雑なセグメンテーションタスクを解決します。更新ルールは、セルの大きなランダムサブセットに並列で繰り返し適用され、収束がセグメンテーションマスクの生成に使用された後、標準の勾配降下法を使用して最適な更新規則を学習するために逆伝播されます。 
[ABSTRACT]オートマトンは、高解像度画像を正常にセグメント化できます。これらのモデルは、世界の軌跡のみで効率的に学習できます。ローカルの情報交換交換のみを使用して、情報を整理し、グローバルに一貫したセグメンテーション結果を生成する優れた機能を示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: MINI-Net: Multiple Instance Ranking Network for Video Highlight
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_63.html">
      <font color="black">MINI-Net: Multiple Instance Ranking Network for Video Highlight
  Detection</font>
    </a>
  </h2>
  <font color="black">特に、最大-最大ランキング損失を形成して、最も可能性の高いポジティブセグメントインスタンスと最も難しいネガティブセグメントインスタンス間の信頼できる相対比較を取得します。この最大-最大ランキング損失により、MINI-Netはすべてのセグメント情報を効果的に活用して、ビデオ内の特定のイベントのハイライトセグメントをローカライズするための、より明確なビデオ機能表現を取得します。各ビデオをセグメントのバッグと見なします。したがって、提案されたMINI-Netは、ポジティブバッグに対してより高いハイライトスコアを適用することを学習します。これには、特定のイベントのハイライトセグメントが含まれます。 
[ABSTRACT]日常生活の動画には、スキーやサーフィンなど、複数のイベントタイプのハイライトセグメントが含まれている可能性があります。各動画はセグメントのバッグと見なされるため、ミニネットは、特定のイベントのハイライトセグメントを含むポジティブバッグ。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-20">
        <br><font color="black">2020-07-20</font>
      </time>
    </span>
</section>
<!-- paper0: Alleviating Human-level Shift : A Robust Domain Adaptation Method for
  Multi-person Pose Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_64.html">
      <font color="black">Alleviating Human-level Shift : A Robust Domain Adaptation Method for
  Multi-person Pose Estimation</font>
    </a>
  </h2>
  <font color="black">広範な実験が2つの一般的なベンチマークで行われ、結果は既存の教師ありアプローチと比較して、この手法の能力を実証しています。したがって、人間レベルのトポロジ構造の配置と細かい調整を行うための複数人姿勢推定のための新しいドメイン適応手法を提案します。きめの細かい機能の配置..私たちの方法は、3つのモジュールで構成されています：クロスアテンティブ機能配置（CAFA）、ドメイン内構造適応（ISA）、およびドメイン間ヒューマントポロジー配置（IHTA）モジュール。 
[要約]ポーズは、本来、ローカルキーポイントにきめの細かい機能が必要です。代わりに、複数人のポーズを推定するための新しいドメイン適応方法を提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Weight Equalizing Shift Scaler-Coupled Post-training Quantization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_65.html">
      <font color="black">Weight Equalizing Shift Scaler-Coupled Post-training Quantization</font>
    </a>
  </h2>
  <font color="black">この大幅な精度の低下を軽減するために、新しい重みイコライジングシフトスケーラーを提案します。その結果、提案された方法はMobileNetsで69.78％から70.96％のトップ1の精度を達成し、さまざまなネットワークモデルおよびタスクで堅牢なパフォーマンスを示しました。これは、チャネルごとの量子化結果に匹敵します。元の出力範囲を回復するには、逆バイナリシフトは、カスタムニューラル処理ユニットの固定コンピューティングたたみ込み演算子の既存のレイヤーごとのスケール複合に効率的に融合されます。 
[要約]提案された方法は、69のトップ1の精度を達成しました。78％。それは、さまざまなネットワークモデルおよびタスクで堅牢なパフォーマンスを示しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: CAMAL: Context-Aware Multi-layer Attention framework for Lightweight
  Environment Invariant Visual Place Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_66.html">
      <font color="black">CAMAL: Context-Aware Multi-layer Attention framework for Lightweight
  Environment Invariant Visual Place Recognition</font>
    </a>
  </h2>
  <font color="black">ただし、リソースに制約のある移動ロボットのおおよその位置を決定するために使用される計算と電力集約型のD-CNNベースのVPRアルゴリズムに対しては、軽量のVPR技術が推奨されます。場所認識データセットは、現在のVPR方法論に比べて画像検索パフォーマンスが約4倍向上した、精度再現率（AUC-PR）曲線の下のより優れた比較可能な領域を明らかにします。変化する条件とカメラの視点の下で特定の場所の認識に対して堅牢であることが判明している永続的な画像領域。 
[ABSTRACT] d-cnnsの権威ある一般化能力は、大規模な場所のデータセットでのトレーニングで得られました。変化する条件とカメラの視点の下で特定の場所の認識に対して堅牢であることが判明している永続的な画像領域を学習しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-18">
        <br><font color="black">2019-09-18</font>
      </time>
    </span>
</section>
<!-- paper0: Testing the Safety of Self-driving Vehicles by Simulating Perception and
  Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_67.html">
      <font color="black">Testing the Safety of Self-driving Vehicles by Simulating Perception and
  Prediction</font>
    </a>
  </h2>
  <font color="black">2つの大規模なデータセットの定量結果は、シミュレーションを使用してモーションプランニングを現実的にテストできることを示しています。代わりに、自動運転車両の知覚および予測システムの出力を直接シミュレートして、リアルなモーションプランニングテストを可能にします。重要なのは、入力私たちのシステムには、高解像度のマップ、境界ボックス、および軌跡が含まれており、テストエンジニアは数分で簡単にスケッチできます。 
[概要]システムは高価であり、ドメインギャップが大きいため、センサーシミュレーションの代替案を提案します。代わりに、グラウンドトゥルースラベルと実際の知覚および予測出力の形式でペアのデータを使用して、オンラインで何を予測するモデルをトレーニングしますシステムが生成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Temporally Invariant and Localizable Features via Data
  Augmentation for Video Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_68.html">
      <font color="black">Learning Temporally Invariant and Localizable Features via Data
  Augmentation for Video Recognition</font>
    </a>
  </h2>
  <font color="black">さらに、学習された機能は時間的にローカライズ可能であり、空間増強アルゴリズムを使用して達成することはできません。ディープラーニングベースのビデオ認識は、大規模なデータセットおよび時空間ネットワークアーキテクチャの開発とともに有望な改善を示しています。新規の時間データ増強に基づいてアルゴリズム、ビデオ認識パフォーマンスは、データ効率の高いアクション認識チャレンジのための第1視覚帰納的事前確率（VIPriors）を含む、空間のみのデータ拡張アルゴリズムと比較して、限られた量のトレーニングデータのみを使用して改善されます。 
[ABSTRACT]空間的に方向付けられた機能を学習することは、パフォーマンスを改善する上で重要な要素です。画像認識では、空間的に応答する機能を学習することが重要です。ビデオ認識のパフォーマンスは、限られた量のトレーニングデータのみを使用する方が優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: ISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked
  Global-Local Attention Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_69.html">
      <font color="black">ISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked
  Global-Local Attention Network</font>
    </a>
  </h2>
  <font color="black">食品認識は、食事管理やセルフサービスのレストランなど、さまざまな現実世界のアプリケーションでマルチメディアコミュニティでますます注目されています。食品認識のさらなる進歩を促進するために、500のカテゴリを持つデータセットISIA Food-500を導入しましたウィキペディアのリストと399,726枚の画像から、カテゴリカバレッジとデータ量により、既存の人気のあるベンチマークデータセットを上回る、より包括的な食品データセット。認識。 
[要約]高度な食品認識を開発するには、食品画像の大規模な検索が必要です。これは、高度な大規模な食品認識アルゴリズムを開発するために必要です。新しい方法を使用して、食品認識ソフトウェアを作成できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Network Architecture Search for Domain Adaptation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_70.html">
      <font color="black">Network Architecture Search for Domain Adaptation</font>
    </a>
  </h2>
  <font color="black">NASDAは、2つの新しいトレーニング戦略で設計されています。最適なアーキテクチャを導出するためのマルチカーネル最大平均不一致によるニューラルアーキテクチャ検索と、特徴ジェネレータと分類子のバッチとの間の敵対的なトレーニングにより、特徴ジェネレータを統合します。このホワイトペーパーでは、ドメイン適応のニューラルアーキテクチャ検索（NASDA）、微分可能なニューラルアーキテクチャ検索を利用してドメイン適応タスクに最適なネットワークアーキテクチャを導出する主要なフレームワーク。深いネットワークは、ドメイン適応の転送可能な表現を学習するために使用されています。 
[要約]ディープドメイン適応メソッドが人工ネットワークを開発しました。nasdaは2つの新しいトレーニング戦略で設計されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Localizing the Common Action Among a Few Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_71.html">
      <font color="black">Localizing the Common Action Among a Few Videos</font>
    </a>
  </h2>
  <font color="black">トリミングされていない長い動画のアクションの開始と終了は、共通のクラスラベルを知らなくても、同じアクションを含むトリミングされた動画のサンプルのほんの一握りに基づいて決定されます。単一または複数のアクションインスタンスは、提案の有効性と一般的な適用性を示しています。 
[ABSTRACT]新しい3Dたたみ込みネットワークアーキテクチャは、サポート動画の表現を検索動画セグメントに合わせるように設計されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: LGNN: a Context-aware Line Segment Detector -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_72.html">
      <font color="black">LGNN: a Context-aware Line Segment Detector</font>
    </a>
  </h2>
  <font color="black">LGNNはさらに、時間に敏感な3Dアプリケーションを可能にします。3D点群にアクセスできる場合、環境の3Dワイヤーフレームをロバストかつ効率的に抽出するためのマルチモーダルラインセグメント分類手法を提示します。最新の状態と比較してアート、LGNNは精度を損なうことなくほぼリアルタイムのパフォーマンスを実現します。 
[ABSTRACT]これは、連携する新しいネットワークの最新の例です。これらには、各ラインの新しい四つ組表現が含まれます。lgnnは、時間追跡とも呼ばれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Hybrid Dynamic-static Context-aware Attention Network for Action
  Assessment in Long Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_73.html">
      <font color="black">Hybrid Dynamic-static Context-aware Attention Network for Action
  Assessment in Long Videos</font>
    </a>
  </h2>
  <font color="black">さらに、4つの異なる種類の体操ルーチンのビデオを含む新しい新体操データセットを収集して注釈を付け、長いビデオでのアクション品質評価を評価しました。アクション品質評価の目的は、スポーツビデオを記録することです。コードとデータセットは\ url {https://github.com/lingan1996/ACTION-NET}で入手できます。 
[ABSTRACT]ほとんどの既存の作品はビデオconvoにのみ焦点を当てていますが、アスリートがビデオで実行している特定のフレームを無視します。代わりに、2つのストリームの機能を組み合わせて最終的なビデオスコアを後退させ、グラウンドトゥルーススコアで監視します専門家による</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_74.html">
      <font color="black">REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets</font>
    </a>
  </h2>
  <font color="black">次に、このツールは、明らかにされたバイアスを軽減するために実行できるアクション可能なステップを提案することにより、ユーザーをさらに支援します。オブジェクトベースのバイアスは、オブジェクト表現のサイズ、コンテキスト、または多様性に関連します。 。 
[ABSTRACT] revise（明らかにする視覚的バイアス）は、視覚的なデータセットの調査に役立つツールです。潜在的なバイアスは、現在、オブジェクトベース、（2）性別ベース、および（3）地理ベースの3つの次元に沿っています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-16">
        <br><font color="black">2020-04-16</font>
      </time>
    </span>
</section>
<!-- paper0: Semi-supervised Visual Feature Integration for Pre-trained Language
  Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_75.html">
      <font color="black">Semi-supervised Visual Feature Integration for Pre-trained Language
  Models</font>
    </a>
  </h2>
  <font color="black">提案されたフレームワークの有効性を検証するために、自然言語推論と読解タスクの両方について実験を行います。この論文では、事前訓練された言語モデルのための新しい半教師付き視覚統合フレームワークを提案します。画像データベースのみが必要で、それ以上の調整は必要ありません。マルチモーダル言語学習のための効率的で実現可能な方法を提供します。 
[ABSTRACT]ほとんどの既存のマルチモーダル言語モデルでは、視覚的および文学的なデータの整列にコストがかかります。フレームワークでは、視覚的機能は視覚化および融合メカニズムを通じて取得されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-01">
        <br><font color="black">2019-12-01</font>
      </time>
    </span>
</section>
<!-- paper0: Weakly Supervised Generative Network for Multiple 3D Human Pose
  Hypotheses -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_76.html">
      <font color="black">Weakly Supervised Generative Network for Multiple 3D Human Pose
  Hypotheses</font>
    </a>
  </h2>
  <font color="black">実験結果は、私たちのアプローチが複数の実行可能な仮説を生成でき、既存の弱く監視されたアプローチと比較して最先端の結果を達成できることを示しています。このために、ネットワークを設計して、未知のマルチモーダルターゲット事後分布。単一の画像からの3D人間の姿勢推定は、欠けている深さの固有の曖昧さのために、逆の問題です。 
[ABSTRACT] human3。 6m、mpii、およびmpi-inf-3dhp。結果はプロジェクトのウェブサイトで入手できます。プロジェクトは、逆問題に対処し、グラウンドトゥルース2dから3dへの対応の必要性を回避するために、監視が不十分なディープジェンスティヴネットワークを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: JUMPS: Joints Upsampling Method for Pose Sequences -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_77.html">
      <font color="black">JUMPS: Joints Upsampling Method for Pose Sequences</font>
    </a>
  </h2>
  <font color="black">GANは高解像度の人間のポーズシーケンスの分布を学習し、エンコーダは入力の低解像度シーケンスをその潜在空間にマップします。修復は、GANジェネレータによるデコードが入力の関節位置に最適に一致する潜在表現を計算することによって行われます。 .. GANとエンコーダーを組み合わせた深い生成モデルに基づいて構築します。 
[アブストラクト]新しい方法はジャンプと呼ばれ、2Dポーズの推定で関節の数を増やし、閉塞または欠落した関節を回復します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-02">
        <br><font color="black">2020-07-02</font>
      </time>
    </span>
</section>
<!-- paper0: Exploit Where Optimizer Explores via Residuals -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_78.html">
      <font color="black">Exploit Where Optimizer Explores via Residuals</font>
    </a>
  </h2>
  <font color="black">私たちの新しい方法は、ASGDやAdamなどの他のオプティマイザーにも適用できます。RSGDは、SGDと比較して、一般化誤差の成長率が小さく、収束率が同じ（経験的には良い）であることを示す理論分析を提供します。画像分類、言語モデリング、グラフ畳み込みニューラルネットワークに関する深層学習実験では、提案されたアルゴリズムは、初期トレーニング段階でSGD（m）/ Adamよりも高速であり、トレーニングの終了時にSGD（m）と同等かそれ以上であるより良い汎化エラー。 
[要約]残差（rsgd）を使用したsgd（m）という名前の新しいメソッドを提案します。これにより、サミュエルとグラードの両方のパフォーマンスが向上します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-11">
        <br><font color="black">2020-04-11</font>
      </time>
    </span>
</section>
<!-- paper0: Revisiting Temporal Modeling for Video Super-resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_79.html">
      <font color="black">Revisiting Temporal Modeling for Video Super-resolution</font>
    </a>
  </h2>
  <font color="black">広範な実験により、提案されたRRNは非常に計算効率が高く、他の時間モデリング方法よりも細かい時間一貫性のあるVSR結果を生成することが示されています。研究コミュニティと産業コミュニティの両方で多くの注目を集めました。さらに、提案された方法は、いくつかの広く使用されているベンチマークで最先端の結果を達成します。 
[要約]提案された方法は、広く使用されているいくつかのベンチマークで状態および現在の残余ネットワーク結果を達成しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: An Exploration of Target-Conditioned Segmentation Methods for Visual
  Object Trackers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_80.html">
      <font color="black">An Exploration of Target-Conditioned Segmentation Methods for Visual
  Object Trackers</font>
    </a>
  </h2>
  <font color="black">一般に、境界ボックスは状態を表すために使用されており、コミュニティは、そのような表現でターゲットを特定できる効率的な因果アルゴリズムを作成するために多大な努力を費やしてきました。セグメンテーショントラッカー、準リアルタイムで実行中。ビジュアルオブジェクトトラッキングは、ビデオ内のターゲットオブジェクトの状態を予測する問題です。 
[ABSTRACT]効果的な因果関係のアルゴリズムを作成するために多大な労力が費やされました。これらのアルゴリズムにより、トラッカーは最近提案されたセグメンテーショントラッカーと競合できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Powers of layers for image-to-image translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_81.html">
      <font color="black">Powers of layers for image-to-image translation</font>
    </a>
  </h2>
  <font color="black">各タスクについて、潜在的な領域で動作する残差ブロックを学習します。これは、ターゲットドメインに到達するまで繰り返し呼び出されます。私たちは、対になっていない画像から画像への変換タスクに対処するシンプルなアーキテクチャを提案します。スタイルまたはクラスの転送、ノイズ除去、ぼかし解除、デブロッキングなど。これは、たとえば、抑制するノイズのタイプまたは量が事前にわからない場合に役立ちます。 
[ABSTRACT]重みを固定した画像オートエンコーダアーキテクチャから始めます。モデルのパフォーマンスは、cycleganと同等かそれ以上です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by
  Implicitly Unprojecting to 3D -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_82.html">
      <font color="black">Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by
  Implicitly Unprojecting to 3D</font>
    </a>
  </h2>
  <font color="black">オブジェクトセグメンテーションやマップセグメンテーションなどの標準的な鳥瞰図タスクでは、モデルはすべてのベースラインと以前の作業よりも優れています。モーションプランニングの密な表現を学習するという目標を追求することで、モデルによって推論される表現が解釈可能であることを示しますテンプレートの軌跡をネットワークから鳥瞰図のコストマップに &quot;射撃&quot;することによるエンドツーエンドのモーションプランニング。LIDARからのOracle深度を使用するモデルに対してアプローチのベンチマークを行います。 
[ABSTRACT]私たちのモデルは、任意の数のカメラから画像データを与えられたシーンの鳥瞰図表現を抽出できます。この構造により、テンプレートの軌跡を鳥に「撮影」することで解釈可能なエンドツーエンドのモーションプランニングが可能になりますの-ネットワークが出力したコストマップを表示</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: An Ensemble of Knowledge Sharing Models for Dynamic Hand Gesture
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CV/paper_83.html">
      <font color="black">An Ensemble of Knowledge Sharing Models for Dynamic Hand Gesture
  Recognition</font>
    </a>
  </h2>
  <font color="black">各サブネットワークは、スケルトンジョイントのみを使用してジェスチャー認識のタスクを実行するようにトレーニングされています。さらに、循環学習率を使用して、より一般化可能な予測を生成するために、アンサンブルで結合される一連のモデルを生成できます。 ..各サブネットワークは、アーキテクチャの違いにより異なるタイプの機能を抽出するため、サブネットワーク間で知識を共有できます。 
[ABSTRACT]モデルは、2つのサブネットワーク、トランスフォーマーと順序付きニューロンのロングショートショートメモリーで構成されます。各サブネットワークは、アーキテクチャの違いにより異なるタイプの機能を抽出し、それらの間で知識を共有できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Large-scale Transfer Learning for Low-resource Spoken Language
  Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CL/paper_0.html">
      <font color="black">Large-scale Transfer Learning for Low-resource Spoken Language
  Understanding</font>
    </a>
  </h2>
  <font color="black">これは、標準のトランスフォーマアーキテクチャに依存する大量の自動音声認識アノテーション付きデータを使用してエンコーダコンポーネントを事前トレーニングし、少量のターゲットラベル付きデータでSLUモデルを微調整することで実装されます。 -基礎となるエンコーダーの機能を間接的にフィッティングおよび増強します。FluentAIデータセットの実験では、言語をまたがる転移学習およびマルチタスク戦略が、それぞれ最大で4：52％および3：89％改善されていることが示されています。ベースライン。 
[ABSTRACT] sluモデルモデルモデルモデルは、過剰適合のリスクを排除するように設計されています。sluモデルモデルには、モデルの構築に使用できるモデルがあります。sluモデルは、slu sluタスクでも使用できますが、モデルは終わり</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: MASRI-HEADSET: A Maltese Corpus for Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CL/paper_1.html">
      <font color="black">MASRI-HEADSET: A Maltese Corpus for Speech Recognition</font>
    </a>
  </h2>
  <font color="black">この論文では、自動音声認識（ASR）を目的として設計された最初の音声マルタ語コーパスを紹介します。MASRI-HEADSETコーパスは研究/学術目的で公開されています。マルタ語、マルタ語は約50万人によって話されています人。 
[要約]マルタの音声認識はまだ開発の初期段階にあります。マスリ-ヘッドセットコーパスはマルタ大学のマスリプロジェクトによって開発されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Navigating Language Models with Synthetic Agents -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CL/paper_2.html">
      <font color="black">Navigating Language Models with Synthetic Agents</font>
    </a>
  </h2>
  <font color="black">さらに、モデルがチェス盤の正確な潜在的表現を作成し、この知識を使用して法的移動の軌跡をプロットできることがわかります。この研究では、コーパスでGPT-2のバージョンをトレーニングします歴史的なチェスゲームのパターンを学習し、モデルで学習した単語の関係を、チェス盤の既知のグラウンドトゥルースと比較し、合法性とプレーの歴史的パターンを分析します。モデルを使用した駒ごとの移動の割合は、人間のパターンと似ています。 
[ABSTRACT]モデルを使用した動きのパーセンテージはよく知られていることがわかります。また、これらの動きは人間のパターンと実質的に類似していることがわかります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-10">
        <br><font color="black">2020-08-10</font>
      </time>
    </span>
</section>
<!-- paper0: Evaluating the Impact of Knowledge Graph Context on Entity
  Disambiguation Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CL/paper_3.html">
      <font color="black">Evaluating the Impact of Knowledge Graph Context on Entity
  Disambiguation Models</font>
    </a>
  </h2>
  <font color="black">私たちの経験的な結果は、提案されたKGコンテキストを一般化できること（Wikipediaの場合）を検証し、変圧器アーキテクチャでのKGコンテキストの提供は、バニラ変圧器モデルを含む既存のベースラインをかなり上回っています。事前トレーニング済みの変圧器モデルは、最先端の技術として登場しました。テキストからコンテキスト情報を学習して、いくつかのNLPタスクのパフォーマンスを向上させるアプローチ。このホワイトペーパーでは、ナレッジグラフ（この場合はWikidata）から導出されたコンテキストが、事前学習済みの変換モデルに通知し、そのパフォーマンスを向上させるのに十分な信号を提供することを主張します。 Wikidata KGの名前付きエンティティの明確化（NED）の場合。 
[要旨]提案されたkgコンテキストはwikipediaに対して標準化できます。既存のモデルにはまだ特殊なコンテキストが必要です。これらのモデルには特殊なコンテキストが必要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-12">
        <br><font color="black">2020-08-12</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Dialog Policies from Weak Demonstrations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CL/paper_4.html">
      <font color="black">Learning Dialog Policies from Weak Demonstrations</font>
    </a>
  </h2>
  <font color="black">ラベル付き、ラベル付き、さらにはラベルなしのデータを使用して、必要なデータに関する仮定を段階的に減らし、専門家のデモ参加者をトレーニングします。挑戦的なマルチドメインダイアログシステムフレームワークでの実験は、私たちのアプローチを検証し、トレーニングを受けた場合でも高い成功率を獲得しますドメイン外のデータ..難しいAtariゲームで高いスコアを示すアルゴリズムであるデモからのディープQラーニング（DQfD）に基づいて、ユーザーの要求にエージェントが正しく応答するようにガイドデータを活用します。 
[ABSTRACT]データセットと環境の間のドメインギャップは学習の鍵です。システムは、難しいatariゲームで高いスコアを獲得するシステムに基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-23">
        <br><font color="black">2020-04-23</font>
      </time>
    </span>
</section>
<!-- paper0: On the Importance of Local Information in Transformer Based Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CL/paper_5.html">
      <font color="black">On the Importance of Local Information in Transformer Based Models</font>
    </a>
  </h2>
  <font color="black">一部の研究では、この注意をローカルに制限することの約束、つまり、周囲の小さな近隣でのみ他のトークンに参加するトークンを特定しました。ローカルの注意ヘッドの重要性を確立した後、アテンションヘッドはローカルに制限されています。この作業では、学習されたモデルにおける局所性情報の役割を体系的に分析し、構文情報の役割と対比します。 
[要旨]これらの頭部は、構文的、意味的、または局所的な振る舞いを示します。これらの構造は局所的な注意に関連付けられています。ただし、複数のnlpタスクで高精度を達成するには、局所情報だけで十分であるという決定的な証拠はありません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: MICE: Mining Idioms with Contextual Embeddings -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CL/paper_6.html">
      <font color="black">MICE: Mining Idioms with Contextual Embeddings</font>
    </a>
  </h2>
  <font color="black">両方の埋め込みを使用したディープニューラルネットワークは、既存のアプローチよりもはるかに優れたパフォーマンスを発揮し、トレーニングセットに存在しなかった表現でも、慣用的な単語の使用を検出できることを示しています。慣用的な意味と、それを使用して、2つの最先端のコンテキスト単語の埋め込み（ELMoとBERT）に基づいて分類子をトレーニングします。開発されたモデルのクロスリンガル転送を示し、必要なデータセットのサイズを分析します。 
[ABSTRACT]リテラルおよび慣用的な意味を持つマルチワード式の新しいデータセットを提示します。これを使用して、2つの最先端のingesttttttttttttttttワードに基づいて分類子をトレーニングします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Ranking Enhanced Dialogue Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CL/paper_7.html">
      <font color="black">Ranking Enhanced Dialogue Generation</font>
    </a>
  </h2>
  <font color="black">たとえば、広く使用されているアーキテクチャは、単語のシャッフル、欠落した発話、発話の並べ替えなど、対話履歴の摂動に鈍感です。これらのアーキテクチャには、対話履歴のダイナミクスを理解およびモデリングする機能がないことが示されています。結果は、最新の対話生成モデルと比較して、私たちのモデルが定量的測定と人間の判断の両方の点でより良い応答を生成することを示しています。 
[ABSTRACT]以前の作品は通常、さまざまなニューラルネットワークアーキテクチャを採用しています。これらのアーキテクチャには、対話履歴のダイナミクスを理解およびモデリングする機能がありません。この問題に取り組むために、ランキングが強化された対話生成フレームワークを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: The COVID-19 Infodemic: Can the Crowd Judge Recent Misinformation
  Objectively? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CL/paper_8.html">
      <font color="black">The COVID-19 Infodemic: Can the Crowd Judge Recent Misinformation
  Objectively?</font>
    </a>
  </h2>
  <font color="black">また、送信されたクエリ、見つかった/選択されたURL、テキストの正当化、およびアドホックロガーによって収集されたクリックやマウスアクションなどのその他の行動データの観点から、労働者の行動を分析します。ステートメントの真実性、私たちはまた、労働者間の合意、異なる集計関数、スケール変換の影響、および労働者のバックグラウンド/バイアスの影響を含む、多くの異なる側面に関する結果も報告します。特に、COVID-19の健康に関連するステートメントを対象としています。緊急時、それは調査の時点でまだ進行中であり、間違いなくオンラインで広まっている誤報の量の増加を引き起こしている（「情報」という用語が使用されている現象）。 
[要旨]群衆の労働者は、声明の真実性を評価し、評価の証拠をURLおよびテキストの正当化として提供するよう求められます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic Speech Recognition Benchmark for Air-Traffic Communications -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CL/paper_9.html">
      <font color="black">Automatic Speech Recognition Benchmark for Air-Traffic Communications</font>
    </a>
  </h2>
  <font color="black">このペーパーは、170時間を超えるATCo音声データでトレーニングされたいくつかの最新のASRモデルの探索的ベンチマークを伝えています。スピーカーのアクセントによるクロスアクセントの欠陥は、データは、ATC環境でシステムを実現可能にします。ATCo環境のASRシステムは、英語以外の話者からのアクセント、コックピットノイズ、話者に依存するバイアス、およびトレーニング用のドメイン内の小さなATCデータベースにより、複雑さが増しています。 
[要旨]航空管制官は、パイロットと航空管制官（atco）との間の唯一の接触方法です。前者は最も広く使用されており、後者は海洋メッセージに必須であり、一部の国内問題では制限されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-18">
        <br><font color="black">2020-06-18</font>
      </time>
    </span>
</section>
<!-- paper0: Exploration of Gender Differences in COVID-19 Discourse on Reddit -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CL/paper_10.html">
      <font color="black">Exploration of Gender Differences in COVID-19 Discourse on Reddit</font>
    </a>
  </h2>
  <font color="black">私たちの分析はまた、パンデミック関連の自然発生的な議論における男性と女性の著者間の局所的な好みのかなりの違いを確認します。 、そしてこれらの違いが、COVID-19に関連する感情を帯びた談話を含むソーシャルメディアの投稿で増幅されることを示しています。男性と女性の言語の違いに関する研究の何十年もの間、語彙、トピック、および感情表現の好みに関する仮定が確立されています2つの性別、およびその社会学的基盤。 
[要約] redditデータセットは、redditディスカッションプラットフォームからの男性と女性の言語表現の新しいデータセットに基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Prosody Learning Mechanism for Speech Synthesis System Without Text
  Length Limit -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CL/paper_11.html">
      <font color="black">Prosody Learning Mechanism for Speech Synthesis System Without Text
  Length Limit</font>
    </a>
  </h2>
  <font color="black">英語と標準中国語での実験により、モデルではより満足な韻律を伴う音声が得られたことがわかります。さらに、局所的注意と呼ばれる新しい自己注意構造が、相対位置情報である入力テキストの長さのこの制限を取り除くために提案されています。シーケンスの相対的な位置行列によってモデル化されるため、位置エンコーディングは不要になります。特に、マンダリン合成では、提案されたモデルはMOSギャップが0.08のベースラインモデルよりも優れており、合成音声の全体的な自然さは大幅に向上しています。改善されました。 
[要約] ttsシステムに基づいて音声の韻律をモデル化するために、新しい学習形式が提案されます。音声の韻律情報は、韻律学習者によってメルスペクトラムから抽出され、音素シーケンスと組み合わせて、メル-スペクトルを再構築します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Commonsense Knowledge Graph Reasoning by Selection or Generation? Why? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CL/paper_12.html">
      <font color="black">Commonsense Knowledge Graph Reasoning by Selection or Generation? Why?</font>
    </a>
  </h2>
  <font color="black">観察結果を踏まえて、ニューラルテキストエンコーダーとナレッジグラフ埋め込みモデルの構造をさらに組み合わせて、選択方法の2つの問題を解決し、競争力のある結果を達成します。各方法には独自の利点があります。2つの方法を理論的および経験的に比較し、選択方法は、CKGRの生成方法よりも適しています。 
[要約]既存のメソッドは2つのカテゴリに分類できます。既存のメソッドは2つのカテゴリに分類されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Dialogue State Induction Using Neural Latent Variable Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CL/paper_13.html">
      <font color="black">Dialogue State Induction Using Neural Latent Variable Models</font>
    </a>
  </h2>
  <font color="black">ただし、ラベル付けプロセスはコストがかかり、時間がかかり、エラーが発生しやすくなる可能性があり、さらに重要なのは、カスタマーサービスの実際のダイアログで広範囲のドメインをカバーできないことです。結果は、モデルが有効なスロットを効果的に見つけることができることを示しています。 、誘導対話状態を備えた最先端の対話システムは、対話状態モジュールを使用しない場合と比較して、パフォーマンスが向上します。 
[ABSTRACT]従来の方法では、ニューラルモデルがトレーニングされるトレーニングコーパスに手動でラベルを付けることで対話状態を検出します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Compression of Deep Learning Models for Text: A Survey -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CL/paper_14.html">
      <font color="black">Compression of Deep Learning Models for Text: A Survey</font>
    </a>
  </h2>
  <font color="black">一方、実際のアプリケーションでは、小さなモデルサイズ、短い応答時間、低い計算電力ワット数が要求されます。近年、自然言語処理（NLP）と情報検索（IR）の分野は、ディープラーニングのおかげで飛躍的な進歩を遂げています。 Recurrent Neural Networks（RNNs）、Gated Recurrent Units（GRUs）and Long Short-Term Memory（LSTMs）networksなどのトランスフォーマーベースのモデルと、トランスフォーマーからの双方向エンコーダー表現（BERT）のようなトランスフォーマーベースのモデル。そして、このモデルに関する小さなモデル、そして最近発表された大量の研究は、この調査が過去数年の「NLPのためのディープラーニング」コミュニティによって行われた大量の研究をまとめ、一貫した物語として提示していると信じています。 
[ABSTRACT]これらのモデルを圧縮するための6種類の方法について説明します。これらのモデルは、プロジェクトの一部として再作成する必要があります。これらには、米国および米国からのデータデータの認識が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-12">
        <br><font color="black">2020-08-12</font>
      </time>
    </span>
</section>
<!-- paper0: Continuous Speech Separation with Conformer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CL/paper_15.html">
      <font color="black">Continuous Speech Separation with Conformer</font>
    </a>
  </h2>
  <font color="black">この論文では、分離システムでリカレントニューラルネットワークの代わりにトランスフォーマーとコンフォーマーを使用します。これは、自己注意ベースの方法でグローバル情報をキャプチャすることが音声分離に不可欠であると考えているためです。LibriCSSデータセット、コンフォーマー分離の評価モデルは、発話ごとの評価で双方向LSTM（BLSTM）から23.5％のワードエラーレート（WER）を削減し、連続評価で15.4％のWERを削減して、最先端の結果を達成します。分離モデルは、混合音声からの単一のスピーカー信号。 
[要約]より厳密な分離モデルは、混合音声から単一の話者信号を抽出します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Semi-supervised Visual Feature Integration for Pre-trained Language
  Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CL/paper_16.html">
      <font color="black">Semi-supervised Visual Feature Integration for Pre-trained Language
  Models</font>
    </a>
  </h2>
  <font color="black">一意性には次のものが含まれます。1）統合は、半教師付きアプローチを介して行われます。これは、すべての文に画像を揃える必要はありません。2）視覚的特徴は外部コンポーネントとして統合され、事前トレーニング済みの言語モデルで直接使用できます。フレームワークでは、視覚的特徴は視覚化および融合メカニズムを通じて取得されます。この論文では、事前トレーニング済みの言語モデルのための新しい半教師付き視覚統合フレームワークを提案します。 
[ABSTRACT]ほとんどの既存のマルチモーダル言語モデルでは、視覚的および文学的なデータの整列にコストがかかります。フレームワークでは、視覚的機能は視覚化および融合メカニズムを通じて取得されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-01">
        <br><font color="black">2019-12-01</font>
      </time>
    </span>
</section>
<!-- paper0: Cognitive Representation Learning of Self-Media Online Article Quality -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CL/paper_17.html">
      <font color="black">Cognitive Representation Learning of Self-Media Online Article Quality</font>
    </a>
  </h2>
  <font color="black">また、大規模な実世界の評価データセットを構築しました。これは、専門家による記事の評価を表現する認知スタイルとより一致しています。広範な実験結果は、提案されたフレームワークが最新の方法を大幅に上回っていることを示しています。オンライン記事の品質評価のさまざまな要素を効果的に学習して統合します。 
[ABSTRACT]セルフメディアオンライン記事は主にユーザーによって作成され、さまざまなテキストレベルの外観特性とマルチモーダルハイブリッド編集があります。また、多様なコンテンツ、さまざまなスタイル、大きなセマンティックスパン、優れたインタラクティブエクスペリエンスといった潜在的な特性もあります。要件</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Conv-Transformer Transducer: Low Latency, Low Frame Rate, Streamable
  End-to-End Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/cs.CL/paper_18.html">
      <font color="black">Conv-Transformer Transducer: Low Latency, Low Frame Rate, Streamable
  End-to-End Speech Recognition</font>
    </a>
  </h2>
  <font color="black">インターリーブされたコンボリューションレイヤーは、パフォーマンスにとって重要な将来のコンテキストのモデリングに使用されます。さらに、各復号化ステップの計算コストを一定に保つために、履歴コンテキストの長さを自主的に制限します。オーディオエンコーディングには、インターリーブされた畳み込み層。 
[ABSTRACT]元のトランスフォーマーは、エンコーダー/デコーダーアーキテクチャーを備え、オフラインasrにのみ適しています。このアーキテクチャーはconv-transformerトランスデューサーと呼ばれ、外部言語モデルなしで競争力のあるパフォーマンスを実現します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: Masked Pre-trained Encoder base on Joint CTC-Transformer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.AS/paper_0.html">
      <font color="black">Masked Pre-trained Encoder base on Joint CTC-Transformer</font>
    </a>
  </h2>
  <font color="black">この研究（Tencent AIラボでのインターンシップ中に行われた作業）は、半教師付き音響モデリング、つまり2段階トレーニング方法の実験は、完全に監視されたモデルよりもはるかに優れたパフォーマンスを提供します。JCTフレームワークでは、元のトランスフォーマーと比較して、プレーンテキストの代わりに音響機能が入力として適用されます。 
[ABSTRACT] MPEフレームワークでは、入力フレームの一部がマスクされ、大規模な教師なしデータのあるエンコーダの後に再構築されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-25">
        <br><font color="black">2020-05-25</font>
      </time>
    </span>
</section>
<!-- paper0: Large-scale Transfer Learning for Low-resource Spoken Language
  Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.AS/paper_1.html">
      <font color="black">Large-scale Transfer Learning for Low-resource Spoken Language
  Understanding</font>
    </a>
  </h2>
  <font color="black">これは、標準のトランスフォーマアーキテクチャに依存する大量の自動音声認識アノテーション付きデータを使用してエンコーダコンポーネントを事前トレーニングし、少量のターゲットラベル付きデータでSLUモデルを微調整することで実装されます。 -基礎となるエンコーダーの機能を間接的にフィッティングおよび増強します。FluentAIデータセットの実験では、言語をまたがる転移学習およびマルチタスク戦略が、それぞれ最大で4：52％および3：89％改善されていることが示されています。ベースライン。 
[ABSTRACT] sluモデルモデルモデルモデルは、過剰適合のリスクを排除するように設計されています。sluモデルモデルには、モデルの構築に使用できるモデルがあります。sluモデルは、slu sluタスクでも使用できますが、モデルは終わり</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: MLNET: An Adaptive Multiple Receptive-field Attention Neural Network for
  Voice Activity Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.AS/paper_2.html">
      <font color="black">MLNET: An Adaptive Multiple Receptive-field Attention Neural Network for
  Voice Activity Detection</font>
    </a>
  </h2>
  <font color="black">この問題を解決するために、このペーパーでは、VADタスクを完了するために、MLNETと呼ばれる適応型複数受容野注意ニューラルネットワークを提案しました。音声アクティビティ検出（VAD）は、音声と非音声を区別し、そのパフォーマンスは重要です音声ベースのサービスの重要性。最近、ディープニューラルネットワーク（DNN）ベースのVADは、従来の信号処理方法よりも優れたパフォーマンスを実現しています。 
[ABSTRACT]ディープニューラルネットワーク（dnn）-ベースのVADは、より優れたコンテキスト音声を実現しました。コンテキスト音声情報の固定ウィンドウは、予測できないさまざまなノイズ環境を処理できません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Sum-Product Networks for Robust Automatic Speaker Identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.AS/paper_3.html">
      <font color="black">Sum-Product Networks for Robust Automatic Speaker Identification</font>
    </a>
  </h2>
  <font color="black">さらに、この作業は、SPNがロバストな自動音声認識（ASR）や自動スピーカー検証（ASV）などの関連タスクに潜在的可能性があることを示しています。SPNスピーカーモデルは、複数の実世界の非定常および色付きノイズソースで評価されます。信号対雑音比（SNR）レベル。可用性：SPN ASIシステムは、https：//github.com/anicolson/SPN-ASIで入手できます。 
[要約]私たちは、spnが将来の堅牢な音声処理のための有用なツールになる可能性があることを示すことを目指しています。2つの最近の畳み込みニューラルネットワークよりも堅牢であると識別されたasiシステム</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-26">
        <br><font color="black">2019-10-26</font>
      </time>
    </span>
</section>
<!-- paper0: Textual Echo Cancellation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.AS/paper_4.html">
      <font color="black">Textual Echo Cancellation</font>
    </a>
  </h2>
  <font color="black">さらに、テキストシーケンスは、TTS再生の生の音響信号と比較してサイズがはるかに小さく、再生が合成される前でも、デバイスとASRサーバーにすぐに送信できます。この論文では、テキストエコーキャンセレーションを提案します。 （TEC）-オーバーラップしたスピーチ録音からの音声合成（TTS）再生エコーをキャンセルするためのフレームワーク。両方のマイクの混合をとるマルチソースアテンションを備えた新しいシーケンスツーシーケンスモデルを使用して、このシステムを実装します。信号とTTS再生のソーステキストを入力として使用し、拡張オーディオを予測します。 
[要約]このシステムは、スマートスピーカーの音声認識とユーザーエクスペリエンスを大幅に向上させることができます。アコースティックエコーキャンセレーションなどの代替アプローチと比較して、インターネット通信と遅延を削減するために使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic Speech Recognition Benchmark for Air-Traffic Communications -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.AS/paper_5.html">
      <font color="black">Automatic Speech Recognition Benchmark for Air-Traffic Communications</font>
    </a>
  </h2>
  <font color="black">このペーパーは、170時間を超えるATCo音声データでトレーニングされたいくつかの最新のASRモデルの探索的ベンチマークを伝えています。スピーカーのアクセントによるクロスアクセントの欠陥は、開発されたASRシステムは、4つのデータベースにわたって7.75％の平均ワードエラー率（WER）を達成します。 
[要旨]航空管制官は、パイロットと航空管制官（atco）との間の唯一の接触方法です。前者は最も広く使用されており、後者は海洋メッセージに必須であり、一部の国内問題では制限されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-18">
        <br><font color="black">2020-06-18</font>
      </time>
    </span>
</section>
<!-- paper0: Enhancing Speech Intelligibility in Text-To-Speech Synthesis using
  Speaking Style Conversion -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.AS/paper_6.html">
      <font color="black">Enhancing Speech Intelligibility in Text-To-Speech Synthesis using
  Speaking Style Conversion</font>
    </a>
  </h2>
  <font color="black">Intelligibility in Bits（SIIB-Gauss）メジャーによって定量化された了解度の向上は、提案されたLombard-SSDRC TTSシステムが、音声整形ノイズ（SSN）で110％から130％、最先端のTTSアプローチに対する競合スピーカーノイズ（CSN）。このような制限を克服するために、タコトロンとWaveRNNベースのTTS合成を使用した新しい転移学習アプローチを提案しました。この拡張をLombard-SSDRCと呼びます。 TTSシステム。 
[ABSTRACT] lombard-ssdrc ttsが音声csnを正常に増加させました。これには、中央値のキーワード修正率でssnが455％、csnが104％含まれていました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic Quality Assessment for Audio-Visual Verification Systems. The
  LOVe submission to NIST SRE Challenge 2019 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.AS/paper_7.html">
      <font color="black">Automatic Quality Assessment for Audio-Visual Verification Systems. The
  LOVe submission to NIST SRE Challenge 2019</font>
    </a>
  </h2>
  <font color="black">最近のNIST SRE19オーディオビジュアルチャレンジデータセットで、この品質依存の融合によってもたらされた改善を示します。スコアの融合は、独立した単一モードのパーツで構成されるマルチモーダルバイオメトリクスシステムの基礎です。このために、顔と話者の両方のモダリティの自動品質評価をトレーニングできます。 
[要約]このモデルは、単峰型システムによって生成された表現の品質を推定します。これらのモデルは、スコアを強化するために使用されます-話者のレベル融合-顔認証モジュール</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Prosody Learning Mechanism for Speech Synthesis System Without Text
  Length Limit -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.AS/paper_8.html">
      <font color="black">Prosody Learning Mechanism for Speech Synthesis System Without Text
  Length Limit</font>
    </a>
  </h2>
  <font color="black">さらに、ローカル注意と呼ばれる新しい自己注意構造が、入力テキストの長さのこの制限をなくすために提案されています。ここで、シーケンスの相対位置情報は相対位置行列によってモデル化されているため、位置エンコーディングは必要ありません。 ..一方、韻律予測結果を改善するために、事前トレーニング済みの言語モデルからのテキストのセマンティック機能が導入されています。英語と北京語の実験では、モデルでより満足できる韻律を持つ音声が得られたことが示されています。 
[要約] ttsシステムに基づいて音声の韻律をモデル化するために、新しい学習形式が提案されます。音声の韻律情報は、韻律学習者によってメルスペクトラムから抽出され、音素シーケンスと組み合わせて、メル-スペクトルを再構築します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Cross attentive pooling for speaker verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.AS/paper_9.html">
      <font color="black">Cross attentive pooling for speaker verification</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、参照クエリペア全体のコンテキスト情報を利用して、ペアワイズマッチング問題の最も特徴的な情報を含む発話レベルの埋め込みを生成するCross Attentive Pooling（CAP）を提案します。VoxCelebで実験が行われます。私たちの方法が同等のプーリング戦略よりも優れたデータセットです。このホワイトペーパーの目的は、発話が「野生の」ビデオから発生し、無関係な信号を含む可能性があるテキストに依存しない話者検証です。 
[要約]スピーカーの埋め込みを生成する既存の方法は疑問です。テストはボクセレブデータセットのデータに基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Continuous Speech Separation with Conformer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.AS/paper_10.html">
      <font color="black">Continuous Speech Separation with Conformer</font>
    </a>
  </h2>
  <font color="black">この論文では、分離システムでリカレントニューラルネットワークの代わりにトランスフォーマーとコンフォーマーを使用します。これは、自己注意に基づく方法でグローバル情報をキャプチャすることが音声分離に不可欠であると考えているためです。モデルは、発話ごとの評価で双方向LSTM（BLSTM）から23.5％のワードエラーレート（WER）を削減し、連続評価で15.4％のWERを削減して、最先端の結果を達成します。会話の書き起こしなどの複雑なスピーチ関連タスクにおける重要な役割。 
[要約]より厳密な分離モデルは、混合音声から単一の話者信号を抽出します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Speaking Speed Control of End-to-End Speech Synthesis using
  Sentence-Level Conditioning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.AS/paper_11.html">
      <font color="black">Speaking Speed Control of End-to-End Speech Synthesis using
  Sentence-Level Conditioning</font>
    </a>
  </h2>
  <font color="black">発話速度値を制御するために提案されたシステムでは、発話速度値（入力音声の長さに対する入力音素の数の比率）が採用されています。さらに、高速、通常、低速のリスニングテストSCTTSは、特に低速のスピーチの場合、文全体で同じレートで持続時間を増減する他の音素持続時間制御アプローチよりも自然なスピーチを生成できることを示しました。提案されたSCTTSでは、追加のスピーチは必要ありません。よく訓練されたモデルまたは外部音声データベース。音素レベルの継続時間情報を抽出し、エンドツーエンドの方法で訓練できます。 
[ABSTRACT]提案されたscttsは、追加の十分にトレーニングされたモデルまたは外部音声データベースを必要としません。エンドツーエンドの方法でトレーニングできます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-30">
        <br><font color="black">2020-07-30</font>
      </time>
    </span>
</section>
<!-- paper0: Channel-wise Subband Input for Better Voice and Accompaniment Separation
  on High Resolution Music -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.AS/paper_12.html">
      <font color="black">Channel-wise Subband Input for Better Voice and Accompaniment Separation
  on High Resolution Music</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、周波数領域での畳み込みニューラルネットワーク（CNN）ベースの音楽ソース分離（MSS）モデル用の新しい入力フォーマットであるチャネル単位のサブバンド入力（CWS）を紹介します。比較のため、音声と伴奏の分離（VAS）を実行します。 ）異なるスケール、アーキテクチャ、CWS設定のモデルで。実験は、CWS入力が多くの面で有益であることを示しています。 
[要約]私たちはcnnベースの高解像度mssモデルの主要な問題に対処することを目指しています。提案されたアプローチは、各サブバンドでの効果的な重み共有を可能にし、チャネル間の柔軟性を高めます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-12">
        <br><font color="black">2020-08-12</font>
      </time>
    </span>
</section>
<!-- paper0: Conv-Transformer Transducer: Low Latency, Low Frame Rate, Streamable
  End-to-End Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.AS/paper_13.html">
      <font color="black">Conv-Transformer Transducer: Low Latency, Low Frame Rate, Streamable
  End-to-End Speech Recognition</font>
    </a>
  </h2>
  <font color="black">インタリーブされた畳み込み層は、パフォーマンスにとって重要な将来のコンテキストのモデリングに使用されます。計算コストを削減するために、インタリーブされた畳み込み層を使用して音響入力を徐々にダウンサンプリングします。TransformerをストリーミングASRに適したものにするために、トランスデューサフレームワークを配置を学習するためのストリーミング可能な方法。 
[ABSTRACT]元のトランスフォーマーは、エンコーダー/デコーダーアーキテクチャーを備え、オフラインasrにのみ適しています。このアーキテクチャーはconv-transformerトランスデューサーと呼ばれ、外部言語モデルなしで競争力のあるパフォーマンスを実現します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Evolutionary Algorithm Enhanced Neural Architecture Search for
  Text-Independent Speaker Verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-14/eess.AS/paper_14.html">
      <font color="black">Evolutionary Algorithm Enhanced Neural Architecture Search for
  Text-Independent Speaker Verification</font>
    </a>
  </h2>
  <font color="black">実験結果は、NASベースのモデルが最先端のスピーカー検証モデルよりも優れていることを示しています。最先端のスピーカー検証モデルは、ディープラーニング技術に基づいており、専門家またはさらに、この論文は、話者確認タスクのための有望なネットワークを自動的に発見するために、Auto-Vectorと呼ばれる進化的アルゴリズム強化ニューラルアーキテクチャ検索方法を提案します。 
[ABSTRACT]話者確認タスクにニューラルアーキテクチャ検索（nas）のアイデアを取り入れます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
