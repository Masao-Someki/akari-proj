<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-06-23の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Sound Event Localization and Detection Using Activity-Coupled Cartesian
  DOA Vector and RD3net -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/cs.SD/paper_0.html">
      Sound Event Localization and Detection Using Activity-Coupled Cartesian
  DOA Vector and RD3net
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      単一ステージシステムとして、アクティビティ結合デカルトDOAベクトル〜（ACCDOA）表現をSEDタスクとSELタスクの両方の単一ターゲットとして使用する統合トレーニングフレームワークを提案します。DCASE2020タスク〜3に提出されたシステム：サウンドイベントの定位と検出（SELD）については、このレポートで説明します。モデルを一般化するために、3つのデータ拡張手法を適用します。 SpecAugmentの拡張。 
[ABSTRACT]システムは、sedタスクとselタスクを個別に組み合わせて機能し、後でそれらの結果を組み合わせます。サウンドイベントの場所とアクティビティを効率的に推定するには、rd3netをさらに提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-22">
        <br>2020-06-22
      </time>
    </span>
  </h3>
</article>
<!-- paper0: MuSe 2020 -- The First International Multimodal Sentiment Analysis in
  Real-life Media Challenge and Workshop -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/cs.SD/paper_1.html">
      MuSe 2020 -- The First International Multimodal Sentiment Analysis in
  Real-life Media Challenge and Workshop
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      MuSe 2020の目的は、さまざまな分野のコミュニティをまとめることです。主に、視聴覚感情認識コミュニティ（信号ベース）、および感情分析コミュニティ（シンボルベース）です。3つの異なるサブチャレンジを提示します。MuSe-Wildは、継続的な感情（覚醒および価数）予測に焦点を当てています; MuSe-Topic。参加者はドメイン固有のトピックを3クラス（低、中、高）感情のターゲットとして認識します。信頼性の新しい側面が予測されるMuSe-Trust。各サブチャレンジでは、参加者の競争力のあるベースラインが設定されます。つまり、テストでは、MuSe-Wildの結合した（価数と覚醒）CCCが2568、MuSe-Topicのスコア（0.34 $ \ cdot $ UAR + 0.66 $ \ cdot $ F1として計算）が76.78％ 10クラスのトピック、3クラスの感情予測では40.64％、MuSe-Trustでは.4359のCCC。 
[ABSTRACT] muse-車はその種の最初の-より包括的なデータベースです。これは、課題に使用されます。また、8.museの目的とは、さまざまな分野のコミュニティをまとめることです
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-30">
        <br>2020-04-30
      </time>
    </span>
  </h3>
</article>
<!-- paper0: FastSpeech 2: Fast and High-Quality End-to-End Text to Speech -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/cs.SD/paper_2.html">
      FastSpeech 2: Fast and High-Quality End-to-End Text to Speech
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、1）FastSpeech 2および2sが音声品質でFastSpeechよりもはるかに簡素化されたトレーニングパイプラインと短縮されたトレーニング時間を示しています。 2）FastSpeech 2および2sは、推論速度を大幅に向上させながら、自己回帰モデルの音声品質に一致させることができます。FastSpeech2sをさらに設計します。これは、テキストから音声波形を直接並列生成する最初の試みであり、フルエンドの利点を享受します。トレーニングを終了し、FastSpeechよりもさらに高速に推論します。ただし、FastSpeechにはいくつかの欠点があります。1）教師と生徒の蒸留パイプラインが複雑である、2）教師モデルから抽出された期間が十分に正確ではなく、ターゲットのメルスペクトログラム教師モデルから抽出されたデータは、データの単純化による情報の損失に悩まされますが、どちらも音声品質を制限します。 
[要約] fastspeechモデルのトレーニングは、継続時間予測のための自己回帰教師モデルに依存しています。fastspeech2は、テキストから音声波形を並列で直接生成する最初の試みです。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-08">
        <br>2020-06-08
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: EPIC30M: An Epidemics Corpus Of Over 30 Million Relevant Tweets -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/cs.CL/paper_0.html">
      EPIC30M: An Epidemics Corpus Of Over 30 Million Relevant Tweets
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      EPIC30Mには、エボラ出血熱、コレラ、豚インフルエンザに関連する3つの一般的な疾患に関連する2,620万件のツイートのサブセットと、2009年のH1N1豚インフルエンザ、2010年のハイチコレラ、2012年中東呼吸器を含む6つの世界的な流行の470万件のツイートのサブセットが含まれています。シンドローム（MERS）、2013年西アフリカエボラ、2016年イエメンコレラ、2018年キブエボラ..COVID-19関連の研究に関する他の取り組みの間に、このようなクロスをサポートするのに十分な規模で豊富な疾患関連のコーパスを文献でほとんど発見していません-流行分析タスク..このペーパーでは、2006年から2020年までに、Twitterからクロールされたツイートである3,000万件のマイクロブログ投稿を含む大規模な流行コーパスEPIC30Mを紹介します。
[ABSTRACT]研究者は、より多くのベンチマークコーパスを必要とします相互流行パターンと傾向分析タスクを容易にする他の流行が含まれています。epic30mは、3,000万件のマイクロブログ投稿、つまり、twitterからクロールされたツイート、fを含む大規模な流行コーパスです。 2006年から2020年まで
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-09">
        <br>2020-06-09
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Word Sense Disambiguation using Knowledge-based Word Similarity -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/cs.CL/paper_1.html">
      Word Sense Disambiguation using Knowledge-based Word Similarity
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      WSDシステムの有効性を検証するために、5つのベンチマーク英語WSDコーパス（Senseval-02、Senseval-03、SemEval-07、SemEval-13、およびSemEval-15）で実験を行いました。この問題に対処するために、新しい知識ベースのWSDシステム。自然言語処理では、単語の意味の曖昧性解消（WSD）は、特定のコンテキストで単語の正しい意味を識別することに関連する未解決の問題です。 
[要約]システムに加えて、新しい知識ベースのwsdシステムを導入します。「視覚的表現」という単語をエンコードする新しい方法を提案します。システムは既存の知識ベースシステムよりも優れ、同等のパフォーマンスを示しました状態-最新の監視付きwsdシステム
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-11">
        <br>2019-11-11
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Clinical Predictive Keyboard using Statistical and Neural Language
  Modeling -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/cs.CL/paper_2.html">
      Clinical Predictive Keyboard using Statistical and Neural Language
  Modeling
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      神経言語モデルは、放射線医学レポートで正確に51.3％の精度を達成できることを示します（2つの単語のうちの1つは正しく予測されます）。また、モデルが頻繁な単語に対してのみ使用されている場合でも、医師が貴重な時間を節約できることを示します..しかし、医師への支援とともに、患者の退院を促進するコンピュータベースのシステムは、病院の感染を減らすのにも役立ちます。 
[ABSTRACT]言語モデルは、オーサリング中に医師を支援するために非常に小規模でのみ適用されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-22">
        <br>2020-06-22
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Novel Cascade Binary Tagging Framework for Relational Triple
  Extraction -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/cs.CL/paper_3.html">
      A Novel Cascade Binary Tagging Framework for Relational Triple
  Extraction
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      重複するトリプルのさまざまなシナリオの詳細な分析は、メソッドがこれらすべてのシナリオにわたって一貫したパフォーマンスの向上を提供することを示しています。実験により、CasRelフレームワークは、エンコーダーモジュールがランダムに初期化されたBERTを使用する場合でも、最先端のメソッドよりも優れていることが示されていますエンコーダー、新しいタグ付けフレームワークの威力を示します。以前の作品のように関係を個別のラベルとして扱う代わりに、新しいフレームワークは関係をモデル内のオブジェクトにマップする関数として関係をモデル化し、自然に重複問題を処理します。 
[ABSTRACT]新しいシステムは事前トレーニング済みの条件付きエンコーダーを使用します。最も強いベースラインよりも17.5および30優れています。2f1の絶対ゲイン
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-07">
        <br>2019-09-07
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Efficient text generation of user-defined topic using generative
  adversarial networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/cs.CL/paper_4.html">
      Efficient text generation of user-defined topic using generative
  adversarial networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最初の弁別子は、複数のLSTMで構成される段落レベルの情報と文の構文構造を学習するようにジェネレーターをガイドすることを目的としています。次に、テキスト生成のトピックまたは感情が変更された場合、2番目の弁別子はジェネレーターで再トレーニングされます。 ..トピックの関連性を決定するために、TF-IDFと長さペナルティに基づくコサイン類似度が採用されます。 
[要約]提案された方法は、他よりも短い時間でテキストを生成することができます。生成されたテキストはユーザーに関連しています-定義されたトピックと感情
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-22">
        <br>2020-06-22
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Modeling Global and Local Node Contexts for Text Generation from
  Knowledge Graphs -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/cs.CL/paper_5.html">
      Modeling Global and Local Node Contexts for Text Generation from
  Knowledge Graphs
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      グローバルノードエンコーディングにより、2つの離れたノード間の明示的な通信が可能になり、すべてのノードが直接接続されているため、グラフのトポロジーが無視されます。私たちの実験では、2つのグラフからテキストへのデータセットで私たちのアプローチがBLEUスコア18.01 AGENDAデータセット、およびWebNLGデータセットの63.69は見られたカテゴリであり、それぞれ最新のモデルより3.7ポイントおよび3.1ポイント優れています。最近のグラフからテキストへのモデルは、グローバルまたはローカルを使用してグラフベースのデータからテキストを生成します。ノード表現を学習するための集約。 
[ABSTRACT]ソーシャルネットワークを使用すると、ユーザーは相互の友達に関する情報を共有できます。これにより、ソーシャルネットワークにアクセスできます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-29">
        <br>2020-01-29
      </time>
    </span>
  </h3>
</article>
<!-- paper0: MuSe 2020 -- The First International Multimodal Sentiment Analysis in
  Real-life Media Challenge and Workshop -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/cs.CL/paper_6.html">
      MuSe 2020 -- The First International Multimodal Sentiment Analysis in
  Real-life Media Challenge and Workshop
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      サブチャレンジごとに、参加者の競争力のあるベースラインが設定されます。つまり、テストでは、MuSe-Wildの結合した（価数と覚醒）CCCが2568、MuSe-Topicのスコア（0.34 $ \ cdot $ UAR + 0.66 $ \ cdot $ F1として計算）が76.78％ 10クラスのトピックと3クラスの感情予測に関する40.64％、およびMuSe-Trustの場合、.4359のCCC。このホワイトペーパーでは、MuSe-CaRに関する詳細情報を提供します。挑戦に使用されるデータベース、および最先端の機能と適用されたモデリングアプローチ。3つの異なるサブチャレンジを提示します。MuSe-Wildは、継続的な感情（覚醒と価数）の予測に焦点を当てています; MuSe-Topic。参加者はドメイン固有のトピックを3クラス（低、中、高）感情のターゲットとして認識します。 MuSe-Trustでは、信頼性の新しい側面が予測されます。 
[ABSTRACT] muse-車はその種の最初の-より包括的なデータベースです。これは、課題に使用されます。また、8.museの目的とは、さまざまな分野のコミュニティをまとめることです
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-30">
        <br>2020-04-30
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Cluster Ranking Model for Full Anaphora Resolution -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/cs.CL/paper_7.html">
      A Cluster Ranking Model for Full Anaphora Resolution
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの貢献は次のとおりです。このホワイトペーパーでは、非参照式（虚辞、述語、およびその他の型を含む）を同時に識別し、シングルトンを含む共参照チェーンを構築するアーキテクチャを紹介します。3番目に、モデルにもかかわらず、 CONLLデータ用に特別に設計されているわけではないため、Kantor and Globerson（2019）によるそのデータセットの最新システムと同等のスコアを達成しています。 
[ABSTRACT] crac 2018共有タスクのデータセットをその目的で使用できるようになりました。ただし、シングルトンクラスターと非参照式の可用性により、非シングルトンのパフォーマンスが向上することを示しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-21">
        <br>2019-11-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: FastSpeech 2: Fast and High-Quality End-to-End Text to Speech -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/cs.CL/paper_8.html">
      FastSpeech 2: Fast and High-Quality End-to-End Text to Speech
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、1）FastSpeech 2および2sが音声品質でFastSpeechよりもはるかに簡素化されたトレーニングパイプラインと短縮されたトレーニング時間を示しています。 2）FastSpeech 2および2sは、推論速度を大幅に向上させながら、自己回帰モデルの音声品質に一致させることができます。FastSpeech2sをさらに設計します。これは、テキストから音声波形を直接並列生成する最初の試みであり、フルエンドを終了するトレーニングとFastSpeechよりもさらに高速な推論..このホワイトペーパーでは、FastSpeechの問題に対処し、TTSでの1対多マッピングの問題を1）で直接解決するFastSpeech 2を提案します。教師からの単純化された出力の代わりに真実のターゲット、および2）条件付き入力として音声のより多くのバリエーション情報（たとえば、ピッチ、エネルギー、およびより正確な継続時間）を導入します。 
[要約] fastspeechモデルのトレーニングは、継続時間予測のための自己回帰教師モデルに依存しています。fastspeech2は、テキストから音声波形を並列で直接生成する最初の試みです。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-08">
        <br>2020-06-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal
  Transformers -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/cs.CL/paper_9.html">
      Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal
  Transformers
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      下流のタスクにより適切な表現を提供するために、ビジュアルゲノムデータセットとMS-COCOデータセットからの画像と文のペアを使用して、ユニバーサルエンドツーエンドモデルを事前トレーニングします。特に、単一モデルのパフォーマンスを向上させます。 VQAタスクは、SOTAと比較して2.17ポイントが公平に比較されています。事前トレーニング済みのモデルを使用したダウンストリームタスクに関する広範な実験では、このアプローチがVisual Question Answering（VQA）を含む、ダウンストリームタスクの最先端技術であることが示されています。画像テキスト検索、リアルのための視覚推論のための自然言語（NLVR）。 
[要旨]最新のビジョンと言語のタスクとして、領域ベースの画像機能を使用するのではなく、画像と文のペアから直接、画像のピクセルと言語の意味のより詳細な接続を構築することを目指しています。また、境界ボックスの注釈のコストを軽減し、視覚課題の意味ラベルと言語の意味の不均衡を克服
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Students Need More Attention: BERT-based AttentionModel for Small Data
  with Application to AutomaticPatient Message Triage -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/cs.CL/paper_10.html">
      Students Need More Attention: BERT-based AttentionModel for Small Data
  with Application to AutomaticPatient Message Triage
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      具体的には、（i）LESA-BERTと呼ばれるBERTの各層に自己注意のためのラベル埋め込みを導入し、（ii）LESA-BERTをより小さなバリアントに蒸留することにより、小さなデータセットで作業するときに過剰適合とモデルサイズを削減することを目指します。 。アプリケーションとして、私たちのフレームワークを使用して、メッセージの緊急度を3つのカテゴリー（非緊急、中、緊急）に分類する患者ポータルメッセージトリアージのモデルを構築します。ヘルスケアで一般的に見られる小規模で不均衡なデータセットは、分類子をトレーニングする際の課題を表します深層学習モデルに基づいています。 
[要約]このプロジェクトのコードはgithubで公開されています。 com
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-22">
        <br>2020-06-22
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Relation Adversarial Network for Low Resource Knowledge Graph Completion -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/cs.CL/paper_11.html">
      Relation Adversarial Network for Low Resource Knowledge Graph Completion
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ただし、KGではリソースの少ない関係が非常に一般的であり、新しく追加された関係には、トレーニング用の既知のサンプルがあまりないことがよくあります。加重関係の敵対的ネットワークと呼ばれる一般的なフレームワークを提案します。高いリソース関係から異なるが関連する低いリソース関係へ..ナレッジグラフ補完（KGC）は、リンク予測または関係抽出を介して失われた接続を埋めることによってナレッジグラフを改善するために提案されました。 
[ABSTRACT]低リソースの関係はkgsで非常に一般的であり、新しく追加された文にはトレーニング用の既知のサンプルがあまりありません。これらの新しく追加された接続には、多くのサンプルがありません。調査では、提案されたアプローチが低に関する以前の方法よりも優れていることが示されていますリンク予測と関係抽出の両方のリソース設定
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-08">
        <br>2019-11-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: SOLOIST: Few-shot Task-Oriented Dialog with A Single Pre-trained
  Auto-regressive Model -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/cs.CL/paper_12.html">
      SOLOIST: Few-shot Task-Oriented Dialog with A Single Pre-trained
  Auto-regressive Model
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの実験は、（i）SOLOISTが2つの有名なベンチマーク、CamRestとMultiWOZで新しい最先端の結果を作成することを示しています。（ii）少数ショット学習設定で、SOLOISTが開発したダイアログシステムは、開発されたダイアログシステムを大幅に上回っています。既存の方法によって、および（iii）マシンティーチングを使用すると、ラベリングコストが大幅に削減されます。ユーザーの目標と現実の世界に基づいたダイアログ応答を生成できる大規模なトランスフォーマーモデルである、大規模な異種ダイアログコーパスを事前トレーニングします。タスク完了のための知識。事前トレーニング済みのモデルを効率的に適応させて、マシンティーチングを介して少数のタスク固有のダイアログで新しいダイアログタスクを実行できます。 
[要約]トランスフォーマーベースの自動回帰言語設定は、新しいツールを使用します。異なるダイアログモジュールを単一のニューラルモデルに包含します。このモデルは、新しいタスクを生成するために効率的に適応できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-11">
        <br>2020-05-11
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Neural Mention Detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/cs.CL/paper_13.html">
      Neural Mention Detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの最高のモデル（biaffine分類器を使用）は、HIGH RECALL共参照アノテーション設定の強力なベースラインと比較した場合、言及想起で最大1.8パーセントのポイントを達成します。同じモデルは、最大5.3および6.2 ppの改善を実現します。モデルは最大1.7および0.7 ppの絶対的な改善を達成しました
[要約]最高のモデル（ビアフィン分類器を使用）は最大1. 8のゲインを達成しました。 。7 p ..ネストされたnerの場合、genia corporaでのモデルの評価は、このタスク用に特別に設計されていなくても、モデルが最新のモデルと一致または優れていることを示しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-07-29">
        <br>2019-07-29
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Adding A Filter Based on The Discriminator to Improve Unconditional Text
  Generation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/cs.CL/paper_14.html">
      Adding A Filter Based on The Discriminator to Improve Unconditional Text
  Generation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このように、元の生成分布は修正されて不一致が減少します。弁別器はジェネレータよりも多くの情報をエンコードできるため、弁別器はジェネレータを改善する可能性があります。失敗の重要な理由は、弁別器入力とALMの違いです入力。 
[ABSTRACT]生成されたテキストは依然として低品質と多様性に苦しんでいます。ディスクリミネーターはこの不一致を検出できます。これは露出バイアスが原因です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-05">
        <br>2020-04-05
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Musical Smart City: Perspectives on Ubiquitous Sonification -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/eess.AS/paper_0.html">
      Musical Smart City: Perspectives on Ubiquitous Sonification
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このホワイトペーパーでは、最初にスマートシティパラダイムを紹介します。最後に、ユビキタスソニフィケーションというタイトルのアプローチを提案し、ユーザーと都市のデータを利用して行動を通知する、音楽スマートシティシステムの投機的ユースケースの初期設計を紹介します。次に、ユビキタスおよびモバイルミュージックのトピックをカバーし、続いてソニフィケーションリサーチの概要を説明します。 
[ABSTRACT]ソニフィケーションは、スマートシティのデータを解釈し、新しい音楽体験を生み出すための有望な方法です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-22">
        <br>2020-06-22
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Sound Event Localization and Detection Using Activity-Coupled Cartesian
  DOA Vector and RD3net -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/eess.AS/paper_1.html">
      Sound Event Localization and Detection Using Activity-Coupled Cartesian
  DOA Vector and RD3net
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      シングルステージシステムとして、アクティビティ結合デカルトDOAベクトル〜（ACCDOA）表現をSEDタスクとSELタスクの両方の単一ターゲットとして使用する統合トレーニングフレームワークを提案します。モデルを一般化するために、3つのデータ拡張を適用します。手法：等化混合データの増強〜（EMDA）、1次アンビソニック〜（FOA）信号の回転、SpecAugmentのマルチチャネル拡張。DCASE2020タスクに提出された当社のシステム〜3：サウンドイベントのローカリゼーションと検出（SELD）について説明します。このレポートで。 
[ABSTRACT]システムは、sedタスクとselタスクを個別に組み合わせて機能し、後でそれらの結果を組み合わせます。サウンドイベントの場所とアクティビティを効率的に推定するには、rd3netをさらに提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-22">
        <br>2020-06-22
      </time>
    </span>
  </h3>
</article>
<!-- paper0: MuSe 2020 -- The First International Multimodal Sentiment Analysis in
  Real-life Media Challenge and Workshop -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/eess.AS/paper_2.html">
      MuSe 2020 -- The First International Multimodal Sentiment Analysis in
  Real-life Media Challenge and Workshop
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちは3つの異なるサブチャレンジを提示します。MuSe-Wildは、継続的な感情（覚醒と価数）の予測に焦点を当てています。 MuSe-Topic。参加者はドメイン固有のトピックを3クラス（低、中、高）感情のターゲットとして認識します。 MuSe-Trustでは、信頼性の新しい側面が予測されます。MuSe2020の目的は、さまざまな分野のコミュニティをまとめることです。主に、視聴覚感情認識コミュニティ（信号ベース）と感情分析コミュニティ（記号ベース）です。このホワイトペーパーでは、この種の野生で最初のMuSe-CaRに関する詳細情報を提供します。挑戦のために利用されるデータベース、ならびに適用される最先端の機能およびモデリングアプローチ。 
[ABSTRACT] muse-車はその種の最初の-より包括的なデータベースです。これは、課題に使用されます。また、8.museの目的とは、さまざまな分野のコミュニティをまとめることです
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-30">
        <br>2020-04-30
      </time>
    </span>
  </h3>
</article>
<!-- paper0: FastSpeech 2: Fast and High-Quality End-to-End Text to Speech -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/eess.AS/paper_3.html">
      FastSpeech 2: Fast and High-Quality End-to-End Text to Speech
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、1）FastSpeech 2および2sが音声品質でFastSpeechよりもはるかに簡素化されたトレーニングパイプラインと短縮されたトレーニング時間を示しています。 2）FastSpeech 2および2sは、推論速度を大幅に向上させながら、自己回帰モデルの音声品質に一致させることができます。FastSpeech2sをさらに設計します。これは、テキストから音声波形を直接並列生成する最初の試みであり、フルエンドの利点を享受します。トレーニングを終了し、FastSpeechよりもさらに高速に推論します。ただし、FastSpeechにはいくつかの欠点があります。1）教師と生徒の蒸留パイプラインが複雑である、2）教師モデルから抽出された期間が十分に正確ではなく、ターゲットのメルスペクトログラム教師モデルから抽出されたデータは、データの単純化による情報の損失に悩まされますが、どちらも音声品質を制限します。 
[要約] fastspeechモデルのトレーニングは、継続時間予測のための自己回帰教師モデルに依存しています。fastspeech2は、テキストから音声波形を並列で直接生成する最初の試みです。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-08">
        <br>2020-06-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Self-Supervised Representations Improve End-to-End Speech Translation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-23/eess.AS/paper_4.html">
      Self-Supervised Representations Improve End-to-End Speech Translation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      エンドツーエンドのスピーチからテキストへの翻訳は、よりシンプルで小さなシステムを提供できますが、データ不足の課題に直面しています。私たちは、自己管理型の事前トレーニング済み機能により、一貫して翻訳パフォーマンスとクロスリンガル転送を向上できることを示していますチューニングなしで、またはほとんどチューニングせずに、さまざまな言語に拡張できます。事前トレーニング方法では、ラベルなしデータを活用でき、データ不足の設定で効果的であることが示されています。 
[ABSTRACT]事前-ラベル付けされていないデータを活用できるトレーニング方法。これらは、データ-不十分な設定で効果的であることが示されています。また、安全で安全な音声を保護するためにも使用できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-22">
        <br>2020-06-22
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="">
      
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本日更新された論文はありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="">
        <br>
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
