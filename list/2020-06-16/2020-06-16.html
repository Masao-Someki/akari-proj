<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-06-16の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Mel-spectrogram augmentation for sequence to sequence voice conversion -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-16/cs.SD/paper_0.html">
      Mel-spectrogram augmentation for sequence to sequence voice conversion
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果では、時間軸ワーピングベースのポリシー（つまり、時間の長さの制御と時間ワーピング）。シーケンスからシーケンスへの音声変換モデルをトレーニングするには、同じ発話で構成される音声ペアの数に関するデータが不十分であるという問題を処理する必要があります。これらの結果は、メルスペクトログラム拡張の使用がより有益であることを示しています。 VCモデルのトレーニング用。 
[ABSTRACT]この研究では、メル-スペクトログラム拡張がシステムを最初からトレーニングする効果を実験的に調査しました。スペクトログラム拡張は、さまざまなサイズのトレーニングセットと拡張ポリシーに基づいています。さらに、データバリエーションを増やすための新しいポリシーを提案しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-06">
        <br>2020-01-06
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Voice Separation with an Unknown Number of Multiple Speakers -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-16/cs.SD/paper_1.html">
      Voice Separation with an Unknown Number of Multiple Speakers
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの方法は、現在の技術水準を大幅に上回っています。これは、私たちが示すように、2人を超える話者に対して競争力がありません。可能なモデルの数ごとに異なるモデルがトレーニングされ、話者の数が最も多いモデルは特定のサンプルの実際の話者数を選択するために使用されます。複数の音声が同時に話す混合オーディオシーケンスを分離する新しい方法を紹介します。 
[要旨]新しい方法は、複数の処理ステップで音声を分離するように訓練されているゲート付きニューラルネットワークを採用し、各出力チャネルのスピーカーを固定したままにします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-29">
        <br>2020-02-29
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Robust Estimation of Hypernasality in Dysarthria with Acoustic Model
  Likelihood Features -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-16/cs.SD/paper_2.html">
      Robust Estimation of Hypernasality in Dysarthria with Acoustic Model
  Likelihood Features
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらの音響モデルから派生した機能が高鼻腔音声に固有であることを示すために、異なる構音障害コーパス全体でそれらを評価します。ここでは、これらの補完的な次元をキャプチャする新しい音響機能のセットを提案します。機能は、トレーニングされた2つの音響モデルに基づいています。健康なスピーチの大きなコーパス。 
[ABSTRACT] hypernasalityは、低周波数に追加の共鳴を導入します。hypernasalに基づいて、鼻腔から空気が漏れるため、調音の精度が低下します。これらの機能は、1つの疾患からの高鼻腔の高鼻声のトレーニングでも一般化します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-26">
        <br>2019-11-26
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Towards Hierarchical Importance Attribution: Explaining Compositional
  Semantics for Neural Sequence Models -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-16/cs.CL/paper_0.html">
      Towards Hierarchical Importance Attribution: Explaining Compositional
  Semantics for Neural Sequence Models
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      コンテキスト分解、目的のプロパティを数学的に満たしていないため、さまざまなモデルで説明の品質に一貫性がありません。自然言語処理タスクでのニューラルネットワークの印象的なパフォーマンスは、複雑な単語や句の構成をモデル化する能力に起因します。モデルによって取得された意味構成、分類ルールを抽出し、モデルに対する人間の信頼を向上させます。 
[要約]モデルがセマンティック構成をどのように処理するかを説明するために、進化関数を研究します。各単語やフレーズの重要性を定量化するための正式で一般的な方法を提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-08">
        <br>2019-11-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Information Extraction of Clinical Trial Eligibility Criteria -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-16/cs.CL/paper_1.html">
      Information Extraction of Clinical Trial Eligibility Criteria
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      臨床試験では、患者の人口統計から食物アレルギーに至るまで、さまざまな基準で被験者の適格性を予測します。問題を新しい知識ベースの母集団タスクとして組み立て、機械学習と文脈自由文法を組み合わせたソリューションを実装します。リソースとコアをリリースしますGitHub上のシステムのコンポーネント。 
[要約]私たちは、臨床試験での試験からの根拠基準に関する情報抽出（つまり）アプローチを調査します。共有ナレッジベースに移動
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-12">
        <br>2020-06-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Highway Transformer: Self-Gating Enhanced Self-Attentive Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-16/cs.CL/paper_2.html">
      Highway Transformer: Self-Gating Enhanced Self-Attentive Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      擬似情報ハイウェイを通じて、LSTMスタイルのゲーティングユニットを組み込んだゲートコンポーネントセルフディペンデンシーユニット（SDU）を導入し、個々の表現の多次元潜在スペース内の内部セマンティックの重要性を補充します。補助的なコンテンツベースのSDUゲートにより、スキップされた接続を介して変調された潜在的埋め込みの情報フローのために、勾配降下アルゴリズムで収束速度の明確なマージンが得られます。SDUゲートを仮定して、コンテキストベースのトランスフォーマーモジュールを支援するゲーティングメカニズムの役割を明らかにします。特に浅いレイヤーでは、最適化プロセス中に次善のポイントに向けてステップが速くなる可能性があります。 
[要旨] lstmスタイルのゲーティングユニットを組み込んだゲートコンポーネント自己依存性ユニット（sdu）は、内部のセマンティックの重要性を補充します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-17">
        <br>2020-04-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Generating Diverse and Consistent QA pairs from Contexts with
  Information-Maximizing Hierarchical Conditional VAEs -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-16/cs.CL/paper_3.html">
      Generating Diverse and Consistent QA pairs from Contexts with
  Information-Maximizing Hierarchical Conditional VAEs
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      生成されたQAペアのみを使用してQAモデル（BERTベース）のパフォーマンスを評価するか（QAベースの評価）、または生成されたものと両方を使用して、いくつかのベンチマークデータセットで情報最大化階層型条件付き変分オートエンコーダ（Info-HCVAE）を検証します最先端のベースラインモデルに対する、トレーニング用の人間がラベル付けしたペア（半教師あり学習）。問題に取り組む別のアプローチは、問題のコンテキストまたは大量の非構造化のいずれかから自動的に生成されたQAペアを使用することです。テキスト（例：結果は、トレーニングのためにデータの一部のみを使用して、モデルが両方のタスクのすべてのベースラインで印象的なパフォーマンスの向上を得ていることを示しています。
[要約]システムは、人間ベースのオートエンコーダの数に基づいています。これらのオートエンコーダは、非構造化テキストのqaペアを作成する
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-28">
        <br>2020-05-28
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Incorporating Uncertain Segmentation Information into Chinese NER for
  Social Media Text -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-16/cs.CL/paper_4.html">
      Incorporating Uncertain Segmentation Information into Chinese NER for
  Social Media Text
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      不確実な単語セグメンテーション情報をエンコードし、適切な単語レベルの表現を取得するために、三部作（つまり、候補位置埋め込み-&gt;位置選択的注意-&gt;適応型単語畳み込み）を提案します。エラーが効果的にカスケードし、以前の最先端の方法と比べて2％以上の大幅なパフォーマンスの向上を達成します。しかし、セグメンテーションエラーの伝播は、ソーシャルメディアテキストのような口語的なデータを処理する際の中国のNERにとって課題です。 
[ABSTRACT]中国のnerのソーシャルメディアコーパスの代表は、セグメンテーションエラー通信が中国のnerへの挑戦であることを発見しました。ただし、このモデルは、セグメンテーションエラーのカスケードのトラブルを効果的に軽減し、以前の状態より2％以上の大幅なパフォーマンスの向上を実現します-アートメソッドの
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-14">
        <br>2020-04-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Global Voices: Crossing Borders in Automatic News Summarization -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-16/cs.CL/paper_5.html">
      Global Voices: Crossing Borders in Automatic News Summarization
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      クロスリンガル要約における翻訳品質の影響を調査し、翻訳してから要約するアプローチをいくつかのベースラインと比較します。グローバルボイスのニュース記事のソーシャルネットワークの説明を抽出して、英語から英語および英語からの評価データを安価に収集します15言語での要約..データセットは、https：//forms.gle/gpkJDT6RJWHM1Ztz9からダウンロードできます。 
[ABSTRACT]私たちはソーシャルを抽出します-グローバルボイスのニュース記事のネットワークの説明.forに-英語およびfrom-15言語での英語の要約
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-01">
        <br>2019-10-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Counterfactual VQA: A Cause-Effect Look at Language Bias -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-16/cs.CL/paper_6.html">
      Counterfactual VQA: A Cause-Effect Look at Language Bias
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちが提案する因果関係の外観は、1）すべてのベースラインVQAアーキテクチャに一般的であり、2）言語バイアスの影響を受けやすいVQA-CPデータセットで大幅な改善を達成し、3）最近の言語の以前のベースの作業における理論的なギャップを埋めます。想像上のシナリオでは画像が存在しなかった、事実に反するVQAによってキャプチャされます。視覚的質問応答（VQA）モデルは言語バイアスに依存する傾向があるため、視覚的知識から推論を学ぶことができません。 VQA。 
[要約]論文では、言語バイアスの新しい因果関係の見方を提案します。バイアスは、因果的結論の観点からの質問に対する回答への直接的な影響として設計されています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-08">
        <br>2020-06-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Early Detection of Social Media Hoaxes at Scale -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-16/cs.CL/paper_7.html">
      Early Detection of Social Media Hoaxes at Scale
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちのデータセットは、将来の研究でベンチマークとしてさらに使用するためにリリースする、真実、記念、および虚偽の物語の実際の分布を伴う現実的なシナリオを表しています。これにより、1300万を超えるツイート、15％を含む4,007レポートのデータセットを作成できます。デマの自動検出に関する既存の研究では、ラベルの付いたデータを取得することが困難なため、比較的小さなデータセットを使用するという制限があります。 
[要約]半自動化された方法は、wikidata知識ベースを使用して、有名人の死亡報告に焦点を当て、真実性分類のための大規模なデータセットを構築します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-01-22">
        <br>2018-01-22
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Regularized Forward-Backward Decoder for Attention Models -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-16/eess.AS/paper_0.html">
      Regularized Forward-Backward Decoder for Attention Models
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      小さいTEDLIUMv2と大きいLibriSpeechデータセットでアプローチを評価し、両方で一貫した改善を達成します。ただし、ほとんどの場合、デコーダーを無視します。これはトレーニング中にのみ追加されるため、ネットワークの基本構造やデコード中に複雑さを追加します。 
[ABSTRACT]多くの研究は、これらのモデルのパフォーマンスを向上させるために、主にエンコーダー構造またはアテンションモジュールに焦点を当てています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-15">
        <br>2020-06-15
      </time>
    </span>
  </h3>
</article>
<!-- paper0: COALA: Co-Aligned Autoencoders for Learning Semantically Enriched Audio
  Representations -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-16/eess.AS/paper_1.html">
      COALA: Co-Aligned Autoencoders for Learning Semantically Enriched Audio
  Representations
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      埋め込みモデルの品質を評価し、3つの異なるタスク（つまり、サウンドイベントの認識、音楽のジャンルと楽器の分類）での特徴抽出としてのパフォーマンスを測定し、モデルがキャプチャする特性のタイプを調査します。我々の方法は、考慮されたタスクの最先端技術と同等であり、我々の方法で生成された埋め込みは、いくつかの音響記述子と十分に相関している。この論文では、音声表現を学習する方法を提案し、音声と関連タグの潜在表現を学習しました。 
[要約]埋め込みモデルの品質を評価し、3つの異なるタスクでの特徴抽出としてのパフォーマンスを測定し、モデルがキャプチャする特性のタイプを調査します。また、モデルがキャプチャする特性のタイプを確認します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-15">
        <br>2020-06-15
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Voice Separation with an Unknown Number of Multiple Speakers -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-16/eess.AS/paper_2.html">
      Voice Separation with an Unknown Number of Multiple Speakers
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      複数の音声が同時に話す混合オーディオシーケンスを分離するための新しい方法を紹介します。この方法は、現在の技術水準を大幅に上回っています。これは、私たちが示すように、3人以上のスピーカーでは競争力がありません。異なるモデルは、可能性のある話者の数ごとにトレーニングされ、話者数が最大のモデルを使用して、所定のサンプルの実際の話者数を選択します。 
[要旨]新しい方法は、複数の処理ステップで音声を分離するように訓練されているゲート付きニューラルネットワークを採用し、各出力チャネルのスピーカーを固定したままにします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-29">
        <br>2020-02-29
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Emotion Recognition in Audio and Video Using Deep Neural Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-16/eess.AS/paper_3.html">
      Emotion Recognition in Audio and Video Using Deep Neural Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      音声、テキスト、視覚..さまざまなアーキテクチャを検討した結果、（CNN + RNN）+ 3DCNNマルチモデルアーキテクチャが見つかり、オーディオスペクトログラムと対応するビデオフレームを処理して、4つの感情で54.0％、3つの感情で71.75％の感情予測精度が得られます。 IEMOCAP 
[2]データセット..精度を向上させるには、まだ多くの課題があります。 
[要約]ディープラーニングテクノロジーの感情認識の精度と待ち時間が改善されました。感情認識の精度を改善する方法があります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-15">
        <br>2020-06-15
      </time>
    </span>
  </h3>
</article>
<!-- paper0: An Iterative Graph Spectral Subtraction Method for Speech Enhancement -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-16/eess.AS/paper_4.html">
      An Iterative Graph Spectral Subtraction Method for Speech Enhancement
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、提案された演算子がグラフ音声信号に適していることを示しており、提案された方法は、両方の信号対雑音比（SNR）の点で、従来の基本スペクトル減算（BSS）法および反復基本スペクトル減算（IBSS）法よりも優れています）と平均音声品質の知覚評価（PESQ）。さらに、GSSに基づいて、音声強調性能をさらに改善するために反復グラフスペクトル減算（IGSS）法を提案します。グラフ音声のスペクトルとグラフノイズ信号については、ノイズの多いスピーチのノイズ干渉を抑制するためのグラフスペクトル減算（GSS）法をさらに提案します。 
[要約]最初に一連のシフト演算子を提案し、グラフ音声信号を作成します。次に、スペクトルのスペクトルを分析して分析します。この方法は、音声強調のパフォーマンスをさらに向上させるためのものです。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-15">
        <br>2020-06-15
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Robust Estimation of Hypernasality in Dysarthria with Acoustic Model
  Likelihood Features -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-16/eess.AS/paper_5.html">
      Robust Estimation of Hypernasality in Dysarthria with Acoustic Model
  Likelihood Features
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらの音響モデルから派生した機能が高鼻腔発話に固有であることを示すために、異なる構音障害コーパス全体でそれらを評価します。機械化学習に基づくメトリックは過剰適合する傾向があるのに対し、設計された機能はしばしば高鼻音に関連する複雑な音響パターンをキャプチャできません。訓練されている小さな疾患固有の音声データセット。高鼻音は、多くの運動音声障害に共通の特徴的な症状です。 
[ABSTRACT] hypernasalityは、低周波数に追加の共鳴を導入します。hypernasalに基づいて、鼻腔から空気が漏れるため、調音の精度が低下します。これらの機能は、1つの疾患からの高鼻腔の高鼻声のトレーニングでも一般化します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-26">
        <br>2019-11-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Exploration of End-to-End ASR for OpenSTT -- Russian Open Speech-to-Text
  Dataset -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-16/eess.AS/paper_6.html">
      Exploration of End-to-End ASR for OpenSTT -- Russian Open Speech-to-Text
  Dataset
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      同じ条件の下で、hybridASRシステムは33.5％、20.9％、および18.6％のWERを示しています。CTC/アテンション、RNNトランスデューサー、トランスフォーマーなどの既存のエンドツーエンドアプローチを評価しています。検証セット（電話、YouTube、および書籍）を使用すると、最高のエンドツーエンドモデルで、それぞれ34.8％、19.1％、および18.1％のワードエラー率（WER）を達成します。 
[ABSTRACT]使用可能な3つの検証セットについて、当社の最良のモデルは単語の誤り率を達成します（wer）
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-15">
        <br>2020-06-15
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Improving qBOLD based measures of oxygen extraction fraction using hyperoxia-BOLD derived measures of blood volume -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-16/biorxiv.physiology/paper_0.html">
      Improving qBOLD based measures of oxygen extraction fraction using hyperoxia-BOLD derived measures of blood volume
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この方法は、DBVを推論することに伴う困難を回避するために、高酸素-BOLDコントラストに由来する血液量の別の測定値に置き換えることにより、qBOLDモデルからDBVを推測します。qBOLDフレームワークは、可逆横緩和率（R2 &#39;）の比率からOEFを推論するため、脱酸素化血液量（DBV）、このOEFの過小評価は、主にこの手法を使用して作成されたDBVの過大評価に起因します。ただし、同じグループで、OEFのhqBOLD測定値は、白質領域（&gt; 100％）。 
[ABSTRACT] oefのsqbold測定値は、健康な脳では予想よりも体系的に低いと報告されています。この方法は、qboldのsqbold推定値を改善するための新しい方法、定量的qbringを提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-15">
        <br>2020-06-15
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
