<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-07-16の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Cross-Lingual Speaker Verification with Domain-Balanced Hard Prototype
  Mining and Language-Dependent Score Normalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.SD/paper_0.html">
      <font color="black">Cross-Lingual Speaker Verification with Domain-Balanced Hard Prototype
  Mining and Language-Dependent Score Normalization</font>
    </a>
  </h2>
  <font color="black">クロスリンガルトライアルのスコアリングを強化するために、言語依存のs-normスコアの正規化を提案します。最新のECAPA-TDNN x-ベクトルベースの最先端のECAPA-TDNN x-vectorを微調整するドメインバランスのハードプロトタイプマイニングを紹介しますスピーカー埋め込みエクストラクタ..課題の主な難点は、ドメイン内のDeepMine Farsiトレーニングデータの限られた利用可能性に加えて、オプションのクロスリンガルトライアル間のさまざまな音声オーバーラップにあります。 
[ABSTRACT]サンプルマイニング手法は、人気のaamのスピーカープロトタイプ間のスピーカー距離を効率的に活用します-ソフトマックス損失関数</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Exploring the time-domain deep attractor network with two-stream
  architectures in a reverberant environment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.SD/paper_1.html">
      <font color="black">Exploring the time-domain deep attractor network with two-stream
  architectures in a reverberant environment</font>
    </a>
  </h2>
  <font color="black">本研究では、可変数の話者の条件下で残響除去と分離タスクの両方を効率的に実行する2ストリーム畳み込みネットワークを備えた時間領域ディープアトラクタネットワーク（TD-DAN）を提案します。反響環境で..ディープアトラクタネットワーク（DAN）は、時間周波数領域でスピーカーアトラクタを使用して音声分離を実行します。 
[ABSTRACT]ディープコナーネットワーク（dan）は、スピーカーアトラクタで音声分離を実行します。これらのモデルは、反響環境でパフォーマンスの低下を被ります。この研究では、td-danが達成したスケール対応のソース対歪み比（si-sdr）のゲインを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br><font color="black">2020-07-01</font>
      </time>
    </span>
</section>
<!-- paper0: A survey and an extensive evaluation of popular audio declipping methods -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.SD/paper_2.html">
      <font color="black">A survey and an extensive evaluation of popular audio declipping methods</font>
    </a>
  </h2>
  <font color="black">信号対歪み比の観点から各アルゴリズムを評価し、音質の知覚メトリックを使用します。各アルゴリズムについて、オーディオ信号、モデリングドメイン、および最適化について行われている仮定を提示します。アルゴリズム..さらに、実際のオーディオデータに対して、人気の高いクリッピングアルゴリズムの広範な数値評価を提供します。 
[ABSTRACT]この記事では、文献で提案されているオーディオクリップ除去アルゴリズムの広範なレビューを提供します。実際のオーディオデータについて、一般的なクリップ除去アルゴリズムの広範な数値評価が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Content-based Recommendations for Radio Stations with Deep Learned Audio
  Fingerprints -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.SD/paper_3.html">
      <font color="black">Content-based Recommendations for Radio Stations with Deep Learned Audio
  Fingerprints</font>
    </a>
  </h2>
  <font color="black">したがって、オーディオストリームのクロールとDeep Autoencoderに依存する、オーディオベースのラジオ局のフィンガープリントを生成するための新しいパイプラインを提案します。提案されたフィンガープリントは、オーディオコンテンツによってラジオ局を特徴付けるために特に有用であり、優れていることを示します。ここでは、レコメンダーシステムは通常は介入しますが、既存のコンテンツベースのアプローチはメタデータに依存しているため、利用可能なデータ品質によって制約を受けます。 
[要約]提案されたフィンガープリントは、ラジオコンテンツをオーディオコンテンツで特徴付けるのに特に役立ちます。これらのフィンガープリントは、有意義で信頼性の高いラジオ局の推奨事項の優れた表現です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: DeepSinger: Singing Voice Synthesis with Data Mined From the Web -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.SD/paper_4.html">
      <font color="black">DeepSinger: Singing Voice Synthesis with Data Mined From the Web</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、DeepSingerを開発します。これは、音楽Webサイトからマイニングされた歌のトレーニングデータを使用して、ゼロから構築された多言語のマルチ歌唱歌声合成（SVS）システムです。DeepSingerには、以前のSVSシステムに比べていくつかの利点があります。私たちの知る限りでは、これは音楽ウェブサイトからトレーニングデータを直接マイニングする最初のSVSシステムです。2）歌詞から歌へのアラインメントモデルは、アラインメントのラベリングに対する人間の労力をさらに回避し、ラベリングコストを大幅に削減します。3）シンギングモデルフィードフォワードに基づくトランスフォーマーは、シンプルで効率的です。パラメトリック合成で複雑な音響機能モデリングを削除し、リファレンスエンコーダーを利用して、騒々しい歌声データから歌手の音色をキャプチャします。4）複数の言語で歌声を合成できます。結果は、Webから純粋にマイニングされた歌唱データを使用して、DeepSingerがピッチ精度の両方の点で高品質の歌声を合成できることを示していますと音声の自然さ（脚注：音声サンプルはhttps://speechresearch.github.io/deepsinger/に表示されます。）
[要旨] deepsingerは、音楽Webサイトからトレーニングデータを直接マイニングする最初のsvsシステムです。高品質を合成できます。ピッチの正確さと自然さの観点から歌を歌う</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-09">
        <br><font color="black">2020-07-09</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern
  Architectures -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.SD/paper_5.html">
      <font color="black">End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern
  Architectures</font>
    </a>
  </h2>
  <font color="black">ResNet、Time-Depth Separable ConvNet、および音声認識用のトランスフォーマーの半教師付きトレーニングの疑似ラベル付けを、CTCまたはSeq2Seq損失関数を使用して研究します。最後に、ラベル付けされていないオーディオの異なる量を活用する効果を研究し、提案します。音響モデリングを改善し、より多くの音声でトレーニングされた音響モデルが外部言語モデルに依存しないことを示す、ラベル付けされていない音声の特性を評価するいくつかの方法。Transformerベースの音響モデルは、教師ありデータセットだけで優れたパフォーマンスを発揮する一方で、半-監視は、アーキテクチャ全体のすべてのモデルと損失関数を改善し、それらの間のパフォーマンスのギャップの多くを埋めます。 
[ABSTRACT]標準のlibrispeechデータセットで実験を行い、ラベルなしデータから追加のラベルなしデータを半教師付きトレーニングで活用します。そうすることで、次のようにしてデコードされた、エンドツーフックモデルの新しい状態に到達します。外部言語モデル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-19">
        <br><font color="black">2019-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: MUSICNTWRK: data tools for music theory, analysis and composition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.SD/paper_6.html">
      <font color="black">MUSICNTWRK: data tools for music theory, analysis and composition</font>
    </a>
  </h2>
  <font color="black">ピッチクラスセットとリズミカルシーケンスの分類と操作のためのpythonライブラリであるMUSICNTWRKのAPI、一般化された音楽とサウンドスペースでのネットワークの生成、音色認識のためのディープラーニングアルゴリズム、および任意のデータの音波処理を紹介します。ソフトウェアはGPL 3.0の下で自由に利用でき、www.musicntwrk.comからダウンロードできます。 
[要約]ソフトウェアはgpl 3で無料で入手できます。 deep.itはwwwからダウンロードできます。 musicntwrk.com</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-03">
        <br><font color="black">2019-06-03</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Monocular Retinal Depth Estimation and Joint Optic Disc and Cup
  Segmentation using Adversarial Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/eess.IV/paper_0.html">
      <font color="black">Monocular Retinal Depth Estimation and Joint Optic Disc and Cup
  Segmentation using Adversarial Networks</font>
    </a>
  </h2>
  <font color="black">最先端の5倍クロス検証で0.92という非常に高い平均相関係数が得られます。このために、敵対ネットワークを使用して単一の画像から深度マップを予測する新しい方法を提案します。提案された深度推定手法INSPIRE-stereoデータセットからの個々の網膜画像を使用してトレーニングおよび評価されます。 
[ABSTRACT]深さは通常、光干渉断層計などの画像診断法から明示的に取得されます。単一のRGB画像から深さを推定することは非常に困難です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Multiplicative Interactions with Bayesian Neural Networks for
  Visual-Inertial Odometry -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/eess.IV/paper_1.html">
      <font color="black">Learning Multiplicative Interactions with Bayesian Neural Networks for
  Visual-Inertial Odometry</font>
    </a>
  </h2>
  <font color="black">重要なことに、それによって私たちの研究は、乗法的相互作用の学習がセンサー障害へのロバスト性を高めるための強力な帰納的バイアスをもたらす可能性があるという経験的証拠を提供します。このペーパーは、単眼視覚慣性オドメトリ（VIOのエンドツーエンドマルチモーダル学習アプローチを示します。 ）、センサーの劣化シナリオに照らしてセンサーの相補性を活用するように特別に設計されています。私たちのアプローチのもう1つの設計機能は、スケーラブルなラプラス近似を使用したモデルの不確実性の組み込みです。 
[要約]提案されたネットワークは、情報の複数のストリーム間の乗法的相互作用を学習するマルチヘッド学習メカニズムを利用しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: ESRGAN+ : Further Improving Enhanced Super-Resolution Generative
  Adversarial Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/eess.IV/paper_2.html">
      <font color="black">ESRGAN+ : Further Improving Enhanced Super-Resolution Generative
  Adversarial Network</font>
    </a>
  </h2>
  <font color="black">コードは、https：//github.com/ncarraz/ESRGANplusで入手できます。結果の画像は、よりリアルなテクスチャを示します。このようにして、モデルを拡張して、画像の知覚品質をさらに向上させます。 
[ABSTRACT]結果の画像はよりリアルなテクスチャを示します。画像は高度な高度な高度なテクノロジーを使用して開発されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-21">
        <br><font color="black">2020-01-21</font>
      </time>
    </span>
</section>
<!-- paper0: Cross-Attention in Coupled Unmixing Nets for Unsupervised Hyperspectral
  Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/eess.IV/paper_3.html">
      <font color="black">Cross-Attention in Coupled Unmixing Nets for Unsupervised Hyperspectral
  Super-Resolution</font>
    </a>
  </h2>
  <font color="black">しかし、監視されていないディープネットワークの開発は、このタスクでは依然として困難です。さらに、クロスアテンションモジュールがネットワークでより効果的な空間スペクトル情報転送を生み出すために考案されています。このために、クロスとの新しい結合非混合ネットワークを提案します。 -注意メカニズム、略してCUCaNet、より高い空間解像度のマルチスペクトル画像（MSI）によってHSIの空間解像度を向上させます。 
[要約]教師なしの深い比較の開発は困難でした。msとhssは「統計的に意味のある基礎」に分解されました。これらの特性には、msとhsデータの分解方法の学習が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-10">
        <br><font color="black">2020-07-10</font>
      </time>
    </span>
</section>
<!-- paper0: Real-Time Face and Landmark Localization for Eyeblink Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/eess.IV/paper_4.html">
      <font color="black">Real-Time Face and Landmark Localization for Eyeblink Detection</font>
    </a>
  </h2>
  <font color="black">1秒あたり500フレームより高速です。広範な文献検索に基づいて、顔検出とランドマーク検出のためのさまざまなアルゴリズムが分析および評価されています。このような実験はこれまでに達成されておらず、その動作に関する重要な洞察を提供することが期待されています。神経および精神疾患。 
[ABSTRACT]従来の研究者は、実験中のまぶたの動きを追跡するためにポテンシオムリッドまたは筋電図を使用していました。これらの2つのアルゴリズムはgpuとcpuで加速され、1〜753 $ / / / 11 $の高速化を達成しました。フレームあたり0.333ミリ秒</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-01">
        <br><font color="black">2020-06-01</font>
      </time>
    </span>
</section>
<!-- paper0: Lunar Terrain Relative Navigation Using a Convolutional Neural Network
  for Visual Crater Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/eess.IV/paper_5.html">
      <font color="black">Lunar Terrain Relative Navigation Using a Convolutional Neural Network
  for Visual Crater Detection</font>
    </a>
  </h2>
  <font color="black">LunaNetは、画像の明るさの変化に対するロバスト性が高く、軌跡全体でフレームごとにクレーターを繰り返し再現できるため、シミュレートされた軌跡でより信頼性の高い位置追跡が可能になります。EKAと組み合わせたLunaNetにより、平均最終位置が60％減少します。推定誤差と、標準的な明るさの画像を使用した軌跡でテストした場合、画像処理ベースのクレーター検出方法を使用したEKFと比較した平均最終速度推定誤差の25％の減少。これらの一致したクレーターは、 EKF。 
[要約]システムは、畳み込みニューラルネットワークと画像処理方法を使用して、拡張カルマンフィルター（ekf）でシミュレートされた宇宙船の位置を追跡します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: VidCEP: Complex Event Processing Framework to Detect Spatiotemporal
  Patterns in Video Streams -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/eess.IV/paper_6.html">
      <font color="black">VidCEP: Complex Event Processing Framework to Detect Spatiotemporal
  Patterns in Video Streams</font>
    </a>
  </h2>
  <font color="black">提案されたアプローチは、Fスコアが0.66から0.89の範囲の時空間ビデオイベントパターンを検出します。この作業では、ユーザーがビデオに対して高レベルの表現力豊かなクエリを定義して、時空間イベントパターンの範囲を検出できるCEPフレームワークに焦点を当てます。システムは、ビデオストリームのグラフベースのイベント表現を使用します。これにより、ディープニューラルネットワークモデルのカスケードを使用してビデオから高レベルのセマンティック概念を検出できます。ii）ビデオイベントクエリ言語（VEQL）で高レベルのユーザークエリを表現します。 CEPのビデオストリームの場合、iii）複雑なイベントマッチャーは、ビデオデータに対して表現力豊かなユーザークエリを照合することにより、時空間ビデオイベントパターンを検出します。 
[ABSTRACT]現在のcepシステムは、非構造化データモデルと表現力豊かな検索言語がないため、質問ビデオストリームに固有の制限があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Enabling Mixed-Precision Quantized Neural Networks in Extreme-Edge
  Devices -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/eess.IV/paper_7.html">
      <font color="black">Enabling Mixed-Precision Quantized Neural Networks in Extreme-Edge
  Devices</font>
    </a>
  </h2>
  <font color="black">この作業は、混合精度のディープニューラルネットワークの加速を対象としたPULP-NNライブラリの拡張を示しています。これは、無視できるほどの精度の損失でディープニューラルネットワークのメモリフットプリントを大幅に縮小できる新しいパラダイムです。27個のカーネルで構成されるライブラリは、入力フィーチャマップ、重み、出力フィーチャマップの精度（8ビット、4ビット、2ビットを考慮）の順列ごとに1つあり、RISCの並列超低消費電力（PULP）クラスターでQNNの効率的な推論を可能にします。 RV32IMCXpulpV2 ISAを特徴とするVベースのプロセッサ。そのため、最近の研究では、CMSIS-NNやPULP-NNなどのQNN（8ビットから2ビット）向けに最適化されたライブラリが提案されました。 
[ABSTRACT] 21年前のライブラリは、27個のカーネルで構成されており、各シュリンク用です。データの分析、分析、および分析からのデータを提供します。古いプロセッサのネットワーク上のqnnを特定するために使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Inspector Gadget: A Data Programming-based Labeling System for
  Industrial Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/eess.IV/paper_8.html">
      <font color="black">Inspector Gadget: A Data Programming-based Labeling System for
  Industrial Images</font>
    </a>
  </h2>
  <font color="black">ソフトウェア2.0時代に画像の機械学習が民主化されるようになると、深刻なボトルネックの1つは、トレーニング用に十分なラベル付きデータを確保することです。実際の産業用画像データセットで実験を行い、Inspectorガジェットが他の弱ラベル付け手法よりも優れたパフォーマンスを得ることが示されています。Snuba、GOGGLES、および事前学習なしの畳み込みニューラルネットワーク（CNN）を使用した自己学習ベースライン。 
[ABSTRACT]問題は、スマート工場が製品の品質管理のために機械学習に依存している製造現場で特に重要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-07">
        <br><font color="black">2020-04-07</font>
      </time>
    </span>
</section>
<!-- paper0: A DICOM Framework for Machine Learning Pipelines against Real-Time
  Radiology Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/eess.IV/paper_9.html">
      <font color="black">A DICOM Framework for Machine Learning Pipelines against Real-Time
  Radiology Images</font>
    </a>
  </h2>
  <font color="black">Nifflerはさらに、MLパイプラインからの結果を匿名化された方法で共有できるようにします。NifflerはDigital Imaging and Communications in Medicine（DICOM）プロトコルを使用して画像データをフェッチおよび保存し、メタデータ抽出機能とアプリケーションプログラミングインターフェイス（API）を提供します）画像にフィルターを適用します。このペーパーでは、そのアーキテクチャとその3つの使用例を紹介します。リアルタイムでの画像からの大静脈（IVC）下大静脈フィルターの検出、スキャナーの利用状況の特定、スキャナーの時計のキャリブレーション。 
[ABSTRACT] nifflerは、研究センターでのmlパイプラインの実行を可能にする統合フレームワークを提案します。nifflerは、mlパイプラインからの結果を特定されていない方法で共有できるようにします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-16">
        <br><font color="black">2020-04-16</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Prior-based Domain Adaptive Object Detection for Hazy and Rainy
  Conditions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_0.html">
      <font color="black">Prior-based Domain Adaptive Object Detection for Hazy and Rainy
  Conditions</font>
    </a>
  </h2>
  <font color="black">適応プロセスのトレーニングに使用される事前敵対的損失は、機能内の気象固有の情報を削減することを目的としており、それにより、検出パフォーマンスに対する気象の影響を軽減します。特に、新規の事前敵対的損失を定義するための画像形成。さらに、オブジェクト検出パイプラインに一連の残留特徴回復ブロックを導入して、特徴空間の歪みをなくし、さらなる改善をもたらします。 
[ABSTRACT]私たちは、検出器を乾燥した雨の状態に適応させるための、監視されていない事前ベースのドメイン敵対オブジェクト検出フレームワークを提案します。事前-適応プロセスのトレーニングに使用される敵対的損失は、天候固有の対損失</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-29">
        <br><font color="black">2019-11-29</font>
      </time>
    </span>
</section>
<!-- paper0: Pretraining Image Encoders without Reconstruction via Feature Prediction
  Loss -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_1.html">
      <font color="black">Pretraining Image Encoders without Reconstruction via Feature Prediction
  Loss</font>
    </a>
  </h2>
  <font color="black">ただし、そのような損失ネットワークを考えると、画像全体をデコードする時間のかかるタスクは必要ないことを示しています。この方法を評価するために、3つの標準公開データセット（LunarLander-v2、STL-10、およびSVHN）およびトレーニング画像エンコーダーの6つの異なる手順を比較します（ピクセル単位、知覚的類似性、および機能予測損失、画像の2つのバリエーションと機能のエンコード/デコードを組み合わせたもの）。この作業では、オートエンコーダーベースの損失を計算する3つの方法を調査します。画像エンコーダの事前トレーニング：一般的に使用される再構成損失、最近導入された深い知覚的類似性損失、およびここで提案される機能予測損失。後者が最も効率的な選択肢であることが判明しました。 
[要約]損失ネットワークでトレーニングされたオートエンコーダーは、損失トレーニングネットワークを使用して元の画像と再構成された画像の明示的な比較を実装しました。代わりに、損失ネットワークの機能をデコードすることを提案します。これは、「最近の「機能トレーニング」という名前を意味します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-16">
        <br><font color="black">2020-03-16</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Learn with Variational Information Bottleneck for Domain
  Generalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_2.html">
      <font color="black">Learning to Learn with Variational Information Bottleneck for Domain
  Generalization</font>
    </a>
  </h2>
  <font color="black">包括的なアブレーション研究は、ドメイン一般化のためのMetaVIBの利点を検証します。MetaVIBは、ドメイン一般化のメタ学習設定を活用することにより、相互情報の新しい変分境界から派生します。ドメインシフトに対処するために、ドメイン不変表現を学習します。メタ変分情報のボトルネックの提案された原則をMetaVIBと呼びます。 
[ABSTRACT]これにより、目に見えないドメインの予測の不確実性をより適切に処理できるようになります。クロスドメインの可視化のために、3つのベンチマークで実験を行います</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Self-Supervised Representation Learning for Detection of ACL Tear Injury
  in Knee MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_3.html">
      <font color="black">Self-Supervised Representation Learning for Detection of ACL Tear Injury
  in Knee MRI</font>
    </a>
  </h2>
  <font color="black">私たちの知る限りでは、MRIフレームから損傷分類タスクを実行する教師あり学習モデルはどれも、モデルによって行われた決定についての説明を提供しないため、MRIデータに関するこの種の作業は初めてです。この提案されたアプローチにより、モデルが空間コンテキスト不変の特徴を学習できるようになることで、膝MRIによるACL涙液損傷の分類などのダウンストリームタスクでの信頼性の高い説明可能なパフォーマンスを支援します。 MRIフレームが分割されるパッチ。 
[要旨]教師なし学習のサブセットである自己教師あり学習は、ラベルなしの画像または動画データから意味のある特徴を学習することによってこの問題を処理します。口実タスクモデルは、mriフレームが分割された乱雑な画像パッチの正しい順序を予測するように設計されていますに</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Monocular Retinal Depth Estimation and Joint Optic Disc and Cup
  Segmentation using Adversarial Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_4.html">
      <font color="black">Monocular Retinal Depth Estimation and Joint Optic Disc and Cup
  Segmentation using Adversarial Networks</font>
    </a>
  </h2>
  <font color="black">最先端の5倍クロス検証で0.92という非常に高い平均相関係数が得られます。このために、敵対ネットワークを使用して単一の画像から深度マップを予測する新しい方法を提案します。提案された深度推定手法INSPIRE-stereoデータセットからの個々の網膜画像を使用してトレーニングおよび評価されます。 
[ABSTRACT]深さは通常、光干渉断層計などの画像診断法から明示的に取得されます。単一のRGB画像から深さを推定することは非常に困難です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Approximating the Ideal Observer for joint signal detection and
  localization tasks by use of supervised learning methods -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_5.html">
      <font color="black">Approximating the Ideal Observer for joint signal detection and
  localization tasks by use of supervised learning methods</font>
    </a>
  </h2>
  <font color="black">ジョイント信号検出とローカリゼーションタスクを考慮すると、修正された一般化尤度比テストを使用するIOは、ローカリゼーションレシーバー動作特性（LROC）曲線で特徴付けられるオブザーバーパフォーマンスを最大化します。画像処理システムのパフォーマンスを最適化するための方法が検討されています。医療画像処理システムは、一般に、画質（IQ）の客観的な測定値を使用して評価および最適化されます。 
[ABSTRACT]理想的なカルロ（io）パフォーマンスは、画像システムの評価と最適化に使用するメリットを提供するために提唱されています。mcmcメソッドは、比較的単純なオブジェクトモデルに制限されています。mcmcは、メソッドを使用してジョイントのioをシミュレートします信号検出およびローカリゼーションタスク</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-29">
        <br><font color="black">2020-05-29</font>
      </time>
    </span>
</section>
<!-- paper0: Progressive Cluster Purification for Unsupervised Feature Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_6.html">
      <font color="black">Progressive Cluster Purification for Unsupervised Feature Learning</font>
    </a>
  </h2>
  <font color="black">適切に設計されたクラスター精製メカニズムを使用して、ノイズサンプルをフィルタリングすることでクラスターをさらに精製し、精製されたクラスターを疑似ラベルとして利用することで、後続の特徴学習を容易にします。プログレッシブクラスター精製（PCP）と呼ばれるアプローチは、プログレッシブクラスタリングを実装します。トレーニング中にクラスターの数を徐々に減らし、クラスターのサイズはモデル表現機能の成長に合わせて継続的に拡大します。クラスタリングベースのメソッドは、それぞれに不可避なクラスの一貫性のないサンプルがあるため、完全なクラス境界情報を調査するのにエラーが発生しやすくなります。集まる。 
[ABSTRACT]プログレッシブクラスター浄化と呼ばれるシンプルなアプローチが開発されました。トレーニング中にクラスターの数を徐々に減らしていきます。クラスターのサイズは、モデル表現機能の成長に合わせて継続的に拡大します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-06">
        <br><font color="black">2020-07-06</font>
      </time>
    </span>
</section>
<!-- paper0: Structured Landmark Detection via Topology-Adapting Deep Graph Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_7.html">
      <font color="black">Structured Landmark Detection via Topology-Adapting Deep Graph Learning</font>
    </a>
  </h2>
  <font color="black">画像ランドマーク検出は、事前定義された基準点の位置を自動的に識別することを目的としています。提案された方法は、ローカル画像機能とグローバル形状機能の両方を活用してグラフ信号を作成します。3つの公開顔画像データセット（WFLW、300W、COFW- 68）だけでなく、3つの実際のX線医療データセット（Cephalometric（パブリック）、HandおよびPelvis）。 
[要約]提案された方法は、ローカル画像機能とグローバル形状機能の両方を活用してグラフ信号を構築します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-17">
        <br><font color="black">2020-04-17</font>
      </time>
    </span>
</section>
<!-- paper0: Few-shot Scene-adaptive Anomaly Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_8.html">
      <font color="black">Few-shot Scene-adaptive Anomaly Detection</font>
    </a>
  </h2>
  <font color="black">ビデオの異常検出の問題に対処します。通常、ほとんどの既存のアプローチはデータを大量に消費し、一般化機能が制限されています。このホワイトペーパーでは、以前のアプローチの制限に対処するために、新規の少数ショットシーン適応異常検出問題を提案します。 。 
[ABSTRACT]目標は、通常の動画から独占的に学習して、異常な動作を自動的に特定することです。ほとんどの既存の動画は、通常、ターゲットシーンの多数の動画でトレーニングして、良好な結果を得る必要があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: AdvFlow: Inconspicuous Black-box Adversarial Attacks using Normalizing
  Flows -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_9.html">
      <font color="black">AdvFlow: Inconspicuous Black-box Adversarial Attacks using Normalizing
  Flows</font>
    </a>
  </h2>
  <font color="black">また、実験結果は、提案されたアプローチと、防御された分類子に対する既存の攻撃方法のいくつかとの競争力のあるパフォーマンスを示し、クエリ数と攻撃成功率の両方でそれらを上回っています。このホワイトペーパーでは、AdvFlow：斬新なブラックボックスを紹介します。フローを正規化する能力を利用して、特定のターゲット画像の周囲にある敵対者の例の密度をモデル化する、画像分類子に対する敵対的攻撃手法。提案された方法は、クリーンなデータ分布に厳密に従う敵を生成することがわかります。おそらく。 
[要約]提案された方法は、クリーンなデータ分布に厳密に従う敵を生成します。これにより、検出の可能性が低くなります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Fast and Robust Iterative Closet Point -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_10.html">
      <font color="black">Fast and Robust Iterative Closet Point</font>
    </a>
  </h2>
  <font color="black">反復最接近点（ICP）アルゴリズムとそのバリアントは、ロボット工学から3D再構成までのさまざまな分野で幅広いアプリケーションを使用する2つのポイントセット間の厳密な登録のための基本的な手法です。スパースICPなどの最近の研究は、コストでスパース性最適化により堅牢性を実現しています。私たちの堅牢なICPメソッドは、計算時間で競争力を保ちながら、ベンチマークデータセットの登録精度を向上させます。 
[ABSTRACT] icpの主な欠点は、遅い、遅い、外れ値に対する感度、欠落データ、および部分的なオーバーラップです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Multiplicative Interactions with Bayesian Neural Networks for
  Visual-Inertial Odometry -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_11.html">
      <font color="black">Learning Multiplicative Interactions with Bayesian Neural Networks for
  Visual-Inertial Odometry</font>
    </a>
  </h2>
  <font color="black">提案されたアプローチのパフォーマンスを、KITTIデータセットのエンドツーエンドの最先端のメソッドと比較することにより評価し、優れたパフォーマンスを達成していることを示します。乗法的相互作用は、センサー障害に対する堅牢性を高めるための強力な帰納的バイアスをもたらす可能性があります。このホワイトペーパーでは、単眼視覚慣性オドメトリ（VIO）のエンドツーエンドマルチモーダル学習アプローチを紹介します。センサーの劣化シナリオの観点から。 
[要約]提案されたネットワークは、情報の複数のストリーム間の乗法的相互作用を学習するマルチヘッド学習メカニズムを利用しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Disentangling the Spatial Structure and Style in Conditional VAE -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_12.html">
      <font color="black">Disentangling the Spatial Structure and Style in Conditional VAE</font>
    </a>
  </h2>
  <font color="black">エンコーダーは、$ z_u $がサンプリングされる元の無関係なラベルを提供します。デコーダーは、SPADEやAdaINのような適応正規化によって、各レイヤーで$ z_s $と$ z_u $を使用します。ラベルが空間構造と関連しているかどうかに応じて、条件マッピングネットワークからの出力$ z_s $は、スタイルコードまたは空間構造コードとして使用されます。 
[要約]ジェネレーターは、接続されたエンコーダー-デコーダーとラベル条件マッピングネットワークによって構築されます。エンコーダーは、$ z `u &#39;がサンプリングされる元の無関係なラベルを提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-29">
        <br><font color="black">2019-10-29</font>
      </time>
    </span>
</section>
<!-- paper0: Difficulty-aware Meta-learning for Rare Disease Diagnosis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_13.html">
      <font color="black">Difficulty-aware Meta-learning for Rare Disease Diagnosis</font>
    </a>
  </h2>
  <font color="black">私たちの方法を評価するために、最近のISIC 2018皮膚病変分類データセットを使用し、クラスごとに5つのサンプルしかなく、モデルが目に見えないクラスを83.3％の高いAUCで分類するためにすばやく適応できることを示します。疾患の分類結果は、公のDermofit Image Libraryにあり、実際の臨床診療における私たちの方法の可能性を示しています。したがって、ニューラルネットワークをトレーニングして、クラスごとの少数のデータサンプルで希少疾患を分類することは非常に困難で、これまでのところ、注目はほとんどありません。 
[ABSTRACT]私たちの主なアプローチは、最初に一般的な疾患のデータからメタ学習モデルをトレーニングして構築し、次にモデルを適応させてまれな疾患の分類を実行することです。これを達成するために、動的に監視する難しさを意識したメタ学習法を使用します学習タスクの重要性</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-30">
        <br><font color="black">2019-06-30</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Parse Wireframes in Images of Man-Made Environments -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_14.html">
      <font color="black">Learning to Parse Wireframes in Images of Man-Made Environments</font>
    </a>
  </h2>
  <font color="black">データとソースコードはhttps://github.com/huangkuns/wireframeで入手できます。このホワイトペーパーでは、乱雑な人工画像の「ワイヤーフレーム」表現を自動的に抽出するタスクへの学習ベースのアプローチを提案します。ワイヤーフレーム（図
[ABSTRACT]を参照）は、5,00を超える画像の非常に大規模な新しいデータセットを構築し、人間が完全にラベル付けしたワイヤーフレームを使用しています。ネットワーク表現は、パフォーマンスが大幅に向上しています。機能対応として</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Transformation Consistency Regularization- A Semi-Supervised Paradigm
  for Image-to-Image Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_15.html">
      <font color="black">Transformation Consistency Regularization- A Semi-Supervised Paradigm
  for Image-to-Image Translation</font>
    </a>
  </h2>
  <font color="black">画像の色付け、ノイズ除去、超解像の3つの異なるアプリケーションでアルゴリズムの有効性を評価します。ラベル付きデータの不足により、いくつかのラベル付きサンプルの横にあるラベルなしデータの大部分から学習する半教師あり学習方法の開発が動機になっています。 ..異なる入力摂動の下でのモデルの予測間の整合性の正則化は、特に、半監視フレームワークで最先端の結果を提供することを示しています。 
[ABSTRACT]整合性の正則化は、半監視フレームワークで最先端の結果を提供することが示されています。メソッドは、ビデオ処理アプリケーションで使用でき、数フレームの知識を活用して映画の品質を向上できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Evaluation of Neural Network Classification Systems on Document Stream -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_16.html">
      <font color="black">Evaluation of Neural Network Classification Systems on Document Stream</font>
    </a>
  </h2>
  <font color="black">現実的なケースでは、NNベースのドキュメント分類システムの効率が大幅に低下するという事実を強調しました。多くの場合、実際の産業プロセスでこの数のサンプルを収集することは、不可能ではないにしても非常に困難です。 3つの異なるアプローチ。1つはイメージコンテンツに基づいており、2つはテキストコンテンツに基づいています。 
[ABSTRACT]調査は、会社のドキュメントストリームの状況に基づいて、準最適なトレーニングケースでnnベースのドキュメント分類システムの効率を分析することによって実施されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Confidence Regularized Self-Training -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_17.html">
      <font color="black">Confidence Regularized Self-Training</font>
    </a>
  </h2>
  <font color="black">この方法では、擬似ラベルを交互最適化によって共同最適化された連続潜在変数として扱います。この作業のコードとモデルは、https：//github.com/yzou2/CRSTで入手できます。 （LR）およびモデルの正則化（MR）。 
[ABSTRACT]これらの方法は、ターゲットドメインで予測し、自信のある予測を疑似として繰り返す反復プロセスを伴うことがよくあります-再トレーニングのラベル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-26">
        <br><font color="black">2019-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Video Object Segmentation with Episodic Graph Memory Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_18.html">
      <font color="black">Video Object Segmentation with Episodic Graph Memory Networks</font>
    </a>
  </h2>
  <font color="black">構造化された外部メモリ設計により、視覚情報が限られている場合でも、モデルで新しい知識を包括的にマイニングしてすばやく保存できます。また、微分可能メモリコントローラは、有用な表現をメモリに保存する抽象的な方法と、これらの表現を後で予測に使用する方法をゆっくりと学習します、勾配降下法を使用します。さらに、提案されたグラフメモリネットワークは、ワンショットとゼロショットの両方のビデオオブジェクトセグメンテーションタスクを一般化できる、きちんとした原理のフレームワークを生成します。4つの困難なベンチマークデータセットでの広範な実験により、グラフメモリネットワークは、ケースバイケースのビデオオブジェクトセグメンテーションのためのセグメンテーションネットワークの適応を容易にすることができます。 
[要約]提案されたグラフメモリネットワークは、ネットワークの新しいパターンメモリネットワークに対処するように設計されています。このネットワークは、ワンショットおよびゼロショットのビデオオブジェクトセグメンテーションタスクによって適切に一般化できる有用なモデルを提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: PVSNet: Pixelwise Visibility-Aware Multi-View Stereo Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_19.html">
      <font color="black">PVSNet: Pixelwise Visibility-Aware Multi-View Stereo Network</font>
    </a>
  </h2>
  <font color="black">私たちの知る限りでは、PVSNetは、さまざまな隣接するビューの可視性情報をキャプチャできる最初のディープラーニングフレームワークです。さらに、ピクセル単位の可視性を実現するためにモデルトレーニング中に不快なビューを導入するアンチノイズトレーニング戦略を提示ネットワークは、無関係なビューと区別しやすくなります。これは、トレーニングに2つの最適な隣接ビューのみを使用する既存の学習法とは異なります。最近、学習ベースのマルチビューステレオ法は有望な結果を達成しています。 
[要旨]ピクセルごとの可視性ネットワークを提示して、隣接するさまざまなビューの可視性情報を学習します。次に、可視性情報を使用して適応加重コストボリュームを構築します。私たちの知る限り、pvsnetは最初のディープラーニングフレームワークです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Graph-Based Social Relation Reasoning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_20.html">
      <font color="black">Graph-Based Social Relation Reasoning</font>
    </a>
  </h2>
  <font color="black">人間は基本的に社交的です。つまり、一般的に他の人々との関係で社会生活を整理します。さらに、提案されたGR2Nは、さまざまなタイプの社会関係間の強い論理的制約を明示的に把握するために、いくつかの仮想関係グラフを構築します。画像からの関係は、ソーシャルチャットボットやパーソナルアシスタントなどのインテリジェントシステムにとって大きな可能性を秘めています。 
[ABSTRACT]私たちの方法は、合理的で一貫性のある社会関係グラフを生成します。合理的で一貫性のある社会関係グラフを提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Visual Context by Comparison -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_21.html">
      <font color="black">Learning Visual Context by Comparison</font>
    </a>
  </h2>
  <font color="black">このモジュールは、既存のディープラーニングモデルにプラグインできます。このホワイトペーパーでは、対象オブジェクトとそれに対応するコンテキストの違いを把握するためのAttend-and-Compare Module（ACM）を紹介します。評価のために、 3つの胸部X線認識タスクとCOCOオブジェクト検出およびセグメンテーションタスク。タスク全体で一貫した改善が見られます。 
[ABSTRACT]このタスクを解決するための現在の方法は、胸部X線画像のさまざまな特性を利用していますが、最も重要な特性の1つはまだ欠けています：画像内の関連領域間の比較の必要性</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Non-greedy Gradient-based Hyperparameter Optimization Over Long Horizons -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_22.html">
      <font color="black">Non-greedy Gradient-based Hyperparameter Optimization Over Long Horizons</font>
    </a>
  </h2>
  <font color="black">次に、人気のある運動量ベースのSGDオプティマイザーのフォワードモード微分アルゴリズムを導出します。これにより、ホライズンサイズで一定のメモリコストが可能になります。勾配ベースのハイパーパラメーター最適化は、分布全体でメタ学習を実行する魅力的な方法ですタスクの最適化、または単一のタスクでのオプティマイザのパフォーマンスの向上。手作業で調整された既製のハイパーパラメータのベースラインと比較して、SVHNのような単純なデータセットと比較すると、この方法は有利です。 
[ABSTRACT]最適なローカルハイパーパラメータは貧弱なグローバルソリューションを生み出す可能性があります。さらに、人気の高いハイパーパラメータを共有しています。これにより、これが問題を大幅に軽減する方法が説明されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Controversial stimuli: pitting neural networks against each other as
  models of human recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_23.html">
      <font color="black">Controversial stimuli: pitting neural networks against each other as
  models of human recognition</font>
    </a>
  </h2>
  <font color="black">自然画像とは異なり、物議を醸す刺激は刺激分布モデルに制約されていないため、モデルの帰納的バイアスを明らかにする厳しい分布外テストが提供されます。 MNISTの変分オートエンコーダとCIFAR-10のハイブリッド判別生成関節エネルギーモデルに基づいています。人間の被験者はこれらの刺激の何百も自然の例を見て、それぞれの数字/オブジェクトカテゴリの存在確率を判断しました画像。 
[ABSTRACT]論争の的となる刺激により、敵対的な例の概念が一般化され、分布モデルを仮定する必要がなくなります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-21">
        <br><font color="black">2019-11-21</font>
      </time>
    </span>
</section>
<!-- paper0: Comparing to Learn: Surpassing ImageNet Pretraining on Radiographs By
  Comparing Image Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_24.html">
      <font color="black">Comparing to Learn: Surpassing ImageNet Pretraining on Radiographs By
  Comparing Image Representations</font>
    </a>
  </h2>
  <font color="black">コードとモデルが利用可能です。ただし、自然画像と医療画像の間に明らかなドメインギャップが存在することは否定できません。このギャップを埋めるために、手動アノテーションなしで700kのX線写真から学習する新しい事前トレーニング方法を提案します。 
[要約]異なる画像表現を比較することで堅牢な機能を学習するため、この方法を比較と学習（c2l）と呼びます。研究は、c2lがモデルと以前の分析アプローチを大幅に上回ることができることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Enhancing Generalized Zero-Shot Learning via Adversarial Visual-Semantic
  Interaction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_25.html">
      <font color="black">Enhancing Generalized Zero-Shot Learning via Adversarial Visual-Semantic
  Interaction</font>
    </a>
  </h2>
  <font color="black">4つのベンチマークデータセットでアプローチを評価します。また、一般化されたゼロショット学習タスクの方法をより注意深く分析および理解するためにアブレーション研究を行います。さらに、既存の方法は、合成画像の特徴の生成または表現学習の活用によって生成される潜在的な埋め込みのいずれかでゼロショット分類器をトレーニングします。 
[ABSTRACT]これは、視覚的ドメインと意味論的ドメインの間で知識を効果的に伝達するための強力なクロスモーダル相互作用を提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Part Boundaries from 3D Point Clouds -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_26.html">
      <font color="black">Learning Part Boundaries from 3D Point Clouds</font>
    </a>
  </h2>
  <font color="black">私たちの実験は、私たちの方法が代替案と比較してグラウンドトゥルースに近いより正確な境界を抽出できることを示しています。私たちの境界検出器は非常に汎用的です。3Dモデリングで一般的に使用されるセマンティックパーツまたはジオメトリプリミティブの境界を特定するようにトレーニングできます。また、詳細なセマンティックシェイプセグメンテーションへのネットワークの適用を示し、パーツのラベル付けパフォーマンスの改善も示しています。 
[要約]私たちの方法は、グラフの畳み込みネットワークアーキテクチャに基づいています。この方法は、地面に近い、より正確な境界を抽出できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Captioning Images Taken by People Who Are Blind -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_27.html">
      <font color="black">Captioning Images Taken by People Who Are Blind</font>
    </a>
  </h2>
  <font color="black">このデータセットを分析して、（1）典型的なキャプションを特徴付け、（2）画像にあるコンテンツの多様性を特徴付け、（3）そのコンテンツを8つの人気のあるビジョンデータセットにあるものと比較します。この新しいデータセットを、 VizWiz-Captionsは、それぞれ5つのキャプションとペアになっている視覚障害者からの39,000を超える画像で構成されています。また、最新の画像キャプションアルゴリズムを分析して、この新しいデータセットがビジョンコミュニティにとって何が難しいかを特定します。 
[要約]この実際の使用例を表す最初の画像キャプションデータセット。このデータセットを分析して（39）典型的なキャプションを特徴付けます。これは、8つの一般的なビジョンデータセットで見つかったものと比較されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br><font color="black">2020-02-20</font>
      </time>
    </span>
</section>
<!-- paper0: Data-Efficient Deep Learning Method for Image Classification Using Data
  Augmentation, Focal Cosine Loss, and Ensemble -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_28.html">
      <font color="black">Data-Efficient Deep Learning Method for Image Classification Using Data
  Augmentation, Focal Cosine Loss, and Ensemble</font>
    </a>
  </h2>
  <font color="black">この状況では、事前トレーニング済みのモデルだけでなく、外部の知識も活用することが困難になります。したがって、パフォーマンスを向上させるには、小さなデータセットを効果的に活用することが重要です。データ収集の結果、ほとんどのデータが不足します。エリアの。 
[ABSTRACT]これらの方法を使用して、クラスごとに50の画像のみで構成されるimagenetデータを活用することにより、高精度を取得します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic Image Labelling at Pixel Level -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_29.html">
      <font color="black">Automatic Image Labelling at Pixel Level</font>
    </a>
  </h2>
  <font color="black">このような粗いオブジェクトマスクは疑似ラベルとして扱われ、ターゲットドメインで繰り返しGFNを最適化/洗練するために統合されます。6つの画像セットでの実験は、提案されたアプローチが細かいオブジェクトマスク（つまり、ピクセル）を生成できることを示しています。レベルのオブジェクトのラベル付け）。品質は手動でラベル付けされたものに非常に匹敵します。提案されたアプローチは、ほとんどの既存の弱く監視されているアプローチよりもセマンティック画像のセグメンテーションでより良いパフォーマンスを達成できます。 
[要約]ガイドフィルターネットワーク（gfn）は、最初にソースからセグメンテーション知識を学習するために開発されます。次に、そのようなセグメンテーション知識を転送して、細かいオブジェクトマスク（つまり、ピクセルレベルのオブジェクトラベル）を生成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Train Your Data Processor: Distribution-Aware and Error-Compensation
  Coordinate Decoding for Human Pose Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_30.html">
      <font color="black">Train Your Data Processor: Distribution-Aware and Error-Compensation
  Coordinate Decoding for Human Pose Estimation</font>
    </a>
  </h2>
  <font color="black">DAECは、モデルにとらわれないプラグインとして機能し、トレーニングデータからデコード戦略を学習し、無視できるほどの余分な計算で、さまざまな最先端の人間の姿勢推定モデルのパフォーマンスを著しく改善します。予測プロセス全体で導入されたエラーに特に焦点を当ててヒートマップデコード処理を研究しています。ヒートマップベースの方法のエラーは驚くほど重要であるにもかかわらず、以前は一般的に無視されていました。 
[ABSTRACT]最近使用されたヒートマップデコード。ヒートマップが優勢です。これは、デコードプロセスを分析する作業です。また、分布を認識し、エラーを補正する補正座標デコード</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-12">
        <br><font color="black">2020-07-12</font>
      </time>
    </span>
</section>
<!-- paper0: CenterNet3D:An Anchor free Object Detector for Autonomous Driving -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_31.html">
      <font color="black">CenterNet3D:An Anchor free Object Detector for Autonomous Driving</font>
    </a>
  </h2>
  <font color="black">KITTIベンチマークでは、提案されたCenterNet3Dは、提案された中心点表現の有効性を示す他の1ステージアンカーベースの方法で競争力のあるパフォーマンスを実現します。この問題を解決するために、CNNバックボーンに支払いを強制する補助コーナーアテンションモジュールを提案します。より正確なバウンディングボックスを取得するのに有効なオブジェクト境界に注意を向けます。ただし、点群の固有のスパース性のため、3Dオブジェクトの中心点は空のスペースにある可能性が高く、正確な境界を推定することが困難になります。 
[ABSTRACT]中心点に基づいて、アンカーなしの3次元オブジェクト検出をその境界なしで実行する、アンカーのないcenternet3dネットワークを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: A Single Stream Network for Robust and Real-time RGB-D Salient Object
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_32.html">
      <font color="black">A Single Stream Network for Robust and Real-time RGB-D Salient Object
  Detection</font>
    </a>
  </h2>
  <font color="black">（2）新規の深度拡張デュアルアテンションモジュール（DEDA）を設計して、フォア/バックグラウンドブランチに空間的にフィルター処理された機能を効率的に提供し、デコーダーが中間融合を最適に実行できるようにします。深度マップ自体の効果..この作業では、深度マップを直接使用してRGBと深度の間の早期融合と中間融合を導く単一ストリームネットワークを設計します。これにより、深度ストリームの機能エンコーダーが節約され、軽量化が実現します。とリアルタイムモデル。 
[要約]深度マップは、フュージョンの影響を具体的に調査していません。シングルフュージョンエンコーダーを構築して、早期フュージョンを実現しています。Imagenetバックボーンモデルを最大限に活用して、豊富で差別的な機能を抽出できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Learning From Multiple Experts: Self-paced Knowledge Distillation for
  Long-tailed Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_33.html">
      <font color="black">Learning From Multiple Experts: Self-paced Knowledge Distillation for
  Long-tailed Classification</font>
    </a>
  </h2>
  <font color="black">具体的には、提案されたフレームワークには、2つのレベルの適応学習スケジュールが含まれます。セルフペースのエキスパート選択とカリキュラムインスタンス選択です。これにより、知識が「学生」に適応的に転送されます。最先端の手法と比較して優れたパフォーマンス。私たちの手法は、分布の不均衡の少ないサブセットでトレーニングされたネットワークが、共同でトレーニングされたものよりも優れたパフォーマンスをもたらすことが多いという観察に触発されました。 
[要約]提案されたlfmeフレームワークは、複数の専門家の学習と呼ばれます（lfme）</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-06">
        <br><font color="black">2020-01-06</font>
      </time>
    </span>
</section>
<!-- paper0: Contextual-Relation Consistent Domain Adaptation for Semantic
  Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_34.html">
      <font color="black">Contextual-Relation Consistent Domain Adaptation for Semantic
  Segmentation</font>
    </a>
  </h2>
  <font color="black">提案されたCrCDAは、2つの挑戦的なドメイン適応セグメンテーションタスク（たとえば、GTA5からCityscapesへ、SYNTHIAからCityscapesへ）にわたって広範囲に評価され、実験は最新の方法と比較してその優れたセグメンテーションパフォーマンスを実証しています。具体的には、CrCDAが学習します。ラベル付けされたソースドメインの機能空間でプロトタイプのローカルコンテキスト関係を明示的に適用し、逆伝播ベースの敵対学習を介してラベル付けされていないターゲットドメインに転送します。このホワイトペーパーでは、革新的なローカルコンテキスト関係整合ドメイン適応（CrCDA）手法を紹介します。グローバルレベルの調整中にローカルレベルの一貫性を達成することを目的としています。 
[ABSTRACT]アイデアは、地域-賢明なフィーチャ表現を詳しく調べ、ローカルレベルの決定に合わせて調整することです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-05">
        <br><font color="black">2020-07-05</font>
      </time>
    </span>
</section>
<!-- paper0: s-DRN: Stabilized Developmental Resonance Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_35.html">
      <font color="black">s-DRN: Stabilized Developmental Resonance Network</font>
    </a>
  </h2>
  <font color="black">まず、ノードのアクティブ化プロセス中に従来の選択関数の不安定性を分析し、スケーラブルなアクティブ化関数を設計して、すべての入力データスケールでクラスタリングのパフォーマンスを安定させます。提案されたs-DRNモデルのパフォーマンスを検証するために、比較研究が行われます提案されたノードグループ化アルゴリズムは、増分的に作成されたクラスターから不要なクラスターを効果的に除外し、警戒パラメーターへのパフォーマンスの依存性を減らし、クラスター化プロセスを堅牢にします。 
[要約]提案されたシステムは、新しいモデルの開発に使用できます。これは、多数のデータベースのデータセットのモデルとして使用されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-18">
        <br><font color="black">2019-12-18</font>
      </time>
    </span>
</section>
<!-- paper0: JNR: Joint-based Neural Rig Representation for Compact 3D Face Modeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_36.html">
      <font color="black">JNR: Joint-based Neural Rig Representation for Compact 3D Face Modeling</font>
    </a>
  </h2>
  <font color="black">人間の顔は高度に構造化されており、トポロジーが一貫しているため、データから完全に学習する必要はない、と主張しています。3番目に、スキニングを通じて、私たちのモデルは口の内部と目の追加をサポートしています、および付属品（髪、眼鏡など）と同様に
[ABSTRACT]私たちのモデルは、以前のブレンドシェイプベースのモデルに比べていくつかの重要な利点を享受しています。代わりに、人造の3D顔リグの形で事前知識を活用できます。学習小さなデータセットのみからのコンパクトで強力な顔モデル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: A-MAL: Automatic Movement Assessment Learning from Properly Performed
  Movements in 3D Skeleton Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_37.html">
      <font color="black">A-MAL: Automatic Movement Assessment Learning from Properly Performed
  Movements in 3D Skeleton Videos</font>
    </a>
  </h2>
  <font color="black">脳卒中後の患者の回復プロセスを監視するために作業療法士が通常行う、Fugl-Meyer評価（FMA）テストからの動きについて、私たちの方法を示します。最近、動きの質を評価するタスクがさまざまな分野で高い需要を得ています。ドメインの..この作業では、A-MALを紹介します。A-MALは、適切なパフォーマンスの動きの動画からのみ学習する自動で強力な動きの評価学習アルゴリズムであり、偏差時間セグメンテーションアルゴリズム、パラメーター関連性検出アルゴリズム、共通の時間的関心点の自動検出とテキストフィードバック生成メカニズムに基づく新しいタイムワーピングアルゴリズム。 
[ABSTRACT] a-malは、自動で強力な動きの学習アルゴリズムです。偏差時間-セグメンテーションアルゴリズム、投影メカニズムを搭載しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-07-23">
        <br><font color="black">2019-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Neural networks adapting to datasets: learning network size and topology -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_38.html">
      <font color="black">Neural networks adapting to datasets: learning network size and topology</font>
    </a>
  </h2>
  <font color="black">したがって、取得されたグラフは、特定の分類タスクの重要な特性をエンコードするものとして理解できます。取得されたネットワークは、ゼロからトレーニングして、実質的に同じパフォーマンスを達成することもできます。体系的な規則性を守る。 
[要約]ネットワークは、グラフの構造を持つネットワークで見つけることができます。結果のネットワークは、特定の学習タスクとデータセットに合わせて調整されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-22">
        <br><font color="black">2020-06-22</font>
      </time>
    </span>
</section>
<!-- paper0: Attention as Activation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_39.html">
      <font color="black">Attention as Activation</font>
    </a>
  </h2>
  <font color="black">畳み込みネットワークでよく知られている整流線形ユニットをこのようなATACユニットに置き換えることにより、適度な数の追加パラメーターで大幅に優れた完全注意ネットワークを構築できます。複数のホストネットワークを使用して、ATACユニットの詳細なアブレーション研究を実施しました。ユニットの有効性と効率を実証的に検証するために、ネットワークの深さを変化させました。さらに、ATACユニットのパフォーマンスを、CIFAR-10、CIFAR-100、およびImageNetデータセットの既存のアクティベーション機能やその他の注意メカニズムと比較しました。 
[要旨]非線形のアクティブ化と要素ごとの機能を同時に改善するためのローカルチャネルのアクティブ化モジュールを提案します。これは、ポイントごとのクロスチャネル機能コンテキストをローカルに集約します。これらのユニットは、ネットワークの深さが異なる複数のネットワークに基づいて、ユニットの有効性と効率</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Path Signatures on Lie Groups -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_40.html">
      <font color="black">Path Signatures on Lie Groups</font>
    </a>
  </h2>
  <font color="black">また、リーグループ値ランダムウォークの2つのサンプルの仮説検定を提供して、その特性を説明します。パスシグネチャは、時系列分析の強力なノンパラメトリックツールであり、ユークリッド値の時系列データの普遍的で特徴的な機能マップを形成することが示されています。 。普遍性を実証するために、時系列に$ SO（3）$表現を使用してコンピュータービジョンの人間の行動認識問題を分析し、他の浅い学習アプローチと同等のパフォーマンスを提供しながら、簡単に解釈可能な機能セットを提供します。 
[ABSTRACT]パス署名の理論をライグループ値の時系列の設定に引き上げ、これらのツールを基本的な幾何学的制約のある時系列に適応させます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-02">
        <br><font color="black">2020-07-02</font>
      </time>
    </span>
</section>
<!-- paper0: ContourRend: A Segmentation Method for Improving Contours by Rendering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_41.html">
      <font color="black">ContourRend: A Segmentation Method for Improving Contours by Rendering</font>
    </a>
  </h2>
  <font color="black">そして、グラフ畳み込みネットワーク（GCN）に基づくセグメンテーションモデルにメソッドを実装します。都市景観データセットの単一オブジェクトセグメンテーションタスクでは、GCNベースのセグメンテーションコンターを使用して単一オブジェクトのコンターを生成し、次にコンターレンダラーは、コンターの周りのピクセルに焦点を当て、高解像度でカテゴリを予測します。細かいコンターを取得するために、コンターレンダラーを採用してセグメンテーションコンターを改良するContourRendというセグメンテーションメソッドを提案します。 
[ABSTRACT] contourrendは、コンターレンダラーを使用してフィーチャを調整します。このメソッドは、contourrendと呼ばれます。これは、単一のオブジェクトのコンターを作成するために使用されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Estimating People Flows to Better Count Them in Crowded Scenes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_42.html">
      <font color="black">Estimating People Flows to Better Count Them in Crowded Scenes</font>
    </a>
  </h2>
  <font color="black">これにより、人数の節約をエンコードするはるかに強力な制約を課すことができます。さらに、結果をさらに改善するために、人物の流れとオプティカルフローの間の相関を活用することもできます。ディープネットワークを使用して、個々の画像の人物の密度を推定します。 
[ABSTRACT]これにより、人数にはるかに強い制約を課すことができます。これにより、結果を改善するために人々が協力する必要があります。ただし、人の流れとオプティカルフローの相関を利用することもできます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-25">
        <br><font color="black">2019-11-25</font>
      </time>
    </span>
</section>
<!-- paper0: Cross-Attention in Coupled Unmixing Nets for Unsupervised Hyperspectral
  Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_43.html">
      <font color="black">Cross-Attention in Coupled Unmixing Nets for Unsupervised Hyperspectral
  Super-Resolution</font>
    </a>
  </h2>
  <font color="black">しかし、監視されていないディープネットワークの開発は、このタスクでは依然として困難です。最新のHSI-SRモデルと比較して、広く使用されている3つのHS-MSデータセットに対して広範な実験が行われ、CUCaNetの優位性が示されています。 HSI-SRアプリケーション。このために、クロス注意メカニズム（CUCaNet）を備えた新規の結合非混合ネットワークを提案し、より高い空間解像度のマルチスペクトル画像（MSI）によってHSIの空間解像度を向上させます。 
[要約]教師なしの深い比較の開発は困難でした。msとhssは「統計的に意味のある基礎」に分解されました。これらの特性には、msとhsデータの分解方法の学習が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-10">
        <br><font color="black">2020-07-10</font>
      </time>
    </span>
</section>
<!-- paper0: Iterative Distance-Aware Similarity Matrix Convolution with
  Mutual-Supervised Point Elimination for Efficient Point Cloud Registration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_44.html">
      <font color="black">Iterative Distance-Aware Similarity Matrix Convolution with
  Mutual-Supervised Point Elimination for Efficient Point Cloud Registration</font>
    </a>
  </h2>
  <font color="black">これらの畳み込み層は、幾何学的特徴全体の結合情報と各ポイントペアのユークリッドオフセットに基づいてポイントを照合することを学習し、特徴ベクトルの内積をとるだけで照合の欠点を克服します。新しい相互監視損失は、キーポイントの追加アノテーションなしのモデル。パイプラインは、従来の両方と簡単に統合できます（たとえば、
[ABSTRACT]提案されたモデルには、反復距離を意識した類似性マトリックスが含まれます。これらには、2段階の学習可能なポイント削除手法が含まれており、誤検出対応を減らします。ペア</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-23">
        <br><font color="black">2019-10-23</font>
      </time>
    </span>
</section>
<!-- paper0: Fair DARTS: Eliminating Unfair Advantages in Differentiable Architecture
  Search -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_45.html">
      <font color="black">Fair DARTS: Eliminating Unfair Advantages in Differentiable Architecture
  Search</font>
    </a>
  </h2>
  <font color="black">それでもなお、離散化の不一致という重要な問題があります。具体的には、各操作のアーキテクチャの重みを他の操作から独立させます。DifferentialableArchitecture Search（DARTS）は、広く普及している重み共有ニューラルアーキテクチャ検索方法です。 
[ABSTRACT]アーキテクチャの根本は異なる方法で実行されています。ただし、パフォーマンスの崩壊の欠如に苦しんでいます。テストでは、2つの条件のいずれかが壊れている場合、崩壊が消えることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-27">
        <br><font color="black">2019-11-27</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Intra-domain Adaptation for Semantic Segmentation through
  Self-Supervision -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_46.html">
      <font color="black">Unsupervised Intra-domain Adaptation for Semantic Segmentation through
  Self-Supervision</font>
    </a>
  </h2>
  <font color="black">この作業では、ドメイン間ギャップとドメイン内ギャップを最小化するために、2ステップの自己監視ドメイン適応アプローチを提案します。最後に、ドメイン内ギャップを減らすために、自己監視適応技術を採用することを提案します。簡単な分割からハードな分割へ。畳み込みニューラルネットワークベースのアプローチは、セマンティックセグメンテーションで著しい進歩を遂げました。 
[要約]新しい手法では、ターゲットデータ自体の大きな分散ギャップ（ドメイン内ギャップ）は考慮されません。さらに、モデルのドメイン間適応を行います。この適応から、ターゲットドメインを簡単なネバダベースのランキング関数を使用したハード分割</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-16">
        <br><font color="black">2020-04-16</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Delicate Local Representations for Multi-Person Pose Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_47.html">
      <font color="black">Learning Delicate Local Representations for Multi-Person Pose Estimation</font>
    </a>
  </h2>
  <font color="black">私たちのアプローチは、COCO Keypoint Challenge 2019の1位を獲得し、追加のトレーニングデータや事前トレーニング済みのモデルを使用せずに、COCOとMPIIの両方のベンチマークで最先端の結果を達成しています。 ..私たちの単一モデルは、COCO test-devで78.6、MPIIテストデータセットで93.0を達成します。 
[ABSTRACT]出力フィーチャのローカル表現とグローバル表現のトレードオフを作成し、キーポイントの場所をさらに調整するための効率的な注意メカニズムを提案します。ソースコードは、今後の調査で公開されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-09">
        <br><font color="black">2020-03-09</font>
      </time>
    </span>
</section>
<!-- paper0: Proof of Concept: Automatic Type Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_48.html">
      <font color="black">Proof of Concept: Automatic Type Recognition</font>
    </a>
  </h2>
  <font color="black">タイプ分類については、元々フォントグループ分類に使用されていた深い畳み込みニューラルネットワーク（CNN）に依存し、検索ケースには一般的なライター識別方法を使用します。両方のシナリオで、簡単なタイプを分類/取得できることを示します。難しいケースは確かに難しいですが、高精度です。これは信頼できますが、遅く、専門的なスキルが必要です。 
[ABSTRACT]現在のタイプ認識は、「m」または「qu」の形状とタイプのサイズを使用して手動で行われ、大規模な参照作業でそれを検索します。新しく作成されたタイプ分類とタイプ検索のパフォーマンスを調査しますデータセット</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: SpaceNet: Make Free Space For Continual Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_49.html">
      <font color="black">SpaceNet: Make Free Space For Continual Learning</font>
    </a>
  </h2>
  <font color="black">さらに、モデルを拡張せずに建築ベースの方法よりも優れたパフォーマンスを実現し、リハーサルベースの方法で同等の結果を達成しながら、メモリを大幅に削減します。クラスの増分学習シナリオ）。この作業では、新しい建築ベースの方法を提案します。モデルの利用可能な固定容量をインテリジェントに利用するクラス増分学習シナリオではSpaceNetと呼ばれる方法。 
[ABSTRACT]継続的な学習の課題は、モデルが新しいタスク用に最適化された場合、特にデータにアクセスできない場合に学習されます。新しい方法は、モデルの利用可能な固定容量をインテリジェントに使用するクラス増分学習シナリオではspacenetと呼ばれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Real-Time Face and Landmark Localization for Eyeblink Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_50.html">
      <font color="black">Real-Time Face and Landmark Localization for Eyeblink Detection</font>
    </a>
  </h2>
  <font color="black">これらの2つのアルゴリズムはGPUとCPUで加速され、それぞれ1,753 $ \ times $と11 $ \ times $の高速化を達成しました。毎秒500フレームより高速です。実験中のまぶたの動きを追跡するために、研究者は伝統的に電位差計や筋電図を利用していました。 
[ABSTRACT]従来の研究者は、実験中のまぶたの動きを追跡するためにポテンシオムリッドまたは筋電図を使用していました。これらの2つのアルゴリズムはgpuとcpuで加速され、1〜753 $ / / / 11 $の高速化を達成しました。フレームあたり0.333ミリ秒</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-01">
        <br><font color="black">2020-06-01</font>
      </time>
    </span>
</section>
<!-- paper0: Lunar Terrain Relative Navigation Using a Convolutional Neural Network
  for Visual Crater Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_51.html">
      <font color="black">Lunar Terrain Relative Navigation Using a Convolutional Neural Network
  for Visual Crater Detection</font>
    </a>
  </h2>
  <font color="black">LunaNetは、画像の明るさの変化に対するロバスト性が高く、軌跡全体でフレームごとにクレーターを繰り返し再現できるため、シミュレートされた軌跡でより信頼性の高い位置追跡が可能になります。EKAと組み合わせたLunaNetにより、平均最終位置が60％減少します。標準輝度の画像を使用した軌道でテストした場合、画像処理ベースのクレーター検出方法を使用したEKFと比較して、推定誤差および平均最終速度推定誤差の25％の減少。地形相対ナビゲーションは、宇宙船の位置推定の精度を向上させることができます。慣性航法システムのドリフトを補正する補足測定として機能するグローバル機能を検出することにより。 
[要約]システムは、畳み込みニューラルネットワークと画像処理方法を使用して、拡張カルマンフィルター（ekf）でシミュレートされた宇宙船の位置を追跡します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Closed-Form Factorization of Latent Semantics in GANs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_52.html">
      <font color="black">Closed-Form Factorization of Latent Semantics in GANs</font>
    </a>
  </h2>
  <font color="black">潜在コードをGANのジェネレーターに取り込む完全に接続された層の重要な役割を研究することにより、潜在的な意味の発見のための一般的な閉形式の因数分解法を提案します。豊富なセマンティック属性のセットが、画像を合成するためにトレーニングされたジェネレーティブアドバサリネットワーク（GAN）の潜在スペース。高速かつ効率的な実装により、私たちのアプローチは、潜在的なセマンティクスを最先端の監視対象メソッドと同じくらい正確に見つけるだけでなく、結果として幅広いデータセットでトレーニングされた複数のGANモデルにまたがる、はるかに用途の広いセマンティッククラス。 
[要約]潜在的なオランダ人のソフトウェアを特定するためにgansが情報を取得しました。gansは、監視されていない方法で潜在モデルの役割を明らかにしました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic Recall Machines: Internal Replay, Continual Learning and the
  Brain -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_53.html">
      <font color="black">Automatic Recall Machines: Internal Replay, Continual Learning and the
  Brain</font>
    </a>
  </h2>
  <font color="black">代わりに、評価されたモデル自体の中で学習したサンプルの暗黙的なメモリが活用されます。特に、推論と想起のための単一モデルの使用、想起されたサンプルの現在の環境バッチへの依存性、トップアクティブ化と学習のダウンダウン変調、抽象的な再現、およびタスクが学習された度合いと再現された度合いとの間の依存関係。さらに、既存の作業は、見られたデータ分布全体を強化することに焦点を当てていますが、より効率的でスケーラブルな、実際の各トレーニングバッチに特化したサンプルの生成の呼び出しを忘れないように最適化する。 
[ABSTRACT]これに加えて、これらの補助的な次数がオンザフライで生成される方法を示します。代わりに、外部のバッファやジェネレータネットワークなしで、評価目的のためにトレーニングされているモデル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-22">
        <br><font color="black">2020-06-22</font>
      </time>
    </span>
</section>
<!-- paper0: Visualizing Transfer Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_54.html">
      <font color="black">Visualizing Transfer Learning</font>
    </a>
  </h2>
  <font color="black">この分析のために作成した大規模なデータセットを公開します。転移学習の一時的なプロセス中に、深い画像認識ネットワークの個々のニューロンの視覚化を提供します。これらの視覚化は、転移のさまざまな新しい特性を定性的に示します適応の速度と特性、ニューロンの再利用、表現された画像の特徴の空間スケール、および小さなデータへの転移学習の動作に関する学習プロセス。 
[要約]これらの視覚化は、転移学習プロセスのさまざまな新しい特性を示します。これらには、適応の速度と特性、ニューロンの再利用、表現された画像の特徴の空間スケールが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Focus-and-Expand: Training Guidance Through Gradual Manipulation of
  Input Features -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_55.html">
      <font color="black">Focus-and-Expand: Training Guidance Through Gradual Manipulation of
  Input Features</font>
    </a>
  </h2>
  <font color="black">\ faxは、各入力データポイントから機能のサブセットを抽出し、最初にこれらの機能を学習者に公開し、ソリューションに焦点を当てます。このプロセスは、他の機能よりも必要な機能の検討を促進します。バイアスの場合、バイアス機能を無視するソリューションは、より堅牢で正確です。 
[要約]たとえば、入力の特定の特徴を他の特徴よりも考慮するようにトレーニングプロセスをステアリングすることを提案します。サブセットまたはミキシングツールを使用して、学習プロセスを徐々に拡張して、入力のすべての特徴を含めます。これらの状態-アートバイアスの除去、確立された拡張方法の改善、および画像分類タスクの改善の2つの例</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Relative Pose Estimation of Calibrated Cameras with Known
  $\mathrm{SE}(3)$ Invariants -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_56.html">
      <font color="black">Relative Pose Estimation of Calibrated Cameras with Known
  $\mathrm{SE}(3)$ Invariants</font>
    </a>
  </h2>
  <font color="black">追加の制約付きの従来の相対ポーズ推定とは異なり、制約をカメラフレームに変換するための外部キャリブレーションは必要ありません。$ \ mathrm {SE}（3）$不変式によって制約される相対ポーズ推定の概念を提案することに加えて、相対姿勢推定のための既存の多項式定式化の包括的な研究とそれらの関係の発見。$ \ mathrm {SE}（3）$不変制約は、追加のセンサー測定値またはモーション仮定から生じます。 
[ABSTRACT]新しい論文で、既知の$ tea mathrmによって制約された調整済みカメラの相対姿勢推定問題の完全な包括的な研究を紹介します。これらには、追加のセンサー測定またはモーション仮定から生じる可能性がある合計5つの最小問題が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Learning with Privileged Information for Efficient Image
  Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_57.html">
      <font color="black">Learning with Privileged Information for Efficient Image
  Super-Resolution</font>
    </a>
  </h2>
  <font color="black">教師のエンコーダーは、模倣損失を使用して、劣化プロセス、HR画像のサブサンプリングを学習します。私たちのコードとモデルは、オンラインで利用できます：https://cvlab.yonsei.ac.kr/projects/PISR.。 FSRCNNのパフォーマンスを大幅に向上させることができる教師と生徒のネットワークで構成される、新しい蒸留フレームワークを論文化します。 
[ABSTRACT] cnnに基づくほとんどのsrメソッドは、従来のアプローチよりもパフォーマンスの向上をコピーすることに焦点を当てています。従来のアプローチよりもpsnrやssimなどの品質メトリックの観点からメモリの向上を学習します。fsrcnnは、いくつかのたたみ込みで構成されますレイヤー、非常に少数のネットワークパラメーターを使用しながら有望な結果を示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Augmented Bi-path Network for Few-shot Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_58.html">
      <font color="black">Augmented Bi-path Network for Few-shot Learning</font>
    </a>
  </h2>
  <font color="black">広範な実験により、提案されたABNetが最新の方法よりも優れていることがわかります。具体的には、顕著なパッチが抽出され、すべての画像のローカル機能として埋め込まれます。最後に、モデルはグローバル機能とローカル機能を別々に比較することを学習します。つまり、類似点をマージする前に、2つのパスで。 
[ABSTRACT] fslメソッドは、2つの画像の特徴を連結し、それらをニューラルネットワークに供給するだけで、テストおよびトレーニング（サポート）画像を比較することを学習します。いくつかの単純な画像レベルの比較では、重大な誤分類を引き起こす可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Reorganizing local image features with chaotic maps: an application to
  texture recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_59.html">
      <font color="black">Reorganizing local image features with chaotic maps: an application to
  texture recognition</font>
    </a>
  </h2>
  <font color="black">より具体的には、画像を3次元ユークリッド空間にマッピングし、この3次元構造に対してカオスマップを反復処理して元の画像に変換します。このようなカオス変換された画像から、反復ごとにローカル記述子（ここではローカルバイナリパターンを使用します）およびそれらの記述子はテクスチャの機能表現を構成します。テクスチャ認識における畳み込みニューラルネットワークの最近の成功にもかかわらず、特にアノテーション付きの大量のデータにアクセスできない場合、モデルベースの記述子は依然として競争力がありますトレーニングのためのデータとモデルの解釈は重要な問題です。 
[ABSTRACT]結果は、文献にあるいくつかの学習ベースの方法と比較した場合でも、競争力のあるパフォーマンスに対する私たちの期待を裏付けました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Borrow from Anywhere: Pseudo Multi-modal Object Detection in Thermal
  Imagery -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_60.html">
      <font color="black">Borrow from Anywhere: Pseudo Multi-modal Object Detection in Thermal
  Imagery</font>
    </a>
  </h2>
  <font color="black">私たちのフレームワークは、2つのドメインからのペアのトレーニング例を明示的に必要とせずに、既存のベンチマークよりも優れていることを示しています。また、このフレームワークは、アプローチを使用するときに、熱ドメインからのデータを少なくして学習できることを示しています。コードと事前訓練されたモデルは、https：//github.com/tdchaitanya/MMTODで入手できます。
[要約]自然画像ドメインデータで訓練されたスーパーマルチモーダルオブジェクト検出器を提案し、熱画像での物体検出のパフォーマンスを向上させます。また、私たちのフレームワークは、熱領域からより少ないデータで学習する能力を持っています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-05-21">
        <br><font color="black">2019-05-21</font>
      </time>
    </span>
</section>
<!-- paper0: P$^{2}$Net: Patch-match and Plane-regularization for Unsupervised Indoor
  Depth Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_61.html">
      <font color="black">P$^{2}$Net: Patch-match and Plane-regularization for Unsupervised Indoor
  Depth Estimation</font>
    </a>
  </h2>
  <font color="black">まず、大きな局所勾配を持つポイントを抽出し、各ポイントを中心とするパッチをその表現として採用します。このホワイトペーパーでは、パフォーマンスの低下がポイントベースの非弁別マッチングに影響されていることを主張しています。この操作により、ネットワークトレーニング。 
[ABSTRACT]これらのシーンには非テクスチャ領域の広大な領域があるため、タスクは非常に困難です。ただし、領域がマスクされている場合でも、パフォーマンスポイントはまだ不十分です。スーパーピクセルは事前に平面として使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-end training of a two-stage neural network for defect detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_62.html">
      <font color="black">End-to-end training of a two-stage neural network for defect detection</font>
    </a>
  </h2>
  <font color="black">DAGMとKolektorSDDで、100 \％の検出率を示し、データセットを完全に解決します。学習への追加の拡張として、過剰および不足の問題に対処するために、負のサンプルの使用頻度サンプリングスキームを提案します-トレーニング中の画像のサンプリング。領域ベースのセグメンテーションマスクで距離変換アルゴリズムをポジティブピクセルの重みとして使用しているため、詳細なアノテーションを必要とせずに、欠陥が存在する可能性が高い領域を重視します。不安定な機能による学習の破壊を防ぐために、分類からセグメンテーションネットワークへの勾配フロー。 
[ABSTRACT]結果を改善するためにゼロトレーニングのネットワークが導入されました。これらには、エンドツーエンドのトレーニングとトレーニングプロセスへの拡張が含まれます。提案された拡張は、トレーニング時間を短縮するように設計されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: COCO-FUNIT: Few-Shot Unsupervised Image Translation with a Content
  Conditioned Style Encoder -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_63.html">
      <font color="black">COCO-FUNIT: Few-Shot Unsupervised Image Translation with a Content
  Conditioned Style Encoder</font>
    </a>
  </h2>
  <font color="black">驚くほど成功しているものの、既存の数枚の画像から画像への変換モデルでは、\ textit {content loss}問題と呼ばれる見えないドメインの外観をエミュレートしながら、入力画像の構造を維持することが困難であることがわかります。最先端の技術と比較した広範な実験的検証を通じて、私たちのモデルはコンテンツ損失の問題に対処する有効性を示しています。コードと事前トレーニング済みモデルはhttps://nvlabs.github.io/COCO-FUNIT/で入手できます。 
[要旨]モデルを目に見えない領域に一般化しようとする最近の試み。現在存在する例は見られません。これは、入力画像と例画像のオブジェクトのポーズが非常に異なる場合、特に深刻です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: RGB-IR Cross-modality Person ReID based on Teacher-Student GAN Model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_64.html">
      <font color="black">RGB-IR Cross-modality Person ReID based on Teacher-Student GAN Model</font>
    </a>
  </h2>
  <font color="black">（2）IDのトレーニングを開始するために、ReID TeacherモジュールがIRモダリティの人物画像の下でトレーニングされ、トレーニングで学生の対応者を導くために使用されます。（3）同様に、異なるドメイン機能をより適切に適合させ、強化するモデルReIDのパフォーマンスでは、3つのTeacher-Student損失関数が使用されました。（1）対応するRGB-IR画像ペアを取得するために、RGB-IR生成的敵対的ネットワーク（GAN）を使用してIR画像が生成されました。 
[ABSTRACT]このタスクの課題は、異なるガイドガイドの異なるモダリティの下での機能のモダリティギャップです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: RobustScanner: Dynamically Enhancing Positional Clues for Robust Text
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_65.html">
      <font color="black">RobustScanner: Dynamically Enhancing Positional Clues for Robust Text
  Recognition</font>
    </a>
  </h2>
  <font color="black">経験的に、代表的な文字レベルシーケンスデコーダーはコンテキスト情報だけでなく位置情報も利用することがわかりました。具体的には、独自の空間位置をエンコードするエンコーダー出力特徴ベクトルを作成するための位置認識モジュールと、垣間見るための注意モジュールが含まれています位置情報（つまり、現在のデコードタイムステップ）のみを使用します。コンテキスト情報に大きく依存する既存のアプローチでは、注意のドリフトの問題が発生します。 
[ABSTRACT]代表的な文字レベルのシーケンスデコーダーは、コンテキスト情報だけでなく位置情報も使用します。高度なデコーダーは、その出力をデコーダーアテンションモジュールと組み合わせてシーンテキストを認識します。動的融合は、要素ごとのゲートを介してより堅牢な機能のために行われます機構</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Decoding CNN based Object Classifier Using Visualization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_66.html">
      <font color="black">Decoding CNN based Object Classifier Using Visualization</font>
    </a>
  </h2>
  <font color="black">この調査は、モデルの精度が低い理由を推論するのにも役立ちます。オブジェクト検出モジュールへの信頼を高めるのに役立ちます。アクティベーションのヒートマップを視覚化すると、CNNが画像内のさまざまなオブジェクトを分類およびローカライズする方法を理解するのに役立ちます。ニューラルネットワーク（CNN）は、自律車両の機械知覚のコンテキストで視覚化することで説明できます。 
[要旨] cnnは、コンテキストのさまざまな畳み込みレイヤーで抽出された特徴のタイプを分析します。これは、cnnが画像内のさまざまなオブジェクトを分類およびローカライズする方法を理解するのに役立ちます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Face Recognition by Clustering Unlabeled Faces in the Wild -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_67.html">
      <font color="black">Improving Face Recognition by Clustering Unlabeled Faces in the Wild</font>
    </a>
  </h2>
  <font color="black">これに対処するために、極値理論に基づく新しいアイデンティティー分離方法を提案します。クラスター割り当てを疑似ラベルとして考えると、クラスターエラーからのラベリングノイズも克服する必要があります。監視対象ベースラインに対する改善、たとえば、IJB-A検証の11.6％改善。 
[ABSTRACT]以前の作業は主に制御された設定で行われており、ラベル付きとラベルなしのデータセットには、構成による周回IDがありません。同じIDからの競合データが複数のクラスターに分割されるため、現実的でないスケールでは、重大なラベル付けノイズが発生します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: CANet: Context Aware Network for 3D Brain Tumor Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_68.html">
      <font color="black">CANet: Context Aware Network for 3D Brain Tumor Segmentation</font>
    </a>
  </h2>
  <font color="black">HCA-FEは、畳み込み空間と機能の相互作用グラフの両方からのコンテキストを使用して、高次元で識別可能な機能をキャプチャします。3D磁気共鳴イメージングでの脳腫瘍の自動セグメンテーションは、腫瘍の診断、進行の監視、および手術計画で積極的な役割を果たします。確率的グラフィカルモデルの強力な推論機能。隠れた機能マップを学習し、CG-ACRFを使用してさまざまなコンテキストの機能を融合します。 
[要約]これまでの研究では、脳腫瘍セグメンテーションのためのいくつかの有望な技術が示されています。これらには脳腫瘍セグメンテーションが含まれますが、これらはまだ有望です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: FastReID: A Pytorch Toolbox for General Instance Re-identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_69.html">
      <font color="black">FastReID: A Pytorch Toolbox for General Instance Re-identification</font>
    </a>
  </h2>
  <font color="black">FastReIDは、単一および複数のGPUサーバーをサポートする最も一般的で高性能なツールボックスであり、プロジェクトの結果を非常に簡単に再現できます。コードとモデルはhttps://github.comで入手できます。 / JDAI-CV / fast-reid ..フレンドリーで管理しやすいシステム構成とエンジニアリングデプロイメント機能により、開業医はモデルをプロダクションにすばやくデプロイできます。PersonRe-id、パーシャルリードなど、最新のプロジェクトを実装しましたid、クロスドメインre-id、車両re-id、およびこれらの事前トレーニング済みモデルを複数のベンチマークデータセットでリリースする予定です。 
[要約] fastreidシステムは高度にモジュール化されており、拡張可能です。これにより、研究者は新しい研究アイデアを簡単に達成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br><font color="black">2020-06-04</font>
      </time>
    </span>
</section>
<!-- paper0: Temporal Distinct Representation Learning for Action Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_70.html">
      <font color="black">Temporal Distinct Representation Learning for Action Recognition</font>
    </a>
  </h2>
  <font color="black">2）時間的多様性損失（TD損失）を作成して、カーネルに強制的に集中させ、同様の外観を持つ画像領域ではなく、フレーム間の変動をキャプチャします。この方法は、ベンチマークの時間的推論データセットSomething-Something V1およびV2で評価されます。そして、最高の競合他社に対してそれぞれ2.4％と1.3％の目に見える改善を達成しています。さらに、大規模なデータセットキネティクスの2D-CNNベースの最先端のパフォーマンスの改善も目撃されています。 
[ABSTRACT] cnnは、この方法を使用して、異なるフレームから特徴の特徴的なチャネルを段階的に励起し、繰り返しの情報抽出を回避します。この方法は、ベンチマークデータセットで評価され、最高の競合他社に対して2.4％の目に見える改善を達成し、 1.それぞれ3％</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Weakly Supervised Person Re-ID: Differentiable Graphical Learning and A
  New Benchmark -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_71.html">
      <font color="black">Weakly Supervised Person Re-ID: Differentiable Graphical Learning and A
  New Benchmark</font>
    </a>
  </h2>
  <font color="black">完全に監視されたRe-IDモデルと比較すると、この方法はSYSU-30 $ k $およびその他のデータセットで最先端のパフォーマンスを実現します。合計で29,606,918枚の画像が得られます。バッグ付きのRe-IDモデルの学習レベルの注釈は、弱く監視されたRe-ID問題と呼ばれます。 
[ABSTRACT] re idの新しいベンチコートには、$ 30k $の個人が含まれています。これは、cuhk03（$ 1. 3k $個人）および市場-1501（$ 1,000）の約$ 20 $倍です。新しいベンチマークには、re -バッグ付きIDモデル-レベル注釈</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-08">
        <br><font color="black">2019-04-08</font>
      </time>
    </span>
</section>
<!-- paper0: VidCEP: Complex Event Processing Framework to Detect Spatiotemporal
  Patterns in Video Streams -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_72.html">
      <font color="black">VidCEP: Complex Event Processing Framework to Detect Spatiotemporal
  Patterns in Video Streams</font>
    </a>
  </h2>
  <font color="black">この作業では、ユーザーがビデオに対して高レベルの表現力豊かなクエリを定義して、時空間イベントパターンの範囲を検出できるCEPフレームワークに焦点を当てます。システムは、ビデオストリームにグラフベースのイベント表現を使用して、高ディープニューラルネットワークモデルのカスケードを使用したビデオからのレベルセマンティックコンセプト、ii）CEPでビデオストリームに対する高レベルのユーザークエリを表現するビデオイベントクエリ言語（VEQL）、iii）マッチングにより時空間ビデオイベントパターンを検出する複雑なイベントマッチャービデオデータに対する表現力豊かなユーザークエリ..このコンテキストでは、次のことを提案します。i）ビデオストリーム用のオンザフライでほぼリアルタイムの複雑なイベントマッチングフレームワークであるインメモリのVidCEP。 
[ABSTRACT]現在のcepシステムは、非構造化データモデルと表現力豊かな検索言語がないため、質問ビデオストリームに固有の制限があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Finding Non-Uniform Quantization Schemes usingMulti-Task Gaussian
  Processes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_73.html">
      <font color="black">Finding Non-Uniform Quantization Schemes usingMulti-Task Gaussian
  Processes</font>
    </a>
  </h2>
  <font color="black">ニューラルアーキテクチャの検索問題をハイパーパラメータ検索の1つとしてキャストし、CNNのレイヤー全体で不均一なビット分布を見つけるニューラルネットワーク量子化の新しい方法を提案します。事前にマルチタスクガウスプロセスを想定して検索を実行します。問題を複数のタスクに分割し、それぞれが異なる数のトレーニングエポックに対応し、最大の情報が得られる構成をサンプリングして空間を探索します。VGG、ResNet、およびGoogLeNetアーキテクチャを使用して、CIFAR10およびImageNetデータセットの調査結果をテストします。 
[ABSTRACT] cifar10およびimagenetデータセットで調査結果をテストします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Inspector Gadget: A Data Programming-based Labeling System for
  Industrial Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_74.html">
      <font color="black">Inspector Gadget: A Data Programming-based Labeling System for
  Industrial Images</font>
    </a>
  </h2>
  <font color="black">実際の産業用画像データセットで実験を行い、Inspectorガジェットが他の弱ラベル付け手法よりも優れたパフォーマンスを得ることが示されています。この変換なしで画像に直接適用することにより、データプログラミングの地平を拡大します。これは、産業用アプリケーションの一般的なシナリオです。データプログラミングは、このカテゴリの最近のパラダイムであり、ラベル付け関数の形式で人間の知識を使用し、それらを組み合わせます。生成モデル。 
[ABSTRACT]問題は、スマート工場が製品の品質管理のために機械学習に依存している製造現場で特に重要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-07">
        <br><font color="black">2020-04-07</font>
      </time>
    </span>
</section>
<!-- paper0: CycAs: Self-supervised Cycle Association for Learning Re-identifiable
  Descriptions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_75.html">
      <font color="black">CycAs: Self-supervised Cycle Association for Learning Re-identifiable
  Descriptions</font>
    </a>
  </h2>
  <font color="black">エンドツーエンドのトレーニングが実現可能になるように、離散関連付けプロセスを微分可能な形式に適合させます。この目標を達成するために、モデルは、フレームペアのインスタンス間の対応を適切に説明できる意味のある表現を学習する必要があります。マルチオブジェクト追跡における\ emph {データ関連付け}の概念として、\ textbf {Cyc} le \ textbf {As} sociation（\ textbf {CycAs}）タスクを提案します：一対のビデオフレーム間のデータ関連付けを実行した後、後方では、歩行者のインスタンスはそれ自体に関連付けられることになっています。 
[ABSTRACT]目標は、自己監視された口実タスクを作成することです。目標を達成するには、モデルは意味のある表現を学習する必要があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Human-Expert-Level Brain Tumor Detection Using Deep Learning with Data
  Distillation and Augmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_76.html">
      <font color="black">Human-Expert-Level Brain Tumor Detection Using Deep Learning with Data
  Distillation and Augmentation</font>
    </a>
  </h2>
  <font color="black">これらの課題を克服するために、特に代表的なトレーニング例を抽出し、あるクラスのこれらのサンプルを同じクラスおよび他のクラスのサンプルと混合して追加のトレーニングサンプルを作成することにより、トレーニングデータを増強するディープニューラルネットワークをトレーニングする新しい方法を提案します。医療診断へのディープラーニング（DL）の適用は、多くの場合2つの問題によって妨げられます。この手法によりパフォーマンスが大幅に向上し、数千のトレーニング例でこの手法が専門家レベルの精度に到達できることを示しています。 
[ABSTRACT]トレーニングデータの量は、診断対象の状態と診断された患者の数に限られているため、不足している場合があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-17">
        <br><font color="black">2020-06-17</font>
      </time>
    </span>
</section>
<!-- paper0: AdaptiveReID: Adaptive L2 Regularization in Person Re-Identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_77.html">
      <font color="black">AdaptiveReID: Adaptive L2 Regularization in Person Re-Identification</font>
    </a>
  </h2>
  <font color="black">特に、MSMT17で最新のパフォーマンスが得られます。これは、個人の再識別用の最大のデータセットです。既存のアプローチとは異なり、提案手法の正則化係数は、バックプロパゲーションを通じて適応的に更新されます。個人の再識別の設定において、AdaptiveReIDと呼ばれるL2正則化メカニズム。 
[ABSTRACT]実践には、トレーニング手順全体を通して一定のままである正則化因子の使用が含まれます。これは、正則化因子としてトレーニング可能なスカラー変数を組み込むことによって達成されます。msmt17は、個人の再識別のための最大のデータセットです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: A cellular automata approach to local patterns for texture recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_78.html">
      <font color="black">A cellular automata approach to local patterns for texture recognition</font>
    </a>
  </h2>
  <font color="black">記述子は、セル状態の分布から取得されます。このコンテキストで、ここでは、セルオートマトンによる複雑なオブジェクトの表現力と、テクスチャ分析におけるローカル記述子の既知の有効性を組み合わせたテクスチャ記述子の方法を提案します。提案された記述子ベンチマークデータセットと実際の問題の両方のテクスチャ画像の分類に適用されます。つまり、葉の表面のテクスチャに基づいて植物種を識別する問題です。 
[ABSTRACT]システムはローカルバイナリ記述子に基づいています。これらの記述子はセル状態の分布から収集されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Feature Quantization Improves GAN Training -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_79.html">
      <font color="black">Feature Quantization Improves GAN Training</font>
    </a>
  </h2>
  <font color="black">9つのベンチマークで3つの代表的なGANモデルにFQを適用します。画像生成にはBigGAN、顔合成にはStyleGAN、教師なしの画像から画像への変換にはU-GAT-ITを使用します。したがって、FQは暗黙的にコンパクトなスペースで堅牢な機能照合を可能にします..私たちの方法は、トレーニングの計算オーバーヘッドがほとんどなく、既存のGANモデルに簡単にプラグインできます。 
[ABSTRACT]新しい方法は、既存のガンモデルに簡単にプラグインできます。さまざまなタスクのマージンを大幅に増やして、ベースラインメソッドのfiqスコアを改善し、新しい最先端のパフォーマンスを実現できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-05">
        <br><font color="black">2020-04-05</font>
      </time>
    </span>
</section>
<!-- paper0: Explaining Deep Neural Networks using Unsupervised Clustering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_80.html">
      <font color="black">Explaining Deep Neural Networks using Unsupervised Clustering</font>
    </a>
  </h2>
  <font color="black">事前トレーニング済みのDNNが与えられた画像データセットで、類似のトレーニングサンプルを見つけ、DNNが決定の基礎とする概念に光を当てる方法の強さを示します。トレーニング済みのディープニューラルネットワーク（DNN）を説明する新しい方法を提案します。 、教師なしクラスタリングを使用してそれらを代理モデルに蒸留することにより。ユーザーの研究を通じて、私たちのモデルがモデルの予測におけるユーザーの信頼を改善できることを示しています。 
[ABSTRACT]私たちの方法は、柔軟なdnnアーキテクチャのレイヤーのサブセットに適用できます。低レベルおよび高レベルの情報を組み込むことができます。これらのデータは、情報を提供するために使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: P2D: a self-supervised method for depth estimation from polarimetry -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_81.html">
      <font color="black">P2D: a self-supervised method for depth estimation from polarimetry</font>
    </a>
  </h2>
  <font color="black">さらに、データの特定の利点を活用するために、最新の方法に偏光測定正則化という用語を含めます。この現象に対応して、自己監視モノデプスネットワークの入力として偏光測定を使用することを提案します。 、シーンの正確な再構成を促進するために偏光キューを利用することを提案します。 
[ABSTRACT]方法は遠近法フィールドの義務を減らしながら深度マップに基づいています。この方法はシーンの正確な再構成を促進するために使用されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Guessing State Tracking for Visual Dialogue -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_82.html">
      <font color="black">Guessing State Tracking for Visual Dialogue</font>
    </a>
  </h2>
  <font color="black">初期の監視は、初期のラウンドでGuesserに監視をもたらし、増分の監視は、推測状態に単調性をもたらします。ほとんどの既存の推測は、事前定義された回数のダイアログですべての質問と回答のペアを受け取った後で、1回だけ推測します。実験結果GuessWhatで？！ 
[ABSTRACT]推測者はすべての質問を受け取った後、1回だけ推測します-事前定義されたラウンド数のダイアログで回答ペア</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-24">
        <br><font color="black">2020-02-24</font>
      </time>
    </span>
</section>
<!-- paper0: A DICOM Framework for Machine Learning Pipelines against Real-Time
  Radiology Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_83.html">
      <font color="black">A DICOM Framework for Machine Learning Pipelines against Real-Time
  Radiology Images</font>
    </a>
  </h2>
  <font color="black">Nifflerは19か月以上安定して稼働しており、部門でいくつかの研究プロジェクトをサポートしています。Nifflerは、MLパイプラインからの結果を匿名化して共有できるようにします。Nifflerプロトタイプの評価は、その実現可能性と画像とメタデータのMLパイプラインをリアルタイムかつ遡及的に容易にする効率。 
[ABSTRACT] nifflerは、研究センターでのmlパイプラインの実行を可能にする統合フレームワークを提案します。nifflerは、mlパイプラインからの結果を特定されていない方法で共有できるようにします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-16">
        <br><font color="black">2020-04-16</font>
      </time>
    </span>
</section>
<!-- paper0: Across Scales & Across Dimensions: Temporal Super-Resolution using Deep
  Internal Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CV/paper_84.html">
      <font color="black">Across Scales & Across Dimensions: Temporal Super-Resolution using Deep
  Internal Learning</font>
    </a>
  </h2>
  <font color="black">低フレームレートの入力ビデオから直接抽出された例でビデオ固有のCNNをトレーニングします。このような内部のビデオ固有の例は、データを必要とせず、入力ビデオ自体を必要とする強力な自己監視を引き起こします。入力ビデオの時間ナイキスト制限を超える時間周波数。これにより、時間フレームの補間（多分高度な）で元に戻すことができないモーションブラーとモーションエイリアシングの両方の影響を解決します。 
[ABSTRACT]真の時間的超解像（tsr）は、時間的-補間（フレームレートの増加）だけではありません。新しい方法には、単一のビデオシーケンス内の小さな空間-時間パッチの強力な反復が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-19">
        <br><font color="black">2020-03-19</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: AdapterHub: A Framework for Adapting Transformers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CL/paper_0.html">
      <font color="black">AdapterHub: A Framework for Adapting Transformers</font>
    </a>
  </h2>
  <font color="black">NLPの現在の手口には、数百万または数十億のパラメーターからなる事前トレーニング済みモデルのダウンロードと微調整が含まれます。アダプター-事前トレーニング済みモデルの各レイヤー内に挿入された小さな学習済みボトルネックレイヤー-完全な回避によってこの問題を改善しますモデル全体を微調整します。さまざまなタスクや言語に対応する事前トレーニング済みアダプターの動的な「ステッチイン」を可能にするフレームワークであるAdapterHubを提案します。 
[ABSTRACT]フレームワークは、人気のあるhuggingfaceトランスフォーマーライブラリーの上に構築されています。これにより、最新のモデルを簡単かつ迅速に適応できます。これらには、bert、roberta、xlmを含む最新のモデルが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Dialect Diversity in Text Summarization on Twitter -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CL/paper_1.html">
      <font color="black">Dialect Diversity in Text Summarization on Twitter</font>
    </a>
  </h2>
  <font color="black">一般的な要約アルゴリズムによって生成されたそのようなデータセットの要約における方言バイアスを調査し、複数の方言からの文があるデータセットの場合、ほとんどの要約アルゴリズムが少数派の方言を代表しない要約を返すことを観察します。しかし、Twitterデータセットには重要なのは、データセット内の文に方言ラベルを付ける必要がないため、多様化プロセスが方言分類や言語識別モデルから独立していることを保証することです。 
[ABSTRACT] Twitterデータセットには、さまざまな英語の方言で書かれた投稿のかなりの割合があります。データセットは、Twitterデータセットがかなりの量のマライゼーションプロセスを持っていたという事実に基づいています。標準の要約アプローチに</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on
  Unlabeled Data in Target Language -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CL/paper_2.html">
      <font color="black">Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on
  Unlabeled Data in Target Language</font>
    </a>
  </h2>
  <font color="black">ベンチマークデータセットでの3つのターゲット言語に対する広範な実験は、この方法が単一ソースと複数ソースのクロスリンガルNERの両方で既存の最先端の方法よりも優れていることをよく示しています。提案された方法は、単一ソースと複数ソースの両方で機能します。 -sourceクロスリンガルNER ..ただし、ソース言語のラベル付きデータが利用できない場合、またはターゲット言語のラベルなしデータに含まれる情報を利用しない場合、このような方法は適用できません。 
[ABSTRACT]ソース言語のナーモデルは、ターゲット言語のラベルなしデータで生徒モデルをトレーニングする教師として使用されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-26">
        <br><font color="black">2020-04-26</font>
      </time>
    </span>
</section>
<!-- paper0: Are We There Yet? Evaluating State-of-the-Art Neural Network based
  Geoparsers Using EUPEG as a Benchmarking Platform -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CL/paper_3.html">
      <font color="black">Are We There Yet? Evaluating State-of-the-Art Neural Network based
  Geoparsers Using EUPEG as a Benchmarking Platform</font>
    </a>
  </h2>
  <font color="black">同じジオパーサーが異なるデータセットで非常に異なるパフォーマンスを持つ可能性があることが知られています。このエキサイティングな結果は、「まだそこにいますか？」という疑問をもたらします。したがって、この作業は、これらの最先端のジオパーサーを体系的に評価します。 8つの注釈付きデータセット、9つのベースラインジオパーサー、および8つのパフォーマンスメトリックを備えた最近開発されたベンチマークプラットフォームEUPEG。
[ABSTRACT]ジオパーシングと呼ばれるジオパーシングシステムは、入力としていくつかのテキストを取ります。同じジオパーサーは非常に異なる場合があることが知られています異なるデータセットでのパフォーマンス</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Logic Constrained Pointer Networks for Interpretable Textual Similarity -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CL/paper_4.html">
      <font color="black">Logic Constrained Pointer Networks for Interpretable Textual Similarity</font>
    </a>
  </h2>
  <font color="black">この基本モデルを損失関数で改善して、両方の文のミスアライメントを等しくペナルティし、アライメントが双方向であることを確認します。最後に、構造化された外部知識でネットワークをガイドするために、ConceptNetおよび構文知識に基づく1次論理制約を導入します。意味のテキストの類似性の解釈可能なモデルにつながる文のコンポーネントの整列の問題を研究します。 
[ABSTRACT]文の分析-サブシーケンスアラインメントによるレベルスコアが提案されました。調査は、歩哨ゲーティング機能を備えたネットワークベースのモデルを分析することによって行われました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Sinhala Language Corpora and Stopwords from a Decade of Sri Lankan
  Facebook -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CL/paper_5.html">
      <font color="black">Sinhala Language Corpora and Stopwords from a Decade of Sri Lankan
  Facebook</font>
    </a>
  </h2>
  <font color="black">両方のコーパスには、作成日、起点のページ、およびコンテンツタイプのマーカーがあります。このペーパーでは、LIRNEasiaのデータ、分析、およびポリシーチームの言語努力からの2つの口語的なシンハラ語コーパスと、アルゴリズムによって導出されたリストを示します。ストップワード.. 2つのコーパスの大きい方は2010〜2020年に及び、政治、メディア、有名人、およびその他のカテゴリを含む533のスリランカFacebookページから投稿された28,825,820〜29,549,672ワードの多言語テキストが含まれます。小さいコーパスは、大きいコーパスから抽出されたシンハラ語テキストのみの5,402,76ワードになります。 
[要約] 2010年から2020年までの2つのコーパスの大きい方。コーパスは533のスリランカのFacebookページから投稿された28、825、820から29、549、672語の多言語テキストです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Cross-Lingual Speaker Verification with Domain-Balanced Hard Prototype
  Mining and Language-Dependent Score Normalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CL/paper_6.html">
      <font color="black">Cross-Lingual Speaker Verification with Domain-Balanced Hard Prototype
  Mining and Language-Dependent Score Normalization</font>
    </a>
  </h2>
  <font color="black">クロスリンガルトライアルのスコアリングを強化するために、言語依存のs-normスコアの正規化を提案します。最新のECAPA-TDNN x-ベクトルベースの最先端のECAPA-TDNN x-vectorを微調整するドメインバランスのハードプロトタイプマイニングを紹介しますSpeaker embedding extractor ..偽者コホートには、常にペルシア語である登録データをシミュレートするペルシア語のターゲットドメインからのデータのみが含まれています。 
[ABSTRACT]サンプルマイニング手法は、人気のaamのスピーカープロトタイプ間のスピーカー距離を効率的に活用します-ソフトマックス損失関数</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Fine-Tune Longformer for Jointly Predicting Rumor Stance and Veracity -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CL/paper_7.html">
      <font color="black">Fine-Tune Longformer for Jointly Predicting Rumor Stance and Veracity</font>
    </a>
  </h2>
  <font color="black">多くの調査研究では、そのようなイベントやニュースのディスカッションスレッドで投稿のスタンスを特定することが、噂の真実性を特定する前の重要な先行ステップであることが明らかになっています。SemEval2019タスク7データセットの実験結果は、噂のスタンス分類と真実性予測の両方。 b）上部は、下部から得られるスタンスの進化によって会話スレッドの噂の真実性を予測します。 
[要旨]ソーシャルメディアプラットフォームと使用の増加により、データが大量に利用可能になりました。このような大量のデータを手動で処理する方法は、費用と時間がかかります。このようなコンテンツを自動的に処理して存在を確認することへの関心が高まっています。噂の</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: CAN-NER: Convolutional Attention Network for Chinese Named Entity
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CL/paper_8.html">
      <font color="black">CAN-NER: Convolutional Attention Network for Chinese Named Entity
  Recognition</font>
    </a>
  </h2>
  <font color="black">また、他のモデルと比較して、レキシコンなどの外部リソースに依存せず、小さなサイズのchar埋め込みを採用することで、モデルの実用性が高まります。広範な実験結果は、この手法が、単語埋め込みや外部埋め込みなしで最先端の方法よりも優れていることを示しています。 Weibo、MSRA、Chinese Resume NERデータセットなど、さまざまなドメインデータセットのレキシコンリソース。このホワイトペーパーでは、ローカル注意を持つ文字ベースの畳み込みニューラルネットワーク（CNN）で構成される、中国語NERのCANと呼ばれる畳み込み注意ネットワークを調査します。レイヤーと、隣接する文字や文のコンテキストから情報を取得するグローバルなセルフアテンションレイヤーを備えたゲーテッドリカレントユニット（GRU）。 
[ABSTRACT]中国語の単語のセグメンテーション（cws）は通常、中国語のnerの最初のステップと見なされます。ただし、これらのモデルには、文字ベースの畳み込みニューラルネットワーク（cnn）とグローバルな自己注意層を持つゲート付き反復ユニット（gru）が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-03">
        <br><font color="black">2019-04-03</font>
      </time>
    </span>
</section>
<!-- paper0: UniTrans: Unifying Model Transfer and Data Transfer for Cross-Lingual
  Named Entity Recognition with Unlabeled Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CL/paper_9.html">
      <font color="black">UniTrans: Unifying Model Transfer and Data Transfer for Cross-Lingual
  Named Entity Recognition with Unlabeled Data</font>
    </a>
  </h2>
  <font color="black">さらに、以前の作業では、目的の言語でラベル付けされていないデータをめったに活用しないため、簡単に収集でき、結果を改善するための貴重な情報が含まれている可能性があります。リンガルNER、さらには、拡張ナレッジ蒸留を介して、ラベル付けされていないターゲット言語データから利用可能な情報を活用します。ベンチマークデータセットで、提案されている4つ以上のターゲット言語でUniTransを評価します。 
[ABSTRACT]以前のメソッドタイプは、言語を介してコンテキスト情報を悪用できます-独立した機能ですが、タスクは表示されません-ターゲット言語の特定の情報</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: A Multilingual Parallel Corpora Collection Effort for Indian Languages -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CL/paper_10.html">
      <font color="black">A Multilingual Parallel Corpora Collection Effort for Indian Languages</font>
    </a>
  </h2>
  <font color="black">加えて、機械翻訳の最近の進歩とディープニューラルネットワークベースの方法を使用したクロスリンガル検索によって可能になったツールを使用して、このようなコーパスを構築する方法について報告します。10のインドの言語-ヒンディー語、テルグ語、タミル語、マラヤーラム語、グジャラート語、ウルドゥー語、ベンガル語、オリヤー語、マラーティー語、パンジャブ語、英語-それらの多くは低リソースとして分類されます。コーパスは、言語間で共有されているコンテンツを持つオンラインソースから編集されています。 
[ABSTRACT]インドの言語はさまざまなソースからコンパイルされています。テストを使用して、10のインドの言語でのパフォーマンスを検証できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language
  Model Pre-Training -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CL/paper_11.html">
      <font color="black">InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language
  Model Pre-Training</font>
    </a>
  </h2>
  <font color="black">コードと事前トレーニング済みモデルは、http：//aka.ms/infoxlm。で入手できます。単言語コーパスと並列コーパスの両方を活用することにより、事前トレーニング済みタスクのクロスリンガル転送性を向上させるために、同時実行タスクを共同でトレーニングします。いくつかのベンチマークの結果は、私たちのアプローチがかなり優れたパフォーマンスを達成することを示しています。 
[要約]統一されたビューは、クロスリンガル表現を学習するための既存の方法をよりよく理解するのに役立ちます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Multimodal Word Sense Disambiguation in Creative Practice -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CL/paper_12.html">
      <font color="black">Multimodal Word Sense Disambiguation in Creative Practice</font>
    </a>
  </h2>
  <font color="black">ADARIデータセットとベースラインが主観的なラベル分類への最初のステップを構成することを願っています。さらに、建築、アート、デザイン、ファッション、家具、製品デザイン、テクノロジーのサブドメインに編成されています。この複雑さを理解するために、文の分類に最先端の事前トレーニング済みBERTモデルを使用した、画像に関するテキストのあいまいさと関連性。 
[ABSTRACT]これは、アイデアやデザインの意図が非常に主観的である創造的な実践において特に当てはまります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: GraphFlow: Exploiting Conversation Flow with Graph Neural Networks for
  Conversational Machine Comprehension -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CL/paper_13.html">
      <font color="black">GraphFlow: Exploiting Conversation Flow with Graph Neural Networks for
  Conversational Machine Comprehension</font>
    </a>
  </h2>
  <font color="black">さらに、可視化実験は、提案されたモデルが推論プロセスに優れた解釈可能性を提供できることを示しています。次に、新しいリカレントグラフニューラルネットワークを提案し、それに基づいて、一連のコンテキストの時間依存性をモデル化するフローメカニズムを導入します。グラフ..提案されたGraphFlowモデルは、会話の流れを効果的にダイアログにキャプチャでき、CoQA、QuAC、およびDoQAベンチマークの既存の最先端の方法と比較して競争力のあるパフォーマンスを示します。 
[要約]提案されたグラフフローモデルは、会話型フローを効果的にキャプチャできます。既存の状態と比較して競争力のあるパフォーマンスを示します-coqa、quac、およびdoqaベンチマークの最先端の方法</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-07-31">
        <br><font color="black">2019-07-31</font>
      </time>
    </span>
</section>
<!-- paper0: Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with
  Minimal Resources -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CL/paper_14.html">
      <font color="black">Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with
  Minimal Resources</font>
    </a>
  </h2>
  <font color="black">この目的のために、特定のテストケースにすばやく適応できる適切なモデルパラメーターの初期化を見つけるためのメタ学習アルゴリズムを提示し、文の類似性を計算することにより、メタトレーニング用の複数の疑似NERタスクを構築することを提案します。結果は、私たちのアプローチは、既存の最先端の方法を全面的に大幅に上回っています。5つのターゲット言語で最小限のリソースを使用して、クロスリンガルの名前付きエンティティの認識に関する広範な実験を行っています。 
[ABSTRACT]たとえば、我々はテストケースを与えられたいくつかの類似した例で学習モデルを微調整することを提案します。これは、そのような類似した例で伝えられた構造的および意味論的情報を活用することにより予測に利益をもたらす可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-14">
        <br><font color="black">2019-11-14</font>
      </time>
    </span>
</section>
<!-- paper0: Dual Past and Future for Neural Machine Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CL/paper_15.html">
      <font color="black">Dual Past and Future for Neural Machine Translation</font>
    </a>
  </h2>
  <font color="black">実験結果は、提案された方法がNMT予測の妥当性を大幅に改善し、2つのよく研究された翻訳タスクで以前の方法を上回っていることを示しています。近年、ニューラル機械翻訳（NMT）によって顕著な成功が達成されていますが、それでもまだ不十分です。 -translation problem ..このペーパーでは、ソースからターゲットおよびターゲットからソースのNMTモデルの両方を活用して、過去および未来のモジュールに対してより直接的で正確な監視信号を提供する新しいデュアルフレームワークを紹介します。 
[ABSTRACT]以前の研究では、原文の内容を明示的にモデル化することが翻訳パフォーマンスに有益であることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Align then Summarize: Automatic Alignment Methods for Summarization
  Corpus Creation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CL/paper_16.html">
      <font color="black">Align then Summarize: Automatic Alignment Methods for Summarization
  Corpus Creation</font>
    </a>
  </h2>
  <font color="black">ブートストラップアプローチを使用して、人間のアノテーターによって修正される事前調整を提供し、自動モデルを評価する検証セットを作成します。評価は、調整された公開会議の新しいコーパスである\ publicmeetingsで行われます。より広範なタスクは、一般化と堅牢性の観点から、フィールドの改善につながります。 
[要旨]一般化と堅牢性の観点から、さまざまなタスクを検討すると、フィールドの改善につながることをお勧めします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: DeepSinger: Singing Voice Synthesis with Data Mined From the Web -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CL/paper_17.html">
      <font color="black">DeepSinger: Singing Voice Synthesis with Data Mined From the Web</font>
    </a>
  </h2>
  <font color="black">結果は、Webから純粋にマイニングされた歌唱データを使用して、DeepSingerがピッチの精度と声の自然さの両方の観点から高品質の歌声を合成できることを示しています（脚注：音声サンプルはhttps://speechresearch.github.io/に示されています） deepsinger /。）。 DeepSingerには、以前のSVSシステムに比べていくつかの利点があります。1）私たちの知る限り、これは音楽Webサイトからトレーニングデータを直接マイニングする最初のSVSシステムです。2）歌詞と歌唱のアラインメントモデルは、人間によるアラインメントの労力をさらに回避します。ラベリングし、ラベリングコストを大幅に削減します。3）フィードフォワードトランスフォーマーに基づく歌唱モデルは、パラメトリック合成で複雑な音響機能モデリングを削除し、参照エンコーダーを利用して歌手の音色を騒々しい歌唱データからキャプチャすることにより、シンプルで効率的です。 、および4）複数の言語と複数の歌手で歌声を合成できます。3つの言語（中国語、広東語、英語）の89人の歌手からの約92時間のデータで構成されるマイニングされた歌唱データセットでDeepSingerを評価します。 
[ABSTRACT] deepsingerは、音楽Webサイトからトレーニングデータを直接マイニングする最初のsvsシステムです。ピッチ精度と音声の自然さの点で、高品質の歌声を合成できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-09">
        <br><font color="black">2020-07-09</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern
  Architectures -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CL/paper_18.html">
      <font color="black">End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern
  Architectures</font>
    </a>
  </h2>
  <font color="black">最後に、ラベル付けされていないオーディオのさまざまな量を活用する効果を調査し、ラベル付けされていないオーディオの特性を評価するいくつかの方法を提案して、音響モデリングを改善し、より多くのオーディオでトレーニングされた音響モデルが外部言語モデルにあまり依存しないことを示します。変圧器ベースの音響モデルは、監視対象データセットのみで優れたパフォーマンスを発揮します。半監視は、アーキテクチャと損失関数全体のすべてのモデルを改善し、それらの間のパフォーマンスギャップの多くを埋めます。これにより、新しい状態に到達します標準の教師あり学習設定で外部言語モデルを使用してデコードされたエンドツーエンドの音響モデルのアート、および半教師付きトレーニングを備えた新しい絶対的な最先端技術。 
[ABSTRACT]標準のlibrispeechデータセットで実験を行い、ラベルなしデータから追加のラベルなしデータを半教師付きトレーニングで活用します。そうすることで、次のようにしてデコードされた、エンドツーフックモデルの新しい状態に到達します。外部言語モデル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-19">
        <br><font color="black">2019-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Predicting Clinical Diagnosis from Patients Electronic Health Records
  Using BERT-based Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/cs.CL/paper_19.html">
      <font color="black">Predicting Clinical Diagnosis from Patients Electronic Health Records
  Using BERT-based Neural Networks</font>
    </a>
  </h2>
  <font color="black">ICD-10の265疾患サブセットのマルチクラス分類のタスクについて、他のテキスト表現モデルといくつかの比較実験を実行しました。これにより、このシステムの実装により誤診が減少することを期待できます。経験豊富な医療専門家のパネルによるモデル。 
[ABSTRACT]医学界におけるこの問題の重要性を示します。問題の科学的表現を提示します。また、経験豊富な医療専門家のパネルとモデルの同等のパフォーマンスを示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: Cross-Lingual Speaker Verification with Domain-Balanced Hard Prototype
  Mining and Language-Dependent Score Normalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/eess.AS/paper_0.html">
      <font color="black">Cross-Lingual Speaker Verification with Domain-Balanced Hard Prototype
  Mining and Language-Dependent Score Normalization</font>
    </a>
  </h2>
  <font color="black">クロスリンガル試験のスコアリングを強化するために、言語依存のsノルムスコアの正規化を提案します。偽者コホートには、常にペルシア語である登録データをシミュレートするペルシア語のターゲットドメインからのデータのみが含まれています。バックエンド言語モデルは、英語を含むように埋め込まれたテストスピーカーを検出し、AAM-softmaxスピーカープロトタイプで決定された言語間補正オフセットが、予想される最大の偽者平均スコアから差し引かれます。 
[ABSTRACT]サンプルマイニング手法は、人気のaamのスピーカープロトタイプ間のスピーカー距離を効率的に活用します-ソフトマックス損失関数</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Exploring the time-domain deep attractor network with two-stream
  architectures in a reverberant environment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/eess.AS/paper_1.html">
      <font color="black">Exploring the time-domain deep attractor network with two-stream
  architectures in a reverberant environment</font>
    </a>
  </h2>
  <font color="black">本研究では、可変数の話者の条件下で残響除去と分離タスクの両方を効率的に実行する2ストリーム畳み込みネットワークを備えた時間領域ディープアトラクタネットワーク（TD-DAN）を提案します。 TD-DANはスピーカー情報をモデル化し、さまざまな波形エンコーダーで探索されます。さらに、これらのモデルは、残響のある環境ではパフォーマンスが低下します。 
[ABSTRACT]ディープコナーネットワーク（dan）は、スピーカーアトラクタで音声分離を実行します。これらのモデルは、反響環境でパフォーマンスの低下を被ります。この研究では、td-danが達成したスケール対応のソース対歪み比（si-sdr）のゲインを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br><font color="black">2020-07-01</font>
      </time>
    </span>
</section>
<!-- paper0: A survey and an extensive evaluation of popular audio declipping methods -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/eess.AS/paper_2.html">
      <font color="black">A survey and an extensive evaluation of popular audio declipping methods</font>
    </a>
  </h2>
  <font color="black">信号処理のダイナミックレンジの制限により、信号のクリッピングまたは飽和が発生することがよくあります。各アルゴリズムについて、オーディオ信号、モデリングドメイン、および最適化アルゴリズムについて行われている仮定を示します。各アルゴリズムは、信号対歪み比の用語のほか、音質の知覚指標の使用。 
[ABSTRACT]この記事では、文献で提案されているオーディオクリップ除去アルゴリズムの広範なレビューを提供します。実際のオーディオデータについて、一般的なクリップ除去アルゴリズムの広範な数値評価が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: DeepSinger: Singing Voice Synthesis with Data Mined From the Web -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/eess.AS/paper_3.html">
      <font color="black">DeepSinger: Singing Voice Synthesis with Data Mined From the Web</font>
    </a>
  </h2>
  <font color="black">結果は、Webから純粋にマイニングされた歌唱データを使用して、DeepSingerがピッチの精度と声の自然さの両方の観点から高品質の歌声を合成できることを示しています（脚注：音声サンプルはhttps://speechresearch.github.io/に示されています） deepsinger /。）。このペーパーでは、DeepSingerを開発します。これは、音楽のWebサイトからマイニングされた歌唱トレーニングデータを使用して、ゼロから構築された多言語マルチシンガー歌声合成（SVS）システムです。 3つの言語（中国語、広東語、英語）の89人の歌手による92時間のデータ。 
[ABSTRACT] deepsingerは、音楽Webサイトからトレーニングデータを直接マイニングする最初のsvsシステムです。ピッチ精度と音声の自然さの点で、高品質の歌声を合成できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-09">
        <br><font color="black">2020-07-09</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern
  Architectures -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/eess.AS/paper_4.html">
      <font color="black">End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern
  Architectures</font>
    </a>
  </h2>
  <font color="black">トランスフォーマーベースの音響モデルは監視データセットのみで優れたパフォーマンスを発揮しますが、半監視はアーキテクチャ全体のすべてのモデルと損失関数を改善し、モデル間のパフォーマンスギャップの多くを埋めることを示しています。最後に、さまざまな量を活用する効果を調べますラベル付けされていないオーディオの数を増やし、ラベル付けされていないオーディオの特性を評価するいくつかの方法を提案して、音響モデリングを改善し、より多くのオーディオでトレーニングされた音響モデルが外部言語モデルにあまり依存しないことを示します。そうすることで、新しい状態に到達します標準の教師あり学習設定で外部言語モデルを使用してデコードされたエンドツーエンドの音響モデルのアート、および半教師付きトレーニングを備えた新しい絶対的な最先端技術。 
[ABSTRACT]標準のlibrispeechデータセットで実験を行い、ラベルなしデータから追加のラベルなしデータを半教師付きトレーニングで活用します。そうすることで、次のようにしてデコードされた、エンドツーフックモデルの新しい状態に到達します。外部言語モデル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-19">
        <br><font color="black">2019-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: MUSICNTWRK: data tools for music theory, analysis and composition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-16/eess.AS/paper_5.html">
      <font color="black">MUSICNTWRK: data tools for music theory, analysis and composition</font>
    </a>
  </h2>
  <font color="black">ソフトウェアはGPL 3.0の下で無料で利用でき、www.musicntwrk.comからダウンロードできます。ピッチクラスセットとリズムシーケンスの分類と操作、一般的な音楽とサウンドにおけるネットワークの生成のためのpythonライブラリであるMUSICNTWRKのAPIを提供します空間、音色認識のための深層学習アルゴリズム、および任意のデータの音波処理。 
[要約]ソフトウェアはgpl 3で無料で入手できます。 deep.itはwwwからダウンロードできます。 musicntwrk.com</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-03">
        <br><font color="black">2019-06-03</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
