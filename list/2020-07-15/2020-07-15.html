<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-07-15の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: A Deep Learning Approach for Low-Latency Packet Loss Concealment of
  Audio Signals in Networked Music Performance Applications -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.SD/paper_0.html">
      <font color="black">A Deep Learning Approach for Low-Latency Packet Loss Concealment of
  Audio Signals in Networked Music Performance Applications</font>
    </a>
  </h2>
  <font color="black">この記事では、ディープラーニングアプローチを使用して、失われたパケットの内容をリアルタイムで予測する手法について説明します。接続が少なく信頼性が低いため、UDPを介して送信された送信中に失われたオーディオパケットは再送信されず、レシーバーオーディオにグリッチが発生します。再生..リアルタイムでエラーを隠す機能は、パケット損失によって引き起こされるオーディオ障害を軽減するのに役立ち、それにより、現実世界のシナリオでのオーディオ再生の品質を向上させることができます。 
[ABSTRACT]エラーをリアルタイムで隠す機能は、パケット損失によって引き起こされる損傷を軽減するのに役立ちます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: ASVspoof 2019: A large-scale public database of synthesized, converted
  and replayed speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.SD/paper_1.html">
      <font color="black">ASVspoof 2019: A large-scale public database of synthesized, converted
  and replayed speech</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、データベースの設計、プロトコル、スプーフィング攻撃の実装、ベースラインASVと対策の結果について説明します。また、タンポデム検出コスト関数メトリックの使用も新しくなりました。これは、スプーフィングと対策の信頼性への影響を反映しています。固定ASVシステム。論理アクセス（LA）シナリオ内のスプーフィング攻撃は、最新の音声合成および音声変換技術（最先端の神経音響および波形モデル技術を含む）で生成されます。 
[ABSTRACT] asvはスプーフィングに対して脆弱であり、「プレゼンテーション攻撃」としても知られています。asvシステムは、再生、音声合成、および音声変換攻撃に対しても脆弱です。これらのシステムは、2つの特定のユースケースシナリオで探索されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-05">
        <br><font color="black">2019-11-05</font>
      </time>
    </span>
</section>
<!-- paper0: Sudo rm -rf: Efficient Networks for Universal Audio Source Separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.SD/paper_2.html">
      <font color="black">Sudo rm -rf: Efficient Networks for Universal Audio Source Separation</font>
    </a>
  </h2>
  <font color="black">音声と環境音の両方の分離データセットでの実験により、SuDoRMRFは同等のパフォーマンスを発揮し、計算リソース要件が大幅に高くなるさまざまな最先端の手法をも上回っています。このようにして、高品質のオーディオソース分離を実現できます。限られた数の浮動小数点演算、メモリ要件、パラメータ数、およびレイテンシ。特に、このたたみ込みネットワークのバックボーン構造は、マルチ解像度機能（SuDoRMRF）の連続的なダウンサンプリングと再サンプリング、および単純な一次元畳み込み。 
[ABSTRACT] sudormrfは同等のパフォーマンスを発揮し、さまざまな状態を上回ります-リソース要件が大幅に高い最先端のアプローチ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Pose2RGBD. Generating Depth and RGB images from absolute positions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_0.html">
      <font color="black">Pose2RGBD. Generating Depth and RGB images from absolute positions</font>
    </a>
  </h2>
  <font color="black">このプロセスは、グラフィックシミュレーションと同様に、生成されたシーンをナビゲートするために使用できる関数f：Pose-&gt; RGBDを取得するニューラルレンダリングと見なすことができます。1つは合成データに基づく2つの新しいデータセットを導入します。完全なグラウンドトゥルース情報を使用し、もう1つは大学のキャンパス内のドローンフライトから記録され、ビデオ信号とGPS信号のみを使用します。コンピュータビジョンとコンピュータグラフィックスのフィールドの交点で、RGBD画像を自動的に生成する方法を提案します。以前に見られ、同期されたビデオ、深度、およびポーズ信号に基づくニューラルネットワーク。 
[ABSTRACT]モデルは、テクスチャ（rgb）と構造（深度）の両方を再構築できなければなりません。モデルは、メッシュや点群などの明示的なものではなく、シーンの暗黙的な表現を作成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: A Comparative Study on Early Detection of COVID-19 from Chest X-Ray
  Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_1.html">
      <font color="black">A Comparative Study on Early Detection of COVID-19 from Chest X-Ray
  Images</font>
    </a>
  </h2>
  <font color="black">コロナウイルス病（COVID-19）は、2019年12月に最初の既知の検出が行われた後、急速に世界的な健康問題となっています。感染の痕跡は疾患が進行した場合にのみ見えるため、専門家の医師によると、初期段階でのCOVID-19の検出は胸部X線画像からの単純なタスクではありません。中等度または重度の段階。 
[要約] covid-19の高度な警告システムが優先事項になりました。胸部x線画像からのcovidの早期診断が懸念されます。これが過去にシステムが開発されたのはこれが初めてです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-07">
        <br><font color="black">2020-06-07</font>
      </time>
    </span>
</section>
<!-- paper0: Pasadena: Perceptually Aware and Stealthy Adversarial Denoise Attack -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_2.html">
      <font color="black">Pasadena: Perceptually Aware and Stealthy Adversarial Denoise Attack</font>
    </a>
  </h2>
  <font color="black">さらに、適応知覚領域ローカリゼーションを実装して、ノイズ除去にあまり害を与えずに攻撃がより効果的であるセマンティック関連の脆弱性領域を特定します。したがって、提案された方法は、Pasadena（知覚的かつステルスな敵対者）と呼ばれます。 DENoise Attack）。NeurIPS&#39;17の競合他社のデータセットでメソッドを検証し、ノイズ除去を実現するだけでなく、最新の攻撃よりも高い成功率と転送可能性という利点があることを示しています。 
[ABSTRACT]新しいタスクである敵対的ノイズ除去攻撃は新しい問題です。敵対的なノイズ除去カーネルを生成して、効果的なノイズ除去と敵対的攻撃を同時に行うことができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Semantics-enriched Representation via Self-discovery,
  Self-classification, and Self-restoration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_3.html">
      <font color="black">Learning Semantics-enriched Representation via Self-discovery,
  Self-classification, and Self-restoration</font>
    </a>
  </h2>
  <font color="black">この目的を達成するために、私たちは深いモデルをトレーニングして、医療画像の下の解剖学的構造の自己発見、自己分類、および自己修復によって意味論的に充実した視覚表現を学習し、意味論に富んだ汎用の事前トレーニング済み3Dモデルを生成します、セマンティックジェネシスと名付けられました。医療画像は、人間の解剖学に関する豊富なセマンティクスと自然に関連付けられており、豊富な繰り返しの解剖学的パターンに反映されており、深い意味論的表現の学習を促進し、さまざまな医療アプリケーションに対して意味論的により強力なモデルを生み出す独自の可能性を提供します。広範な実験により、セマンティックジェネシスは、3Dの対応物すべてと、事実上の2DのImageNetベースの転移学習を大幅に上回っています。 
[ABSTRACT]詳細な画像は、自己監視学習に利用できます。このパフォーマンスは、私たちの新しい自己監視学習フレームワークに起因します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Decoupling Inherent Risk and Early Cancer Signs in Image-based Breast
  Cancer Risk Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_4.html">
      <font color="black">Decoupling Inherent Risk and Early Cancer Signs in Image-based Breast
  Cancer Risk Models</font>
    </a>
  </h2>
  <font color="black">結果として、融合モデルは、早期のがんの兆候がすでに見えている場合に医師が予防措置を推奨するように導く可能性があります。これら3つのモデルは、パフォーマンスのコントラストにつながるさまざまなパターンに焦点を当てた特徴的な機能を学習していることがわかります。すべての画像を不注意にトレーニング固有のリスクを初期のがんの兆候で緩和し、両方のレジメンで最適とは言えない見積もりを出します。 
[ABSTRACT]これらのモデルは、さまざまなパターンに焦点を当てた特徴的な機能を学習します。これは、パフォーマンスのコントラストに変換されます。リスクモデルは、より良い結果を開発するために使用できるディープニューラルネットワークに基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-11">
        <br><font color="black">2020-07-11</font>
      </time>
    </span>
</section>
<!-- paper0: MFRNet: A New CNN Architecture for Post-Processing and In-loop Filtering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_5.html">
      <font color="black">MFRNet: A New CNN Architecture for Post-Processing and In-loop Filtering</font>
    </a>
  </h2>
  <font color="black">このネットワークは、HEVC（HM 16.20）とVVC（VTM 7.0）の両方のPPおよびILFコーディングモジュールに統合されており、ランダムアクセス構成を使用したJVET共通テスト条件下で完全に評価されています。両方のアンカーコーデック（HEVC HMとVVC VTM）と、品質評価にPSNRとVMAFの両方を使用するBjontegaard Delta測定に基づく他の既存のCNNベースのPP / ILFアプローチ。VTM7.0のそれぞれのゲインは、最大5.1％です。 ILFおよびPPで最大7.1％。 
[ABSTRACT]このネットワークは、カスケード構造を使用して接続された4つのマルチレベル機能レビュー残差密ブロック（mfrbs）で構成されています。それぞれのブロックは、前のmfrbからの高い神経機能を再利用します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Meta-rPPG: Remote Heart Rate Estimation Using a Transductive
  Meta-Learner -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_6.html">
      <font color="black">Meta-rPPG: Remote Heart Rate Estimation Using a Transductive
  Meta-Learner</font>
    </a>
  </h2>
  <font color="black">展開中の予測不可能な分布の変化に対処するために、テスト（展開）中にラベルなしのサンプルを取り、自己管理による重み調整（伝達的推論とも呼ばれます）のためにラベル付けされていないサンプルを取得し、分布の変化にすばやく適応できるトランスダクティブメタラーナーを提案します。このアプローチを使用して、MAHNOB-HCIおよびUBFC-rPPGで最先端のパフォーマンスを実現します。リモート心拍数推定は、被験者との物理的な接触なしの心拍数の測定であり、リモートフォトプレチスモグラフィ（rPPG）を使用して実行されます。この仕事で。 
[ABSTRACT] rppg信号は通常、複数の要因に敏感であるという制限のあるビデオカメラを使用して収集されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: BézierSketch: A generative model for scalable vector sketches -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_7.html">
      <font color="black">BézierSketch: A generative model for scalable vector sketches</font>
    </a>
  </h2>
  <font color="black">ベンチマーク..人間のスケッチの神経生成モデルの研究は、スケッチ画像の生成と人間の描画プロセスの間のリンクにより、魅力的な現代のモデリング問題です。これにより、スケッチをパラメータ化されたストロークの短いシーケンスとして扱い、繰り返しのトレーニングを行うことができます。スケーラブルな高解像度の結果を生成しながら、より長いスケッチのためのより大きな容量を持つスケッチジェネレーター。 
[ABSTRACT]ランドマークsketchrnnは、スケッチを一連のウェイポイントとして定量的に生成することにより画期的な機能を提供しました。これにより、スケッチをパラメータ化されたストロークの短いシーケンスとして扱うことができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-04">
        <br><font color="black">2020-07-04</font>
      </time>
    </span>
</section>
<!-- paper0: Spatial-Adaptive Network for Single Image Denoising -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_8.html">
      <font color="black">Spatial-Adaptive Network for Single Image Denoising</font>
    </a>
  </h2>
  <font color="black">粗いものから細かいものまでノイズを除去することで、高品質のノイズのない画像を取得できます。マルチスケール情報をキャプチャするために、コンテキストブロックを備えたエンコーダー/デコーダー構造が導入されています。この方法を、合成と実際のノイズの多い画像データセットの両方に適用します。 
[要旨]効率的な単一画像のブラインドノイズ除去のための新しい空間適応型ノイズ除去ネットワークを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-28">
        <br><font color="black">2020-01-28</font>
      </time>
    </span>
</section>
<!-- paper0: Anomaly Detection by One Class Latent Regularized Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_9.html">
      <font color="black">Anomaly Detection by One Class Latent Regularized Networks</font>
    </a>
  </h2>
  <font color="black">半教師付き生成的敵対的ネットワーク（GAN）ベースのメソッドは、異常検出タスクで最近人気を博しています。ある種の分布から出現する、通常のクラスに属する広範囲の画像を考えると、このタスクの目的はモデルを構築することです異常なインスタンスに属する分布外の画像を検出します。さらに、識別器と見なされる補助オートエンコーダは、より安定したトレーニングプロセスを取得できます。 
[ABSTRACT]タスクは、異常なインスタンスの画像に基づいてモデルを構築することです。これらの画像は、正常なクラスに属する一連の画像に表示されます。ただし、トレーニングプロセスはまだ不安定で、やりがいがあります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-05">
        <br><font color="black">2020-02-05</font>
      </time>
    </span>
</section>
<!-- paper0: Cross-Domain Medical Image Translation by Shared Latent Gaussian Mixture
  Model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_10.html">
      <font color="black">Cross-Domain Medical Image Translation by Shared Latent Gaussian Mixture
  Model</font>
    </a>
  </h2>
  <font color="black">放射線医学の重要な例は、非造影CTから造影CTへの一般化です。しかし、このようなモデルは、翻訳プロセス中に微細構造を維持する能力に欠けています。これは、多くの臨床アプリケーションで重要です。大動脈および骨盤動脈..多くの既存のクロスドメイン画像間変換モデルは、大きな臓器のクロスドメインセグメンテーションを改善することが示されています。 
[ABSTRACT]クロスドメインの画像分析ツールは、実際の臨床アプリケーションで高い需要があります。正確な診断を行うには、さまざまな領域のX線画像が必要になることがよくあります。これらのモデルには、変換プロセス中に微細構造を保存する機能がありません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Generating Videos of Zero-Shot Compositions of Actions and Objects -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_11.html">
      <font color="black">Generating Videos of Zero-Shot Compositions of Actions and Objects</font>
    </a>
  </h2>
  <font color="black">ヒューマンオブジェクトインタラクションビデオを生成するために、ビデオのさまざまな側面に焦点を当てた複数の弁別子を含む新しい敵対的なフレームワークHOI-GANを提案します。特に、ゼロショット構成ビデオでヒューマンオブジェクトインタラクションビデオを生成するタスクを紹介します。設定、つまり、ターゲットアクションとターゲットオブジェクトを別々に見て、トレーニング中に見えないアクションオブジェクト構成のビデオを生成します。提案されたフレームワークの有効性を実証するために、2つの挑戦的なデータセットで広範な定量的および定性的評価を実行します：EPIC -キッチンと20BN-Something-Something v2。 
[要約]このペーパーでは、ビデオを作成する方法を開発します。これらは、複雑なシーンでのビデオ生成の問題への対処に向けて進歩しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br><font color="black">2019-12-05</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Abnormality Detection Using Heterogeneous Autonomous
  Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_12.html">
      <font color="black">Unsupervised Abnormality Detection Using Heterogeneous Autonomous
  Systems</font>
    </a>
  </h2>
  <font color="black">また、自律型車両は、画像やその他のアナログまたはデジタルセンサーデータなどのさまざまなデータタイプを提供します。これらのデータはすべて、有効に活用された場合に異常検出に役立つ可能性があります。無人監視ドローンの異常を監視し、監視されていない方法でリアルタイム画像とIMU（慣性計測ユニット）センサーデータを分析します。さらに、社内データセットでこのアプローチをテストし、その堅牢性を検証しました。 
[要約]畳み込みニューラルネットワークは、通常の画像と検討中の別の画像の間の角度を予測します。これらのアルゴリズムの結果は、最終的な異常の程度を推定するためにまとめられます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-05">
        <br><font color="black">2020-06-05</font>
      </time>
    </span>
</section>
<!-- paper0: MeDaS: An open-source platform as service to help break the walls
  between medicine and informatics -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_13.html">
      <font color="black">MeDaS: An open-source platform as service to help break the walls
  between medicine and informatics</font>
    </a>
  </h2>
  <font color="black">RINV（Rapid Implementation aNd Verification）のアイデアに基づく一連のツールキットとユーティリティに基づいて、提案されたMeDaSプラットフォームは、医療画像分析に必要な前処理、後処理、拡張、視覚化、およびその他のフェーズを実装できます。最後に、新しいオープンソースプラットフォーム、つまりMeDaSを提案します。サービスとしてのMeDicalオープンソースプラットフォームです。私たちの知る限り、MeDaSは、研究者向けの共同的かつインタラクティブなサービスを提供する最初のオープンソースプラットフォームです。 DL関連のツールキットを使用して簡単に医学的背景を得ると同時に、情報科学の科学者またはエンジニアが医学知識の側面を理解するために。 
[ABSTRACT] dl関連のツールキットを使用して、医学的背景から研究者にコラボレーションとインタラクティブなサービスを簡単に提供する最初のオープンソースプラットフォーム。肺、肝臓、脳、胸部、歯科などの5つのタスクが検証され、効率的に実現可能</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-12">
        <br><font color="black">2020-07-12</font>
      </time>
    </span>
</section>
<!-- paper0: Symmetric Dilated Convolution for Surgical Gesture Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_14.html">
      <font color="black">Symmetric Dilated Convolution for Surgical Gesture Recognition</font>
    </a>
  </h2>
  <font color="black">自動外科ジェスチャー認識は、術中コンピューター支援と客観的外科技能評価の前提条件です。JIGSAWSデータセットからの基本的なロボット縫合タスクでのアプローチの有効性を検証します。実験結果は、長期的なフレーム依存性。これは、最先端の方法よりもフレーム単位の精度で最大6ポイント、F1 @ 50スコアで最大6ポイントを大幅に上回ります。 
[ABSTRACT]この実験は、長期的なフレームの依存関係をキャプチャする方法の能力を示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Atomic Super-Resolution Tomography -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_15.html">
      <font color="black">Atomic Super-Resolution Tomography</font>
    </a>
  </h2>
  <font color="black">新しい定式化により、原子相互作用ポテンシャルを明示的に定義できるようになりました。これにより、結晶の特性に関する利用可能な物理的なアプリオリな知識が有意義かつ強力に組み込まれます。ここでは、グリッドのない離散トモグラフィーアルゴリズムを提案し、顕微鏡の超解像アプローチと同様の原子位置。計算実験では、提案されたグリッドフリー法を確立されたグリッドベースのアプローチと比較し、私たちのアプローチが実際の原子位置を実際の格子欠陥に対してより正確に回復できることを示します。 
[ABSTRACT]離散トモグラフィーと呼ばれる一般的な再構成アプローチは、原子の位置を分配します。システムは、結晶固体の原子が規則的な格子を形成する傾向があるという物理的なアプリオリな知識に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br><font color="black">2020-02-03</font>
      </time>
    </span>
</section>
<!-- paper0: Continual Adaptation for Deep Stereo -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_16.html">
      <font color="black">Continual Adaptation for Deep Stereo</font>
    </a>
  </h2>
  <font color="black">私たちのパラダイムでは、オンラインでモデルを継続的に適応させるために必要な学習信号は、右から左への画像ワーピングによる自己監視から、または従来のステレオアルゴリズムから供給できます。軽量でモジュール式のアーキテクチャであるModularly Adaptive Network（MADNet）を設計します。そして、ネットワーク全体の独立したサブ部分の効率的な最適化を可能にするモジュラー適応アルゴリズム（MAD、MAD ++）を定式化します。このような仮定は、実際のアプリケーションでは体系的に満たされていないため、目に見えない設定への適応能力が最も重要になります。 
[要約]ネットワークアーキテクチャと適応アルゴリズムは、ディープステレオネットワークの「継続的適応」理論を作成しました。オンラインでモデルを適応させるために必要な学習信号は、右から左への画像ワーピングによる自己監視または従来のステレオから供給できると述べていますアルゴリズム</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-10">
        <br><font color="black">2020-07-10</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Dense People Detection with Deep Learning and Depth images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_17.html">
      <font color="black">Towards Dense People Detection with Deep Learning and Depth images</font>
    </a>
  </h2>
  <font color="black">シミュレーションデータを使用して、最初にネットワークのトレーニングを行った後、比較的少量の実際のデータで微調整します。この戦略が効果的であることを示し、トレーニング中に使用されるシーンとは異なるシーンで一般的に機能するネットワークを生成します。私たちの方法を、従来のソリューションとDNNベースのソリューションの両方を含む既存の最先端技術と比較してください。 
[要旨]私たちのシステムは、パフォーマンスを向上させるために分離された畳み込みを使用します。これらのネットワークは、トレーニング中に使用されたシーンとは異なるシーンで動作するように一般化されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: GIQA: Generated Image Quality Assessment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_18.html">
      <font color="black">GIQA: Generated Image Quality Assessment</font>
    </a>
  </h2>
  <font color="black">さらに、GIQAは、生成モデルのリアリズムと多様性を個別に評価し、GANのトレーニングでオンラインハードネガティブマイニング（OHEM）を有効にして結果を改善するなど、多くのアプリケーションで利用できます。さまざまなデータセットのGANモデルを使用して、それらが人間の評価と一致していることを示します。学習ベースとデータベースの2つの観点から、3つのGIQAアルゴリズムを紹介します。 
[要約]トピックに対していくつかの定量的基準が最近浮上しましたが、それらのどれも定量的画像用に設計されていません。これには、学習ベースとデータベースが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-19">
        <br><font color="black">2020-03-19</font>
      </time>
    </span>
</section>
<!-- paper0: Deep learning approaches for fast radio signal prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_19.html">
      <font color="black">Deep learning approaches for fast radio signal prediction</font>
    </a>
  </h2>
  <font color="black">レイトレーシングの結果から学習し、建物と送信機のプロパティから動的に電力カバレッジを予測するために、ディープニューラルネットワークモデルを提案します。ストライドコンボリューションと開始モジュールを備えた提案されたUNETモデルは、レイトレーシング出力に近い非常に正確な結果を提供します。 32x32フレーム..この作業の目的は、建物と送信機の場所を考慮して、密集した都市環境での電力カバレッジの予測です。 
[ABSTRACT] ray-トレーシングは、エリア内のエネルギー分布パターンを予測する最も正確な方法と見なされています。ただし、従来、レイトレーシングは、最も正確な方法と考えられています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-16">
        <br><font color="black">2020-06-16</font>
      </time>
    </span>
</section>
<!-- paper0: Spatio-Temporal Event Segmentation and Localization for Wildlife
  Extended Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.IV/paper_20.html">
      <font color="black">Spatio-Temporal Event Segmentation and Localization for Wildlife
  Extended Videos</font>
    </a>
  </h2>
  <font color="black">動画を1回パスするだけで、個別のトレーニングセットは必要ありません。標準のディープラーニングバックボーンによって計算された高レベルの機能の予測に依存しています。予測には、注意メカニズムが強化されたLSTMを使用します。予測誤差を使用して自己管理された方法で。 
[ABSTRACT]提案されたアプローチは、予測メカニズムを使用して自己監視された方法でトレーニングされた、注意メカニズムで強化されたlstmの使用を含みます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-05">
        <br><font color="black">2020-05-05</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Vehicle Trajectory Prediction by Transfer Learning of Semi-Supervised
  Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_0.html">
      <font color="black">Vehicle Trajectory Prediction by Transfer Learning of Semi-Supervised
  Models</font>
    </a>
  </h2>
  <font color="black">運転環境の低レベルと中レベルの両方の表現を使用した結果は、実世界の車両軌道予測に対する半教師付き手法の適用性を示しています。教師付きモデルから半教師付きモデルに移動すると、ラベルなしデータを使用してスケールアップできます。事前トレーニングで画像の数を数百万から10億に増やします。半教師付きモデル内で、教師と生徒の手法による対照学習と、少数の軌跡を予測するネットワークと、大きな軌跡セットに対する確率を予測するネットワークとを比較します。 
[ABSTRACT]半監視モデルに移動すると、ラベルなしデータを使用してスケールアップできます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Self-Supervised Monocular Depth Estimation: Solving the Dynamic Object
  Problem by Semantic Guidance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_1.html">
      <font color="black">Self-Supervised Monocular Depth Estimation: Solving the Dynamic Object
  Problem by Semantic Guidance</font>
    </a>
  </h2>
  <font color="black">具体的には、（i）（監視対象の）セマンティックセグメンテーションの相互に有益なクロスドメイントレーニングとタスク固有のネットワークヘッドを使用した自己監視の深度推定、（ii）移動DCオブジェクトがフォトメトリックを汚染しないようにするためのガイダンスを提供するセマンティックマスキングスキームを提案します。損失、および（iii）DCオブジェクトの深さを学習できる、移動しないDCオブジェクトを含むフレームの検出方法。いくつかのベンチマーク、特にEigen分割での方法のパフォーマンスを示します。すべてのメジャーでテスト時間の調整なしにすべてのベースラインを超えます。自己監視単眼深度推定は、LiDARなどの深度ラベルを必要とせずに、任意の画像シーケンスでトレーニング可能な単一のカメラ画像から3Dシーン情報を取得する強力な方法を提供します。センサー。 
[ABSTRACT]移動する車や歩行者などの移動するダイナミッククラス（DC）オブジェクトを処理するための、自己監視による意味論的に誘発された深度推定（sgdepth）メソッド。このメソッドは、一連の自己特別評価に基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Semantically Plausible and Diverse 3D Human Motion Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_2.html">
      <font color="black">Semantically Plausible and Diverse 3D Human Motion Prediction</font>
    </a>
  </h2>
  <font color="black">潜在変数のサンプリングが条件付けから独立している既存のアプローチとは異なり、私たちのアプローチは過去の観測の表現に基づいて潜在変数のサンプリングプロセスを条件付けし、関連情報を運ぶように促します。多様性を維持しながら高品質のモーションを生成するだけでなく、観察された3Dポーズシーケンスに含まれる意味情報を保持するモーションも生成します。多様なモーションを生成する既存のアプローチは、人間のモーションの多様性を捉えられないか、多様であるが意味的に妥当な継続を生成できません観察された動き。 
[ABSTRACT]モーションは、シンプルでシンプルなシステムのオートエンコーダ（cvaes）を組み合わせたものです。これは、さまざまなタイプのモーションの複数の異なる形式に基づいています。これらには、セマンティック情報を保持する記述が含まれています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-18">
        <br><font color="black">2019-12-18</font>
      </time>
    </span>
</section>
<!-- paper0: TCGM: An Information-Theoretic Framework for Semi-Supervised
  Multi-Modality Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_3.html">
      <font color="black">TCGM: An Information-Theoretic Framework for Semi-Supervised
  Multi-Modality Learning</font>
    </a>
  </h2>
  <font color="black">既存の方法は、モダリティ間での効果的な融合または適切な仮定の下での理論的保証の欠如に悩まされています。しかし、大量のデータで各モダリティにラベルを付けることは、法外な費用と時間がかかり、半監視の重大な問題につながりますマルチモーダル学習..特に、すべてのモダリティの分類子に対してTCによって引き起こされる損失（つまりTCゲイン）を最大化することにより、これらの分類子は、グラウンドトゥルース分類子の同等のクラスを協調的に発見できます。ラベル付きデータの限られたパーセンテージを活用して、固有のものを識別します。 
[要約]半教師ありマルチモーダル学習の方法が役立つ場合があります。ラベルのないデータポイントのさまざまなモダリティにわたってデータを使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Progressive Skeletonization: Trimming more fat from a network at
  initialization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_4.html">
      <font color="black">Progressive Skeletonization: Trimming more fat from a network at
  initialization</font>
    </a>
  </h2>
  <font color="black">直感的に、すべての可能なサブネットワークの中から、摂動したときに接続が損失に最大の影響を与えるサブネットワークを見つけることを提案します。このため、最大の予測接続感度（FORCE）を持つスケルトンネットワークを見つけることを提案します。多くの場合、私たちのアプローチでは、ネットワークをトレーニング可能に保ち、最近のアプローチよりも大幅に優れたパフォーマンスを提供しながら、最大99.9％のパラメーターを削除できます。 
[要約]特定のレベルのスパース性を超えると、これらのアプローチはネットワークパフォーマンスを維持できなくなります。驚いたことに、多くの場合、些細なプルーニングよりもパフォーマンスが低下します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-16">
        <br><font color="black">2020-06-16</font>
      </time>
    </span>
</section>
<!-- paper0: Our Evaluation Metric Needs an Update to Encourage Generalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_5.html">
      <font color="black">Our Evaluation Metric Needs an Update to Encourage Generalization</font>
    </a>
  </h2>
  <font color="black">モデルパフォーマンスのインフレを停止し、AIシステムの機能を過大評価するのを防ぐために、評価中に一般化を促進するシンプルで新しい評価指標であるWOODスコアを提案します。最近の研究では、モデルが偽のバイアスに適合していることが示されています。人間などの一般化可能な機能を学習する代わりに、データセットを「ハック」します。いくつかの一般的なベンチマークで人間のパフォーマンスを超えるモデルは、Out of Distribution（OOD）データへの露出でパフォーマンスの大幅な低下を示します。 
[ABSTRACT]モデルは、人間のような一般化可能な機能を学習する代わりに、偽の劣化と「ハッキング」データセットに適合している</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Pose2RGBD. Generating Depth and RGB images from absolute positions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_6.html">
      <font color="black">Pose2RGBD. Generating Depth and RGB images from absolute positions</font>
    </a>
  </h2>
  <font color="black">このプロセスは、グラフィックシミュレーションと同様に、生成されたシーンをナビゲートするために使用できる関数f：Pose-&gt; RGBDを取得するニューラルレンダリングと考えることができます。最後に、データセットを生成する完全に監視されていない方法を提案します。 Pose2RGBDネットワークをトレーニングするために、ビデオのみから。2つの新しいデータセットを導入します。1つは完全なグラウンドトゥルース情報を含む合成データに基づいており、もう1つはビデオとGPSのみを使用して大学のキャンパス内のドローン飛行から記録されています。シグナル。 
[ABSTRACT]モデルは、テクスチャ（rgb）と構造（深度）の両方を再構築できなければなりません。モデルは、メッシュや点群などの明示的なものではなく、シーンの暗黙的な表現を作成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Rethinking Image Inpainting via a Mutual Encoder-Decoder with Feature
  Equalizations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_7.html">
      <font color="black">Rethinking Image Inpainting via a Mutual Encoder-Decoder with Feature
  Equalizations</font>
    </a>
  </h2>
  <font color="black">ディープエンコーダーデコーダーベースのCNNには、穴埋めのための高度な画像修復方法があります。深いレイヤーフィーチャは構造ブランチに送信され、浅いレイヤーフィーチャはテクスチャブランチに送信されます。このドキュメントでは、相互エンコーダーデコーダーを提案します。両方の共同回復のためのCNN。 
[要約]エンコーダの深い層と浅い層を使用して、構造とテクスチャを復元できます。これらのエンコーダ機能がないと、構造とテクスチャの両方を復元するパフォーマンスが制限されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Re-ranking for Writer Identification and Writer Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_8.html">
      <font color="black">Re-ranking for Writer Identification and Writer Retrieval</font>
    </a>
  </h2>
  <font color="black">考えられる理由としては、公開されているベンチマークデータセットに含まれるライターあたりのサンプル数が少ないため、再ランキングの見込みが低くなることが考えられます。これらの相互関係は、最初に提案されたように新しいベクトルにエンコードするか、用語で統合するかの2つの方法で使用します。 query-expansion ..ライターごとのサンプル数が少ない場合でも、kの相互最近傍関係に基づく再ランク付けステップがライターの識別に有利であることを示します。 
[ABSTRACT]新しい方法は通常、従来の、またはディープラーニングベースの手法を使用した特徴抽出ステップに焦点を当てています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: A Comparative Study on Early Detection of COVID-19 from Chest X-Ray
  Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_9.html">
      <font color="black">A Comparative Study on Early Detection of COVID-19 from Chest X-Ray
  Images</font>
    </a>
  </h2>
  <font color="black">コロナウイルス病（COVID-19）は、2019年12月の最初の既知の検出後、急速に世界的な健康問題となっています。さらに、拡張データで微調整されたディープCheXNetを介した転移学習は、他のディープネットワークの中で97.14％の優れたパフォーマンスを発揮します。感度および99.49％の特異性。詳細な一連の実験では、CSENが96％を超える特異性で最高（98.5％以上）の感度を達成することを示しています。 
[要約] covid-19の高度な警告システムが優先事項になりました。胸部x線画像からのcovidの早期診断が懸念されます。これが過去にシステムが開発されたのはこれが初めてです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-07">
        <br><font color="black">2020-06-07</font>
      </time>
    </span>
</section>
<!-- paper0: An Uncertainty-based Human-in-the-loop System for Industrial Tool Wear
  Analysis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_10.html">
      <font color="black">An Uncertainty-based Human-in-the-loop System for Industrial Tool Wear
  Analysis</font>
    </a>
  </h2>
  <font color="black">これらの問題に対処するために、システムの透明性とパフォーマンスを向上させるために、ヒューマンインザループシステムのコンテキストでモンテカルロドロップアウトに基づく不確実性測定を使用します。不確実性ベースのヒューマンインザループシステム内で、重回帰は画像レベルで失敗した予測を特定することを目的としています。一般化可能性を確保するために、提示されたアプローチが一般に入手可能なCityscapesデータセットで同様の結果を達成することを示します。 
[ABSTRACT]以前の研究は、予測の質がモデルの不確実性と相関していることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Pasadena: Perceptually Aware and Stealthy Adversarial Denoise Attack -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_11.html">
      <font color="black">Pasadena: Perceptually Aware and Stealthy Adversarial Denoise Attack</font>
    </a>
  </h2>
  <font color="black">さらに、適応知覚領域のローカリゼーションを実装して、ノイズ除去にあまり害を与えずに攻撃がより効果的になるセマンティック関連の脆弱性領域を特定します。この新しいタスクをカーネル予測問題として定式化し、敵対的な効果的なノイズ除去と敵対的攻撃の敵対ノイズのないカーネルを同時に生成できるノイズ予測カーネル予測。NeurIPS&#39;17の敵対的競争データセットでメソッドを検証し、このメソッドがノイズ除去を実現するだけでなく、高い成功率と転送可能性の利点があることを示します最先端の攻撃。 
[ABSTRACT]新しいタスクである敵対的ノイズ除去攻撃は新しい問題です。敵対的なノイズ除去カーネルを生成して、効果的なノイズ除去と敵対的攻撃を同時に行うことができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Semantics-enriched Representation via Self-discovery,
  Self-classification, and Self-restoration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_12.html">
      <font color="black">Learning Semantics-enriched Representation via Self-discovery,
  Self-classification, and Self-restoration</font>
    </a>
  </h2>
  <font color="black">医用画像は、人間の解剖学に関する豊富なセマンティクスに自然に関連付けられており、豊富な繰り返しの解剖学的パターンに反映されており、深い意味論的表現の学習を促進し、さまざまな医療アプリケーションのための意味論的により強力なモデルを生み出す独自の可能性を提供します。医療画像の下の解剖学的構造の自己発見、自己分類、および自己修復によって意味論的に豊富な視覚表現を学習するモデル。その結果、セマンティックジェネシスという名前の汎用の事前トレーニング済み3Dモデルが生成されます。コードおよび事前トレーニング済みのセマンティックジェネシスは、https：//github.com/JLiangLab/SemanticGenesisで入手できます。 
[ABSTRACT]詳細な画像は、自己監視学習に利用できます。このパフォーマンスは、私たちの新しい自己監視学習フレームワークに起因します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Decoupling Inherent Risk and Early Cancer Signs in Image-based Breast
  Cancer Risk Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_13.html">
      <font color="black">Decoupling Inherent Risk and Early Cancer Signs in Image-based Breast
  Cancer Risk Models</font>
    </a>
  </h2>
  <font color="black">ただし、トレーニングデータの選択はネットワークが特定する学習パターンに影響を与えるため、このようなモデルを使用する場合は注意が必要です。その結果、融合モデルでは、早期のがんの兆候がすでに見えている場合に、医師が予防措置を推奨する可能性があります。がんを発症する患者から）：がんの目に見える兆候のない画像でトレーニングされた固有のリスクモデル、がんまたはがんの初期の兆候を含む画像でトレーニングされたがんの兆候モデル、および癌の患者からのすべての画像でトレーニングされた融合モデル診断。 
[ABSTRACT]これらのモデルは、さまざまなパターンに焦点を当てた特徴的な機能を学習します。これは、パフォーマンスのコントラストに変換されます。リスクモデルは、より良い結果を開発するために使用できるディープニューラルネットワークに基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-11">
        <br><font color="black">2020-07-11</font>
      </time>
    </span>
</section>
<!-- paper0: RGB-D Salient Object Detection with Cross-Modality Modulation and
  Selection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_14.html">
      <font color="black">RGB-D Salient Object Detection with Cross-Modality Modulation and
  Selection</font>
    </a>
  </h2>
  <font color="black">RGB-D顕著オブジェクト検出（SOD）のモダリティ間の補完性を段階的に統合および改善する効果的な方法を提示します。3番目に、顕著性に基づく位置エッジ注意（sg-PEA）モジュールを使用して、ネットワークに顕著性に関連する領域により多く焦点を当てます。AFSモジュールは、マルチモダリティの空間的特徴の融合を利用して、チャネルモダリティの自己モダリティとクロスモダリティの相互依存性を検討します。 
[要約]提案されたネットワークは、rgb画像とそれに対応する深度マップからの情報を統合することを目的としています。より顕著性に関連する機能を除外し、下位機能をフィルタリングすることを目的としています。機能にさらに焦点を合わせるネットワーク</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Knowledge Distillation for Multi-task Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_15.html">
      <font color="black">Knowledge Distillation for Multi-task Learning</font>
    </a>
  </h2>
  <font color="black">マルチタスク学習（MTL）は、すべてのタスクで良好なパフォーマンスと計算コストの削減を実現するために複数のタスクを実行する単一のモデルを学習することです。最初に、各タスクのタスク固有のモデルを学習します。広範な実験結果により、メソッドは、よりバランスのとれた方法でマルチタスク学習モデルを最適化し、全体的なパフォーマンスを向上させることができます。 
[ABSTRACT]そのようなモデルを学習するには、さまざまな難易度、大きさ、特性を持つ一連のタスクの損失を共同最適化する必要があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Real-Time Multi-Object Tracking -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_16.html">
      <font color="black">Towards Real-Time Multi-Object Tracking</font>
    </a>
  </h2>
  <font color="black">2つのモデルを個別に実行すると、実行時間は2つのステップの合計であり、モデル間で共有できる潜在的な構造を調査しないため、効率の問題が発生する可能性があります。このホワイトペーパーでは、ターゲットの検出と共有モデルで学習する外観埋め込み。コードとモデルは\ url {https://github.com/Zhongdao/Towards-Realtime-MOT}で入手できます。 
[ABSTRACT]リアルタイムモットに関する最新の研究活動は通常、関連付けのステップに焦点を当てているため、本質的にはリアルタイムの関連付け方法であり、リアルタイムのモットシステムではありません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-27">
        <br><font color="black">2019-09-27</font>
      </time>
    </span>
</section>
<!-- paper0: MvMM-RegNet: A new image registration framework based on multivariate
  mixture model and neural network estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_17.html">
      <font color="black">MvMM-RegNet: A new image registration framework based on multivariate
  mixture model and neural network estimation</font>
    </a>
  </h2>
  <font color="black">外観と解剖学的情報の両方を統合する生成モデルが確立され、グループごとの登録を実装できる新しい損失関数が導出されます。2つの公開データセット、つまり、現在の深層学習ベースの登録アルゴリズムは、強度ベースの類似性測定値を損失関数として利用することがよくあります。この場合、トレーニング中の一対の移動画像と固定画像間の密な対応が逆伝播によって最適化されます。 
[要約]この論文では、多変量混合モデル（mvmm）とニューラルネットワーク推定に基づく新しい画像登録フレームワークを提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-28">
        <br><font color="black">2020-06-28</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Bijective Feature Maps for Linear ICA -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_18.html">
      <font color="black">Learning Bijective Feature Maps for Linear ICA</font>
    </a>
  </h2>
  <font color="black">このようなハイブリッドモデルのトレーニングの複雑さを考慮して、非相関行列の多様体であるStiefel多様体の近くに線形ICAを制約する新しい理論を導入します。これらの2つの方法を組み合わせることにより、ICAメソッドをスケーリングして、高い解釈可能な潜在構造を学習します。 3次元画像。私たちのハイブリッドモデルは、大規模な画像データセットに対して、フローベースのモデルや線形ICAよりも優れた教師なし潜在因子の発見を実現します。 
[要約]高速追跡ハイブリッドモデルを使用して、新しいモデルを作成できます。これを組み合わせて、線形独立成分分析（ica）モデルを非線形全単射機能マップで学習できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-18">
        <br><font color="black">2020-02-18</font>
      </time>
    </span>
</section>
<!-- paper0: JSENet: Joint Semantic Segmentation and Edge Detection Network for 3D
  Point Clouds -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_19.html">
      <font color="black">JSENet: Joint Semantic Segmentation and Edge Detection Network for 3D
  Point Clouds</font>
    </a>
  </h2>
  <font color="black">さらに、ネットワークがより良い境界でセマンティックセグメンテーション結果を生成するのを促進する新しい損失関数を提案します。この論文では、3Dセマンティックエッジ検出タスクに初めて取り組み、新しい2ストリームの完全たたみ込みネットワークを提示します。 2つのタスクを共同で実行します。コードリリース：https://github.com/hzykent/JSENet 
[ABSTRACT] 3Dセマンティックエッジ検出器の理解の欠如により、ほとんど注意が払われていません。ただし、2つのタスクの共同学習方法ではなく、私たちの方法は、同等以上のパフォーマンスを達成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Socially and Contextually Aware Human Motion and Pose Forecasting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_20.html">
      <font color="black">Socially and Contextually Aware Human Motion and Pose Forecasting</font>
    </a>
  </h2>
  <font color="black">人間との対話中のスムーズでシームレスなロボットナビゲーションは、人間の動きの予測に依存します。この現実世界の問題に対処するために、この予測タスクの重要な手掛かりとして、シーンと社会の両方のコンテキストを提案するフレームワークに組み込むことを検討します。ソーシャルプーリングレイヤーを使用して、シーンからのすべての個人の動きとポーズから関節特徴表現を生成することにより、ソーシャルの手がかりを含めます。 
[要約]この論文は、統合されたエンドツーエンドのパイプラインで人間の動きと体骨格の姿勢予測の両方のタスクに取り組むための新しいフレームワークを提案します。このために、最初に重要なメトリックを損失として適用することにより、これら2つのタスクを結合します。各タスクのエラーの原因を単一の距離として共同で測定します。ソーシャルプーリングレイヤーを使用して、シーンからのすべての個人の動きとポーズから共同の特徴表現を生成することにより、ソーシャルの手がかりも含めます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: MFRNet: A New CNN Architecture for Post-Processing and In-loop Filtering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_21.html">
      <font color="black">MFRNet: A New CNN Architecture for Post-Processing and In-loop Filtering</font>
    </a>
  </h2>
  <font color="black">このネットワークは、HEVC（HM 16.20）とVVC（VTM 7.0）の両方のPPおよびILFコーディングモジュールに統合されており、ランダムアクセス構成を使用したJVET共通テスト条件で完全に評価されています。両方のアンカーコーデック（HEVC HMとVVC VTM）と、品質評価のためにPSNRとVMAFの両方を使用するBjontegaard Delta測定に基づく他の既存のCNNベースのPP / ILFアプローチ。MFRNetがHM 16.20に統合されている場合、最大16.0 ％（BDレートVMAF）はILFで、最大21.0％（BDレートVMAF）はPPで示されています。 
[ABSTRACT]このネットワークは、カスケード構造を使用して接続された4つのマルチレベル機能レビュー残差密ブロック（mfrbs）で構成されています。それぞれのブロックは、前のmfrbからの高い神経機能を再利用します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Topology-Change-Aware Volumetric Fusion for Dynamic Scene Reconstruction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_22.html">
      <font color="black">Topology-Change-Aware Volumetric Fusion for Dynamic Scene Reconstruction</font>
    </a>
  </h2>
  <font color="black">実験は、最先端の方法と比較して、トポロジー変更の動的シーンの説得力のある再構成結果を示しています。トポロジー変更は、動的シーンの4D再構成にとって難しい問題です。この論文では、古典的なフレームワークが再構成されています。 TSDFとEDGの両方の再設計にNon-manifold Volumetric Gridの新しい構造を導入することにより、トポロジの変更の下で動的シーンの4D再構築を可能にするように設計されています。これにより、セルの分割と複製による接続の更新が可能になります。 
[ABSTRACT]クラシックボリュームフュージョンベースのアートでは、メッシュは通常、tsdfボリュームからプレミアムサーフェス表現として抽出されます。新しい方法は、4d再構成を可能にするように再設計されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Wavelet-Based Dual-Branch Network for Image Demoireing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_23.html">
      <font color="black">Wavelet-Based Dual-Branch Network for Image Demoireing</font>
    </a>
  </h2>
  <font color="black">ネットワークは、大規模な受容野をサポートする密なたたみ込みモジュールと拡張されたたたみ込みモジュールを組み合わせます。WDNetは、画像のデモアリング用に設計されていますが、他の2つの低レベルビジョンタスクに適用され、最新の画像の排出と排出Rain100hデータセットとRaindrop800データセットをそれぞれ使用します。広範な実験により、この方法の効果が実証され、さらに、WDNetが非画面画像のモアレアーティファクトの除去に一般化されていることが示されています。 
[要約]ウェーブレットベースのデュアルブランチネットワーク（wdnet）には、画像のデモアリングのための空間的注意メカニズムがあります。これらの方法とは異なり、私たちのネットワークは、ウェーブレットドメインのモアレパターンを削除して、画像コンテンツからモアレパターンの周波数を分離します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Realistic 3D Embedding via View Alignment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_24.html">
      <font color="black">Towards Realistic 3D Embedding via View Alignment</font>
    </a>
  </h2>
  <font color="black">2つの合成タスク（KITTIを使用した自動車の合成とCityscapesを使用した歩行者の合成）に関する広範な実験は、VA-GANが最新の生成方法と比較して質的および量的に高忠実度の合成を達成することを示しています。学習のための微分弁別ガイド合成された3Dモデルを現実的なポーズやビューで背景画像に合わせることができるように、背景画像からの幾何学的変換。テクスチャジェネレーターは、推定ビューの下で3Dモデルの正確なオブジェクトテクスチャを生成するための新しいビューエンコーディングメカニズムを採用しています。 
[ABSTRACT] va-ganは、相互に接続されており、ベッドの踏み台になっているテクスチャジェネレータと差分画像で構成されています。テクスチャジェネレータは、推定ビューの下で3dモデルの正確な画像を生成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Meta-rPPG: Remote Heart Rate Estimation Using a Transductive
  Meta-Learner -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_25.html">
      <font color="black">Meta-rPPG: Remote Heart Rate Estimation Using a Transductive
  Meta-Learner</font>
    </a>
  </h2>
  <font color="black">展開中の予測不可能な分布の変化に対処するために、テスト（展開）中にラベルなしのサンプルを取り、自己管理の重み調整（伝達的推論とも呼ばれます）のためにラベル付けされていないサンプルを取り、分布の変化への迅速な適応を提供するトランスダクティブメタラーナーを提案します。このアプローチを使用して、MAHNOB-HCIおよびUBFC-rPPGで最先端のパフォーマンスを実現します。エンドツーエンドの教師あり学習アプローチは、トレーニングデータが豊富で、あまり逸脱しない分布をカバーする場合にうまく機能します。テストデータの配布から、または展開中。 
[ABSTRACT] rppg信号は通常、複数の要因に敏感であるという制限のあるビデオカメラを使用して収集されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Conditional Image Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_26.html">
      <font color="black">Conditional Image Retrieval</font>
    </a>
  </h2>
  <font color="black">CIRシステムのパフォーマンスを評価するための2つの新しいデータセットを提示し、さまざまな設計の選択肢を評価します。最後に、CIRデータ構造が生成的敵対的ネットワーク（GAN）の「盲点」を識別できることを示します：GANが失敗する領域実際のデータ分布を適切にモデル化します。これにより、条件付きクエリが高速化され、条件付けなしでクエリが遅くなることはありません。 
[要約]これらのシステムはクエリのネットワークを広げます。非常に異なるメディアと文化的起源の芸術作品の間で共有された意味論的コンテンツを探索できるアルゴリズムを提示します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Wavelet Integrated CNNs for Noise-Robust Image Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_27.html">
      <font color="black">Wavelet Integrated CNNs for Noise-Robust Image Classification</font>
    </a>
  </h2>
  <font color="black">低周波コンポーネントは、基本的なオブジェクト構造を含む主要な情報を格納します。これは、後続のレイヤーに送信されて、堅牢な高レベルの特徴を抽出します。 -sampling ..ほとんどのデータノイズを含む高周波コンポーネントは、WaveCNetsのノイズ耐性を向上させるために、推論中に削除されます。 
[ABSTRACT] cnnsは、max-pooling、strided-convolution、average-poolingを同じようなdwtバージョンに置き換えています。これらの変更は、ウェーブネットの復元力を向上させるために使用されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-07">
        <br><font color="black">2020-05-07</font>
      </time>
    </span>
</section>
<!-- paper0: Multitask Learning Strengthens Adversarial Robustness -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_28.html">
      <font color="black">Multitask Learning Strengthens Adversarial Robustness</font>
    </a>
  </h2>
  <font color="black">敵対的防御は未解決の課題ですが、我々の結果は、ディープネットワークはあまりにも少ないタスクでトレーニングされているために脆弱であることを示唆しています。モデルの敵対的なロバスト性を、トレーニングされるタスクの数に結びつける実証分析。 
[ABSTRACT]調査では、モデルが複数のタスクで一度にトレーニングされると、コンピュータへの攻撃に対してより堅牢になることが示されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Face Recognition by Clustering Unlabeled Faces in the Wild -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_29.html">
      <font color="black">Improving Face Recognition by Clustering Unlabeled Faces in the Wild</font>
    </a>
  </h2>
  <font color="black">深い顔認識は、大規模なラベル付きデータから大きなメリットを得ていますが、現在の研究は、ラベルなしデータを活用してパフォーマンスをさらに向上させ、人間による注釈のコストを削減することに焦点を当てています。制御された設定と実際の設定の両方での広範な実験は、監視下での方法の一貫した改善を示していますベースライン、たとえば、IJB-A検証の11.6％の改善。これに対処するために、極値理論に基づく新規のアイデンティティ分離方法を提案します。 
[ABSTRACT]以前の作業は主に制御された設定で行われており、ラベル付きとラベルなしのデータセットには、構成による周回IDがありません。同じIDからの競合データが複数のクラスターに分割されるため、現実的でないスケールでは、重大なラベル付けノイズが発生します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Hierarchical Kinematic Human Mesh Recovery -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_30.html">
      <font color="black">Hierarchical Kinematic Human Mesh Recovery</font>
    </a>
  </h2>
  <font color="black">単一の画像から3Dヒューマンメッシュのパラメトリックモデルを推定する問題を検討します。標準ベンチマークデータセットでの広範な実験によってこれらの側面を実証し、提案された新しい設計がいくつかの既存の一般的な方法を上回り、新しい状態を確立する方法を示します。最新の結果。これにより、リグレッサーアーキテクチャの強力な事前通知された設計と、3Dヒューマンメッシュリカバリーの現在の標準フレームワークと組み合わせて柔軟に使用できる関連階層最適化が実現します。 
[要約]これらの方法は人体の運動学的構造のみを利用しており、以前のモデルの次善の使用につながっています。これにより、3Dヒューマンメッシュリカバリの現在の標準フレームワークと組み合わせて柔軟に使用できる強力なモデルが得られます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-09">
        <br><font color="black">2020-03-09</font>
      </time>
    </span>
</section>
<!-- paper0: Bridging Knowledge Graphs to Generate Scene Graphs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_31.html">
      <font color="black">Bridging Knowledge Graphs to Generate Scene Graphs</font>
    </a>
  </h2>
  <font color="black">この目的のために、2つのグラフ間および各グラフ内で情報を反復的に伝達しながら、各反復でブリッジを徐々に調整しながら、新しいグラフベースのニューラルネットワークを提案します。広範な実験を通じて、 GB-Netを最新の方法と比較して、最新の技術を生み出しました。この新しい視点に基づいて、シーングラフの生成を、シーンと常識グラフの間のブリッジの推論として再定式化します。シーングラフの述語インスタンスは、常識グラフの対応するエンティティまたは述語クラスにリンクする必要があります。 
[ABSTRACT]常識知識グラフは、世界がどのように構造化され、一般的な概念がどのように相互作用するかをエンコードする豊富なリポジトリです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-07">
        <br><font color="black">2020-01-07</font>
      </time>
    </span>
</section>
<!-- paper0: Joint Layout Analysis, Character Detection and Recognition for
  Historical Document Digitization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_32.html">
      <font color="black">Joint Layout Analysis, Character Detection and Recognition for
  Historical Document Digitization</font>
    </a>
  </h2>
  <font color="black">拡張された中国の歴史的文書MTHv2データセットの実験結果は、提案されたフレームワークの有効性を示しています。文字ブランチは、ドキュメントイメージ内の個々の文字をローカライズし、それらを同時に認識します。 
[ABSTRACT]このフレームワークでは、文字ブランチとレイアウトブランチが追加されています。これらには、テキスト行にグループ化する後処理メソッドが含まれています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: REPrune: Filter Pruning via Representative Election -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_33.html">
      <font color="black">REPrune: Filter Pruning via Representative Election</font>
    </a>
  </h2>
  <font color="black">私たちの方法は、精度をより迅速に回復し、微調整中に必要なフィルターのシフトを小さくします。経験的に、REPruneは49％以上のFLOPを削減し、CIFAR-10のResNet-110で0.53％の精度向上を実現します。 ImageNetのResNet-18でトップ1の検証損失が1.67％の41.8％FLOPを超える
[ABSTRACT] repruneは、0％で49％を超えるフロップを削減します。 repruneの過程における他のフィルターの</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: BézierSketch: A generative model for scalable vector sketches -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_34.html">
      <font color="black">BézierSketch: A generative model for scalable vector sketches</font>
    </a>
  </h2>
  <font color="black">この目的のために、最初に、各ストロークを最適なB \ &#39;ezier曲線に埋め込むようにエンコーダーをトレーニングする、ストロークの埋め込みに対する新しいインバースグラフィックスアプローチを紹介します。定性的および定量的な結果をQuick、Draw！で報告します。基準。 
[ABSTRACT]ランドマークsketchrnnは、スケッチを一連のウェイポイントとして定量的に生成することにより画期的な機能を提供しました。これにより、スケッチをパラメータ化されたストロークの短いシーケンスとして扱うことができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-04">
        <br><font color="black">2020-07-04</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Active Learning via Open Set Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_35.html">
      <font color="black">Deep Active Learning via Open Set Recognition</font>
    </a>
  </h2>
  <font color="black">アクティブラーニングの目的は、オラクルへのリクエスト数を最小限に抑えるために、ラベルのないサンプルの有益性を推測することです。これらの制約の下で、ラベルのないプールから最も有益なインスタンスのみを選択し、オラクル（例：人間の専門家）がこれらのサンプルにラベルを提供します。オラクルがラベル付けする必要があるサンプルを選択するために、この信頼度尺度の逆数を使用します。 
[ABSTRACT]アクティブ学習の目的は、ラベルのないサンプルの有益性を推測することです。したがって、vnnが不明確なラベルのないサンプルは、将来のトレーニングにより有益です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-04">
        <br><font color="black">2020-07-04</font>
      </time>
    </span>
</section>
<!-- paper0: Spatial-Adaptive Network for Single Image Denoising -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_36.html">
      <font color="black">Spatial-Adaptive Network for Single Image Denoising</font>
    </a>
  </h2>
  <font color="black">粗いものから細かいものまでノイズを除去することで、高品質のノイズのない画像を取得できます。変形可能な畳み込みを導入して、空間的に相関する特徴を重み付けのためにサンプリングします。この方法を、合成と実際のノイズの多い画像データセットの両方に適用します。 
[要旨]効率的な単一画像のブラインドノイズ除去のための新しい空間適応型ノイズ除去ネットワークを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-28">
        <br><font color="black">2020-01-28</font>
      </time>
    </span>
</section>
<!-- paper0: Video Object Segmentation with Episodic Graph Memory Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_37.html">
      <font color="black">Video Object Segmentation with Episodic Graph Memory Networks</font>
    </a>
  </h2>
  <font color="black">構造化された外部メモリ設計により、視覚的な情報が限られている場合でも、モデルが新しい知識を包括的にマイニングし、すばやく保存できます。また、微分可能メモリコントローラは、有用な表現をメモリに保存する抽象的な方法と、これらの表現を後で予測に使用する方法をゆっくりと学習します、勾配降下を介して。。さらに、学習可能なコントローラが埋め込まれているため、メモリの読み取りと書き込みが簡単になり、固定メモリスケールが維持されます。さらに、提案されたグラフメモリネットワークは、両方を一般化できるきちんとした原理のフレームワークを生成します。 -ショットおよびゼロショットのビデオオブジェクトセグメンテーションタスク。 
[要約]提案されたグラフメモリネットワークは、ネットワークの「フレームを更新することを学ぶ」という考えに対処するために開発されています。ワンショットおよびゼロショットのビデオオブジェクトセグメンテーションタスクでうまく一般化できる詳細な詳細モデルを提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Human 3D Pose Representation with Viewpoint and Pose
  Disentanglement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_38.html">
      <font color="black">Unsupervised Human 3D Pose Representation with Viewpoint and Pose
  Disentanglement</font>
    </a>
  </h2>
  <font color="black">コードとモデルは、\ url {https://github.com/NIEQiang001/unsupervised-human-pose.git}で入手できます。これらの2つの絡み合っていない機能は、3Dポーズの表現として一緒に利用されます。人間の3Dポーズの推定とアクションの認識。 
[ABSTRACT]デジタルモデルを使用して、ポーズ依存およびビュー依存の機能を人間のスケルトンデータから分離することができます。これらには、自動エンコーダ、自動エンコーダ、自動エンコーダが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Transposer: Universal Texture Synthesis Using Feature Maps as Transposed
  Convolution Filter -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_39.html">
      <font color="black">Transposer: Universal Texture Synthesis Using Feature Maps as Transposed
  Convolution Filter</font>
    </a>
  </h2>
  <font color="black">この研究では、従来のテクスチャ合成における組み立て/ステッチング操作が転置たたみ込み演算に類似しているという発見に基づいて、転置たたみ込み演算を使用する新しい方法を提案します。具体的には、転置たたみ込みフィルターとしての入力テクスチャと、転置たたみ込みへの入力として自己相関情報を取得する機能の自己相似性マップ。妥協点として、最近の多くの方法では、同じ単一（またはテクスチャー画像の固定セット。目に見えない画像の再トレーニングに多大な時間を要します。 
[ABSTRACT] cnnのプロジェクトは、従来のテクスチャ合成における組み立てまたはスティッチング操作が転置たたみ込み操作に匹敵するという発見に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: P-KDGAN: Progressive Knowledge Distillation with GANs for One-class
  Novelty Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_40.html">
      <font color="black">P-KDGAN: Progressive Knowledge Distillation with GANs for One-class
  Novelty Detection</font>
    </a>
  </h2>
  <font color="black">最初のステップでは、生徒GANは、事前に重み付けされた事前トレーニング済みの教師GANのガイドを介して、教師から基本的な知識を完全に学習します。知識抽出の漸進的学習は、生徒GANのパフォーマンスを継続的に改善する2段階のアプローチです。シングルステップ方式よりも優れたパフォーマンスを実現します。P-KDGANは、教師から生徒に知識を転送するために設計された蒸留損失によって2つの標準GANを接続する新しい試みです。 
[ABSTRACT] gansを使用したプログレッシブ知識抽出は、コンパクトで高速な新規性検出ネットワークを学習するために提案されています。pkdganは、知識抽出の標準であり、学生のganのパフォーマンスを向上させます。教師と生徒のガン</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Anomaly Detection by One Class Latent Regularized Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_41.html">
      <font color="black">Anomaly Detection by One Class Latent Regularized Networks</font>
    </a>
  </h2>
  <font color="black">さらに、弁別器と見なされる補助オートエンコーダーは、より安定したトレーニングプロセスを取得できます。これらの問題を解決するために、潜在的な特徴空間でのみトレーニングデータの基になる構造がキャプチャされない、新しい敵対的なデュアルオートエンコーダーネットワークが提案されます。しかし、潜在的な表現の空間で差別的にさらに制限され、より正確な検出器につながる可能性もあります。しかし、GANのトレーニングプロセスはまだ不安定でやりがいがあります。 
[ABSTRACT]タスクは、異常なインスタンスの画像に基づいてモデルを構築することです。これらの画像は、正常なクラスに属する一連の画像に表示されます。ただし、トレーニングプロセスはまだ不安定で、やりがいがあります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-05">
        <br><font color="black">2020-02-05</font>
      </time>
    </span>
</section>
<!-- paper0: Cross-Domain Medical Image Translation by Shared Latent Gaussian Mixture
  Model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_42.html">
      <font color="black">Cross-Domain Medical Image Translation by Shared Latent Gaussian Mixture
  Model</font>
    </a>
  </h2>
  <font color="black">私たちのモデルの優れたパフォーマンスは、翻訳された画像で2つのタスクを実行することによって確認されます-大動脈プラークの検出とセグメンテーションおよび膵臓セグメンテーション。多くの既存のクロスドメイン画像から画像への翻訳モデルは、大きな臓器。しかし、そのようなモデルは、翻訳プロセス中に微細構造を維持する能力に欠けています。これは、大動脈や骨盤動脈の小さな石灰化プラークのセグメント化など、多くの臨床応用にとって重要です。 
[ABSTRACT]クロスドメインの画像分析ツールは、実際の臨床アプリケーションで高い需要があります。正確な診断を行うには、さまざまな領域のX線画像が必要になることがよくあります。これらのモデルには、変換プロセス中に微細構造を保存する機能がありません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Multiple Sound Sources Localization from Coarse to Fine -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_43.html">
      <font color="black">Multiple Sound Sources Localization from Coarse to Fine</font>
    </a>
  </h2>
  <font color="black">これらの結果は、効果的にサウンドを特定のビジュアルソースに合わせるモデルの能力を示しています。次に、ローカリゼーションの結果を使用してサウンドを分離し、既存の方法と同等のパフォーマンスを得ています。コードはhttps://github.com/shvdiwnkozbw/Multi-で入手できます。 Source-Sound-Localization 
[ABSTRACT] 2ステージオーディオビジュアル学習フレームワークは、さまざまなカテゴリのオーディオとビジュアルの表現を複雑なシーンから解きほぐします。次に、サウンドの分離にローカリゼーションの結果を使用し、既存の方法と同等のパフォーマンスを得ます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: AQD: Towards Accurate Quantized Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_44.html">
      <font color="black">AQD: Towards Accurate Quantized Object Detection</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、オブジェクト検出における量子化ネットワークのパフォーマンスの低下が、バッチ正規化の不正確なバッチ統計に起因することを示します。具体的には、マルチレベルバッチ正規化（マルチレベルBN）を使用してバッチ統計を推定することを提案します。各検出ヘッドを個別に設定します。さらに、量子化器自体の構成方法を改善するために、学習された間隔量子化方法を提案します。 
[ABSTRACT]これを解決するために、正確な量子化オブジェクト検出-aqd）方法を提案します。オブジェクト検出でのビット幅量子化の量を減らすには、まだ課題です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Generating Videos of Zero-Shot Compositions of Actions and Objects -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_45.html">
      <font color="black">Generating Videos of Zero-Shot Compositions of Actions and Objects</font>
    </a>
  </h2>
  <font color="black">ヒューマンオブジェクトインタラクションビデオを生成するために、ビデオのさまざまな側面に焦点を当てた複数の弁別子を含む新しい敵対的なフレームワークHOI-GANを提案します。特に、ゼロショット構成ビデオでヒューマンオブジェクトインタラクションビデオを生成するタスクを紹介します。設定、つまり、ターゲットアクションとターゲットオブジェクトを別々に見て、トレーニング中に見えないアクションオブジェクト構成のビデオを生成します。提案されたフレームワークの有効性を実証するために、2つの挑戦的なデータセットで広範な定量的および定性的評価を実行します：EPIC -キッチンと20BN-Something-Something v2。 
[要約]このペーパーでは、ビデオを作成する方法を開発します。これらは、複雑なシーンでのビデオ生成の問題への対処に向けて進歩しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br><font color="black">2019-12-05</font>
      </time>
    </span>
</section>
<!-- paper0: Visually Guided Sound Source Separation using Cascaded Opponent Filter
  Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_46.html">
      <font color="black">Visually Guided Sound Source Separation using Cascaded Opponent Filter
  Network</font>
    </a>
  </h2>
  <font color="black">プロジェクトページ：https://ly-zhu.github.io/cof-net ..このようなタスクは、通常、視覚的にガイドされた音源分離と呼ばれます。システムは、ソースの外観と動きによってガイドされます。目的として、ビデオフレーム、オプティカルフロー、動的画像、およびそれらの組み合わせに基づいてさまざまな表現を研究します。 
[要約] cofの主要な要素は、ソース間の残余コンポーネントを識別して再配置する新しいフィルターモジュールです。システムは、cofとともにソースのビジュアルの混合を生成します。また、ピクセルのレベルの視覚効果の組み合わせを生成します。オリジナルの視覚効果</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br><font color="black">2020-06-04</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Abnormality Detection Using Heterogeneous Autonomous
  Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_47.html">
      <font color="black">Unsupervised Abnormality Detection Using Heterogeneous Autonomous
  Systems</font>
    </a>
  </h2>
  <font color="black">そのために、この論文では、無人監視ドローンの異常の程度を推定し、監視対象外の方法でリアルタイム画像とIMU（慣性計測ユニット）センサーデータを分析する異種システムを提案します。デバイスの誤動作を検出します。さらに、社内データセットでこのアプローチをテストして、その堅牢性を検証しました。 
[要約]畳み込みニューラルネットワークは、通常の画像と検討中の別の画像の間の角度を予測します。これらのアルゴリズムの結果は、最終的な異常の程度を推定するためにまとめられます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-05">
        <br><font color="black">2020-06-05</font>
      </time>
    </span>
</section>
<!-- paper0: From Symmetry to Geometry: Tractable Nonconvex Problems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_48.html">
      <font color="black">From Symmetry to Geometry: Tractable Nonconvex Problems</font>
    </a>
  </h2>
  <font color="black">これらの問題は特徴的な幾何学的構造を示します：ローカルミニマイザーは単一の「グラウンドトゥルース」ソリューションの対称コピーですが、他の臨界点はグラウンドトゥルースの対称コピーのバランスのとれた重ね合わせで発生し、対称性を壊す方向に負の曲率を示します..イメージング、信号処理、およびデータ分析におけるさまざまな問題から発生するこの現象の例について説明します。それでも、単純な方法（例：勾配降下法）は、実際には驚くほどよく機能します。 
[ABSTRACT]問題に対するシンプルでシンプルな答えを見つけるためのシンプルでシンプルなアプローチ。問題の理解を助けるために呼び出され、これらの問題は解決されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: A Single Stream Network for Robust and Real-time RGB-D Salient Object
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_49.html">
      <font color="black">A Single Stream Network for Robust and Real-time RGB-D Salient Object
  Detection</font>
    </a>
  </h2>
  <font color="black">彼らは深度マップ自体の効果を深く探求していません。この作業では、単一のストリームネットワークを設計して深度マップを直接使用し、RGBと深度の間の早期融合と中間融合をガイドします。これにより、深度の機能エンコーダーが節約されます。ストリームし、軽量でリアルタイムのモデルを実現します。（2）新しい奥行き強調デュアルアテンションモジュール（DEDA）を設計して、前景/背景ブランチに空間的にフィルタリングされた機能を効率的に提供し、デコーダーが中間融合を最適に実行します。 
[要約]深度マップは、フュージョンの影響を具体的に調査していません。シングルフュージョンエンコーダーを構築して、早期フュージョンを実現しています。Imagenetバックボーンモデルを最大限に活用して、豊富で差別的な機能を抽出できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Top-Related Meta-Learning Method for Few-Shot Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_50.html">
      <font color="black">Top-Related Meta-Learning Method for Few-Shot Detection</font>
    </a>
  </h2>
  <font color="black">トレーニング全体は、基本クラスモデルと微調整フェーズで構成されます。大量ショットデータとより多くのパラメーターに依存する多くのメタ学習手法が、少数ショット検出用に提案されています。ただし、カテゴリの不均衡のため機能が少ないと、以前の方法には明らかな問題があり、少数ショット検出のバイアスが強く、分類が不十分です。 
[ABSTRACT]カテゴリの場合、true-ラベルexample.itを利用するTCLを提案します。これは、カテゴリベースのグループ化メカニズムで構成され、外観と環境によってカテゴリをグループ化し、類似したカテゴリ間の意味的機能を強化します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: MeDaS: An open-source platform as service to help break the walls
  between medicine and informatics -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_51.html">
      <font color="black">MeDaS: An open-source platform as service to help break the walls
  between medicine and informatics</font>
    </a>
  </h2>
  <font color="black">この目的のために、私たちは新しいオープンソースプラットフォーム、つまりMeDaSを提案します。サービスとしてのMeDicalオープンソースプラットフォームです。私たちの知る限り、MeDaSは、協調的かつインタラクティブなサービスを提供する最初のオープンソースプラットフォームですDL関連のツールキットを使用して簡単に医学的背景の研究者、および同時に情報科学の科学者またはエンジニアが医学知識の側面を理解するために..一方では、医用画像分析にDLの力を活用する多大なニーズが生じています。医学、臨床、情報学のバックグラウンドの研究コミュニティから、彼らの専門知識、知識、スキル、および経験を共同で共有します。 
[ABSTRACT] dl関連のツールキットを使用して、医学的背景から研究者にコラボレーションとインタラクティブなサービスを簡単に提供する最初のオープンソースプラットフォーム。肺、肝臓、脳、胸部、歯科などの5つのタスクが検証され、効率的に実現可能</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-12">
        <br><font color="black">2020-07-12</font>
      </time>
    </span>
</section>
<!-- paper0: Symmetric Dilated Convolution for Surgical Gesture Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_52.html">
      <font color="black">Symmetric Dilated Convolution for Surgical Gesture Recognition</font>
    </a>
  </h2>
  <font color="black">JIGSAWSデータセットからの基本的なロボット縫合タスクに対するアプローチの有効性を検証します。これらの課題に取り組むために、RGBビデオのみを使用して対応する境界を持つ外科ジェスチャーを自動的に検出およびセグメント化する新しい時間畳み込みアーキテクチャを提案します。運動学データを収集するために追加のセンサーを必要とするか、トリミングされていない長い手術ビデオからの時間情報のキャプチャに制限があります。 
[ABSTRACT]この実験は、長期的なフレームの依存関係をキャプチャする方法の能力を示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Multi-Target Domain Adaptation Through Knowledge
  Distillation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_53.html">
      <font color="black">Unsupervised Multi-Target Domain Adaptation Through Knowledge
  Distillation</font>
    </a>
  </h2>
  <font color="black">たとえば、ビデオ監視では、各カメラは異なる視点（ターゲットドメイン）に対応できます。また、マルチドメイン適応のターゲットデータをブレンドして共通モデルをトレーニングすることで対処されていますが、これによりパフォーマンスが低下する可能性があります.. MT-MTDAは、OfficeHome、Office31、およびDigits-5データセットの最先端の方法と比較され、実証結果は、提案されたモデルが複数のターゲットドメインにわたってかなり高いレベルの精度を提供できることを示しています。 
[要約]単一のターゲットドメインのシナリオはudaの文献でよく研究されていますが、マルチスティティドmtdaの設定は、その重要性にもかかわらず大部分が未踏のままです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: 360$^\circ$ Depth Estimation from Multiple Fisheye Images with Origami
  Crown Representation of Icosahedron -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_54.html">
      <font color="black">360$^\circ$ Depth Estimation from Multiple Fisheye Images with Origami
  Crown Representation of Icosahedron</font>
    </a>
  </h2>
  <font color="black">さらに、抽出された特徴から二十面体のコストボリュームを生成するための二十面体ベースの球形スイープを提案します。この研究では、屋内環境の複数の全方位画像から全周深さを推定する方法を紹介します。CrownConvを適用できます。魚眼画像と正距円筒画像の両方に適用して、特徴を抽出します。 
[ABSTRACT] crownconvは、魚眼画像と正距円筒画像の両方に適用して特徴を抽出できます。コスト量は正則化され、最終量はコスト量から深さ相関によって取得されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Continual Adaptation for Deep Stereo -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_55.html">
      <font color="black">Continual Adaptation for Deep Stereo</font>
    </a>
  </h2>
  <font color="black">私たちのパラダイムでは、オンラインでモデルを継続的に適応させるために必要な学習信号は、右から左への画像ワーピングによる自己監視から、または従来のステレオアルゴリズムから供給できます。ステレオ画像からの深度推定は、畳み込みニューラルネットワークによる比類のない結果で実行されますエンドツーエンドで密な格差を後退するようにトレーニングします。軽量でモジュール式のアーキテクチャであるモジュール式適応ネットワーク（MADNet）を設計し、ネットワーク全体の独立した部分の効率的な最適化を可能にするモジュール式適応アルゴリズム（MAD、MAD ++）を策定します。 。 
[要約]ネットワークアーキテクチャと適応アルゴリズムは、ディープステレオネットワークの「継続的適応」理論を作成しました。オンラインでモデルを適応させるために必要な学習信号は、右から左への画像ワーピングによる自己監視または従来のステレオから供給できると述べていますアルゴリズム</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-10">
        <br><font color="black">2020-07-10</font>
      </time>
    </span>
</section>
<!-- paper0: Semi-supervised Learning with a Teacher-student Network for Generalized
  Attribute Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_56.html">
      <font color="black">Semi-supervised Learning with a Teacher-student Network for Generalized
  Attribute Prediction</font>
    </a>
  </h2>
  <font color="black">私たちの実験は、私たちの方法がファッション属性予測のさまざまなベンチマークで競争力のあるパフォーマンスを達成するだけでなく、見えないドメインの堅牢性とクロスドメインの適応性も向上させることを示しています。そのことを念頭に置いて、マルチティーチャーシングル学生（MTSS ）マルチタスク学習と半教師あり学習の蒸留に触発されたアプローチです。これは、属性のクラス階層の定義が不明確であるため、トレーニングデータが必然的にクラスの不均衡とラベルのスパース性の影響を受け、効果的な注釈が欠落するためです。 
[ABSTRACT]競合するオブジェクトを正確に認識することは重要ですが、それでも困難です。私たちのMTSS-特定のドメインの専門家-学習タスク-ラベル埋め込み技術を使用する特定の企業</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: JNR: Joint-based Neural Rig Representation for Compact 3D Face Modeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_57.html">
      <font color="black">JNR: Joint-based Neural Rig Representation for Compact 3D Face Modeling</font>
    </a>
  </h2>
  <font color="black">3番目に、スキニングを通じて、私たちのモデルは口の内部と目、およびアクセサリ（髪、眼鏡など）の追加をサポートします。これは、よりシンプルで、より正確で、原理的な方法です。これは、モバイルデバイスとエッジデバイスのグラフィックスアプリケーションとビジョンアプリケーションの両方に幅広い価値があることを示唆しています。 
[ABSTRACT]私たちのモデルは、以前のブレンドシェイプに基づくいくつかの重要な利点を享受しています-ベースのモデル。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Temporal Self-Ensembling Teacher for Semi-Supervised Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_58.html">
      <font color="black">Temporal Self-Ensembling Teacher for Semi-Supervised Object Detection</font>
    </a>
  </h2>
  <font color="black">上記の自己強調戦略は、協力して、ラベルのない画像に対してより良い教師予測をもたらします。この作品のソースコードは、http：//github.com/SYangDong/tse-t。で公開されています。さらに、このメソッドは新しい状態を設定しますベースのSSODメソッドより1.44％優れたVOCベンチマークでのSSODの芸術の割合。 
[要約]知識蒸留-kd）教師モデルと生徒モデルで構成されるフレームワークは、ラベルなしの画像をうまく活用するために広く使用されています。準教師付き学習方法は、ベースラインssod方法よりも1.44％優れています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Heterogeneous Autoencoder for Subspace Clustering of Sequential
  Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_59.html">
      <font color="black">Deep Heterogeneous Autoencoder for Subspace Clustering of Sequential
  Data</font>
    </a>
  </h2>
  <font color="black">最後に、シーケンシャルデータの事前知識に基づいて制約グラフの形で追加のペナルティを組み込んで、クラスタリングのロバスト性を向上させます。結果は、提案された手法が、困難なビデオシーケンスに対していくつかの最新の手法よりも優れていることを示しています。一般的な潜在的な特徴を生成するために、タスク依存の不確実性の重みを使用して、オートエンコーダーを共同でトレーニングします。 
[ABSTRACT]事前に訓練されたセマンティックセグメンテーションネットワークからマスクを抽出します。次に、結合する境界ボックスからマスクを抽出します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Dense People Detection with Deep Learning and Depth images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_60.html">
      <font color="black">Towards Dense People Detection with Deep Learning and Depth images</font>
    </a>
  </h2>
  <font color="black">最初にネットワークをトレーニングするためにシミュレーションデータを使用し、その後、比較的少量の実際のデータで微調整します。この方法を、従来のソリューションとDNNベースのソリューションの両方を含む既存の最先端技術と完全に比較します。この論文では、単一の深度画像から複数の人物を検出するDNNベースのシステムを提案します。 
[要旨]私たちのシステムは、パフォーマンスを向上させるために分離された畳み込みを使用します。これらのネットワークは、トレーニング中に使用されたシーンとは異なるシーンで動作するように一般化されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: UDBNET: Unsupervised Document Binarization Network via Adversarial Game -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_61.html">
      <font color="black">UDBNET: Unsupervised Document Binarization Network via Adversarial Game</font>
    </a>
  </h2>
  <font color="black">実験結果は、広く使用されているDIBCOデータセットに対する既存の最先端のアルゴリズムよりも、提案されたモデルの優れたパフォーマンスを示しています。提案されたシステムのソースコードは、https：//github.com/VIROBO-15で公開されています。 / UDBNET ..このように、共同弁別器はUDBNetを強制して、実際の劣化した画像に対してより良いパフォーマンスを実現します。 
[ABSTRACT]敵対的なテクスチャ拡張ネットワーク（atanet）は、まず、劣化した参照画像のテクスチャをクリーンな画像の上に重ね合わせます。共同弁別器は、udbnetに画像のパフォーマンスを向上させる</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: High-Resolution Image Inpainting with Iterative Confidence Feedback and
  Guided Upsampling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_62.html">
      <font color="black">High-Resolution Image Inpainting with Iterative Confidence Feedback and
  Guided Upsampling</font>
    </a>
  </h2>
  <font color="black">これを実現するには、コンテキストアテンションモジュールを拡張して、入力画像の高解像度機能パッチを借用します。さらに、ガイド付きアップサンプリングネットワークを提案して、高解像度の修復結果を生成できるようにします。さらに、実際のオブジェクト削除シナリオを模倣します。 、大きなオブジェクトマスクデータセットを収集し、ユーザー入力をよりよくシミュレートする、より現実的なトレーニングデータを合成します。 
[ABSTRACT]インペインティングでは、フィードバックメカニズムを備えた反復的なインペインティングメソッドを提案します。このマップは、各反復で穴内の高信頼ピクセルのみを信頼することで徐々に穴を埋め、残りの結果に焦点を当てます。コンテキストでは、大きなオブジェクトを収集しますデータセットをマスクし、より現実的なトレーニングデータを合成してユーザー入力をよりよくシミュレート</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-24">
        <br><font color="black">2020-05-24</font>
      </time>
    </span>
</section>
<!-- paper0: Modeling Artistic Workflows for Image Generation and Editing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_63.html">
      <font color="black">Modeling Artistic Workflows for Image Generation and Editing</font>
    </a>
  </h2>
  <font color="black">上記の観察を動機として、与えられた芸術的ワークフローに従う生成モデルを提案し、既存の作品の多段階画像生成と多段階画像編集の両方を可能にします。3つの異なる芸術的データセットの定性的および定量的結果画像生成と編集の両方のタスクで提案されたフレームワークの有効性を実証します。人々は、全体的なデザインを通知する複数の段階を含む芸術的なワークフローに従って芸術を作成することがよくあります。 
[ABSTRACT]アーティストが以前の決定を変更したい場合、この新しい決定を最終的なアートワークに転送するには、かなりの作業が必要になる場合があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: A Graph-based Interactive Reasoning for Human-Object Interaction
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_64.html">
      <font color="black">A Graph-based Interactive Reasoning for Human-Object Interaction
  Detection</font>
    </a>
  </h2>
  <font color="black">広範な実験により、提案されたフレームワークがV-COCOとHICO-DETの両方のベンチマークで既存のHOI検出方法よりも優れ、ベースラインが約9.4％と15％向上し、HOIの検出における有効性が検証されました。提案されたモデルはプロジェクト関数で構成されています。関連するターゲットを畳み込み空間からグラフベースの意味空間にマッピングします。すべてのノード間でセマンティクスを伝達するメッセージ受け渡しプロセスと、推論されたノードを畳み込み空間に変換する更新関数です。さらに、グラフ内で組み立てるための新しいフレームワークを構築HOIを検出するためのモデル、つまりIn-GraphNet。 
[要旨]ホイの検出方法は主に追加のアノテーションに依存しています。ホイはホイスを使用してホイスを推測します。ホイスベースのツールはホイスを検出できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: BUNET: Blind Medical Image Segmentation Based on Secure UNET -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_65.html">
      <font color="black">BUNET: Blind Medical Image Segmentation Based on Secure UNET</font>
    </a>
  </h2>
  <font color="black">実験では、プロトコルのパラメーター空間を徹底的に調べ、精度の低下がほとんどないベースラインアーキテクチャで、最新の安全な推論手法と比較して、最大14倍の推論時間の短縮を実現できることを示します。さらに、高次元の入力データを備えたGCベースの安全なアクティブ化プロトコルの計算上のボトルネックを減らすために、広範なアーキテクチャ検索を実行します。BUNETでは、準同型暗号化や文字化け回路（GC）などの暗号プリミティブを効率的に使用して、完全なUNETニューラルアーキテクチャ用の安全なプロトコル。 
[要旨]プライバシーを実装するプリミティブプロトコルであるブラインドユネット（ブネット）を提案します。さらに、安全なアクティベーションプロトコルのコンピューティングボトルネックを減らすために広範なアーキテクチャ検索を実行します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Lifelong Learning using Eigentasks: Task Separation, Skill Acquisition,
  and Selective Transfer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_66.html">
      <font color="black">Lifelong Learning using Eigentasks: Task Separation, Skill Acquisition,
  and Selective Transfer</font>
    </a>
  </h2>
  <font color="black">生涯学習のための固有タスクフレームワークを紹介します。監視付きの継続的な学習で最先端のパフォーマンスを改善し、ゲームStarcraft2の生涯RLアプリケーションで前向きな知識の伝達の証拠を示します。リプレイアプローチは、主に壊滅的な忘却を回避するために使用され、フォワードナレッジトランスファーなどの他の生涯学習の目標にも対処します。 
[ABSTRACT] eigentaskは学習スキルとeigentaskのゲームのペアです。efertaskはeigentaskとefertaskの組み合わせです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Object Permanence from Video -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_67.html">
      <font color="black">Learning Object Permanence from Video</font>
    </a>
  </h2>
  <font color="black">この学習問題を4つのコンポーネントに分解する必要がある理由を説明します。オブジェクトは、（1）表示、（2）閉塞、（3）別のオブジェクトに含まれ、（4）含まれるオブジェクトによって運ばれます。これらの4つのシナリオでオブジェクトの場所を予測することを学習するアーキテクチャ。オブジェクトの永続性により、直接見えない場合でもオブジェクトが存在し続けることを理解することで、見えないオブジェクトの場所を推論できます。 
[ABSTRACT]オブジェクトの永続性を学習するタスクは、世界のモデルを構築するために重要です。これは、自然な視覚シーン内のオブジェクトが隠れて互いに含まれているためです。このタスクは、データからオブジェクトを学習するために必要です。見えない物体の移動位置を説明するシステム</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-23">
        <br><font color="black">2020-03-23</font>
      </time>
    </span>
</section>
<!-- paper0: PolyLaneNet: Lane Estimation via Deep Polynomial Regression -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_68.html">
      <font color="black">PolyLaneNet: Lane Estimation via Deep Polynomial Regression</font>
    </a>
  </h2>
  <font color="black">さらに、2つの追加のパブリックデータセットに関する広範な定性的な結果が、レーン検出のための最近の研究で使用された評価指標の制限とともに提示されます。最後に、ソースコードとトレーニング済みモデルを提供し、他のユーザーがこのペーパーに示されているすべての結果を複製できるようにします、これは、最先端のレーン検出メソッドでは驚くほどまれです。提案されたメソッドは、効率（115 FPS）を維持しながら、TuSimpleデータセット内の既存の最先端のメソッドと競合することが示されています。 
[ABSTRACT]この作業では、車線検出の新しい方法を紹介します。車両の前方カメラからの画像を入力として使用します。この方法は、画像内の各車線マーキングを表すチップを出力するために提案されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-23">
        <br><font color="black">2020-04-23</font>
      </time>
    </span>
</section>
<!-- paper0: GIQA: Generated Image Quality Assessment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_69.html">
      <font color="black">GIQA: Generated Image Quality Assessment</font>
    </a>
  </h2>
  <font color="black">さらに、GIQAは、生成モデルのリアリズムと多様性を個別に評価し、GANのトレーニングでオンラインハードネガティブマイニング（OHEM）を有効にして結果を改善するなど、多くのアプリケーションで利用できます。さまざまなデータセットのGANモデルを使用して、それらが人間の評価と一致していることを示します。学習ベースとデータベースの2つの観点から、3つのGIQAアルゴリズムを紹介します。 
[要約]トピックに対していくつかの定量的基準が最近浮上しましたが、それらのどれも定量的画像用に設計されていません。これには、学習ベースとデータベースが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-19">
        <br><font color="black">2020-03-19</font>
      </time>
    </span>
</section>
<!-- paper0: Multiview Detection with Feature Perspective Transformation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_70.html">
      <font color="black">Multiview Detection with Feature Perspective Transformation</font>
    </a>
  </h2>
  <font color="black">コードとMultiviewXデータセットは、https：//github.com/hou-yz/MVDetで入手できます。マルチビューシステムでは、オクルージョンから生じるあいまいさを処理するときに、2つの重要な質問に答える必要があります。複数のビューからの手がかり？ 
[ABSTRACT]マルチビューシステムでは、2つの重要な質問に答える必要があります。オクルージョンによって汚染された信頼できない2Dおよび3D空間情報をどのように集約すればよいですか？</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: DeepMSRF: A novel Deep Multimodal Speaker Recognition framework with
  Feature selection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_71.html">
      <font color="black">DeepMSRF: A novel Deep Multimodal Speaker Recognition framework with
  Feature selection</font>
    </a>
  </h2>
  <font color="black">DeepMSRFは、2つのストリームVGGNETを使用して両方のモダリティでトレーニングし、話者のアイデンティティを正確に認識することができる包括的なモデルに到達します。ビデオストリームで話者を認識するために、重要な調査研究が行われ、高顔の表情、感情、性別などのレベルスピーカーの機能。2つのモダリティ、つまりスピーカーの音声と顔の画像の機能をフィードすることにより、DeepMSRFを実行します。 
[要約] 2つのモダリティの機能をフィードすることにより、deepmsrfを実行します。これには、スピーカーの音声と顔の画像の学習が含まれます。結果は、deepmsrfが少なくとも3％の精度でシングルモダリティのスピーカー認識方法よりも優れていることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Reference-guided Face Component Editing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_72.html">
      <font color="black">Reference-guided Face Component Editing</font>
    </a>
  </h2>
  <font color="black">広範な実験的検証と比較を通じて、提案されたフレームワークの有効性を検証します。フレームワークがターゲットの顔のコンポーネントに集中するのを促すために、例に基づく注意モジュールが、注意機能と抽出されたターゲットの顔コンポーネントの機能を融合するように設計されています参照画像から..制限を解除するには（たとえば、
[ABSTRACT]以前の方法では、顔のコンポーネントの形状を制御できませんでした。これらには、実験的なツールを使用して、注目の特徴と参照画像から抽出されたターゲットの顔のコンポーネントの特徴を融合させることが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-03">
        <br><font color="black">2020-06-03</font>
      </time>
    </span>
</section>
<!-- paper0: Correlation filter tracking with adaptive proposal selection for
  accurate scale estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_73.html">
      <font color="black">Correlation filter tracking with adaptive proposal selection for
  accurate scale estimation</font>
    </a>
  </h2>
  <font color="black">次に、色の類似性に基づく適応戦略を策定して、高品質の提案を選択します。具体的には、まずHSV色空間のカラーヒストグラムを使用してインスタンス（つまり、最初のフレームの初期ターゲットと予測されたフレーム）を表します。前のフレームのターゲット）と提案。提案されたトラッカーの一般化と効率を検証するために、提案された適応提案選択アルゴリズムを粗から細までの深い機能とさらに統合します。 
[要約]多数の冗長な提案があると、提案されたトラッカーのパフォーマンスと速度が低下する可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Point-Set Anchors for Object Detection, Instance Segmentation and Pose
  Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_74.html">
      <font color="black">Point-Set Anchors for Object Detection, Instance Segmentation and Pose
  Estimation</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、この汎用的なアプローチが、これらの各タスクの最先端の方法と競争力のあるパフォーマンスを達成できることを示しています。 、これらの変換をサンプリングして追加のポイントセット候補を生成するアンカーボックス手法を採用します。推論を容易にするために、代わりに、より有利な位置に配置されたポイントのセットから回帰を実行することを提案します。 
[要約]ジェネレーターの中心はシンプルで効率的ですが、中心点で抽出された画像の特徴には、離れたキーポイントを予測するための限られた情報が含まれていると主張します。姿勢推定のトレーニングデータ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-06">
        <br><font color="black">2020-07-06</font>
      </time>
    </span>
</section>
<!-- paper0: Alleviating Over-segmentation Errors by Detecting Action Boundaries -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_75.html">
      <font color="black">Alleviating Over-segmentation Errors by Detecting Action Boundaries</font>
    </a>
  </h2>
  <font color="black">BRBによって予測されたアクション境界により、ASBからの出力が改善され、パフォーマンスが大幅に向上します。私たちの貢献は3つあります。（i）時間アクションセグメンテーションのフレームワークであるASRFは、時間アクションセグメンテーションを分割します。フレームごとのアクション分類とアクション境界回帰に。私たちのコードはまもなく公開されます。 
[ABSTRACT]私たちのモデルアーキテクチャは、長期の特徴抽出と2つのブランチで構成されています。アクションセグメンテーションブランチ（asb）と境界53歳のbrb.asbは、アクションクラスでビデオフレームを分類し、brbはアクション境界確率</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Water level prediction from social media images with a multi-task
  ranking approach -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_76.html">
      <font color="black">Water level prediction from social media images with a multi-task
  ranking approach</font>
    </a>
  </h2>
  <font color="black">回帰とペアワイズランキング損失の両方を使用してモデルをトレーニングするマルチタスク（ディープ）学習アプローチを提案します。注釈付きの水位の小さなセットと弱い水位の大きなセットから予測子を効率的に学習する方法を示します2つの画像のどちらで水位が高く、取得がはるかに簡単であるかを示す注釈。画像ベースの洪水位推定の主なボトルネックはトレーニングデータであるという観察に動機付けられています。これは難しいため、制御されていない画像に正しい水深で注釈を付けるための多大な労力。 
[要約]リアルタイムで（ほぼ）洪水マップを構築するために、洪水イベント中に取得されたソーシャルメディア画像から水深を推定するコンピュータービジョンシステムを導入します。8145のタスクレベルの画像を含む、deepfloodという名前の新しいデータセットを提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Generalized Shortest Path-based Superpixels for Accurate Segmentation of
  Spherical Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_77.html">
      <font color="black">Generalized Shortest Path-based Superpixels for Accurate Segmentation of
  Spherical Images</font>
    </a>
  </h2>
  <font color="black">私たちのアプローチは、球面形状を尊重し、3D球面取得空間上のピクセルとスーパーピクセルの中心間の最短経路の概念を一般化します。この論文では、SphSPS（球面最短経路ベースのスーパーピクセル）。このようなパスの特徴情報をクラスタリングフレームワークに効率的に統合でき、オブジェクトの輪郭の尊重と形状の規則性を共同で改善できることを示しています。 
[ABSTRACT]スーパーピクセルアプローチは、スーパーピクセルアプローチ用に作成されました。これらには、スーパーピクセル、スーパーピクセル、スーパーピクセルが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-15">
        <br><font color="black">2020-04-15</font>
      </time>
    </span>
</section>
<!-- paper0: Hierarchical Dynamic Filtering Network for RGB-D Salient Object
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_78.html">
      <font color="black">Hierarchical Dynamic Filtering Network for RGB-D Salient Object
  Detection</font>
    </a>
  </h2>
  <font color="black">予測に鋭いエッジと一貫した顕著性領域を持たせるために、結果をさらに最適化するためにハイブリッド拡張損失関数を設計します。最後に、一種のより柔軟で効率的なマルチスケールのクロスモーダル機能処理を実装します。すなわち。 6つのメトリックに関して、提案された方法は、8つの挑戦的なベンチマークデータセットで既存の12の方法よりも優れています。 
[要約]提案された方法は、8つの挑戦的なベンチマークデータセットで既存の12の方法よりも優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Neural Networks on Random Graphs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_79.html">
      <font color="black">Neural Networks on Random Graphs</font>
    </a>
  </h2>
  <font color="black">最高のパフォーマンスのネットワークの大部分は、確かにこれらの新しいファミリにありました。古典的な数値グラフの不変量だけでは、最高のネットワークを選別することはできないようなので、一連の準1次元を選択する新しい数値特性を導入しましたグラフは、最もパフォーマンスの高いネットワークの大多数でした。ニューラルネットワークのテストの精度に関連して、グラフのさまざまな構造的および数値的特性を調査しました。 
[要約]ランダムな有向非循環グラフ（dag）を直接生成するための新しい柔軟なアルゴリズムを導入しました。また、グラフをフィードに必要なdagに変換する一般的な手順を提案しました-ニューラルネットワーク</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-19">
        <br><font color="black">2020-02-19</font>
      </time>
    </span>
</section>
<!-- paper0: Spatio-Temporal Event Segmentation and Localization for Wildlife
  Extended Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_80.html">
      <font color="black">Spatio-Temporal Event Segmentation and Localization for Wildlife
  Extended Videos</font>
    </a>
  </h2>
  <font color="black">自己学習された注意マップは、各フレーム内のイベント関連オブジェクトを効果的にローカライズおよび追跡します。提案されたアプローチはラベルを必要としません。予測には、自己監視された方法で訓練された注意メカニズムで強化されたLSTMを使用します予測誤差を使用します。 
[ABSTRACT]提案されたアプローチは、予測メカニズムを使用して自己監視された方法でトレーニングされた、注意メカニズムで強化されたlstmの使用を含みます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-05">
        <br><font color="black">2020-05-05</font>
      </time>
    </span>
</section>
<!-- paper0: Face to Purchase: Predicting Consumer Choices with Structured Facial and
  Behavioral Traits Embedding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_81.html">
      <font color="black">Face to Purchase: Predicting Consumer Choices with Structured Facial and
  Behavioral Traits Embedding</font>
    </a>
  </h2>
  <font color="black">消費者の購買行動を予測することは、eコマースのターゲットを絞った広告と販売促進にとって重要です。しかし、消費者の顔は以前の研究ではほとんど未踏であり、既存の顔関連の研究は性格特性などの高レベルの機能に焦点を当て、顔のデータから学ぶことのビジネス上の重要性。私たちは、階層的埋め込みネットワークに基づく半教師付きモデルを設計して、消費者の高レベルの機能を抽出し、消費者の上位$ N $の購入先を予測します。 
[ABSTRACT]人間の顔は、消費者の性格や行動特性に関する洞察を得るための貴重な情報源です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Automated Synthetic-to-Real Generalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_82.html">
      <font color="black">Automated Synthetic-to-Real Generalization</font>
    </a>
  </h2>
  <font color="black">コードはhttps://github.com/NVlabs/ASG。で入手できます。この作業では、ImageNetの事前トレーニング済みモデルで同様の表現を維持するように合成トレーニング済みモデルを明示的に推奨し、\ textit {learning-to -optimize（L2O）}戦略を使用してレイヤーごとの学習率の選択を自動化します。提案されたフレームワークは、実際のデータを見たりトレーニングしたりせずに、合成から実際の汎化パフォーマンスを大幅に改善できると同時に、そのようなダウンストリームタスクにもメリットがあることを示します。ドメイン適応として。 
[ABSTRACT]提案されたフレームワークは、実際の役割を見たりトレーニングしたりせずに、合成から現実への汎化パフォーマンスを大幅に改善できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Visual Tracking by TridentAlign and Context Embedding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_83.html">
      <font color="black">Visual Tracking by TridentAlign and Context Embedding</font>
    </a>
  </h2>
  <font color="black">複数のベンチマークデータセットで得られた実験結果は、提案されたトラッカーがリアルタイムの速度で実行されている間、提案されたトラッカーのパフォーマンスは最先端のトラッカーのパフォーマンスに匹敵することを示しています。シャムのネットワークベースの視覚追跡方法用のTridentAlignおよびコンテキスト埋め込みモジュール。一方、コンテキスト埋め込みモジュールは、オブジェクト間のグローバルコンテキスト情報を考慮して、ターゲットとディストラクタオブジェクトを区別することを目的としています。 
[要約] tridentalignモジュールは、大規模なスケールバリエーションへの適応性を支援します。ターゲットのフィーチャ表現を複数の空間次元にプールして、フィーチャピラミッドを形成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Training Object Detectors from Few Weakly-Labeled and Many Unlabeled
  Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_84.html">
      <font color="black">Training Object Detectors from Few Weakly-Labeled and Many Unlabeled
  Images</font>
    </a>
  </h2>
  <font color="black">最近の代表的な弱監視付きパイプラインPCLに基づいて、私たちの方法は、より多くのラベルなし画像を使用して、最近の多くの弱監視付き検出ソリューションと同等またはそれ以上のパフォーマンスを実現できます。教師分類子モデルによってラベルなしセットで生成され、ラベル付き画像との領域レベルの類似性によってブートストラップされる疑似ラベル。これは、ラベル付きデータが検出器の学習をブートストラップするのに十分でない半教師付き学習の極端なケースです。 
[要約]私たちの方法は、画像付きの1つまたはいくつかの画像からオブジェクト検出器をトレーニングすることです-レベルラベルとラベルなし画像のより大きなセット</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-01">
        <br><font color="black">2019-12-01</font>
      </time>
    </span>
</section>
<!-- paper0: Patch-wise Attack for Fooling Deep Neural Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_85.html">
      <font color="black">Patch-wise Attack for Fooling Deep Neural Network</font>
    </a>
  </h2>
  <font color="black">人間の知覚できないノイズをクリーンな画像に追加することにより、結果として生じる敵対的な例は他の未知のモデルをだますことができます。これによって動機付けられ、パッチごとの反復アルゴリズムを提案します-通常は訓練された主流モデルと防御モデルへのブラックボックス攻撃ピクセル単位のノイズを操作する既存の攻撃方法から。このように、ホワイトボックス攻撃のパフォーマンスを犠牲にすることなく、敵対的な例は強力な転送能力を持つことができます。 
[要旨]私たちのツールは、一般に、あらゆる勾配ベースのフロー攻撃に統合できます。また、ホワイトボックス攻撃の敵対的なバージョンを作成するために使用することもできます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: OVC-Net: Object-Oriented Video Captioning with Temporal Graph and Detail
  Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_86.html">
      <font color="black">OVC-Net: Object-Oriented Video Captioning with Temporal Graph and Detail
  Enhancement</font>
    </a>
  </h2>
  <font color="black">ビデオベースのオブジェクト指向ビデオキャプションネットワーク（OVC）-Netを時系列グラフと詳細な拡張機能を介して紹介し、時間に沿って活動を効果的に分析し、小さなサンプル条件下で視覚言語接続を安定してキャプチャします。実験結果から、 OVC-Netは、並行オブジェクトを正確に記述する能力を示し、最先端のパフォーマンスを実現します。有効性を実証するために、新しいデータセットで実験を行い、それを最新の状態と比較しますアートビデオのキャプション方法。 
[要約]このホワイトペーパーでは、オブジェクトレベルのオブジェクトと呼ばれる拡張ビデオキャプションのビデオを理解する新しいタスクを提案します。ビジュアルビジュアル機能を使用して、空間からのアクティビティを説明できます特徴</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-08">
        <br><font color="black">2020-03-08</font>
      </time>
    </span>
</section>
<!-- paper0: Relational Deep Feature Learning for Heterogeneous Face Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_87.html">
      <font color="black">Relational Deep Feature Learning for Heterogeneous Face Recognition</font>
    </a>
  </h2>
  <font color="black">顔内パーツ間の各アイデンティティーの関係情報はどのモダリティでも類似しているため、機能間のモデリング関係は、ドメインを越えたマッチングに役立ちます。小さなHFRデータベースの制限を克服するための認識バックボーン。提案された方法は、5つのHFRデータベースで他の最先端の方法よりも優れています。 
[要約]提案された方法は、大規模なビジュアルデータベースに基づいています。これらには、2つの顔の特徴に加えてグローバルな関係情報を抽出するグラフ構造化モジュールが含まれています。これらの特徴は、これらの特徴と同じ色に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-02">
        <br><font color="black">2020-03-02</font>
      </time>
    </span>
</section>
<!-- paper0: Personalized Face Modeling for Improved Face Reconstruction and Motion
  Retargeting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_88.html">
      <font color="black">Personalized Face Modeling for Improved Face Reconstruction and Motion
  Retargeting</font>
    </a>
  </h2>
  <font color="black">具体的には、事前に3DMM上でパーソナライズされた補正を予測することにより、ユーザー固有の式blendshapeと動的（式固有）アルベドマップを学習します。画像ベースの3D顔再構成と顔のモーションリターゲティングの従来の方法は、3Dモーフィブルモデル（3DMM ）モデリング能力が限られていて、野生のデータにうまく一般化できない顔に。さらに、既存の方法は、ユーザーごとに単一のアルベドを学習しますが、これは表情固有の皮膚反射率の変化をキャプチャするには不十分です。 
[ABSTRACT]私たちは、顔の表情を使用するための最新のコンセプトを提案します。ユーザーごとおよびフレームごとの顔のモーションパラメータは、ユーザーの表情の大量の動画に基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Compare and Reweight: Distinctive Image Captioning Using Similar Images
  Sets -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CV/paper_89.html">
      <font color="black">Compare and Reweight: Distinctive Image Captioning Using Similar Images
  Sets</font>
    </a>
  </h2>
  <font color="black">私たちの測定基準は、各画像の人間の注釈が識別性に基づいて同等ではないことを示しています。したがって、加重損失関数でのCIDErBtwの使用に基づいて、または各画像の生成されたキャプションの識別性を促進するために、いくつかの新しいトレーニング戦略を提案します。強化学習報酬..幅広い画像キャプションモデルが開発され、BLEU、CIDEr、SPICEなどの一般的なメトリックに基づいて大幅な改善を実現しました。 
[ABSTRACT]生成されたキャプションは画像を正確に説明できますが、類似した画像の総称であり、識別性に欠けています。これは、各画像の一意性を適切に説明できないことを意味します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature
  and PRESupposition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_0.html">
      <font color="black">Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature
  and PRESupposition</font>
    </a>
  </h2>
  <font color="black">MultiNLIにはこれらの推論タイプを示す非常に少数のペアが含まれているように見えますが、BERTが実用的な推論を描くことを学習することがわかります。十分に研究された実用的な推論を示す&gt; 25kの半自動生成された文のペアで構成される、IMPlicatureおよびPRESupposition診断データセット（IMPPRES）を作成します。タイプ..「のみ」のような一部の前提トリガーの場合、BERTは、トリガーが否定のような含意取り消し演算子の下に埋め込まれている場合でも、含意として前提を確実に認識します。 
[ABSTRACT] imppresを使用して、bert、infersent、およびbow nliモデルが実用的な説明を学習するかどうかを評価します。ただし、含意形式の機能はまだ十分に研究されていません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-07">
        <br><font color="black">2020-04-07</font>
      </time>
    </span>
</section>
<!-- paper0: COVID-19 Twitter Dataset with Latent Topics, Sentiments and Emotions
  Attributes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_1.html">
      <font color="black">COVID-19 Twitter Dataset with Latent Topics, Sentiments and Emotions
  Attributes</font>
    </a>
  </h2>
  <font color="black">データセットの使用方法を説明するために、トピック、感情、感情の属性とそれらの時間的分布に関する記述統計を提示し、コミュニケーション、心理学、公衆衛生、経済学、疫学での可能なアプリケーションについて説明します。このリソースペーパーでは、大規模なデータセットについて説明します。 2020年1月28日から7月1日までの間に1,300万人を超えるユニークユーザーからの6,300万件を超えるコロナウイルス関連のTwitter投稿をカバーしています。ツイートには強い懸念と感情が表れているため、自然言語処理技術と機械学習を使用してツイートの内容を分析しましたベースのアルゴリズム、および各ツイートに関連付けられた推定された17の潜在的意味属性（1）10の検出されたトピックに対するツイートの関連性を示す10の属性、2）価数（すなわち、不快/快感）および感情の強さの程度を示す5つの量的属性恐怖、怒り、悲しみと喜びの4つの主要な感情にまたがる強度、および3）感情カテゴリと最も支配的な感情カテゴリをそれぞれ示す2つの定性的属性。 
[要約]自然言語処理技術と機械学習ベースのアルゴリズムを使用してツイートの内容を分析しました。各ツイートに関連付けられた17の潜在的意味属性を推定しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Our Evaluation Metric Needs an Update to Encourage Generalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_2.html">
      <font color="black">Our Evaluation Metric Needs an Update to Encourage Generalization</font>
    </a>
  </h2>
  <font color="black">モデルパフォーマンスのインフレを停止し、AIシステムの機能を過大評価するのを防ぐために、評価中に一般化を促進するシンプルで新しい評価指標であるWOODスコアを提案します。最近の研究では、モデルが偽のバイアスに適合していることが示されています。人間などの一般化可能な機能を学習する代わりに、データセットを「ハック」します。いくつかの一般的なベンチマークで人間のパフォーマンスを超えるモデルは、Out of Distribution（OOD）データへの露出でパフォーマンスの大幅な低下を示します。 
[ABSTRACT]モデルは、人間のような一般化可能な機能を学習する代わりに、偽の劣化と「ハッキング」データセットに適合している</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological
  Inflection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_3.html">
      <font color="black">SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological
  Inflection</font>
    </a>
  </h2>
  <font color="black">受賞した4つのシステムはすべてニューラルでした（2つの単一言語トランスフォーマーと2つの大規模な多言語RNNベースのモデル（ゲート付き注意））。システムは、45の言語と5つの言語ファミリのデータを使用して開発され、追加の45の言語と10のデータで微調整されました言語ファミリ（合計13）、および90のすべての言語で評価されました。10チームからの合計22のシステム（19ニューラル）がタスクに提出されました。 
[ABSTRACT]システムは45言語と5言語ファミリのデータを使用して開発されました。追加の45言語、10言語ファミリのデータで微調整され、すべての90言語で評価されました。4つの勝利システムはすべてニューラル、2つの単一言語トランスフォーマーでした。と2つの大規模な多言語rnn-注目を集めるベースのモデル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-20">
        <br><font color="black">2020-06-20</font>
      </time>
    </span>
</section>
<!-- paper0: Calling Out Bluff: Attacking the Robustness of Automatic Scoring Systems
  with Simple Adversarial Testing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_4.html">
      <font color="black">Calling Out Bluff: Attacking the Robustness of Automatic Scoring Systems
  with Simple Adversarial Testing</font>
    </a>
  </h2>
  <font color="black">二次加重カッパ（QWK）などの標準的なパフォーマンスメトリックによって一般的に測定されるパフォーマンス、および正確さは同じことを指します。試験中の一般的な学生の行動に触発され、AESシステムの自然言語理解をテストするためのタスクにとらわれない敵対的評価スキームを提案します機能と全体的な堅牢性..過去20年間に、ディープラーニングベースの自動エッセイ採点（AES）システムで大きな進歩がありました。 
[ABSTRACT] aesシステムは、自然言語理解能力と全体的な堅牢性をテストします。プロジェクトは、試験中の一般的な学生の行動に触発されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Structure-Invariant Testing for Machine Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_5.html">
      <font color="black">Structure-Invariant Testing for Machine Translation</font>
    </a>
  </h2>
  <font color="black">具体的には、SIT（1）は、指定された文の1つの単語を、意味的に類似し、構文的に同等の単語で置き換えることにより、類似したソース文を生成します。 （2）構文解析ツリー（支持性または依存関係の解析を介して取得）によって文の構造を表します。 （3）構造が一定のしきい値を超えて量的に異なる文のペアを報告します。この課題に取り組むために、機械翻訳ソフトウェアを検証するための新しい変成テストアプローチである構造不変テスト（SIT）を導入します。機械翻訳システムの堅牢性は非常に困難であるため、十分に検討されていません。 
[ABSTRACT]システムは、誤解、誤診、個人の安全に対する脅威、または政治的対立につながる劣った結果を返す可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-07-19">
        <br><font color="black">2019-07-19</font>
      </time>
    </span>
</section>
<!-- paper0: Can neural networks acquire a structural bias from raw linguistic data? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_6.html">
      <font color="black">Can neural networks acquire a structural bias from raw linguistic data?</font>
    </a>
  </h2>
  <font color="black">この結論が正しければ、生来のバイアスなしに学習者が一部の言語の普遍性を獲得できるという暫定的な証拠です。これらの結果は、構造的バイアスを生から獲得できるという命題を支持する人工学習者からこれまでで最も強い証拠であると主張しますデータ。しかし、人間はBERTよりもはるかに少ないデータから言語を学習するため、人間の言語習得の正確な意味は不明です。 
[ABSTRACT]私たちは4つの実験を実施して、構造v-線形一般化-異なる構造に依存する現象に基づいてその好みをテストしています。これらの結果は、人工学習者からこれまでで最も強力な証拠であると主張しています。ただし、人間に対する正確な影響言語習得が不明確</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Sudo rm -rf: Efficient Networks for Universal Audio Source Separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_7.html">
      <font color="black">Sudo rm -rf: Efficient Networks for Universal Audio Source Separation</font>
    </a>
  </h2>
  <font color="black">このようにして、限られた数の浮動小数点演算、メモリ要件、パラメーター数、およびレイテンシで高品質の音源分離を実現できます。音声と環境音の両方の分離データセットに対する実験により、SuDoRMRFは同等以上のパフォーマンスを発揮します。非常に高い計算リソース要件を備えたさまざまな最先端のアプローチ。具体的には、このたたみ込みネットワークのバックボーン構造は、多重解像度機能（SuDoRMRF）の連続的なダウンサンプリングと再サンプリング、および単純な一次元畳み込み。 
[ABSTRACT] sudormrfは同等のパフォーマンスを発揮し、さまざまな状態を上回ります-リソース要件が大幅に高い最先端のアプローチ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Phylogenetic signal in phonotactics -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_8.html">
      <font color="black">Phylogenetic signal in phonotactics</font>
    </a>
  </h2>
  <font color="black">111のパマニュンガン語彙から音韻データを抽出し、系統発生信号のテストを適用して、データが系統発生履歴を反映する度合いを定量化します。系統発生法は、言語推論を超えて言語学において幅広い可能性を持っています。系統発生信号は、きめの細かい頻度で大きくなります。バイナリデータよりもデータが多く、自然クラスベースのデータが最も優れています。 
[ABSTRACT]たとえばバイナリデータを含む3つのデータセットをテストしますが、すべてのデータセットで進化のシグナルを検出します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br><font color="black">2020-02-03</font>
      </time>
    </span>
</section>
<!-- paper0: An Empirical Study on Robustness to Spurious Correlations using
  Pre-trained Language Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_9.html">
      <font color="black">An Empirical Study on Robustness to Spurious Correlations using
  Pre-trained Language Models</font>
    </a>
  </h2>
  <font color="black">自然言語の推論と言い換えの識別に関する実験では、適切な補助タスクを使用したMTLは、分布内のパフォーマンスを損なうことなく、挑戦的な例のパフォーマンスを大幅に向上させることを示しています。さらに、MTLからの利益は、主にマイノリティの例からの一般化の向上から得られることを示しています。 ..私たちの結果は、偽の相関を克服するためのデータ多様性の重要性を強調しています。 
[ABSTRACT]成功の鍵は、偽の相関関係が保持されない少量の反例からの一般化です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Extracting Structured Data from Physician-Patient Conversations By
  Predicting Noteworthy Utterances -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_10.html">
      <font color="black">Extracting Structured Data from Physician-Patient Conversations By
  Predicting Noteworthy Utterances</font>
    </a>
  </h2>
  <font color="black">方法論上の課題の1つは、会話が長い（約1500ワード）ため、最新のディープラーニングモデルで入力として使用するのが難しいことです。この課題に対処するために、注目に値する発話を抽出します---会話の可能性が高い部分この探索的研究では、会話の筆記録、訪問後の要約、対応する裏付けとなる証拠（筆記録）、および構造化ラベルで構成される新しいデータセットについて説明します。 
[ABSTRACT]このデータを使用して、医師が電子カルテの訪問後のドキュメントを作成するのに役立つ可能性のある情報を抽出し、事務的な負担を軽減する可能性があります。臓器システムのレビューで関連する診断と異常を認識するタスクに焦点を当てます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: HuggingFace's Transformers: State-of-the-art Natural Language Processing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_11.html">
      <font color="black">HuggingFace's Transformers: State-of-the-art Natural Language Processing</font>
    </a>
  </h2>
  <font color="black">ライブラリは\ url {https://github.com/huggingface/transformers}で入手できます。このライブラリの裏付けは、コミュニティによって作成され、コミュニティで利用できる事前トレーニング済みモデルのキュレーションされたコレクションです。\ textit {Transformers}は、研究者によって拡張可能であり、開業医にとってシンプルであり、産業展開で高速かつ堅牢です。 
[要約]ライブラリは、慎重に設計された最先端のトランスアーキテクチャで構成され、統合されたAPIの下にあります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-09">
        <br><font color="black">2019-10-09</font>
      </time>
    </span>
</section>
<!-- paper0: Questionnaire analysis to define the most suitable survey for port-noise
  investigation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_12.html">
      <font color="black">Questionnaire analysis to define the most suitable survey for port-noise
  investigation</font>
    </a>
  </h2>
  <font color="black">この調査の結果は、個人の感情と技術的側面との関連を理解するために、音響モニタリングと組み合わせて行われる言語調査の出発点になります。最初のデータ収集キャンペーンの予備結果は、調査の数、質問の種類、サンプルノイズの種類の妥当性。バックポートエリアの騒音にさらされている住民に提出するアンケートの準備による調査キャンペーンは、理解を深めるのに役立ちます。主観的な視点。 
[ABSTRACT]アンケートは、triploプロジェクトで配布されるように最適化されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Emergent Multi-Agent Communication in the Deep Learning Era -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_13.html">
      <font color="black">Emergent Multi-Agent Communication in the Deep Learning Era</font>
    </a>
  </h2>
  <font color="black">この記事では、これら2つの角度の両方からの代表的な最近の言語出現研究について概説します。言語を通じて協力する能力は人間の決定的な特徴です。科学的な観点から、言語が深いエージェントのコミュニティで進化する条件とその出現を理解します機能は人間の言語の進化に光を当てることができます。 
[ABSTRACT]研究者は、人間と対話するための共有言語を開発できるかどうかを調査しています。深い人工ネットワークは、相互に通信することでインタラクティブに問題を解決できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-03">
        <br><font color="black">2020-06-03</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Entity Matching with Pre-Trained Language Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_14.html">
      <font color="black">Deep Entity Matching with Pre-Trained Language Models</font>
    </a>
  </h2>
  <font color="black">Dittoでは、マッチングの決定を行う際に重要となる可能性のある重要な入力情報を強調表示することで、ドメインの知識を注入できます。また、Dittoのマッチング機能をさらに向上させる3つの最適化手法を開発しました。789Kおよび412Kレコードで構成される2つの会社のデータセットのマッチングについて、同上は96.5％の高いF1スコアを達成しています。 
[ABSTRACT] dittoは、少なくとも半分の数のラベル付きデータで以前のsotaの結果を達成できます。モデルのマッチング機能を改善するために「より難しい」ことを学ぶのは難しいと彼は言います</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-01">
        <br><font color="black">2020-04-01</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Transformer based Data Augmentation with Subword Units for
  Morphologically Rich Online ASR -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_15.html">
      <font color="black">Deep Transformer based Data Augmentation with Subword Units for
  Morphologically Rich Online ASR</font>
    </a>
  </h2>
  <font color="black">この方法により、語彙のサイズとメモリ要件を大幅に削減しながらWERを大幅に削減できることを示します。Transformerで生成されたテキストを使用したデータ拡張は、言語の分離には効果的ですが、形態学的に豊富な言語で語彙が急増することを示しています。 、サブワードベースのニューラルテキストの拡張が、WER全体だけでなくOOVワードの認識においても、ワードベースのアプローチよりも優れていることも示しています。 
[ABSTRACT]サブワードベースのニューラルテキスト拡張は新しい方法です。生成されたテキストを統計的に導出されたサブワードに減らします。また、全体的なwerの観点から、ワードベースのアプローチよりも優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Reasoning Strategies in End-to-End Differentiable Proving -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_16.html">
      <font color="black">Learning Reasoning Strategies in End-to-End Differentiable Proving</font>
    </a>
  </h2>
  <font color="black">CTPはスケーラブルであり、CLUTRRデータセットに最先端の結果をもたらすことを示します。これは、小さいグラフを推論し、大きいグラフを評価することで神経モデルの体系的な一般化をテストします。ディープラーニングモデルを解釈可能にしようとする試み、データ効率が高く、堅牢な、たとえばニューラル定理証明（NTP）などのルールベースのシステムとのハイブリッド化により、ある程度の成功が見られました。すべてのソースコードとデータセットは、https：//github.com/uclnlp/からオンラインで入手できます。 ctp。 
[ABSTRACT]これらのニューロシンボリックモデルは、解釈可能なルールを誘導し、バックコミュニケーションを介してデータから表現を学習することができます。これらのモデルは、予測について説明する一方で、予測について説明するように制限されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: What's in a Name? Are BERT Named Entity Representations just as Good for
  any other Name? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_17.html">
      <font color="black">What's in a Name? Are BERT Named Entity Representations just as Good for
  any other Name?</font>
    </a>
  </h2>
  <font color="black">脆弱性は、最近のエンティティ対応のBERTモデルでも継続します。次に、型注釈とラベル予測の不確実性を共同でモデル化しながら、複数の置換からの予測をアンサンブルする簡単な方法を提供します。3つのNLPタスクの実験は、この方法が堅牢性を強化することを示していますまた、自然データセットと敵対データセットの両方の精度が向上します。 
[要約]いくつかのタスクでは、そのような摂動は自然であり、最新のトレーニング済みモデルは驚くほど脆いことを強調します。また、トークン化や発生頻度などの要因を考慮して、この非堅牢性の原因を特定しようとします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Language, communication and society: a gender based linguistics analysis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_18.html">
      <font color="black">Language, communication and society: a gender based linguistics analysis</font>
    </a>
  </h2>
  <font color="black">さらに、与えられた結果は、ジェンダーのステレオタイプ、およびそれらが生み出す期待がペナルティまたは不平等につながる可能性があるかどうかを理解するための良い出発点になる可能性があります。行動特性..この研究の目的は、言語は私たちの思考、私たちの偏見、文化的ステレオタイプの鏡であるという仮説を裏付ける証拠を見つけることです。 
[要約] 537人にアンケートが実施され、537に与えられました。目的は、もしあれば、現代社会における男性と女性の役割を定義する際に明らかになるステレオタイプのイメージを特定することでした</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Modeling Voting for System Combination in Machine Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_19.html">
      <font color="black">Modeling Voting for System Combination in Machine Translation</font>
    </a>
  </h2>
  <font color="black">システムの組み合わせは、さまざまな機械翻訳システムの仮説を組み合わせて翻訳パフォーマンスを向上させるための重要な手法です。基本的な考え方は、さまざまなシステムの仮説に含まれる単語が、代表的で生成プロセスに関与すべき単語に投票できるようにすることです。この問題は、最近、マルチソースのシーケンス間モデルのエンドツーエンドのトレーニングによって緩和されましたが、これらのニューラルモデルは仮説間の関係を明示的に分析せず、同意の捕捉に失敗します。仮説は、単語が複数の仮説で発生する可能性があるという事実を無視して、独立して計算されます。 
[要約]システムシステムシステムの組み合わせは、各有権者の影響と各候補者に対するその選好を定量化することで実行できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Linear Time Neural Machine Translation with Capsule Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_20.html">
      <font color="black">Towards Linear Time Neural Machine Translation with Capsule Networks</font>
    </a>
  </h2>
  <font color="black">\ textsc {CapsNMT}には2つのコアプロパティがあります。シーケンスの長さに対して線形である時間内に実行され、ソースセンテンスの部分全体の情報を選択、表現、および集計するより柔軟な方法を提供します。ドイツ語のタスクとより大きなWMT14英語-フランス語のタスクである\ textsc {CapsNMT}は、最先端のNMTシステムと同等の結果を達成します。前の作品とは異なり、\ cite {sutskever2014sequence}は、パッシブとボトムアップ方式では、動的ルーティングポリシーは、ソースセンテンスを反復プロセスでエンコードして、下位層と上位層のノード間のクレジット属性を決定します。 
[ABSTRACT] `ソース文をあらかじめ決められたサイズの行列にマッピングするための少しの軌道メカニズムがあり、ディープlstmネットワークを適用して、ソース表現からターゲットシーケンスをデコードします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-11-01">
        <br><font color="black">2018-11-01</font>
      </time>
    </span>
</section>
<!-- paper0: Contextualized Code Representation Learning for Commit Message
  Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_21.html">
      <font color="black">Contextualized Code Representation Learning for Commit Message
  Generation</font>
    </a>
  </h2>
  <font color="black">この論文では、コミットメッセージ生成（CoreGen）のための新しいContextualizedコード表現学習方法を提案します。具体的には、既存の研究はコードトークンに静的埋め込みを採用しており、コンテキストに関係なく同じベクトルにトークンをマッピングします。さらに、また、低リソース設定へのソリューションとして、より大きなコードコーパスでコンテキスト化されたコード表現をトレーニングし、事前トレーニングされたコード表現を他のダウンストリームのコードからテキスト生成タスクに適応させる将来の機会を強調します。 
[要約]ソースコードと自然言語の間のセマンティックギャップは、タスクに大きな課題をもたらします。新しい研究では、コードトークンに静的埋め込みを採用しており、コンテキストに関係なく、トークンを同じ視覚的メッセージにマッピングします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Compare and Reweight: Distinctive Image Captioning Using Similar Images
  Sets -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/cs.CL/paper_22.html">
      <font color="black">Compare and Reweight: Distinctive Image Captioning Using Similar Images
  Sets</font>
    </a>
  </h2>
  <font color="black">最後に、広範な実験が行われ、提案されたアプローチにより、さまざまな画像キャプションベースラインの識別性（CIDErBtwと取得メトリックによって測定される）と精度（たとえば、CIDErによって測定される）の両方が大幅に改善されることが示されています。各画像の人間による注釈は、特徴に基づいて同等ではありません。幅広い画像キャプションモデルが開発され、BLEU、CIDEr、SPICEなどの一般的なメトリックに基づいて大幅な改善を実現しました。 
[ABSTRACT]生成されたキャプションは画像を正確に説明できますが、類似した画像の総称であり、識別性に欠けています。これは、各画像の一意性を適切に説明できないことを意味します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: A Deep Learning Approach for Low-Latency Packet Loss Concealment of
  Audio Signals in Networked Music Performance Applications -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.AS/paper_0.html">
      <font color="black">A Deep Learning Approach for Low-Latency Packet Loss Concealment of
  Audio Signals in Networked Music Performance Applications</font>
    </a>
  </h2>
  <font color="black">この記事では、ディープラーニングアプローチを使用して、失われたパケットコンテンツをリアルタイムで予測する手法について説明します。リアルタイムでエラーを隠す機能は、パケット損失によって引き起こされるオーディオ障害を軽減するのに役立つため、現実世界でのオーディオ再生の品質を向上できます。シナリオ..接続が少なく信頼性が低いため、UDPを介して送信されたオーディオパケットは転送中に失われ、再送信されないため、レシーバーのオーディオ再生でグリッチが発生します。 
[ABSTRACT]エラーをリアルタイムで隠す機能は、パケット損失によって引き起こされる損傷を軽減するのに役立ちます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Defense for Black-box Attacks on Anti-spoofing Models by Self-Supervised
  Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.AS/paper_1.html">
      <font color="black">Defense for Black-box Attacks on Anti-spoofing Models by Self-Supervised
  Learning</font>
    </a>
  </h2>
  <font color="black">ASVspoof 2019データセットの実験結果は、Mockingjayによって抽出された高レベルの表現が敵対的な例の転送可能性を防ぎ、ブラックボックス攻撃にうまく対処できることを示しています。元のデータと区別がつかないが、誤った予測をもたらす敵対的な攻撃は、アンチスプーフィングモデルでは危険であり、論争中ではありません。コストをかけずに検出する必要があります。この作業では、敵対的攻撃に対する防御にそれらを使用して、自己監視学習高レベル表現の堅牢性を調査します。 
[ABSTRACT] anti-スプーフィングモデルは敵対的な攻撃に対して脆弱です。これは、ノイズの影響を検出できないためです。これは敵対的な攻撃に対抗するために使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-05">
        <br><font color="black">2020-06-05</font>
      </time>
    </span>
</section>
<!-- paper0: ASVspoof 2019: A large-scale public database of synthesized, converted
  and replayed speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.AS/paper_2.html">
      <font color="black">ASVspoof 2019: A large-scale public database of synthesized, converted
  and replayed speech</font>
    </a>
  </h2>
  <font color="black">物理アクセス（PA）シナリオ内でのリプレイスプーフィング攻撃は、慎重に制御されたシミュレーションを通じて生成され、以前よりもはるかに明確な分析をサポートします。また、2019年版の新機能として、タンデム検出コスト関数メトリックの使用があり、これは固定ASVシステムの信頼性に関するスプーフィングと対策。また、論理アクセスにおけるスプーフィングされたデータに対する人間の評価についても説明します。 
[ABSTRACT] asvはスプーフィングに対して脆弱であり、「プレゼンテーション攻撃」としても知られています。asvシステムは、再生、音声合成、および音声変換攻撃に対しても脆弱です。これらのシステムは、2つの特定のユースケースシナリオで探索されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-05">
        <br><font color="black">2019-11-05</font>
      </time>
    </span>
</section>
<!-- paper0: Sudo rm -rf: Efficient Networks for Universal Audio Source Separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.AS/paper_3.html">
      <font color="black">Sudo rm -rf: Efficient Networks for Universal Audio Source Separation</font>
    </a>
  </h2>
  <font color="black">このようにして、限られた数の浮動小数点演算、メモリ要件、パラメーター数、およびレイテンシで高品質の音源分離を実現できます。音声と環境音の両方の分離データセットに対する実験により、SuDoRMRFは同等以上のパフォーマンスを発揮します。非常に高い計算リソース要件を備えたさまざまな最先端のアプローチ。具体的には、このたたみ込みネットワークのバックボーン構造は、多重解像度機能（SuDoRMRF）の連続的なダウンサンプリングと再サンプリング、および単純な一次元畳み込み。 
[ABSTRACT] sudormrfは同等のパフォーマンスを発揮し、さまざまな状態を上回ります-リソース要件が大幅に高い最先端のアプローチ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Transformer based Data Augmentation with Subword Units for
  Morphologically Rich Online ASR -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-15/eess.AS/paper_4.html">
      <font color="black">Deep Transformer based Data Augmentation with Subword Units for
  Morphologically Rich Online ASR</font>
    </a>
  </h2>
  <font color="black">Transformerで生成されたテキストを使用したデータ拡張は、言語の分離には効果的ですが、形態学的にリッチな言語で語彙が爆発することを示しています。したがって、サブテキストベースのニューラルテキスト拡張と呼ばれる新しい方法を提案します。この方法により、語彙サイズとメモリ要件を大幅に削減しながら、WERを大幅に削減できることを示しています。 
[ABSTRACT]サブワードベースのニューラルテキスト拡張は新しい方法です。生成されたテキストを統計的に導出されたサブワードに減らします。また、全体的なwerの観点から、ワードベースのアプローチよりも優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
