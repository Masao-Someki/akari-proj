<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-09-10の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Cross-domain Adaptation with Discrepancy Minimization for
  Text-independent Forensic Speaker Verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.SD/paper_0.html">
      <font color="black">Cross-domain Adaptation with Discrepancy Minimization for
  Text-independent Forensic Speaker Verification</font>
    </a>
  </h2>
  <font color="black">この調査では、複数の音響環境で収集されたCRSSフォレンジックオーディオデータセットを紹介します。グラウンドトゥルーススピーカーのアイデンティティを持つ実際の自然主義的なフォレンジックオーディオコーパスの欠如は、この分野の大きな課題を表しています。位置/シナリオの不確実性と、リファレンスと自然界のフィールドレコーディングの間の多様性の不一致による課題。 
[ABSTRACT]複数の音響環境に対するクロスドメインスピーカーの検証は、この分野の大きな課題です。クロス-ドメインスピーカーの識別は、オーディオフォレンジックの研究における課題です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-05">
        <br><font color="black">2020-09-05</font>
      </time>
    </span>
</section>
<!-- paper0: Active Learning for Sound Event Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.SD/paper_1.html">
      <font color="black">Active Learning for Sound Event Detection</font>
    </a>
  </h2>
  <font color="black">SEDモデルのトレーニング中、記録はトレーニング入力として使用され、注釈付きセグメントの長期的なコンテキストが維持されます。提案されたシステムは、評価に使用される2つのデータセット（TUT Rare Sound 2017およびTAU Spatial Sound 2019）の参照方法よりも明らかに優れています。 ..驚くべきことに、ターゲットサウンドイベントがまれであるデータセットでは、必要な注釈の労力を大幅に削減できます。トレーニングデータの2％のみに注釈を付けることにより、達成されるSEDパフォーマンスはすべてのトレーニングデータに注釈を付けるのと同じです。 
[要約]提案されたシステムは、限られた注釈作業で学習済みsedモデルの精度を最大化することを目的としています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br><font color="black">2020-02-12</font>
      </time>
    </span>
</section>
<!-- paper0: 1-Dimensional polynomial neural networks for audio signal related
  problems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.SD/paper_2.html">
      <font color="black">1-Dimensional polynomial neural networks for audio signal related
  problems</font>
    </a>
  </h2>
  <font color="black">この非線形性により、計算の複雑さが増しますが、オーディオ信号に関連するさまざまな分類および回帰問題で同じ数のトレーニングパラメーターを持つ通常の1DCNNよりもモデルがより良い結果をもたらすことができることを示しています。非線形性が減少したため、ソリューションの検索スペースが制限されました。実験は3つの公開データセットに対して行われ、提案されたモデルは、取り組む回帰問題で1DCNNよりもはるかに高速な収束を実現できることを示しています。 
[ABSTRACT]モデルが非パーツモデルに提案されたのはこれが初めてです。これを使用して、ディープモデルやワイドモデルの必要性を減らすことができます。ただし、コンパクトトポスは常にディープモデルよりも優先されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Exploiting Multi-Modal Features From Pre-trained Networks for
  Alzheimer's Dementia Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.SD/paper_3.html">
      <font color="black">Exploiting Multi-Modal Features From Pre-trained Networks for
  Alzheimer's Dementia Recognition</font>
    </a>
  </h2>
  <font color="black">課題は、音響データとテキストデータを提供することで、アルツハイマー型認知症の疑いのある患者を識別することです。テスト結果はベースラインの精度を18.75％超えており、回帰タスクの検証結果では、4つのクラスの認知障害を78.70％..マルチモーダル機能を使用して、畳み込みリカレントニューラルネットワークベースの構造を変更し、分類タスクと回帰タスクを同時に実行し、可変長の会話を計算できます。 
[要約]ディープラーニングモデルは、ウィキペディアやウィキペディアなどの簡単に収集できる大規模なデータセットでトレーニングされます。事前トレーニング済みのネットワークから抽出されたさまざまなマルチモーダル機能を利用して、ニューラルネットワークを使用してアルツハイマー型認知症を認識します。これらの機能には、興味深い洞察が含まれますアルツハイマー病と学習へ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: A multi-view approach for Mandarin non-native mispronunciation
  verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.SD/paper_4.html">
      <font color="black">A multi-view approach for Mandarin non-native mispronunciation
  verification</font>
    </a>
  </h2>
  <font color="black">音響埋め込み間の距離は、電話間の類似性と見なされます。したがって、発音ミスの電話の例では、標準的な発音との類似性スコアが小さいと予想されます。このアプローチは、GOPベースのアプローチよりも+ 11.23％とシングル発音間違いの検証タスクの診断精度でアプローチを+ 1.47％で表示します。 
[要約]差別的な特徴表現を組み込むためにマルチビューアプローチが提案されています。これらのモデルは、マンダリンの非ネイティブの誤発音検証に必要なアノテーションが少なくなります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-05">
        <br><font color="black">2020-09-05</font>
      </time>
    </span>
</section>
<!-- paper0: Multiple F0 Estimation in Vocal Ensembles using Convolutional Neural
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.SD/paper_5.html">
      <font color="black">Multiple F0 Estimation in Vocal Ensembles using Convolutional Neural
  Networks</font>
    </a>
  </h2>
  <font color="black">この作業は、追加のリバーブを含むレコーディングを含む、さまざまなシナリオとデータ構成でこのタスクのCNNのセットを提案および評価します。トレーニングのために、F0アノテーションを含むボーカル四重奏の複数のマルチトラックデータセットで構成されるデータセットを構築します。今後の研究の方向性についての議論。 
[要約]この作業では、このタスクの一連のCNNを提案および評価します。これらには、リバーブを追加した録音が含まれます。今後のピッチ顕著性についての議論で締めくくります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: VoiceFilter-Lite: Streaming Targeted Voice Separation for On-Device
  Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.SD/paper_6.html">
      <font color="black">VoiceFilter-Lite: Streaming Targeted Voice Separation for On-Device
  Speech Recognition</font>
    </a>
  </h2>
  <font color="black">また、そのようなモデルが8ビット整数モデルとして量子化され、リアルタイムで実行できることも示します。新しい非対称損失の使用や、適応型実行時抑制強度の採用など、これらの多面的な要件を満たす新しい手法を提案します。ストリーミング音声認識システムの一部として、ターゲットユーザーからの音声信号のみを保持するためにデバイスで実行される単一チャネルの音源分離モデルであるVoiceFilter-Liteを導入します。 
[ABSTRACT]新しいモデルを使用して、ブロードキャスト可能なモデルを作成できます。入力信号が重複した音声で構成されている場合のパフォーマンスが向上し、他のすべての音響条件下での音声認識パフォーマンスを損なうことはありません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-modal Attention for Speech Emotion Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.SD/paper_7.html">
      <font color="black">Multi-modal Attention for Speech Emotion Recognition</font>
    </a>
  </h2>
  <font color="black">提案されたハイブリッドネットワークMMANは、感情認識のためのIEMOCAPデータベースで最先端のパフォーマンスを実現します。感情は、音声韻律で表される人間の音声の重要な側面を表します。cLSTM-MMAは、他のユニモーダルサブ核融合後期のネットワーク。 
[要約]音声感情認識は、視覚的および文学的な手がかりから大幅にメリットを得ます。clstm-mmaだけでも、他の融合方法と同じくらい競争力があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Real-time Plant Health Assessment Via Implementing Cloud-based Scalable
  Transfer Learning On AWS DeepLens -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.IV/paper_0.html">
      <font color="black">Real-time Plant Health Assessment Via Implementing Cloud-based Scalable
  Transfer Learning On AWS DeepLens</font>
    </a>
  </h2>
  <font color="black">クラウド統合は、スケーラビリティと私たちのアプローチへのユビキタスアクセスを提供します。農業部門では、植物の葉の病気の制御は、あらゆる国の経済に影響を与える植物種の品質と生産に影響を与えるため、重要です。経済的損失を減らし、特定の種を保護するためには、初期段階での植物の葉病の分類が不可欠です。 
[要約]提案されたディープレンズ分類および検出モデル（dcdm）アプローチは、限られた制限に対処します。拡張可能な転送を介して、果物（リンゴ、ブドウ、桃、イチゴ）および野菜（ジャガイモ、トマト）における葉の病気の自動検出および分類を提供します学習する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Revealing Lung Affections from CTs. A Comparative Analysis of Various
  Deep Learning Approaches for Dealing with Volumetric Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.IV/paper_1.html">
      <font color="black">Revealing Lung Affections from CTs. A Comparative Analysis of Various
  Deep Learning Approaches for Dealing with Volumetric Data</font>
    </a>
  </h2>
  <font color="black">この論文は、ImageClef 2020結核タスクのコンテキストで、肺CTの結核関連病変を自動的に検出するためのいくつかのディープラーニングアプローチを提示し、比較分析します。報告された作業は、競争で最高の結果を得たSenticLab.UAICチームに属しています。 ..これらすべてには、さまざまなニューラルネットワークアーキテクチャ、さまざまなセグメンテーションアルゴリズム、およびデータ拡張スキームを含む豊富な実験的分析が付属しています。 
[要約] 3種類の方法は、ボリュームデータが脳ネットワークへの入力としてどのように与えられるかに関して異なります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: CS-AF: A Cost-sensitive Multi-classifier Active Fusion Framework for
  Skin Lesion Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.IV/paper_2.html">
      <font color="black">CS-AF: A Cost-sensitive Multi-classifier Active Fusion Framework for
  Skin Lesion Classification</font>
    </a>
  </h2>
  <font color="black">私たちの実験結果は、私たちのフレームワークが常に静的核融合の競合他社をしのいでいることを示しています。少ないデータ（つまり、データ量の観点から）からサンプルを正確に特定することがより重要ですが、より重要な少数派クラス（特定の癌病変など） ..通常、皮膚病変データセットは制限され、統計的にバイアスされているため、効果的な融合アプローチを設計する際、トレーニング/検証データセットでの各分類子のパフォーマンスだけでなく、相対的な識別力（信頼性など）も考慮することが重要です）アクティブなフュージョンアプローチを必要とするテスト段階での個々のサンプルに関する各分類子の。 
[ABSTRACT]複数のcnn分類子の結果はより効果的でロバストです。これらは、1つのcnn分類子と比較して、単一のcnn分類子と比較されます。この、isic研究データセット用に96のアクティブな分類子を準備しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-25">
        <br><font color="black">2020-04-25</font>
      </time>
    </span>
</section>
<!-- paper0: Single Image Super-Resolution for Domain-Specific Ultra-Low Bandwidth
  Image Transmission -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.IV/paper_3.html">
      <font color="black">Single Image Super-Resolution for Domain-Specific Ultra-Low Bandwidth
  Image Transmission</font>
    </a>
  </h2>
  <font color="black">私たちは、再構成された画像の品質と、実用的なユースケースでの一般的なそのような方法の展望を調査することを目指しています。アプリケーション..私たちは、毎秒数フレームの水中音響帯域幅要件を満たす約1 kBの低解像度の低サイズバージョンに画像をダウンサンプリングすることを提案します。 
[ABSTRACT]これにより、このようなチャネルは使用できなくなったり、効率が悪くなったりします。これは、長年の釣りで得られた大規模で多様なデータセットで調査されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Analysis of Hand-Crafted and Automatic-Learned Features for Glaucoma
  Detection Through Raw Circmpapillary OCT Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.IV/paper_4.html">
      <font color="black">Analysis of Hand-Crafted and Automatic-Learned Features for Glaucoma
  Detection Through Raw Circmpapillary OCT Images</font>
    </a>
  </h2>
  <font color="black">さらに、両方の戦略の組み合わせで構成されるハイブリッドアプローチは、ROC曲線の下の面積が0.85で、予測段階での精度が0.82で、最高のパフォーマンスを報告します。結果は、提案された手動学習モデルが新規の記述子では、自動学習を上回ります。新規性として、視野や眼圧測定などの他の高価なテストを使用せずに、生の乳頭周囲OCT画像のみを考慮して予測モデルを構築しました。 
[ABSTRACT]実験は、緑内障194例と正常b 198例で構成されるプライベートデータベースで行われました-眼科医によって診断されたスキャン</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Improved Trainable Calibration Method for Neural Networks on Medical
  Imaging Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.IV/paper_5.html">
      <font color="black">Improved Trainable Calibration Method for Neural Networks on Medical
  Imaging Classification</font>
    </a>
  </h2>
  <font color="black">モデルのキャリブレーションを大幅に改善しながら、全体的な分類精度を維持する新しいキャリブレーションアプローチを提案します。提案されたアプローチは、ミスキャリブレーションを定量化するための一般的な測定基準である予想されるキャリブレーションエラーに基づいています。最近の研究により、ディープニューラルネットワークが超-医用画像処理ドメインにおける幅広い画像分類タスクにおける人間のパフォーマンス。 
[ABSTRACT]提案されたアプローチは予想されるキャリブレーションエラーに基づいています。これにより、さまざまなアーキテクチャとデータセット全体でキャリブレーションエラーが大幅に減少します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: not-so-BigGAN: Generating High-Fidelity Images on a Small Compute Budget -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.IV/paper_6.html">
      <font color="black">not-so-BigGAN: Generating High-Fidelity Images on a Small Compute Budget</font>
    </a>
  </h2>
  <font color="black">BigGANのようにピクセル空間で画像をモデル化する代わりに、not-so-BigGANはウェーブレット変換を使用して次元の呪いを回避し、全体的な計算要件を大幅に削減します。広範な経験的評価を通じて、固定計算予算ではなく、 -so-BigGANはBigGANよりも数倍速く収束し、桁違いに低いコンピューティングバジェットで競争力のある画像品質に達します（4 Telsa-V100 GPU）。この手法は効果的ですが、信じられないほどの量のコンピューティングリソースや時間（256） TPU-v3コア）。モデルを大規模な研究コミュニティーの手の届かないところに置きます。 
[ABSTRACT] bigganは高次元でのトレーニング中に大きなミニバッチサイズを使用します。bigganではなく、固定コンピューティングバジェットでテストする必要があり、桁違いに低いコンピューティングバジェット（4 telsa-v100 gpus）で競争力のある画像品質に到達します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Generalizing Complex/Hyper-complex Convolutions to Vector Map
  Convolutions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.IV/paper_7.html">
      <font color="black">Generalizing Complex/Hyper-complex Convolutions to Vector Map
  Convolutions</font>
    </a>
  </h2>
  <font color="black">複雑で超複雑な値のニューラルネットワークが実数の対応するものよりも改善を提供する主な理由は、重み共有メカニズムと多次元データを単一のエンティティとして扱うことです。これは、固有の線形結合を模倣するシステムを導入することによって達成されます。入力次元。たとえば、四元数のハミルトン積など。ただし、どちらも、2つの複素数と4つの四元数の次元数に制限されています。 
[ABSTRACT]超複雑な畳み込みは、複雑なネットワークの利点を獲得します。これらの実際のシステムは、内部の潜在的な関係を獲得する能力を獲得します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Concise and Effective Network for 3D Human Modeling from Orthogonal
  Silhouettes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.IV/paper_8.html">
      <font color="black">Concise and Effective Network for 3D Human Modeling from Orthogonal
  Silhouettes</font>
    </a>
  </h2>
  <font color="black">3Dヒューマンモデリングのための既存のCNNアプローチは、通常、2つのバイナリイメージから多数のパラメーター（{8.5M}から{355.4M}まで）を学習します。新しいCNN構造は、フロントの識別機能だけでなく、マッピング機能のための側面図とそれらの混合機能。私たちのネットワークのトレーニングは、公的にアクセス可能なデータセットを拡張することによって得られたサンプルで行われます。 
[要旨]マッピング機能は、2つのシルエットから効果的に特徴を抽出し、それらを人体の形状空間のエントリに融合できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-25">
        <br><font color="black">2019-12-25</font>
      </time>
    </span>
</section>
<!-- paper0: Real-time Bayesian personalization via a learnable brain tumor growth
  model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.IV/paper_9.html">
      <font color="black">Real-time Bayesian personalization via a learnable brain tumor growth
  model</font>
    </a>
  </h2>
  <font color="black">神経膠腫患者のコホートのベイジアン腫瘍モデルパーソナライゼーションでサロゲートをテストします。局所腫瘍細胞密度..提案された神経サロゲートを使用したベイジアン推論は、通常の数値ソルバーでフォワードモデルを解くことによって得られるものと同様の推定値をもたらします。 
[ABSTRACT]現在のモデリングアプローチは、腫瘍の進行をシミュレートする数値ソルバーに頼っています。これらはデータ駆動型アプローチに基づいており、最大数万のフォワードモデル評価が必要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Momentum-Net for Low-Dose CT Image Reconstruction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.IV/paper_10.html">
      <font color="black">Momentum-Net for Low-Dose CT Image Reconstruction</font>
    </a>
  </h2>
  <font color="black">NIH AAPM-Mayoクリニック低線量CTグランドチャレンジデータセットの実験結果は、提案されたMomentum-Netアーキテクチャが、最新の非反復画像ノイズ除去ディープニューラルネットワーク（NN）、WavResNetと比較して、画像再構成の精度を大幅に向上させることを示しています。 （LDCTの場合）。非拡張NNプロパティを満たすために画像リファインNN学習に適用されるスペクトル正規化手法も調査しました。ただし、実験結果は、これがMomentum-Netの画像再構成パフォーマンスを改善しないことを示しています。このペーパーは、適切なモデルを使用して、最近の高速反復ニューラルネットワークフレームワークであるMomentum-Netを低線量X線コンピュータ断層撮影（LDCT）に適用します。画像再構成。 
[ABSTRACT]プロジェクトは、運動量の画像再構成パフォーマンスを改善する一連の方法の最新版-ネット</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-27">
        <br><font color="black">2020-02-27</font>
      </time>
    </span>
</section>
<!-- paper0: Nighttime Dehazing with a Synthetic Benchmark -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.IV/paper_11.html">
      <font color="black">Nighttime Dehazing with a Synthetic Benchmark</font>
    </a>
  </h2>
  <font color="black">さらに、MobileNet-v2バックボーンに基づくエンコーダー/デコーダー構造を備えた、シンプルで効果的な学習ベースラインも考案しました。この問題に対処するために、ヘイズから色補正を解く前に、最適なスケールの最大反射率を提案します削除し、それらを順番に処理します。データセットとソースコードの両方は、https：//github.com/chaimi2013/3Rで入手できます。 
[ABSTRACT]大規模なベンチマークデータセットが存在しないと、進行が妨げられます。代わりに、以前の分析分布から実際の明るい色をサンプリングすることで、現実的な夜間ファジーを作成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-10">
        <br><font color="black">2020-08-10</font>
      </time>
    </span>
</section>
<!-- paper0: NTGAN: Learning Blind Image Denoising without Clean Reference -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.IV/paper_12.html">
      <font color="black">NTGAN: Learning Blind Image Denoising without Clean Reference</font>
    </a>
  </h2>
  <font color="black">学習ベースの画像のノイズ除去に関する最近の研究は、さまざまなノイズ低減タスクで有望なパフォーマンスを達成しました。ノイズ伝達を学習すると、破損したサンプルを観察するだけでネットワークがノイズ除去能力を獲得できるようになります。実際の写真。 
[ABSTRACT]ディープデノイザは、クリーンな参照なしでトレーニングされるか、合成ノイズを監視されません。新しい方法を使用して、現実的なノイズを除去できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Anatomical Segmentations for Tractography from Diffusion MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.IV/paper_13.html">
      <font color="black">Learning Anatomical Segmentations for Tractography from Diffusion MRI</font>
    </a>
  </h2>
  <font color="black">組織の種類に応じて、0 .70と0 .87の間の一貫したセグメンテーションの結果を示します。流線のセットではなく、ボリュームラベルとして路を表すことの欠点は、それに沿った微細構造または幾何学的特徴の点ごとの分析ができないことです。拡散MRIのディープラーニングアプローチは、これまで主にボクセルベースの病変または白質繊維路のセグメンテーションに焦点を当ててきました。 
[ABSTRACT]これにより、脳全体または路に沿った幾何学的特徴の点耐性分析ができなくなります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Cephalogram Synthesis and Landmark Detection in Dental Cone-Beam CT
  Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.IV/paper_14.html">
      <font color="black">Cephalogram Synthesis and Landmark Detection in Dental Cone-Beam CT
  Systems</font>
    </a>
  </h2>
  <font color="black">画像の解像度を向上させるために、超解像ディープラーニングテクニックが調査されています。標準化された3Dセファロ分析方法論が欠如しているため、3DコーンビームCT（CBCT）ボリュームから合成された2Dセファログラムは、歯科用CBCTシステムのセファロ分析に広く使用されています。低線量を目的として、2つのCBCT投影から直接2Dセファログラムを合成するために、ピクセル間の生成敵対ネットワーク（pix2pixGAN）が提案されています。 
[要約]シグモイドベースの強度変換が3dセファログラムに提案されています。pix2pixganは、合成セファログラムの検出を増やすために提案されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Anonymization of labeled TOF-MRA images for brain vessel segmentation
  using generative adversarial networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.IV/paper_15.html">
      <font color="black">Anonymization of labeled TOF-MRA images for brain vessel segmentation
  using generative adversarial networks</font>
    </a>
  </h2>
  <font color="black">これにより、医用画像における希少なデータと匿名化の課題を克服する道が開かれます。すべてのモデルのパフォーマンスは、ダイス類似係数（DSC）とハウスドルフ距離（95HD）の95パーセンタイルによって評価されました。脳血管セグメンテーションの分析ユースケースとして、画像ラベル生成用の飛行時間（TOF）磁気共鳴血管造影（MRA）パッチで3つのGANをトレーニングしました：1）深い畳み込みGAN、2）グラディエントペナルティ付きのWasserstein-GAN（WGAN-GP）および3）スペクトル正規化を伴うWGAN-GP（WGAN-GP-SN）。 
[要旨]キャプターは、予測特性を保持しながら匿名の画像を提供できます。これにより、医療用画像における希少なデータと匿名化の課題を克服する道が開かれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Detection of Line Artefacts in Lung Ultrasound Images of COVID-19
  Patients via Non-Convex Regularization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.IV/paper_16.html">
      <font color="black">Detection of Line Artefacts in Lung Ultrasound Images of COVID-19
  Patients via Non-Convex Regularization</font>
    </a>
  </h2>
  <font color="black">誤検出と見落としの数を減らすために、この方法には2段階の検証メカニズムが含まれており、これはラドン領域と画像領域の両方で実行されます。さらに、高速収束により、提案された方法は処理に容易に適用できます。 LUS画像シーケンス。ラドン変換ドメインでは、ラインアーティファクトの既知の臨床定義に関連付けられている単純な極大値検出手法を採用しています。 
[要約]提案された方法は、提案されたコーシー近位分割（cps）方法によって実装が保証されています。それは、lus画像内の垂直および垂直ラインアーティファクトの両方を正確に識別します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-06">
        <br><font color="black">2020-05-06</font>
      </time>
    </span>
</section>
<!-- paper0: Hyperspectral Image Super-Resolution via Deep Prior Regularization with
  Parameter Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.IV/paper_17.html">
      <font color="black">Hyperspectral Image Super-Resolution via Deep Prior Regularization with
  Parameter Estimation</font>
    </a>
  </h2>
  <font color="black">2つの公開データセットの実験結果は、定量的および定性的比較の両方で、いくつかの最先端の方法に対する提案された方法の優位性を示しています。次に、シルベスター方程式を解くことによって融合問題を最適化し、同時に正則化パラメーターを推定します。物理モデルの寄与を自動的に調整し、最終的なHR HSIを再構築する前に学習します。この作業では、物理モデルと深い事前情報を統合する方法を提案します。 
[要約] hsiは、同じシーンの低解像度（lr）hsiと高解像度（hr）の従来の画像を組み合わせて、hr hsiを取得します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-modal Attention for Speech Emotion Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.IV/paper_18.html">
      <font color="black">Multi-modal Attention for Speech Emotion Recognition</font>
    </a>
  </h2>
  <font color="black">提案されたハイブリッドネットワークMMANは、感情認識のためにIEMOCAPデータベースで最先端のパフォーマンスを実現します。このホワイトペーパーでは、ビジュアルモーダルアテンションネットワーク（MMAN）と呼ばれるハイブリッドフュージョン手法を検討します。スピーチの感情認識におけるテキストによる手がかり。感情は、スピーチの韻律で表される人間のスピーチの本質的な側面を表します。 
[要約]音声感情認識は、視覚的および文学的な手がかりから大幅にメリットを得ます。clstm-mmaだけでも、他の融合方法と同じくらい競争力があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Deep Learning for Image and Point Cloud Fusion in Autonomous Driving: A
  Review -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_0.html">
      <font color="black">Deep Learning for Image and Point Cloud Fusion in Autonomous Driving: A
  Review</font>
    </a>
  </h2>
  <font color="black">これらの観察に基づいて、洞察を提供し、有望な研究の方向性を指摘します。さらに、これらの方法を公開されているデータセットで比較します。深度補完、オブジェクト検出、セマンティックセグメンテーションにおけるカメラとLiDARの融合方法の詳細なレビューが続きます。 、追跡、オンラインクロスセンサーキャリブレーション。それぞれの融合レベルに基づいて構成されています。 
[要約]このホワイトペーパーでは、画像と点群の両方を活用する最近のデータフュージョンアプローチについて説明します。これには、詳細な完了、オブジェクト検出、セマンティックセグメンテーション、追跡、オンラインクロスセンサーキャリブレーションが含まれます。現在の学術研究と実際のアプリケーション</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-10">
        <br><font color="black">2020-04-10</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Part Discovery by Unsupervised Disentanglement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_1.html">
      <font color="black">Unsupervised Part Discovery by Unsupervised Disentanglement</font>
    </a>
  </h2>
  <font color="black">場所とセマンティクスの両方をキャプチャするこれらは、教師あり学習アプローチの魅力的なターゲットです。実験では、私たちのアプローチを以前の最先端のアプローチと比較し、セグメンテーションの精度と形状の一貫性の大幅な向上を観察します。ほとんどの既存の教師なしアプローチ抽象表現の学習に焦点を当て、監督によって最終表現に洗練されます。 
[ABSTRACT]教師なしのアプローチは、最終的な表現への監督により洗練される抽象表現の学習に焦点を当てています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Real-time Plant Health Assessment Via Implementing Cloud-based Scalable
  Transfer Learning On AWS DeepLens -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_2.html">
      <font color="black">Real-time Plant Health Assessment Via Implementing Cloud-based Scalable
  Transfer Learning On AWS DeepLens</font>
    </a>
  </h2>
  <font color="black">クラウド統合は、スケーラビリティとユビキタスアクセスを提供します。ディープラーニングモデルのトレーニングに4万枚の画像を使用し、それを1万枚の画像で評価しました。果物と野菜の健康と不健康な葉の広範な画像データセットでの実験植物の葉の病気のリアルタイム診断で98.78％の精度を示しました。 
[要約]提案されたディープレンズ分類および検出モデル（dcdm）アプローチは、限られた制限に対処します。拡張可能な転送を介して、果物（リンゴ、ブドウ、桃、イチゴ）および野菜（ジャガイモ、トマト）における葉の病気の自動検出および分類を提供します学習する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Map-Adaptive Goal-Based Trajectory Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_3.html">
      <font color="black">Map-Adaptive Goal-Based Trajectory Prediction</font>
    </a>
  </h2>
  <font color="black">マルチモーダル、長期的な車両軌道予測のための新しい方法を提示します。大規模な内部運転データセットと公共のnuScenesデータセットの両方に対する実験結果は、モデルが以下の最新のアプローチよりも優れていることを示しています。 6秒間の車両軌跡予測。また、モデルが既存の方法よりも完全に新しい都市の道路シーンに一般化できることを実証します。 
[ABSTRACT]私たちのアプローチは、車線の中心線を使用して各車両に提案されたゴールパスのセットを作成することに依存しています。私たちのモデルは、既存の方法よりも完全に新しい都市の道路シーンに一般化できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Revealing Lung Affections from CTs. A Comparative Analysis of Various
  Deep Learning Approaches for Dealing with Volumetric Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_4.html">
      <font color="black">Revealing Lung Affections from CTs. A Comparative Analysis of Various
  Deep Learning Approaches for Dealing with Volumetric Data</font>
    </a>
  </h2>
  <font color="black">この論文は、ImageClef 2020結核タスクのコンテキストで、肺CTの結核関連病変を自動的に検出するためのいくつかのディープラーニングアプローチを提示し、比較分析します。報告された作業は、競争で最高の結果を得たSenticLab.UAICチームに属しています。 ..これらすべてには、さまざまなニューラルネットワークアーキテクチャ、さまざまなセグメンテーションアルゴリズム、およびデータ拡張スキームを含む豊富な実験的分析が付属しています。 
[要約] 3種類の方法は、ボリュームデータが脳ネットワークへの入力としてどのように与えられるかに関して異なります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: MU-GAN: Facial Attribute Editing based on Multi-attention Mechanism -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_5.html">
      <font color="black">MU-GAN: Facial Attribute Editing based on Multi-attention Mechanism</font>
    </a>
  </h2>
  <font color="black">実験結果は、この方法が属性編集能力と詳細保存能力のバランスをとることができ、属性間の相関を切り離すことができることを示しています。これは、属性操作の精度と画質の点で、最先端の方法よりも優れています。 、私たちは古典的な畳み込みエンコーダー/デコーダーをジェネレーターの対称U-Netのような構造に置き換え、次にアディティブアテンションメカニズムを適用して、エンコーダー表現を適応的に転送するためのアテンションベースのU-Net接続を構築して、デコーダーを属性で補完します-詳細を除外し、属性編集機能を強化します。 
[要約]属性操作の精度と画質は、これらの畳み込みレイヤーで確認できます。これは、能力と品質の点で最新の方法と最新の方法よりも優れています。これらには、次の能力に基づく最新の方法が含まれます。画質に影響を与える</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Toward Hierarchical Self-Supervised Monocular Absolute Depth Estimation
  for Autonomous Driving Applications -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_6.html">
      <font color="black">Toward Hierarchical Self-Supervised Monocular Absolute Depth Estimation
  for Autonomous Driving Applications</font>
    </a>
  </h2>
  <font color="black">私たちの貢献は2つあります。a）より良いオブジェクトレベルの深度推定を提供するために、新規の密結合予測（DCP）レイヤーが提案され、b）特に自動運転シナリオでは、緻密な幾何学的制約（DGC）が導入され、正確なスケールファクターを自動運転車の追加コストなしで回復しました。近年、単眼の深度推定のための自己管理手法は、特に自動運転アプリケーションの深度推定タスクの重要な分岐になりました。DCPレイヤーのおかげで、オブジェクトの境界がより良くなりました。深度マップで区別され、深度はオブジェクトレベルでより継続されます。 
[ABSTRACT]以前のメソッドは、依然として顕著なオブジェクト-レベルの深さの可能性に悩まされています。ただし、それらは依然として不確実なスケールファクターを持っています。これは、現在のメソッドのために依然として不正確なままであるためです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-12">
        <br><font color="black">2020-04-12</font>
      </time>
    </span>
</section>
<!-- paper0: Small-floating Target Detection in Sea Clutter via Visual Feature
  Classifying in the Time-Doppler Spectra -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_7.html">
      <font color="black">Small-floating Target Detection in Sea Clutter via Visual Feature
  Classifying in the Time-Doppler Spectra</font>
    </a>
  </h2>
  <font color="black">検出器のアウティラーはターゲットとして分類されます。この視覚的な手掛かりに従って、ローカルバイナリパターン（LBP）を利用してTDS画像のテクスチャの変化を測定します。実際のIPIXレーダーデータセットでは、機能ベースの検出器は、他の3つの既存のアプローチと比較して、好ましい検出率を示します。 
[ABSTRACT]ターゲットを含むレーダーレーダーリターンとクラッタのみを含むレーダーレーダーは、lbpの特徴空間で分離可能</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Semi-supervised Medical Image Segmentation through Dual-task Consistency -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_8.html">
      <font color="black">Semi-supervised Medical Image Segmentation through Dual-task Consistency</font>
    </a>
  </h2>
  <font color="black">具体的には、ピクセル単位のセグメンテーションマップとターゲットのジオメトリ対応レベルセット表現を共同で予測するデュアルタスクディープネットワークを使用します。マルチ/デュアルタスク学習は、固有の予測を持つさまざまなレベルの情報に対応していることを観察します摂動、私たちはこの作業で質問をします：SSLのネットワークおよび/またはデータレベルの摂動と変換を暗黙的に構築するのではなく、タスクレベルの正規化を明示的に構築できますか？。同時に、ラベル付けされたデータとラベル付けされていないデータの両方について、レベルセットから導出されたセグメンテーションマップと直接予測されたセグメンテーションマップの間にデュアルタスクの一貫性の正規化を導入します。 
[ABSTRACT]デュアルタスク-ネットワーク-一貫性のあるセミフィールド-監視対象フレームワークを初めて。システムは、ラベルなしデータを組み合わせることにより、実質的にパフォーマンスを向上できます。メソッドは、ラベルなしデータを組み込むことで、パフォーマンスを大幅に向上できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Plant Diseases recognition on images using Convolutional Neural
  Networks: A Systematic Review -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_9.html">
      <font color="black">Plant Diseases recognition on images using Convolutional Neural
  Networks: A Systematic Review</font>
    </a>
  </h2>
  <font color="black">系統的レビューの結果から、植物病の同定におけるCNNの使用に関する革新的な傾向を理解し、研究コミュニティの注意を必要とするギャップを特定することが可能です。この意味で、121の論文を提示します。病気の検出、データセットの特徴、調査された作物と病原体に関連する側面を処理するためのさまざまなアプローチで過去10年間に選択されました。植物の病気は、食料生産に影響を与え、生産の損失を最小限に抑える主な要因の1つと考えられています。作物病が迅速に検出および認識されることが不可欠です。 
[要約]ディープラーニング手法の最近の拡大により、植物の病気の検出が使用されるようになりました。これにより、調査されたデータセット、作物、病原体を非常に正確に使用するツールが可能になります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: View-consistent 4D Light Field Depth Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_10.html">
      <font color="black">View-consistent 4D Light Field Depth Estimation</font>
    </a>
  </h2>
  <font color="black">私たちの方法は、他の古典的アプローチとディープラーニングベースのアプローチの両方に対して効率的に実行され、合成ライトフィールドと現実世界のライトフィールドの両方で競争力のある定量的メトリックと定性的パフォーマンスを実現します。私たちの方法は、EPIを介して深度エッジを正確に定義し、これらのエッジを中央ビュー内で空間的に拡散します。これらの深度推定は、オクルージョンを意識した方法で他のすべてのビューに伝播されます。 
[ABSTRACT]私たちの方法は、他の古典的学習と深層学習の両方に基づいて効率的に実行されます-ベースのアプローチ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Investigating Bias in Deep Face Analysis: The KANFace Dataset and
  Empirical Study -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_11.html">
      <font color="black">Investigating Bias in Deep Face Analysis: The KANFace Dataset and
  Empirical Study</font>
    </a>
  </h2>
  <font color="black">最後に、ネットワークエンベディングをデバイアスする方法が導入され、提案されたベンチマークでテストされます。このバイアスは、トレーニングセット内の人口統計全体の多様性の制限と、アルゴリズムの設計の両方に影響されます。データは、手動で注釈が付けられますアイデンティティ、正確な年齢、性別、親族の。 
[要約]顔認識、年齢推定、性別認識、親族検証のディープラーニングモデルは、特定の人口統計に対する偏見に懸念を引き起こしています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-15">
        <br><font color="black">2020-05-15</font>
      </time>
    </span>
</section>
<!-- paper0: Temporal Attribute-Appearance Learning Network for Video-based Person
  Re-Identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_12.html">
      <font color="black">Temporal Attribute-Appearance Learning Network for Video-based Person
  Re-Identification</font>
    </a>
  </h2>
  <font color="black">TALNetは、属性と外観の表現の間の補完を活用し、マルチタスク学習方式によってそれらを共同で最適化します。具体的には、属性ブランチネットワークが、ロバストな属性表現を学習するための空間注意ブロックと時間セマンティックコンテキストブロックで提案されます。人間属性と外観は互いに補完的であり、どちらも歩行者のマッチングに貢献します。 
[ABSTRACT] talnetは、人間の属性と外見を同時に活用して、ビデオから歩行者の包括的かつ効果的な表現を学習します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: CS-AF: A Cost-sensitive Multi-classifier Active Fusion Framework for
  Skin Lesion Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_13.html">
      <font color="black">CS-AF: A Cost-sensitive Multi-classifier Active Fusion Framework for
  Skin Lesion Classification</font>
    </a>
  </h2>
  <font color="black">私たちの実験結果は、私たちのフレームワークが常に静的核融合の競合他社よりも優れていることを示しています。実験評価では、ISIC研究データセットに（12 CNNアーキテクチャの）96の基本分類子を用意しました。 （すなわち、データの量に関して）しかし、より重要な少数派クラス（たとえば、特定の癌性病変）。 
[ABSTRACT]複数のcnn分類子の結果はより効果的でロバストです。これらは、1つのcnn分類子と比較して、単一のcnn分類子と比較されます。この、isic研究データセット用に96のアクティブな分類子を準備しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-25">
        <br><font color="black">2020-04-25</font>
      </time>
    </span>
</section>
<!-- paper0: Single Image Super-Resolution for Domain-Specific Ultra-Low Bandwidth
  Image Transmission -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_14.html">
      <font color="black">Single Image Super-Resolution for Domain-Specific Ultra-Low Bandwidth
  Image Transmission</font>
    </a>
  </h2>
  <font color="black">私たちの方法は、一般的なバイキュービックアップサンプリングよりも優れた知覚品質と優れた再構成を実現し、この分野での水中アプリケーションのさらなる動機付けとなることを示しています。データ伝送のボトルネックと戦うために、海事ドメイン内の実用的なユースケースを検討し、単一画像の超解像方法論の見通し..ニューラルネットワークは、元の画像を再構築することを試みて、アップサンプリングを実行するように訓練されます。 
[ABSTRACT]これにより、このようなチャネルは使用できなくなったり、効率が悪くなったりします。これは、長年の釣りで得られた大規模で多様なデータセットで調査されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Online trajectory recovery from offline handwritten Japanese kanji
  characters -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_15.html">
      <font color="black">Online trajectory recovery from offline handwritten Japanese kanji
  characters</font>
    </a>
  </h2>
  <font color="black">エンコーダーは特徴抽出に焦点を当て、デコーダーは抽出された特徴を参照して座標の時系列を生成します。提案されたモデルには、畳み込みニューラルネットワークベースのエンコーダーと、注意層..提案手法の性能を視覚的検証と手書き文字認識の両方で評価する。 
[要約]提案されたモデルには、畳み込みニューラルネットワークに基づくエンコーダーと長期短期記憶ネットワークに基づく2つの主要コンポーネントがあります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: One-shot Text Field Labeling using Attention and Belief Propagation for
  Structure Information Extraction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_16.html">
      <font color="black">One-shot Text Field Labeling using Attention and Belief Propagation for
  Structure Information Extraction</font>
    </a>
  </h2>
  <font color="black">ドキュメント画像からの構造化情報の抽出は、通常、テキスト検出、テキスト認識、テキストフィールドのラベリングという3つのステップで構成されます。タスクの既存のワンショット学習方法は、ほとんどがルールベースであり、ランドマークが少ない混雑した地域のフィールドにラベルを付けるのは困難です。テキストのラベル付けタスクのための既存の学習ベースの方法は、通常、ドキュメントのタイプごとに特定のモデルをトレーニングするために大量のラベル付けされた例を必要とします。 
[ABSTRACT]テキストフィールドのラベル付けはあまり調査されておらず、依然として多くの課題に直面しています。ただし、大量のドキュメント画像を収集してラベル付けすることは、プライバシーのために困難であり、時には不可能です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: FairMOT: On the Fairness of Detection and Re-Identification in Multiple
  Object Tracking -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_17.html">
      <font color="black">FairMOT: On the Fairness of Detection and Re-Identification in Multiple
  Object Tracking</font>
    </a>
  </h2>
  <font color="black">したがって、トレーニングは検出タスクに大きく偏っていますが、再IDタスクは無視します。 （2）ROI-Alignを使用して、オブジェクト検出から直接借用されたre-ID機能を抽出します。ただし、単一のネットワークで2つのタスクを一緒に実行することにほとんど注意が向けられていませんでした。しかし、これにより多くのあいまいさが生じます多くのサンプリングポイントは、邪魔なインスタンスまたは背景に属している可能性があるため、オブジェクトの特性評価に使用します。 
[ABSTRACT] re-idは、精度がプライマリ検出タスクに大きく依存するセカンダリタスクです。ただし、これにより、オブジェクトの特徴付けに多くのあいまいさが追加されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-04">
        <br><font color="black">2020-04-04</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Flow-based Feature Warping for Face Frontalization with
  Illumination Inconsistent Supervision -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_18.html">
      <font color="black">Learning Flow-based Feature Warping for Face Frontalization with
  Illumination Inconsistent Supervision</font>
    </a>
  </h2>
  <font color="black">ディープラーニングベースの顔の正面化手法における最近の進歩にもかかわらず、トレーニング中に大きなポーズと照明の不一致があるため、写実的で照明を維持する正面の顔の合成は依然として困難です。さらに、ポーズを減らすためにワープ注意モジュール（WAM）が導入されています。機能レベルの不一致により、正面の画像をより効果的に合成し、プロファイル画像の詳細を保持します。IPMには、合成された正面の画像が照明を維持し、細部が確実になるように連携する2つの経路が含まれています。 
[ABSTRACT]私たちは、新しいフローベースのフィーチャーワーピングモデル（ffwm）を提案します。それは、写真を合成することを学ぶことができます-正面画像からのリアルでイラスト</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-16">
        <br><font color="black">2020-08-16</font>
      </time>
    </span>
</section>
<!-- paper0: Unconstrained Text Detection in Manga: a New Dataset and Baseline -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_19.html">
      <font color="black">Unconstrained Text Detection in Manga: a New Dataset and Baseline</font>
    </a>
  </h2>
  <font color="black">この作品は、高度なテキストスタイルである日本のジャンルのコミックジャンルのテキストを2値化することを目的としています。..ピクセルレベルでテキスト注釈を使用したマンガデータセットの不足を克服するために、独自の独自のデータセットを作成しています。最適化モデルでは、2値化の標準メトリックに加えて、他の特別なメトリックを実装します。 
[ABSTRACT]日本の検索の巨人は、ピクセルレベルのテキストアノテーションでマンガデータセットの欠如と闘うための特別なメトリックを作成し、独自に作成しました。これらのリソースを使用して、深い問題モデルを設計および評価し、現在のテキストの方法よりも優れていますほとんどのメトリックでのマンガの二値化</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Bootstrap your own latent: A new approach to self-supervised Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_20.html">
      <font color="black">Bootstrap your own latent: A new approach to self-supervised Learning</font>
    </a>
  </h2>
  <font color="black">BYOLは、転送と半教師付きベンチマークの両方で、現在の最新技術と同等以上のパフォーマンスを発揮することを示しています。最先端の方法は負のペアに依存していますが、BYOLはそれらなしで新しい最新技術を実現します。 。自己監視画像表現学習への新しいアプローチであるBootstrap Your Own Latent（BYOL）を紹介します。 
[ABSTRACT] byolは、相互に作用し、相互に学習する2つのニューラルネットワークに依存しています。ターゲットネットワークの拡張バージョンは、低速です-オンラインネットワークの移動平均</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-13">
        <br><font color="black">2020-06-13</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Metric Learning Meets Deep Clustering: An Novel Unsupervised
  Approach for Feature Embedding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_21.html">
      <font color="black">Deep Metric Learning Meets Deep Clustering: An Novel Unsupervised
  Approach for Feature Embedding</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、この課題を克服する新しいUDMLメソッドを提案します。データポイントをアンカーします。ベンチマークデータセットの実験結果は、提案されたアプローチが他のUDMLメソッドよりも優れていることを示しています。 
[ABSTRACT]新しいudmlメソッドは、通常、トリプレット損失またはペアワイズ損失を使用します。これらの方法では、ポジティブサンプルとネガティブサンプルのマイニングが必要ですが、ラベル情報が利用できないため、教師なし設定では困難です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: TanhExp: A Smooth Activation Function with High Convergence Speed for
  Lightweight Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_22.html">
      <font color="black">TanhExp: A Smooth Activation Function with High Convergence Speed for
  Lightweight Neural Networks</font>
    </a>
  </h2>
  <font color="black">ノイズが追加され、データセットが変更されても、その動作は安定したままです。TanhExpの定義は、f（x）= xtanh（e ^ x）です。さまざまなデータセットとネットワークモデルでのTanhExpのシンプルさ、効率、および堅牢性を示します。 TanhExpは、収束速度と精度の両方で同等のものよりも優れています。 
[ABSTRACT] tanh指数関数的アクティベーション関数（tanhexp）は、画像分類タスクでのネットワークのパフォーマンスを大幅に向上させることができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-22">
        <br><font color="black">2020-03-22</font>
      </time>
    </span>
</section>
<!-- paper0: Improved Trainable Calibration Method for Neural Networks on Medical
  Imaging Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_23.html">
      <font color="black">Improved Trainable Calibration Method for Neural Networks on Medical
  Imaging Classification</font>
    </a>
  </h2>
  <font color="black">モデルのキャリブレーションを大幅に改善しながら、全体的な分類精度を維持する新しいキャリブレーションアプローチを提案します。経験的に、ニューラルネットワークはしばしばキャリブレーションが誤っており、予測に自信がありません。このミスキャリブレーションは、自動意思決定システムでは問題になる可能性がありますが、ニューラルネットワークのキャリブレーション不良が重大な治療エラーにつながる可能性がある医療分野。 
[ABSTRACT]提案されたアプローチは予想されるキャリブレーションエラーに基づいています。これにより、さまざまなアーキテクチャとデータセット全体でキャリブレーションエラーが大幅に減少します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: not-so-BigGAN: Generating High-Fidelity Images on a Small Compute Budget -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_24.html">
      <font color="black">not-so-BigGAN: Generating High-Fidelity Images on a Small Compute Budget</font>
    </a>
  </h2>
  <font color="black">BigGANのようにピクセル空間で画像をモデル化する代わりに、not-so-BigGANはウェーブレット変換を使用して次元の呪いを回避し、全体的な計算要件を大幅に削減します。このペーパーでは、not-so-BigGAN、シンプルで高次元の自然画像でディープジェネレーティブモデルをトレーニングするためのスケーラブルなフレームワーク。広範囲な経験的評価を通じて、固定された計算予算では、BigGANはBigGANよりも数倍速く収束し、桁違いの競争力のある画質に到達するより低いコンピューティングバジェット（4 Telsa-V100 GPU）。 
[ABSTRACT] bigganは高次元でのトレーニング中に大きなミニバッチサイズを使用します。bigganではなく、固定コンピューティングバジェットでテストする必要があり、桁違いに低いコンピューティングバジェット（4 telsa-v100 gpus）で競争力のある画像品質に到達します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Generalizing Complex/Hyper-complex Convolutions to Vector Map
  Convolutions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_25.html">
      <font color="black">Generalizing Complex/Hyper-complex Convolutions to Vector Map
  Convolutions</font>
    </a>
  </h2>
  <font color="black">これは、クォータニオンのハミルトン積など、入力次元の一意の線形結合を模倣するシステムを導入することによって実現されます。これらの新しいベクトルマップのたたみ込みが、複雑および超複雑のすべての利点を捉えているように見えることを示すために、3つの実験を行います。次元の制限を回避しながら、内部の潜在的な関係をキャプチャする能力などのネットワーク。ただし、どちらも複雑な次元の2つと四元数の4つの次元数に制限されています。 
[ABSTRACT]超複雑な畳み込みは、複雑なネットワークの利点を獲得します。これらの実際のシステムは、内部の潜在的な関係を獲得する能力を獲得します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Concise and Effective Network for 3D Human Modeling from Orthogonal
  Silhouettes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_26.html">
      <font color="black">Concise and Effective Network for 3D Human Modeling from Orthogonal
  Silhouettes</font>
    </a>
  </h2>
  <font color="black">その結果、係数が{2.4M}のネットワークでは、より正確なモデルを生成できます。3Dヒューマンモデリングの既存のCNNアプローチでは、通常、2つのパラメータから多数のパラメータ（{8.5M}から{355.4M}まで）を学習します。バイナリ画像..新しいCNN構造は、正面図と側面図の特徴的な特徴だけでなく、マッピング関数の混合特徴も正確にするために、私たちの作業で提案されています。 
[要旨]マッピング機能は、2つのシルエットから効果的に特徴を抽出し、それらを人体の形状空間のエントリに融合できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-25">
        <br><font color="black">2019-12-25</font>
      </time>
    </span>
</section>
<!-- paper0: Diversified Mutual Learning for Deep Metric Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_27.html">
      <font color="black">Diversified Mutual Learning for Deep Metric Learning</font>
    </a>
  </h2>
  <font color="black">最後に、従来のトリプレット損失を伴う提案された方法は、標準データセットでのRecall @ 1の最先端のパフォーマンスを実現します。CUB-200-2011では69.9、CARS-196では89.1です。大規模なデータが不足している状態で転移学習を行います。埋め込みモデルは事前学習済みモデルで初期化され、ターゲットデータセットで微調整されます。この作業では、多様化と呼ばれるディープメトリック学習の効果的な相互学習方法を提案します。相互メトリック学習。これは、多様な相互学習で埋め込みモデルを強化します。 
[要約]提案された方法は、多様な相互計量学習と呼ばれます。ディープ計量学習は、多様な相互学習で埋め込みモデルを強化します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Nighttime Dehazing with a Synthetic Benchmark -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_28.html">
      <font color="black">Nighttime Dehazing with a Synthetic Benchmark</font>
    </a>
  </h2>
  <font color="black">合成ベンチマークの実験では、劣化要因が一緒になって画像の品質を低下させることが示されています。データセットとソースコードの両方がhttps://github.com/chaimi2013/3Rで入手できます。この問題に対処するために、最適な-ヘイズ除去から色補正をほどく前に最大反射率をスケーリングし、それらを順番に処理します。 
[ABSTRACT]大規模なベンチマークデータセットが存在しないと、進行が妨げられます。代わりに、以前の分析分布から実際の明るい色をサンプリングすることで、現実的な夜間ファジーを作成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-10">
        <br><font color="black">2020-08-10</font>
      </time>
    </span>
</section>
<!-- paper0: Joint Architecture and Knowledge Distillation in CNN for Chinese Text
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_29.html">
      <font color="black">Joint Architecture and Knowledge Distillation in CNN for Chinese Text
  Recognition</font>
    </a>
  </h2>
  <font color="black">バニラたたみ込み層の場合、深さ方向の分離可能なたたみ込みと点ごとのたたみ込みのみで構成される提案された節約型たたみ込み（ParConv）ブロックは、ネットワークの幅や深さなどの他の調整なしで直接置換として使用されます。最も人気のあるデータセット：MNISTは、提案されたアプローチが主流のバックボーンネットワークにも正常に適用できることを示しています。このホワイトペーパーでは、事前トレーニング済みの標準CNNのアーキテクチャと知識を同時に抽出するためのガイドラインを提案します。 
[要約]蒸留ベースのアプローチには、ほとんどの市販のディープラーニングソフトウェアでサポートされているシンプルなトレーニングプロセスが含まれ、ハードウェアの特別な要件はありません。提案されたアルゴリズムは、オフラインの手書きの中国語のテキスト認識（HCTR）で最初に検証されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-17">
        <br><font color="black">2019-12-17</font>
      </time>
    </span>
</section>
<!-- paper0: NTGAN: Learning Blind Image Denoising without Clean Reference -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_30.html">
      <font color="black">NTGAN: Learning Blind Image Denoising without Clean Reference</font>
    </a>
  </h2>
  <font color="black">合成ノイズの仮定は、実際の写真に直面した場合の一般化につながります。この問題に対処するために、ノイズ低減タスクをノイズ伝達タスクの特殊なケースと見なして、新しい深い教師なし画像のノイズ除去方法を提案します。転移により、ネットワークは、破損したサンプルを観察するだけでノイズ除去能力を獲得できます。 
[ABSTRACT]ディープデノイザは、クリーンな参照なしでトレーニングされるか、合成ノイズを監視されません。新しい方法を使用して、現実的なノイズを除去できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Learning 2D Temporal Adjacent Networks for Moment Localization with
  Natural Language -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_31.html">
      <font color="black">Learning 2D Temporal Adjacent Networks for Moment Localization with
  Natural Language</font>
    </a>
  </h2>
  <font color="black">この2Dテンポラルマップは、隣接する関係を表現しながら、さまざまな長さの多様なビデオモーメントをカバーできます。2Dマップに基づいて、モーメントローカリゼーションの単一ショットフレームワークであるTemporal Adjacent Network（2D-TAN）を提案します。隣接する時間的関係をエンコードできる一方で、ビデオモーメントを参照式と照合するための識別機能を学習します。 
[ABSTRACT]トリミングされていない動画の他の一時的な瞬間との関連でターゲットモーメントが発生する可能性があります。これは、動画の他の秒との関連でキャプチャされた動画のターゲットのため、難しい問題です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-08">
        <br><font color="black">2019-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: HSFM-$Σ$nn: Combining a Feedforward Motion Prediction Network and
  Covariance Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_32.html">
      <font color="black">HSFM-$Σ$nn: Combining a Feedforward Motion Prediction Network and
  Covariance Prediction</font>
    </a>
  </h2>
  <font color="black">私たちの提案する方法は、2つの異なるアプローチを組み合わせたものです。共分散予測のために、HSFMを使用したレイヤーがモデルベースの遷移関数であるフィードフォワードネットワークと各レイヤーのニューラルネットワーク（NN）です。この方法を、従来の方法と比較します。それらの限界を示す共分散推定..この論文では、動き予測のための新しい方法、HSFM-$ \ Sigma $ nnを提案します。 
[要約]私たちの提案する方法は、2つの異なるアプローチを組み合わせたものです。学習ベースのアプローチ、ソーシャル-lstmと比較します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Cephalogram Synthesis and Landmark Detection in Dental Cone-Beam CT
  Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_33.html">
      <font color="black">Cephalogram Synthesis and Landmark Detection in Dental Cone-Beam CT
  Systems</font>
    </a>
  </h2>
  <font color="black">画像の解像度を改善するために、超解像深層学習技術が調査されています。しかし、従来のX線フィルムベースのセファログラムと比較すると、このような合成セファログラムには画像のコントラストと解像度がありません。 3DコーンビームCTボリューム（CBCT）ボリュームは、歯科用CBCTシステムでの頭部計測分析に広く使用されています。 
[要約]シグモイドベースの強度変換が3dセファログラムに提案されています。pix2pixganは、合成セファログラムの検出を増やすために提案されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: LaSOT: A High-quality Large-scale Single Object Tracking Benchmark -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_34.html">
      <font color="black">LaSOT: A High-quality Large-scale Single Object Tracking Benchmark</font>
    </a>
  </h2>
  <font color="black">LaSOTの平均ビデオ長は約2,500フレームです。各ビデオには、ターゲットが消えたり再出現したりするなど、実際のビデオ映像に存在するさまざまな課題が含まれています。LaSOTのリリースにおける私たちの目標は、専用の高品質プラットフォームを提供することですトラッカーのトレーニングと評価の両方に使用します。各ビデオフレームには、境界ボックスで注意深く手動で注釈が付けられます。 
[要旨]高品質の大規模な単一オブジェクト追跡のベンチマークであるラソトを紹介します。これには、境界ボックスで注釈が付けられた一連のビデオが含まれています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-08">
        <br><font color="black">2020-09-08</font>
      </time>
    </span>
</section>
<!-- paper0: Detection of Line Artefacts in Lung Ultrasound Images of COVID-19
  Patients via Non-Convex Regularization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_35.html">
      <font color="black">Detection of Line Artefacts in Lung Ultrasound Images of COVID-19
  Patients via Non-Convex Regularization</font>
    </a>
  </h2>
  <font color="black">この論文では、COVID-19患者の肺超音波（LUS）画像におけるラインアーチファクト定量化のための新しい方法を提示します。さらに、その高速収束により、提案された方法はLUS画像シーケンスの処理に容易に適用できます。現在の最先端のBライン識別方法と比較して提案された方法のパフォーマンスを評価し、9人のCOVID-19患者のLUS画像で87％の正しく検出されたBラインでかなりのパフォーマンスの向上を示します。 
[要約]提案された方法は、提案されたコーシー近位分割（cps）方法によって実装が保証されています。それは、lus画像内の垂直および垂直ラインアーティファクトの両方を正確に識別します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-06">
        <br><font color="black">2020-05-06</font>
      </time>
    </span>
</section>
<!-- paper0: Language Guided Networks for Cross-modal Moment Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CV/paper_36.html">
      <font color="black">Language Guided Networks for Cross-modal Moment Retrieval</font>
    </a>
  </h2>
  <font color="black">次に、第2融合段階でマルチモーダル融合モジュールを採用します。第1特徴抽出段階では、視覚的および言語的特徴を共同で学習して、文クエリの複雑なセマンティクスをカバーできる強力な視覚情報を取得することを提案します。具体的には、初期変調ユニットは、言語的埋め込みによって視覚的特徴抽出器の特徴マップを変調するように設計されています。 
[要約]言語ガイドネットワーク（lgn）を提示します。これは、文章キャプチャを活用して瞬間検索のプロセス全体をガイドする新しいフレームワークです。ビジョンと言語ドメイン間の適切なセマンティックアラインメントに大きな課題をもたらします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-18">
        <br><font color="black">2020-06-18</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Data Weighted Training Strategies for Grammatical Error Correction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CL/paper_0.html">
      <font color="black">Data Weighted Training Strategies for Grammatical Error Correction</font>
    </a>
  </h2>
  <font color="black">この作業では、経験的調査を実行して、GECのトレーニングスケジュールに、例としてスコアリングの一種であるDelta-log-perplexityを最適に組み込む方法を発見します。スコアリングされたデータでトレーニングされたモデルは、一般的なGECテストセット..そうすることで、デルタログの複雑さの機能と適用性に光を当てる実験を実行します。 
[ABSTRACT]私たちは例を導出することで両方の種類のデータを利用します-より小さく、より高品質のデータセットに基づいた大規模な事前トレーニングデータのレベルスコア</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-07">
        <br><font color="black">2020-08-07</font>
      </time>
    </span>
</section>
<!-- paper0: Comparative Study of Language Models on Cross-Domain Data with Model
  Agnostic Explainability -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CL/paper_1.html">
      <font color="black">Comparative Study of Language Models on Cross-Domain Data with Model
  Agnostic Explainability</font>
    </a>
  </h2>
  <font color="black">さらに、事前学習と一貫性のある言語モデルの説明可能性が提示され、モデルにとらわれないアプローチを通じてこれらのモデルのコンテキストキャプチャ機能を検証します。最後に、ここで与えられた研究は、パフォーマンスまたは計算効率。実験結果により、Yelp 2013の評価分類タスクとFinancial Phrasebank感情検出タスクがそれぞれ69％の精度と88.2％の精度で確立されました。 
[要約]これらの言語モデルのパフォーマンスは、ra以外のデータセットでは調査されていません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Central Yup'ik and Machine Translation of Low-Resource Polysynthetic
  Languages -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CL/paper_2.html">
      <font color="black">Central Yup'ik and Machine Translation of Low-Resource Polysynthetic
  Languages</font>
    </a>
  </h2>
  <font color="black">Yup&#39;ikと英語の対訳テキストコーパスをコンパイルし、文法規則に基づいてYup&#39;ikの形態素パーサーを開発しました。トークン化された入力を使用すると、未解析の入力と比較して翻訳精度が向上することがわかります。次に、影響を比較しました。 Yup&#39;ikから英語への翻訳のBLEUスコアの精度に関する、さまざまなトークン化方式、つまり、ルールベース、監視なし（バイトペアエンコーディング）、および監視なし形態（Morfessor）の解析。 
[要約]私たちは、yup &#39;ikとenglish.developed aographic translateの対訳テキストコーパスをコンパイルし、文法規則に基づいて作成しました。次に、さまざまなトークン化方法の影響を比較しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: kk2018 at SemEval-2020 Task 9: Adversarial Training for Code-Mixing
  Sentiment Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CL/paper_3.html">
      <font color="black">kk2018 at SemEval-2020 Task 9: Adversarial Training for Code-Mixing
  Sentiment Classification</font>
    </a>
  </h2>
  <font color="black">ただし、この領域、特にコード混合感情分類では、研究とデータがほとんどありません。さらに、多言語モデルを使用した敵対的トレーニングを使用して、SemEval-2020タスク9ヒンディー語-英語感情分類コンテストの1位を達成します。 ..異なる言語のグループ間のコミュニケーションの増加に伴い、この現象はますます人気があります。 
[ABSTRACT]言語グループでの言語の人気が高まっています。これは、異なる言語のグループ間の言語コミュニケーションが原因です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-08">
        <br><font color="black">2020-09-08</font>
      </time>
    </span>
</section>
<!-- paper0: Exploiting Multi-Modal Features From Pre-trained Networks for
  Alzheimer's Dementia Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CL/paper_4.html">
      <font color="black">Exploiting Multi-Modal Features From Pre-trained Networks for
  Alzheimer's Dementia Recognition</font>
    </a>
  </h2>
  <font color="black">私たちのテスト結果はベースラインの精度を18.75％超えており、回帰タスクの検証結果は、78.70％の精度で認知障害の4つのクラスを分類できる可能性を示しています。マルチモーダル機能を使用して、畳み込みリカレントニューラルネットワークを変更します分類と回帰のタスクを同時に実行し、さまざまな長さの会話を計算できるベースの構造。課題は、音響データとテキストデータを提供することで、アルツハイマー型認知症の疑いのある患者を識別することです。 
[要約]ディープラーニングモデルは、ウィキペディアやウィキペディアなどの簡単に収集できる大規模なデータセットでトレーニングされます。事前トレーニング済みのネットワークから抽出されたさまざまなマルチモーダル機能を利用して、ニューラルネットワークを使用してアルツハイマー型認知症を認識します。これらの機能には、興味深い洞察が含まれますアルツハイマー病と学習へ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Impact of News on the Commodity Market: Dataset and Results -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CL/paper_5.html">
      <font color="black">Impact of News on the Commodity Market: Dataset and Results</font>
    </a>
  </h2>
  <font color="black">このフレームワークを商品「ゴールド」に適用し、2000〜2019年の期間から収集された11,412人の人間が注釈を付けたニュース見出し（この調査でリリース）のデータセットを使用して機械学習モデルをトレーニングします。金価格に関するニュースの流れと、私たちのフレームワークから生成された情報が将来の金価格に大きな影響を与えることを観察します。しかし、この情報は主に株価を中心にニュースの見出しに含まれる金融感情の形をとっています。 
[要約]ニュースニュースニュースから情報を抽出するために使用できるフレームワークを提案します。これらのデータは、ニュースフローの影響を説明するために使用されています。調査により、当社のフレームワークから生成された情報が将来の金価格に大きな影響を与えることが判明しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Making the Most of Context in Neural Machine Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/cs.CL/paper_6.html">
      <font color="black">Towards Making the Most of Context in Neural Machine Translation</font>
    </a>
  </h2>
  <font color="black">実験結果は、モデルがTransformerベースラインと、最新のベースラインで最大2.1 BLEUの大幅なマージンを備えた以前のドキュメントレベルのNMTモデルよりも優れていることを示しています。単一の文を含む文の数。また、以前の研究で通常組み込まれている、隣接する2つまたは3つの文をはるかに超えるコンテキストの利点を示す分析も提供します。 
[ABSTRACT]新しいドキュメント-レベルnmtコンセプトは、標準のデータセットでエレガントにトレーニングできるように設計されています。以前の研究ではグローバルコンテキストを明確に使用していなかったと主張しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-19">
        <br><font color="black">2020-02-19</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: Cross-domain Adaptation with Discrepancy Minimization for
  Text-independent Forensic Speaker Verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.AS/paper_0.html">
      <font color="black">Cross-domain Adaptation with Discrepancy Minimization for
  Text-independent Forensic Speaker Verification</font>
    </a>
  </h2>
  <font color="black">結果から、多様な音響環境が話者検証のパフォーマンスに影響を与えること、およびクロスドメイン適応の提案されたアプローチがこのシナリオの結果を大幅に改善できることを示しています。参照と自然主義的フィールドの記録間の不確実性と多様性の不一致..この微調整されたモデルに基づいて、埋め込み空間のドメイン固有の分布を不一致損失と最大平均不一致（MMD）に合わせます。 
[ABSTRACT]複数の音響環境に対するクロスドメインスピーカーの検証は、この分野の大きな課題です。クロス-ドメインスピーカーの識別は、オーディオフォレンジックの研究における課題です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-05">
        <br><font color="black">2020-09-05</font>
      </time>
    </span>
</section>
<!-- paper0: Active Learning for Sound Event Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.AS/paper_1.html">
      <font color="black">Active Learning for Sound Event Detection</font>
    </a>
  </h2>
  <font color="black">提案されたシステムは、評価に使用される2つのデータセット（TUT Rare Sound 2017とTAU Spatial Sound 2019）の参照メソッドよりも明らかに優れています。SEDモデルのトレーニング中、記録はトレーニング入力として使用され、注釈付きセグメントの長期的なコンテキストが保持されます..驚くべきことに、ターゲットサウンドイベントがまれであるデータセットでは、必要な注釈の労力を大幅に削減できます。トレーニングデータの2％のみに注釈を付けることにより、達成されるSEDパフォーマンスはすべてのトレーニングデータに注釈を付けるのと同じです。 
[要約]提案されたシステムは、限られた注釈作業で学習済みsedモデルの精度を最大化することを目的としています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br><font color="black">2020-02-12</font>
      </time>
    </span>
</section>
<!-- paper0: 1-Dimensional polynomial neural networks for audio signal related
  problems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.AS/paper_2.html">
      <font color="black">1-Dimensional polynomial neural networks for audio signal related
  problems</font>
    </a>
  </h2>
  <font color="black">1次元畳み込みニューラルネットワーク（1DCNN）の自動多項式カーネル推定を使用し、深い層の必要性を補償できる最初の層から高度な非線形性を導入する1次元多項式ニューラルネットワーク（1DPNN）モデルを提案しますこの非線形性により、計算の複雑さが増しますが、オーディオ信号に関連するさまざまな分類および回帰問題で同じ数のトレーニングパラメーターを持つ通常の1DCNNよりもモデルがより良い結果を生成できることがわかります。実験は、3つの公的に利用可能なデータセットで行われ、提案されたモデルは、取り組むべき回帰問題で1DCNNよりもはるかに高速な収束を達成できることを示しています。 
[ABSTRACT]モデルが非パーツモデルに提案されたのはこれが初めてです。これを使用して、ディープモデルやワイドモデルの必要性を減らすことができます。ただし、コンパクトトポスは常にディープモデルよりも優先されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Exploiting Multi-Modal Features From Pre-trained Networks for
  Alzheimer's Dementia Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.AS/paper_3.html">
      <font color="black">Exploiting Multi-Modal Features From Pre-trained Networks for
  Alzheimer's Dementia Recognition</font>
    </a>
  </h2>
  <font color="black">課題は、音響データとテキストデータを提供することで、アルツハイマー型認知症の疑いのある患者を識別することです。テスト結果はベースラインの精度を18.75％超えており、回帰タスクの検証結果では、4つのクラスの認知障害を78.70％..マルチモーダル機能を使用して、畳み込みリカレントニューラルネットワークベースの構造を変更し、分類タスクと回帰タスクを同時に実行し、可変長の会話を計算できます。 
[要約]ディープラーニングモデルは、ウィキペディアやウィキペディアなどの簡単に収集できる大規模なデータセットでトレーニングされます。事前トレーニング済みのネットワークから抽出されたさまざまなマルチモーダル機能を利用して、ニューラルネットワークを使用してアルツハイマー型認知症を認識します。これらの機能には、興味深い洞察が含まれますアルツハイマー病と学習へ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Speaker Representation Learning using Global Context Guided Channel and
  Time-Frequency Transformations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.AS/paper_4.html">
      <font color="black">Speaker Representation Learning using Global Context Guided Channel and
  Time-Frequency Transformations</font>
    </a>
  </h2>
  <font color="black">提案されたモジュールのパフォーマンスに影響を与える可能性のあるさまざまな要因を分析するために、詳細なアブレーション研究も行われます。この研究では、長距離、非局所的な時間周波数をモデル化するために、グローバルコンテキストガイドチャネルおよび時間周波数変換を提案しますスピーカー表現における依存性とチャネル分散..提案されたL2-tf-GTFC変換ブロックを使用することにより、イコールエラーレートは4.56％から3.07％に減少し、相対的に32.68％減少し、相対的に27.28％向上することがわかります。 DCFスコアの。 
[ABSTRACT]軽量ブロックはcnnモデルに簡単に組み込むことができます。ベースラインresnet-ldeモデルおよびスクイーズ＆励起ブロックに比べて、スピーカーの検証パフォーマンスが大幅に向上します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-02">
        <br><font color="black">2020-09-02</font>
      </time>
    </span>
</section>
<!-- paper0: A multi-view approach for Mandarin non-native mispronunciation
  verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.AS/paper_5.html">
      <font color="black">A multi-view approach for Mandarin non-native mispronunciation
  verification</font>
    </a>
  </h2>
  <font color="black">音響埋め込み間の距離は、電話間の類似性と見なされます。このアプローチは、GOPベースのアプローチと比較して、+ 11.23％が改善され、シングルビューアプローチが+ 1.47％改善されたことを示しています。誤って発音された電話は、標準的な発音との類似性スコアが小さいことが予想されます。 
[要約]差別的な特徴表現を組み込むためにマルチビューアプローチが提案されています。これらのモデルは、マンダリンの非ネイティブの誤発音検証に必要なアノテーションが少なくなります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-05">
        <br><font color="black">2020-09-05</font>
      </time>
    </span>
</section>
<!-- paper0: Multiple F0 Estimation in Vocal Ensembles using Convolutional Neural
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.AS/paper_6.html">
      <font color="black">Multiple F0 Estimation in Vocal Ensembles using Convolutional Neural
  Networks</font>
    </a>
  </h2>
  <font color="black">トレーニングのために、F0アノテーション付きのボーカルカルテットの複数のマルチトラックデータセットで構成されるデータセットを構築します。この作業は、追加のリバーブを含む録音を含む、さまざまなシナリオとデータ構成でこのタスクのCNNのセットを提案および評価します。モデルF0の解像度を上げて評価すると、同じ音楽ジャンルを対象とした最先端の方法と、マルチF0推定の汎用的な方法よりも優れています。 
[要約]この作業では、このタスクの一連のCNNを提案および評価します。これらには、リバーブを追加した録音が含まれます。今後のピッチ顕著性についての議論で締めくくります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: VoiceFilter-Lite: Streaming Targeted Voice Separation for On-Device
  Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.AS/paper_7.html">
      <font color="black">VoiceFilter-Lite: Streaming Targeted Voice Separation for On-Device
  Speech Recognition</font>
    </a>
  </h2>
  <font color="black">また、そのようなモデルが8ビット整数モデルとして量子化され、リアルタイムで実行できることも示しています。VoiceFilter-Liteを導入しました。これは、デバイスで実行され、ターゲットからの音声信号のみを保持する単一チャネルソース分離モデルです。ストリーミング音声認識システムの一部としてのユーザー。新しい非対称損失の使用や、適応型実行時抑制強度の採用など、これらの多面的な要件を満たす新しい手法を提案します。 
[ABSTRACT]新しいモデルを使用して、ブロードキャスト可能なモデルを作成できます。入力信号が重複した音声で構成されている場合のパフォーマンスが向上し、他のすべての音響条件下での音声認識パフォーマンスを損なうことはありません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-modal Attention for Speech Emotion Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-10/eess.AS/paper_8.html">
      <font color="black">Multi-modal Attention for Speech Emotion Recognition</font>
    </a>
  </h2>
  <font color="black">提案されたハイブリッドネットワークMMANは、感情認識のためにIEMOCAPデータベースで最先端のパフォーマンスを実現します。cLSTM-MMAは、後期フュージョンで他のユニモーダルサブネットワークと融合されます。このホワイトペーパーでは、ハイブリッドフュージョンを研究します。方法、マルチモーダル注意ネットワーク（MMAN）と呼ばれ、音声感情認識で視覚的およびテキストによる手がかりを利用します。 
[要約]音声感情認識は、視覚的および文学的な手がかりから大幅にメリットを得ます。clstm-mmaだけでも、他の融合方法と同じくらい競争力があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
