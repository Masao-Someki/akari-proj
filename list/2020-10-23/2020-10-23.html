<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-10-23の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: End-to-End Classification of Reverberant Rooms using DNNs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_0.html">
      <font color="black">End-to-End Classification of Reverberant Rooms using DNNs</font>
    </a>
  </h2>
  <font color="black">DNNによって学習された残響音声表現と音響パラメータの間の関係が調査されます。文献の既存のアプローチは、分類器への入力として音響パラメータを手動で選択するドメインの専門知識に依存しています。以前に提案された方法の制限を克服するために、これは論文は、DNNが残響音声スペクトルを直接操作することによって分類を実行する方法を示しており、注意メカニズムを備えたCRNNがタスクに提案されています。 
[要約]残響音声による研究は、深層学習が音声に対する空気の影響を使用して、録音された部屋の観点から録音を分類する方法を示唆しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-12-21">
        <br><font color="black">2018-12-21</font>
      </time>
    </span>
</section>
<!-- paper0: Microsoft Speaker Diarization System for the VoxCeleb Speaker
  Recognition Challenge 2020 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_1.html">
      <font color="black">Microsoft Speaker Diarization System for the VoxCeleb Speaker
  Recognition Challenge 2020</font>
    </a>
  </h2>
  <font color="black">次に、Res2Netベースのスピーカー埋め込みエクストラクタ、リークフィルタリングを使用したコンフォーマーベースの連続音声分離、システムフュージョン用の修正DOVER（Diarization Output Voting Error Reductionの略）メソッドなど、コンポーネントの詳細を示します。開発セットと評価セットでそれぞれ3.71％と6.23％のダイアリゼーションエラー率（DER）を達成し、チャレンジのダイアリゼーショントラックで1位にランクされています。このペーパーでは、モノラルマルチトーカー録音用のMicrosoftスピーカーダイアリゼーションシステムについて説明します。野生では、VoxCelebスピーカー認識チャレンジ（VoxSRC）2020のダイアリゼーショントラックで評価されます。
[要約]最初に、実際のマルチトーカー録音の処理における問題に対処するためのシステム設計について説明します。voxsrcchallenge2020によって提供されるデータセットには、実際の-youtubeからのライフマルチトークセルオーディオセット</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Class-Conditional Defense GAN Against End-to-End Speech Attacks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_2.html">
      <font color="black">Class-Conditional Defense GAN Against End-to-End Speech Attacks</font>
    </a>
  </h2>
  <font color="black">したがって、この再構成は信号に余分なノイズを追加せず、実験結果によると、防御GANは、単語誤り率と文レベルの認識精度の両方の点で、従来の防御アルゴリズムを大幅に上回っています。与えられたテスト入力とジェネレータネットワーク間の相対的な弦距離調整を最小化することによるクラス条件付き生成的敵対的ネットワークの最適な入力ベクトル。次に、合成されたスペクトログラムと与えられた入力信号から導出された元の位相情報から1D信号を再構築します。 。 
[概要]特定の入力信号を自動エンコードする代わりに、提案されたアプローチは機能しません。代わりに、1d信号と元の位相情報をテストします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: All for One and One for All: Improving Music Separation by Bridging
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_3.html">
      <font color="black">All for One and One for All: Improving Music Separation by Bridging
  Networks</font>
    </a>
  </h2>
  <font color="black">実験結果は、音楽分離のための有名で最先端のオープンソースライブラリであるOpen-Unmix（UMX）のパフォーマンスが、上記のスキームを利用することによって改善できることを示しています。MDLとCLは簡単に適用できます。多くの既存のDNNベースの分離方法は、トレーニング中にのみ使用され、推論ステップに影響を与えない単なる損失関数であるためです。一方、新しい組み合わせ損失（CL）を使用して機器推定の組み合わせを検討します。 。 
[ABSTRACT] mdlとclは、多くの既存のdnnベースの分離方法に簡単に適用できます。これらは、トレーニング中にのみ使用され、結論ステップに影響を与えない単なる損失関数です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-08">
        <br><font color="black">2020-10-08</font>
      </time>
    </span>
</section>
<!-- paper0: Mood Classification Using Listening Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_4.html">
      <font color="black">Mood Classification Using Listening Data</font>
    </a>
  </h2>
  <font color="black">この新しいデータセットに関する私たちの結果は、現在のオーディオベースのモデルの限界を明らかにするだけでなく、このタイムリーなトピックに関するさらに再現性のある研究を促進することも目的としています。曲のムードは、 music ..この作品では、ムードを分類する際に、リスニングベースの機能がコンテンツベースの機能よりも優れていることを示しています。リスニングデータのマトリックス因数分解によって得られた埋め込みは、オーディオコンテンツに基づく埋め込みよりもトラックムードの情報が多いようです。 
[概要]百万曲のデータセットに加えて、百万曲のデータセットのサブセットをコンパイルします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Backdoor Attack against Speaker Verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_5.html">
      <font color="black">Backdoor Attack against Speaker Verification</font>
    </a>
  </h2>
  <font color="black">具体的には、検証タスクの理解に基づいて、さまざまなクラスターからのポイズニングされたサンプルにさまざまなトリガー（$ ie $、事前定義された発話）が含まれるクラスタリングベースの攻撃スキームを設計します。感染したモデルは、良性のサンプルで正常に動作しますが、攻撃者が登録したスピーカーに関する情報を持っていなくても、攻撃者が指定した未登録のトリガーは検証に合格します。また、既存のバックドア攻撃をスピーカー検証の攻撃に直接採用できないことも示しています。 
[概要]トレーニングデータをポイズニングすることで、話者検証モデルに感染するための隠しバックドアを挿入できることを実証しました。感染したモデルは良性のサンプルで正常に動作しますが、攻撃者が指定した未登録のトリガーは検証に合格します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Noise-Robust Adaptation Control for Supervised Acoustic System
  Identification Exploiting A Noise Dictionary -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_6.html">
      <font color="black">Noise-Robust Adaptation Control for Supervised Acoustic System
  Identification Exploiting A Noise Dictionary</font>
    </a>
  </h2>
  <font color="black">継続的に活性化を推測しながら、システムが励起されていないときはいつでもオフラインまたはオンラインで収集できるトレーニングデータからノイズ辞書を学習することを提案します。ブロックオンライン教師あり音響システムのノイズに強い適応制御戦略を提示します。ノイズ辞書を利用することによる同定..提案されたアルゴリズムは、ノイズに強い適応制御への新しい機械学習ベースのアプローチを表しており、高レベルで非定常の干渉ノイズ信号と突然のシステム変更を特徴とするアプリケーションでより高速な収束を可能にします。 
[ABSTRACT]提案されたアルゴリズムは、干渉ノイズ信号の顕著なスペクトルを利用します。提案されたアルゴリズムは、新しい機械学習ベースのノイズアプローチを表しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-03">
        <br><font color="black">2020-07-03</font>
      </time>
    </span>
</section>
<!-- paper0: Rethinking Evaluation in ASR: Are Our Models Robust Enough? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_7.html">
      <font color="black">Rethinking Evaluation in ASR: Are Our Models Robust Enough?</font>
    </a>
  </h2>
  <font color="black">最後に、最も広く使用されているデータセットで単一の音響モデルをトレーニングすると、組み合わせて、研究と実世界のベンチマークの両方で競争力のあるパフォーマンスが得られることを示します。音響モデリングの研究結果は、通常、単一のデータセットでのパフォーマンスに基づいて評価されます。 ..さらに、十分な数のベンチマークセットを使用すると、ベンチマークの平均ワードエラー率（WER）パフォーマンスが、実際のデータのパフォーマンスの優れたプロキシになることを示します。 
[要約]ベンチマークに関する調査結果は、通常、一般化に基づいて評価されます。調査によると、十分な数のベンチマークセットを使用すると、ベンチマークの平均ワードエラー率（wer）のパフォーマンスが、実際のデータセットでのパフォーマンスの優れたプロキシになります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Parallel Tacotron: Non-Autoregressive and Controllable TTS -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_8.html">
      <font color="black">Parallel Tacotron: Non-Autoregressive and Controllable TTS</font>
    </a>
  </h2>
  <font color="black">自然性をさらに向上させるために、ローカルコンテキストを効率的にキャプチャできる軽量の畳み込みを使用し、反復的な改良に触発された反復的なスペクトログラム損失を導入します。\ emph {Parallel Tacotron}と呼ばれるこのモデルは、トレーニングと推論の両方で高度に並列化できます。最新の並列ハードウェアでの効率的な合成を可能にします。ニューラルエンドツーエンドのテキストから音声へのモデルは非常に自然な音声を合成できますが、その効率と自然さにはまだ改善の余地があります。 
[概要]自己回帰ニューラルオートエンコーダーを使用すると、テキストの1つから多くのマッピングの性質が音声合成の問題に緩和されます。また、自然性が向上し、自然性が向上します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: NU-GAN: High resolution neural upsampling with GAN -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_9.html">
      <font color="black">NU-GAN: High resolution neural upsampling with GAN</font>
    </a>
  </h2>
  <font color="black">生成音声技術の生産には高いサンプリングレートでの動作が必要なため、オーディオのアップサンプリングは重要な問題です。ABX優先テストでは、NU-GANリサンプラーが22kHzから44.1kHzのオーディオをリサンプリングできることが示されています。これは、元のオーディオよりも7.4％高いだけです。シングルスピーカーデータセットのランダムチャンス、およびマルチスピーカーデータセットのチャンスより10.8％高い。このようなアプリケーションは、44.1kHzまたは48kHzの解像度でオーディオを使用しますが、現在の音声合成方法は、最大24kHzの解像度を処理するように装備されています。 
[概要] nu-ganは、テキスト内の個別のコンポーネントとしてオーディオのアップサンプリングを解決することに飛躍します-最大解像度。テクニックに加えて、nuganはgansを使用したオーディオ生成のテクニックを活用します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: The HUAWEI Speaker Diarisation System for the VoxCeleb Speaker
  Diarisation Challenge -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_10.html">
      <font color="black">The HUAWEI Speaker Diarisation System for the VoxCeleb Speaker
  Diarisation Challenge</font>
    </a>
  </h2>
  <font color="black">実験結果は、提案されたシステムがVoxCeleb話者認識チャレンジ2020のダイアリゼーションタスクのベースライン方法と比較して大幅な改善をもたらすことを示しています。次のダイアリゼーションシステムは、xベクトルの凝集型階層的クラスタリング（AHC）と変分ベイジアンに基づいて構築されています。隠れマルコフモデル（VB-HMM）ベースの反復クラスタリング..十分に訓練されたニューラルネットワークベースの音声強調モデルが前処理に使用され、ニューラルネットワークベースの音声アクティビティ検出（VAD）システムに従って、バックグラウンドミュージックとノイズが除去されます。話者ダイアリゼーションシステムに有害です。 
[概要]話者ダイアリゼーションシステムに有害なBGMやノイズを除去するための新しいシステムが採用されています。提案されたシステムは、voxceleb話者認識チャレンジ2020のダイアリゼーションタスクのベースライン方法と比較して大幅な改善をもたらします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: wav2vec 2.0: A Framework for Self-Supervised Learning of Speech
  Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_11.html">
      <font color="black">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech
  Representations</font>
    </a>
  </h2>
  <font color="black">wav2vec 2.0は、潜在空間での音声入力をマスクし、共同で学習される潜在表現の量子化で定義された対照的なタスクを解決します。ラベル付けされたデータの量を1時間に減らすと、wav2vec2.0は以前の最先端技術を上回ります。 100分の1のラベル付きデータを使用しながら、100時間のサブセット。音声音声のみから強力な表現を学習した後、転写された音声を微調整することで、概念的に単純でありながら、最良の半監視方式よりも優れたパフォーマンスを発揮できることを初めて示しました。 
[ABSTRACT] wav2vec 2. 0は、潜在空間での音声入力をマスクします。これは、3年前の350万ケースのラベル付きデータの量子化で定義された対照的なタスクを解決します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-20">
        <br><font color="black">2020-06-20</font>
      </time>
    </span>
</section>
<!-- paper0: Graph Attention Networks for Speaker Verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_12.html">
      <font color="black">Graph Attention Networks for Speaker Verification</font>
    </a>
  </h2>
  <font color="black">残りの接続を持ついくつかのグラフアテンションレイヤーの後、各ノードはアフィン変換を使用して1次元空間に投影され、続いて読み出し操作がスカラー類似性スコアになります。この作業は、を使用したスピーカー検証のための新しいバックエンドフレームワークを示します。グラフ注意ネットワーク..発話内の複数の作物から抽出されたセグメントごとのスピーカー埋め込みは、グラフのノード表現として解釈されます。 
[概要]カリフォルニア大学の研究者は、新しい方法を提案しました。彼らは、注意を引くためのグラフの使用に関連していると言います。これには、話者の埋め込みを異なる発話から分離することが含まれます。結果は、平均的な等しいエラー率の改善と一致しています。コサイン類似性バックエンドを20％上回っています-テスト時間の増加なしで終了</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Transformer based unsupervised pre-training for acoustic representation
  learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_13.html">
      <font color="black">Transformer based unsupervised pre-training for acoustic representation
  learning</font>
    </a>
  </h2>
  <font color="black">実験は、音声感情認識、音声イベント検出、音声翻訳の3種類の音響タスクで実施されました。音声感情認識のために、MuST-C、Librispeech、およびESC-USデータセットを組み合わせたより大きな事前トレーニングデータを使用して、UARはIEMOCAPデータセットをさらに4.3％改善します。最近、さまざまな音響タスクと関連アプリケーションが発生しました。 
[ABSTRACT]音響タスクには、音声感情認識、音声イベント検出、音声翻訳が含まれます。テストは、3種類の音響タスクで実施されました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-29">
        <br><font color="black">2020-07-29</font>
      </time>
    </span>
</section>
<!-- paper0: Momentum Contrast Speaker Representation Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_14.html">
      <font color="black">Momentum Contrast Speaker Representation Learning</font>
    </a>
  </h2>
  <font color="black">また、学習した表現の分布を分析することにより、音声領域での対照学習の特徴を経験的に示します。人間の監督なしで話者表現を学習することは、オープンセットの話者認識に対処するのに役立つと期待します。さらに、どの口実タスクが話者認証に適しています。 【概要】学習メカニズムの一形態として、voxceleb（mocovox）の運動量コントラストを提案する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: How Similar or Different Is Rakugo Speech Synthesizer to Professional
  Performers? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_15.html">
      <font color="black">How Similar or Different Is Rakugo Speech Synthesizer to Professional
  Performers?</font>
    </a>
  </h2>
  <font color="black">たとえば、自然性は、従来の音声合成分野で評価される最も重要なポイントとして一般的に強調されてきましたが、最も重要な要素ではありませんでした。また、聴衆をより楽しませるために、モデリングの基本周波数をさらに改善する必要があることもわかりました。 ..より重要な要素は、内容の理解度と落語物語の登場人物の識別可能性であり、どちらも合成された落語のスピーチはプロのパフォーマーと比較して比較的劣っていました。 
[概要]研究は国際大学によって実施されました。結果は、本物の面白い音声合成に到達するための重要なステップを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: A Qualitative Analysis of Haptic Feedback in Music Focused Exercises -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_16.html">
      <font color="black">A Qualitative Analysis of Haptic Feedback in Music Focused Exercises</font>
    </a>
  </h2>
  <font color="black">つまり、制約なしにさまざまなフィードバックタイプを調査および評価するための十分な時間が与えられました。創造性を伴う調査と同様に、参加者が急いでいる、または制限されていると感じないことが重要でした。このタイプの音楽指向の分析を適用する調査プロセスを包括的な期間にわたって行うだけでなく、日常の作曲の実践におけるDMI統合の調査を可能にしました。 
[ABSTRACT]ハプティックスはデジタル機器におけるハプティックスの役割であることがわかりました。ハプティックスは4つの別々のフィードバック段階を探索することができました。ハプティックスはデジタル機器（dmi）にとって最も効果的なツールです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: A Framework for Contrastive and Generative Learning of Audio
  Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_17.html">
      <font color="black">A Framework for Contrastive and Generative Learning of Audio
  Representations</font>
    </a>
  </h2>
  <font color="black">この論文では、グラウンドトゥルースラベルにアクセスすることなく、自己監視フレームワークで音声表現の対照学習のフレームワークを提示します。ここでは、音声信号をより小さなスケールで個別の辞書要素にマッピングし、トランスフォーマーをトレーニングして予測します。次の辞書要素..それらが互いに接近し、他の異なる信号から分離されているスペースに。 
[ABSTRACT]自作の対照学習は、音声信号とそのさまざまな拡張バージョンをマッピングすることです。システムは、完全に監視された方法と比較して、グラウンドトゥルースラベルにアクセスして、かなりのタスクを実行します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Perceptual Loss based Speech Denoising with an ensemble of Audio Pattern
  Recognition and Self-Supervised Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_18.html">
      <font color="black">Perceptual Loss based Speech Denoising with an ensemble of Audio Pattern
  Recognition and Self-Supervised Models</font>
    </a>
  </h2>
  <font color="black">知覚損失は、特定の音声特性への歪みを防ぎ、6つの大規模な事前トレーニング済みモデル（話者分類、音響モデル、話者埋め込み、感情分類、および2つの自己監視音声エンコーダー（PASE +、wav2vec 2.0））を使用して分析します。最初に、VCTK-DEMANDと呼ばれる人気のある拡張ベンチマークでConformer Transformer Networksを使用して強力なベースライン（PERLなし）を構築します。知覚損失の概念に基づいて構築された知覚アンサンブル正規化損失（PERL）と呼ばれる一般化されたフレームワークを紹介します。 
[概要]知覚損失の概念に基づいて構築された知覚アンサンブル正則化損失（perl）と呼ばれるフレームワークを紹介します。最初に、vctoisoisデマンドと呼ばれる人気のある拡張ベンチマークでコンフォーマートランスフォーマーネットワークを使用して強力なベースライン（w / o perl）を構築します。モデル（perl --ae）は、音響イベントモデル（audiosetを利用）のみを使用して、主要な知覚メトリックで最先端の方法を上回ります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: WHALETRANS: E2E WHisper to nAturaL spEech conversion using modified
  TRANSformer network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_19.html">
      <font color="black">WHALETRANS: E2E WHisper to nAturaL spEech conversion using modified
  TRANSformer network</font>
    </a>
  </h2>
  <font color="black">エンドツーエンドのASRを使用して単語誤り率を測定し、生成された音声のBLEUスコアを測定することにより、wTIMITおよびCHAINSデータセットの結果を示します。さらに、フォルマント分布wrtを測定することにより、そのスペクトル形状を測定します。変更されたトランスアーキテクチャを提案することにより、シーケンスからシーケンスへのアプローチを使用して、ささやきから自然な音声への変換を調査します。 
[概要]ささやきから自然に変換された音声フォルマントの確率分布は、地面に似ています-真理分布。これまでに、ささやきから自然に変換されたフォルマントが見つかりました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-20">
        <br><font color="black">2020-04-20</font>
      </time>
    </span>
</section>
<!-- paper0: Position-Agnostic Multi-Microphone Speech Dereverberation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_20.html">
      <font color="black">Position-Agnostic Multi-Microphone Speech Dereverberation</font>
    </a>
  </h2>
  <font color="black">このようなネットワークをトレーニングおよびテストするためのセットアップを提供します。この目的のために、私たちのアプローチは、ディープセットフレームワークの最近の進歩を利用して、残響ログスペクトルを強化するアーキテクチャを設計します。REVERBチャレンジデータセットを使用した実験では、提案された位置に依存しないセットアップは、位置認識フレームワークと同等に機能し、マイクの数が少なくてもわずかに優れている場合があります。 
[概要]私たちのアプローチは、ディープセットアプリの最近の進歩を利用して、残響ログスペクトルを強化するアーキテクチャを設計します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Dual-Signal Transformation LSTM Network for Real-Time Noise Suppression -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_21.html">
      <font color="black">Dual-Signal Transformation LSTM Network for Real-Time Noise Suppression</font>
    </a>
  </h2>
  <font color="black">これら2種類の信号変換を組み合わせることで、DTLNはマグニチュードスペクトルから情報を確実に抽出し、学習した特徴ベースから位相情報を組み込むことができます。このペーパーでは、 Deep Noise Suppression Challenge（DNS-Challenge）..このアプローチは、短時間フーリエ変換（STFT）と、100万未満のパラメーターを使用するスタックネットワークアプローチで学習した分析および合成の基礎を組み合わせたものです。 
[概要]この方法は、短時間セオドア変換（stft）と、100万未満のパラメーターを使用したスタックネットワークアプローチで学習した分析と合成の基礎を組み合わせたものです。ネットワークは、実際の処理（1フレームイン、1フレームアウト）そして競争力のある結果に達する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-15">
        <br><font color="black">2020-05-15</font>
      </time>
    </span>
</section>
<!-- paper0: Robust Audio-Based Vehicle Counting in Low-to-Moderate Traffic Flow -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_22.html">
      <font color="black">Robust Audio-Based Vehicle Counting in Low-to-Moderate Traffic Flow</font>
    </a>
  </h2>
  <font color="black">実験結果は、新しい高周波パワー機能を導入することにより、ノイズの多い環境での回帰精度が向上することを示しています。誤検出と誤検出の確率が一致するポイントに最小検出しきい値を設定して、統計的に互いに打ち消し合うことを提案します。この方法は、合計$ 1421 $の車両がマイクを通過する、$ 422 $の短い、$ 20 $秒の1チャネルサウンドファイルで構成されるトラフィック監視データセットでトレーニングおよびテストされます。 
[概要]この方法は、交通監視データセットでトレーニングおよびテストされています。これには、1421ドルの範囲が含まれます。マイクを通過する車両です。この方法は、新しい高周波電力機能によって開発されました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: The NTU-AISG Text-to-speech System for Blizzard Challenge 2020 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_23.html">
      <font color="black">The NTU-AISG Text-to-speech System for Blizzard Challenge 2020</font>
    </a>
  </h2>
  <font color="black">対応するマンダリン音節から分解された文字を入力として使用すると、合成音声の自然さと元の話者の類似性は良好ですが、主観的な評価結果は、合成音声の理解度が上海方言TTSシステムで大幅に損なわれていることを示しています。つまり、最初に外部のマンダリンデータを使用して、エンドツーエンドの音響モデルとWaveNetボコーダーの両方をトレーニングし、次に上海の方言を使用して、音響モデルとWaveNetボコーダーをそれぞれ調整します。制約を克服するために、平均スピーカーを採用します。モデリング方法。 
[概要]今年の課題は、リソースの制約が少ないttsシステムを構築することです。上海語の方言は、外部の北京語データを使用して、エンドツーエンドエンドツーエンドの音響モデルとウェーブネットボコーダーの両方をトレーニングします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Low-Resource StarGAN Voice Conversion using Weight Adaptive
  Instance Normalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_24.html">
      <font color="black">Towards Low-Resource StarGAN Voice Conversion using Weight Adaptive
  Instance Normalization</font>
    </a>
  </h2>
  <font color="black">実験は、トレーニングサンプルの数がスピーカーあたり20と5である、2つの低リソース状況で109のスピーカーを使用して実行されます。データ効率を向上させるために、提案されたモデルは、スピーカー埋め込みを抽出するためにスピーカーエンコーダーを使用し、適応インスタンス正規化を実行します。 （AdaIN）畳み込み重みについて..非並列トレーニングデータを使用した多対多の音声変換は、近年大きな進歩を遂げています。 
[概要]この作業では、モデルのデータ効率の向上を目指しています。実験は、2つの低リソース状況で109人のスピーカーを使用して実施されます。自然性と類似性の両方について、提案されたモデルはベースラインメソッドよりも優れています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Listening to 10 People Simultaneously: An Efficient Permutation
  Invariant Training of Audio Source Separation Using Sinkhorn's Algorithm -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_25.html">
      <font color="black">Towards Listening to 10 People Simultaneously: An Efficient Permutation
  Invariant Training of Audio Source Separation Using Sinkhorn's Algorithm</font>
    </a>
  </h2>
  <font color="black">$ N $が増加すると階乗の複雑さが非常に急速に爆発するため、PITベースのトレーニングは、ソース信号の数が少ない場合（$ N = 2 $や$ 3 $など）にのみ機能します。ただし、通常のPITではすべてを試す必要があります。 $ N $グラウンドトゥルースと$ N $推定値の間の$ N！$順列..SinkPITは、シンクホーンの行列バランシングアルゴリズムに基づいており、微分可能な方法で最良の順列を近似する二重確率行列を効率的に見つけます。 
[ABSTRACT]ライターは、シンクピットを使用して単一チャネル混合物を10個のソースに分解するニューラルネットワークモデルをトレーニングする実験を実施しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Analysis of the BUT Diarization System for VoxConverse Challenge -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_26.html">
      <font color="black">Analysis of the BUT Diarization System for VoxConverse Challenge</font>
    </a>
  </h2>
  <font color="black">各ステップの比較を提供し、システムの最も関連性の高いモジュールの実装を共有します。システムは、プライマリメトリック（ダイアリゼーションエラー率）に関してチャレンジで2番目にスコアを付け、セカンダリメトリック（ジャッカードエラー）に従って最初にスコアを付けました。このシステムは、信号の前処理、音声アクティビティの検出、話者の埋め込み抽出、初期の凝集型階層クラスタリングと、それに続くベイジアン隠れマルコフモデルを使用したダイアリゼーション、話者ごとのグローバル埋め込みと重複音声検出に基づく再クラスタリングステップで構成されます。と取り扱い。 
[概要]私たちのシステムは、一次メトリック（ダイアリゼーションエラー率）に関してチャレンジで2番目に、二次メトリックに従って最初にスコアを付けました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: AISHELL-3: A Multi-speaker Mandarin TTS Corpus and the Baselines -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_27.html">
      <font color="black">AISHELL-3: A Multi-speaker Mandarin TTS Corpus and the Baselines</font>
    </a>
  </h2>
  <font color="black">我々の実験からの客観的評価結果は、提案されたマルチスピーカー合成システムが、スピーカー埋め込み類似性と等しいエラーレート測定の両方に関して高い音声類似性を達成することを示しています。マルチスピーカーマダリン音声合成にAISHELL-3を使用するベースラインシステムを提示します。このデータセットでトレーニングされたシステムは、トレーニングプロセスでは決して見られない話者でも一般化されます。 
[概要]コーパスには約85時間の感情が含まれています-ニュートラルな録音。これらの録音は218人の中国語のネイティブスピーカーによって話されています。システムはタコトロンの拡張です-2</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Robust Text-Dependent Speaker Verification via Character-Level
  Information Preservation for the SdSV Challenge 2020 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_28.html">
      <font color="black">Robust Text-Dependent Speaker Verification via Character-Level
  Information Preservation for the SdSV Challenge 2020</font>
    </a>
  </h2>
  <font color="black">この問題を軽減するために、語彙コンテンツを考慮に入れるためにCTCベースの自動音声認識（ASR）モデルを活用する新しいプーリングおよびスコア補正方法を提案します。提出されたシステムはTDNNベースとResNetベースのフロントで構成されていました。 -フレームレベルの機能がさまざまなプーリング方法（統計、自己注意、ghostVLADプーリングなど）で集約されたエンドアーキテクチャ。どちらの方法も従来の手法よりも改善されており、すべてを融合することで最高のパフォーマンスが達成されました。実験されたシステムでは、チャレンジの評価サブセットで0.0785％のMinDCFと2.23％のEERが示されました。 
[ABSTRACT]タスク1は、テキストに依存する話者検証タスクです。これは、text --to-33333333 text --33333333333333333333333textに基づいています。このメソッドはテストシステムのテスターによって開発されました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Neural Network-based Acoustic Vehicle Counting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_29.html">
      <font color="black">Neural Network-based Acoustic Vehicle Counting</font>
    </a>
  </h2>
  <font color="black">最小値ベースのカウントに加えて、極小値を検出せずに予測距離で動作する深層学習カウントを提案します。実験によると、NNベースの距離回帰は以前に提案されたサポートベクター回帰よりもはるかに優れています。$ 95 \％車両カウントエラーの平均の$信頼区間は$ 
[0.28 \％、-0.55 \％] $以内です。 
[要約]結果は、nnベースの距離エラーが以前に提案されたサポート利他的をはるかに上回っていることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Noise Robust TTS for Low Resource Speakers using Pre-trained Model and
  Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_30.html">
      <font color="black">Noise Robust TTS for Low Resource Speakers using Pre-trained Model and
  Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">実験結果は、提案されたアプローチによって生成された音声が、ノイズ除去された新しい話者データを使用して事前訓練されたマルチ話者音声合成モデルを直接微調整する方法よりも優れた主観的評価結果を有することを示している。この論文では、提案されたエンドツーエンド。音声合成モデルは、話者とノイズ情報をそれぞれモデル化するための条件付き入力として、話者の埋め込みとノイズ表現の両方を使用します。堅牢な音声合成モデルは、多くの収集作業を必要とする高品質でカスタマイズされたデータに依存します。 
[概要]音声合成では、低品質で低リソースの音声データを使用します。モデルは、クリーンなデータとノイズの多い拡張データの両方で事前トレーニングされています。このモデルは、新しいスピーカーのクリーンな音声を合成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-26">
        <br><font color="black">2020-05-26</font>
      </time>
    </span>
</section>
<!-- paper0: Urban Sound Classification : striving towards a fair comparison -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_31.html">
      <font color="black">Urban Sound Classification : striving towards a fair comparison</font>
    </a>
  </h2>
  <font color="black">このフレームワークがこの分野の新しいアーキテクチャの評価に役立つことを願っています。次に、公正な比較を見つけて既存のモデルのパフォーマンスを再現することは容易ではありません。その結果、同じ入力表現を使用して公正な比較を提供します。 、メトリクス、およびパフォーマンスを評価するためのオプティマイザー。 
[概要]都市の騒音公害は大都市で懸念が高まっています。これにより、都市では依然として懸念されている騒音公害を監視できます。コードはgithubリポジトリで入手でき、再現性が向上しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Similarity Analysis of Self-Supervised Speech Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_32.html">
      <font color="black">Similarity Analysis of Self-Supervised Speech Representations</font>
    </a>
  </h2>
  <font color="black">同じ入力が与えられた場合にさまざまな自己教師ありモデルがどのように異なる動作をするかを示すことに加えて、私たちの研究では、トレーニングの目的が、ビルディングブロック（RNN /トランスフォーマー/ CNN）や方向性（uni / bidirection）..具体的には、既存の類似性尺度を使用して、異なる自己監視表現間の類似性を定量化します。ただし、既存のアプローチの特性を理解することに焦点を当てた研究はほとんどありません。 
[概要]有用な表現を学習するために多くのアルゴリズムが提案されています。これらには、幅広い音声タスクへの広範なアプリケーションが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: CycleGAN-VC3: Examining and Improving CycleGAN-VCs for Mel-spectrogram
  Conversion -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_33.html">
      <font color="black">CycleGAN-VC3: Examining and Improving CycleGAN-VCs for Mel-spectrogram
  Conversion</font>
    </a>
  </h2>
  <font color="black">自然性と類似性の主観的評価では、すべてのVCペアで、CycleGAN-VC3が2種類のCycleGAN-VC2よりも優れているか、競合していることが示されました。一方はメルケプストラムに適用され、もう一方はメルスペクトログラムに適用されました。メルスペクトログラム変換に対するCycleGAN-VC / VC2の有効性はあいまいであるため、比較方法でメルスペクトログラムを変換ターゲットとして使用する場合でも、通常はメルケプストラム変換に使用されます。最近、サイクル整合性のある敵対ネットワーク（ CycleGAN）-VCおよびCycleGAN-VC2は、この問題に関して有望な結果を示しており、ベンチマーク手法として広く使用されています。 
[概要] cycle-一貫した敵対的ネットワーク（cyclegan）とcyclegan-vc2は、ベンチマーク手法として広く使用されています。それらの問題に対処するために、cyclegan-vc / vc2からmel-スペクトログラムへの変換の適用性を調べました。サイクルガン-時間の過度の使用-周波数適応正規化（tfan）</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Representation Learning for Speaker Recognition via
  Contrastive Equilibrium Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_34.html">
      <font color="black">Unsupervised Representation Learning for Speaker Recognition via
  Contrastive Equilibrium Learning</font>
    </a>
  </h2>
  <font color="black">実験結果は、提案されたCELが最先端の教師なし話者検証システムを大幅に上回り、最高のパフォーマンスのモデルがVoxCeleb1とVOiCESの評価セットでそれぞれ8.01％と4.01％のEERを達成したことを示しました。 CELを介して事前に訓練された初期パラメータで訓練された監視付き話者埋め込みネットワークのいくつかは、ランダムに初期化されたパラメータで訓練されたものよりも優れたパフォーマンスを示しました。 ）、均一性の損失を採用することにより、埋め込みに潜む厄介な要因の不確実性を高めます。 
[要約]研究は、話者の識別可能性を維持する必要があることを示しています。また、対照的な類似性損失関数が一緒に使用されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Prediction of Object Geometry from Acoustic Scattering Using
  Convolutional Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_35.html">
      <font color="black">Prediction of Object Geometry from Acoustic Scattering Using
  Convolutional Neural Networks</font>
    </a>
  </h2>
  <font color="black">現在の作業では、モデルから作成された予測がグラウンドトゥルースと高精度で一致することがわかりました。さらに、使用するデータチャネルが少なくても解像度が低くても精度は低下しません。トレーニングデータは、で開発された高速数値ソルバーから生成されます。 CUDA。 
[概要]本研究では、散乱特徴からオブジェクトの彫刻を推測する方法を提案します。シミュレーションの完全なセットをサンプリングして、複数のデータセットを生成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-21">
        <br><font color="black">2020-10-21</font>
      </time>
    </span>
</section>
<!-- paper0: Self-training and Pre-training are Complementary for Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_36.html">
      <font color="black">Self-training and Pre-training are Complementary for Speech Recognition</font>
    </a>
  </h2>
  <font color="black">Librispeechのすべてのラベル付きデータのトレーニングは1.5％/ 3.1％のWERを達成します。Libri-lightからのわずか10分のラベル付きデータとLibriVoxからの53k時間のラベルなしデータを使用すると、クリーンで3.0％/ 5.2％のWERが達成されます。 Librispeechの他のテストセット-わずか1年前に960時間のラベル付きデータでトレーニングされた最高の公開システムに匹敵します。このペーパーでは、wav2vec2.0による疑似ラベル付けと事前トレーニングがさまざまなラベル付きデータ設定で補完的であることを示します。 
[概要] libriからのわずか10分のラベル付きデータの使用-軽いだけでなく、53k時間のラベルなしデータも効果的です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Neural Kalman Filtering for Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_37.html">
      <font color="black">Neural Kalman Filtering for Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">2つの中間クリーンスピーチ推定値は、最初にリカレントニューラルネットワーク（RNN）と線形ウィーナーフィルタリング（WF）から別々に生成され、次に学習されたNKFゲインによって線形結合されてNKF出力を生成します。NKFメソッドは専門家の知識を使用していると見なすことができます。 WFからRNN推定を正規化して、トレーニングでは見られないノイズ条件への一般化能力を向上させます。統計信号処理ベースの音声強調方法は、専門家の知識を採用して、ディープニューラルネットワークを補完する統計モデルと線形フィルターを設計します（ DNN）データ駆動型のメソッド。 
[要約]提案された方法は、音声強調のための新しいシステムの構築を支援するために使用できます。また、ニューラルカルマンフィルター処理（nkf）にも使用できます。また、ニューラルカルマンフィルター処理（nkf&#39;s）でも採用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-28">
        <br><font color="black">2020-07-28</font>
      </time>
    </span>
</section>
<!-- paper0: Muse: Multi-modal target speaker extraction with visual cues -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_38.html">
      <font color="black">Muse: Multi-modal target speaker extraction with visual cues</font>
    </a>
  </h2>
  <font color="black">唇の画像シーケンスのみを条件とするMuSEという名前のマルチモーダルスピーカー抽出ネットワークを提案します。スピーカー抽出アルゴリズムは、ターゲットスピーカーからの音声サンプルを参照ポイントとして使用して注意を集中します。MuSEはパフォーマンスが優れているだけではありません。 SI-SDRおよびPESQに関する他の競合ベースラインですが、クロスドメイン評価でも優れた堅牢性を示しています。 
[要約]スピーチは通常、税引前です。平均して、それは通常、事前に記録されています..museは、si-sdrおよびpesqに関して他の競合ベースラインよりも優れていますが、優れたターゲットポイントも示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-15">
        <br><font color="black">2020-10-15</font>
      </time>
    </span>
</section>
<!-- paper0: Compositional embedding models for speaker identification and
  diarization with simultaneous speech from 2+ speakers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_39.html">
      <font color="black">Compositional embedding models for speaker identification and
  diarization with simultaneous speech from 2+ speakers</font>
    </a>
  </h2>
  <font color="black">AMIヘッドセットミックスコーパスでのスピーカーダイアリゼーション実験では、最新の精度（DER = 22.93％）を達成し、以前の最良の結果（
[3]から23.82％）よりもわずかに高くなっています。入力音声内の話者のセットを推測するために、埋め込みスペースでの集合和集合演算を計算する合成関数gを含めます。合成されたLibriSpeechデータを使用した複数人の話者識別の実験では、提案された方法は従来の埋め込み方法よりも優れています。 （スピーカーセットではなく）単一のスピーカーを分離するようにトレーニングされているだけです。 
[要約]提案された方法は、構成的埋め込みに基づいています。それは、異なる話者からの音声を分離する関数fを含みます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: BERT for Joint Multichannel Speech Dereverberation with Spatial-aware
  Tasks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_40.html">
      <font color="black">BERT for Joint Multichannel Speech Dereverberation with Spatial-aware
  Tasks</font>
    </a>
  </h2>
  <font color="black">マルチチャネルスペクトルの大きさとさまざまな長さの発話のスペクトル位相情報の両方がエンコードされます。実験結果は、提案された方法の有効性を示しています。提案された方法は、シーケンスからシーケンスへのマッピング問題として関連するタスクに対処します。 -音声強調タスクを終了します。 
[要約]提案された方法は、シーケンスからシーケンスへのマッピング問題として関連するタスクに対処します。これは、さまざまなフロントエンド音声強調タスクに十分一般的です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-21">
        <br><font color="black">2020-10-21</font>
      </time>
    </span>
</section>
<!-- paper0: LaSAFT: Latent Source Attentive Frequency Transformation for Conditioned
  Source Separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.SD/paper_41.html">
      <font color="black">LaSAFT: Latent Source Attentive Frequency Transformation for Conditioned
  Source Separation</font>
    </a>
  </h2>
  <font color="black">ソース依存の周波数パターンをキャプチャするために、潜在ソース注意周波数変換（LaSAFT）ブロックを提案します。これら2つの新しい方法を採用することにより、マルチソース分離のために条件付きU-Net（CUNet）を拡張し、実験結果はLaSAFTとGPoCMがCUNetのパフォーマンスを向上させ、いくつかのMUSDB18ソース分離タスクで最先端のSDRパフォーマンスを実現できること。また、機能ごとの線形の拡張であるゲート付きポイントごとの畳み込み変調（GPoCM）も提案します。内部機能を変調するための変調（FiLM）。 
[概要]このホワイトペーパーの目的は、ftブロックを拡張してマルチソースタスクに適合させることです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Deep Learning for Distinguishing Normal versus Abnormal Chest
  Radiographs and Generalization to Unseen Diseases -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.IV/paper_0.html">
      <font color="black">Deep Learning for Distinguishing Normal versus Abnormal Chest
  Radiographs and Generalization to Unseen Diseases</font>
    </a>
  </h2>
  <font color="black">
[概要] cxrの発見は、いくつかのaiシステムの主な焦点となっています。これらの結果は、aiを安全に使用して、これまでに見られなかった異常が存在するケースにフラグを立てることができるかどうかを評価するための重要なステップです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: DeepCSR: A 3D Deep Learning Approach for Cortical Surface Reconstruction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.IV/paper_1.html">
      <font color="black">DeepCSR: A 3D Deep Learning Approach for Cortical Surface Reconstruction</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT] deepcsrは、画像をキャプチャする高解像度で皮質表面を効率的に再構築および分析します。deepcsrはまた、ハイパーカラム機能を使用して、脳テンプレート空間内のポイントの詳細な表面表現を予測します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Progressive Multi-Scale Residual Network for Single Image
  Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.IV/paper_2.html">
      <font color="black">Progressive Multi-Scale Residual Network for Single Image
  Super-Resolution</font>
    </a>
  </h2>
  <font color="black">
[概要]この論文は、制限されたパラメータで特徴を修正することにより、単一画像の超解像問題のためのプログレッシブマルチスケール残余ネットワーク（pmrn）を提案します。特徴活用の組み合わせは、非線形性を導入するために再帰的に定義されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-19">
        <br><font color="black">2020-07-19</font>
      </time>
    </span>
</section>
<!-- paper0: AMPA-Net: Optimization-Inspired Attention Neural Network for Deep
  Compressed Sensing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.IV/paper_3.html">
      <font color="black">AMPA-Net: Optimization-Inspired Attention Neural Network for Deep
  Compressed Sensing</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT] amp-netとampa-netは、4つの標準的なcs再構成ベンチマークデータセットにあります。ネットワークは、amp-ampの頭脳の発案-新しいシステムと呼ばれます。これは初めてのamp-amp-です。限られたシステムを使用することができました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-14">
        <br><font color="black">2020-10-14</font>
      </time>
    </span>
</section>
<!-- paper0: Fourier Spectrum Discrepancies in Deep Network Generated Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.IV/paper_4.html">
      <font color="black">Fourier Spectrum Discrepancies in Deep Network Generated Images</font>
    </a>
  </h2>
  <font color="black">
[要約]調査によると、ネットワークで生成された深い画像は、観察可能な体系的な欠点を共有しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-15">
        <br><font color="black">2019-11-15</font>
      </time>
    </span>
</section>
<!-- paper0: DeepGalaxy: Deducing the Properties of Galaxy Mergers from Images Using
  Deep Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.IV/paper_5.html">
      <font color="black">DeepGalaxy: Deducing the Properties of Galaxy Mergers from Images Using
  Deep Neural Networks</font>
    </a>
  </h2>
  <font color="black">DeepGalaxyは、3D潜在空間でアクティベーションマップを生成する完全畳み込みオートエンコーダー（FCAE）、アクティベーションマップを1Dベクトルに圧縮する変分オートエンコーダー（VAE）、およびアクティベーションマップからラベルを生成する分類器で構成されます。エンコーダ-デコーダアーキテクチャに基づいて、DeepGalaxyは入力画像を圧縮された潜在空間$ z $にエンコードし、潜在空間の距離に従って画像の類似性を決定します。2つの銀河が衝突する動的プロセスである銀河合併は次のとおりです。宇宙で最も壮観な現象の1つです。 
[ABSTRACT] deepgalaxyは、銀河の合体の物理的特性を予測するようにトレーニングされた視覚分析フレームワークです。3D潜在空間でアクティベーションマップを生成する完全畳み込みオートエンコーダー（fcae）と変分オートエンコーダー（vae）で構成されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Convolutional Neural Network and decision support in medical imaging:
  case study of the recognition of blood cell subtypes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.IV/paper_6.html">
      <font color="black">Convolutional Neural Network and decision support in medical imaging:
  case study of the recognition of blood cell subtypes</font>
    </a>
  </h2>
  <font color="black">
[概要]研究者は、特定のタイプの深層学習、つまり、4つの血球タイプ（好中球、好酸球、リンパ球、単球）の画像認識に畳み込みニューラルネットワーク（cnnsまたはconvnets）を使用しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-19">
        <br><font color="black">2019-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Effective training of deep convolutional neural networks for
  hyperspectral image classification through artificial labeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.IV/paper_7.html">
      <font color="black">Effective training of deep convolutional neural networks for
  hyperspectral image classification through artificial labeling</font>
    </a>
  </h2>
  <font color="black">
[概要]転移学習アプローチは、特定のデータセットの2番目の要件を緩和するために使用できます。特定の画像タイプやニューラルネットワークアーキテクチャに制限されることなく、分類精度を向上させるのに非常に効果的です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-12">
        <br><font color="black">2019-09-12</font>
      </time>
    </span>
</section>
<!-- paper0: Iterative Network for Image Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.IV/paper_8.html">
      <font color="black">Iterative Network for Image Super-Resolution</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT] cnnベースのメソッドは、一般に、低解像度の画像を、高度なネットワーク構造と損失関数を備えた対応する高解像度バージョンにマッピングします。これらのメソッドメソッドメソッドは、ネットワーク構造と損失関数を組み合わせて、印象的なパフォーマンスを示します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-20">
        <br><font color="black">2020-05-20</font>
      </time>
    </span>
</section>
<!-- paper0: COVID-19-CT-CXR: a freely accessible and weakly labeled chest X-ray and
  CT image collection on COVID-19 from biomedical literature -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.IV/paper_9.html">
      <font color="black">COVID-19-CT-CXR: a freely accessible and weakly labeled chest X-ray and
  CT image collection on COVID-19 from biomedical literature</font>
    </a>
  </h2>
  <font color="black">
[概要]データセット、コード、およびdlモデルはwwwで公開されています。インフルエンザ.covid-19-ct-cxrおよびct画像は、pubmed中央オープンアクセス（pmc-oa）サブセットから自動的に抽出されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-11">
        <br><font color="black">2020-06-11</font>
      </time>
    </span>
</section>
<!-- paper0: Correlation Filter for UAV-Based Aerial Tracking: A Review and
  Experimental Evaluation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.IV/paper_10.html">
      <font color="black">Correlation Filter for UAV-Based Aerial Tracking: A Review and
  Experimental Evaluation</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]風の状態、UAVの機械構造の振動、限られた計算リソースに基づいて、オンボードの追跡方法はすべて重要です。これは、20の最先端のdcfベースのトラッカーが整然としていることに基づいています。彼らの革新によると</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-13">
        <br><font color="black">2020-10-13</font>
      </time>
    </span>
</section>
<!-- paper0: Malaria detection from RBC images using shallow Convolutional Neural
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.IV/paper_11.html">
      <font color="black">Malaria detection from RBC images using shallow Convolutional Neural
  Networks</font>
    </a>
  </h2>
  <font color="black">
[概要]これは、アルゴリズムの商用展開に大きな利点をもたらす可能性があります。これは、アフリカの貧しい国やインド亜大陸の一部で使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Segmentation of B-Mode Echocardiograms -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.IV/paper_12.html">
      <font color="black">Unsupervised Segmentation of B-Mode Echocardiograms</font>
    </a>
  </h2>
  <font color="black">
[要約]この方法は、「プロミネンス反復ダイクストラ」アルゴリズムまたはproidと呼ばれます。66人の小児患者の臨床コホートを分析するために使用されました。proidは、手動セグメンテーションと比較した場合、平均ダイク類似度スコア0.93を維持しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: The Detection of Thoracic Abnormalities ChestX-Det10 Challenge Results -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.IV/paper_13.html">
      <font color="black">The Detection of Thoracic Abnormalities ChestX-Det10 Challenge Results</font>
    </a>
  </h2>
  <font color="black">
[概要]チャレンジは2つのラウンドに分かれています。これはdeepwiseailabによって手配されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: Muse: Multi-modal target speaker extraction with visual cues -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.IV/paper_14.html">
      <font color="black">Muse: Multi-modal target speaker extraction with visual cues</font>
    </a>
  </h2>
  <font color="black">
[要約]スピーチは通常、税引前です。平均して、それは通常、事前に記録されています..museは、si-sdrおよびpesqに関して他の競合ベースラインよりも優れていますが、優れたターゲットポイントも示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-15">
        <br><font color="black">2020-10-15</font>
      </time>
    </span>
</section>
<!-- paper0: QISTA-Net: DNN Architecture to Solve $\ell_q$-norm Minimization Problem
  and Image Compressed Sensing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.IV/paper_15.html">
      <font color="black">QISTA-Net: DNN Architecture to Solve $\ell_q$-norm Minimization Problem
  and Image Compressed Sensing</font>
    </a>
  </h2>
  <font color="black">
[概要]新しい方法は、高密度信号再構成問題を解決するためにqista --net --s-と呼ばれます。これは、qista --sと呼ばれるネットワークアーキテクチャに基づいています。これは、再構成のパフォーマンスが依然としてほとんどの状態を上回っていることを意味します。アート自然画像再構成方法</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: GAN based Unsupervised Segmentation: Should We Match the Exact Number of
  Objects -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.IV/paper_16.html">
      <font color="black">GAN based Unsupervised Segmentation: Should We Match the Exact Number of
  Objects</font>
    </a>
  </h2>
  <font color="black">
[概要]簡単なアイデアは、教師なし合成問題として教師あり厳密化タスクにアプローチすることです。調査によると、強度画像はサイクルを使用して注釈ドメインに転送できます-一貫した敵対的学習</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Uncovering the Topology of Time-Varying fMRI Data using Cubical
  Persistence -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.IV/paper_17.html">
      <font color="black">Uncovering the Topology of Time-Varying fMRI Data using Cubical
  Persistence</font>
    </a>
  </h2>
  <font color="black">
[概要]これは、fmri測定からのデータに基づいています。これには、fmriデータセットの各時点をエンコードする方法を見つけることが含まれます。この表現は、当然、ボクセルごとのボクセル対応に依存しません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-14">
        <br><font color="black">2020-06-14</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: F-Siamese Tracker: A Frustum-based Double Siamese Network for 3D Single
  Object Tracking -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_0.html">
      <font color="black">F-Siamese Tracker: A Frustum-based Double Siamese Network for 3D Single
  Object Tracking</font>
    </a>
  </h2>
  <font color="black">
[概要] 3D単一オブジェクト追跡では、3D錐台でオンライン精度検証を実行します。これにより、既存の3D追跡バックボーンに直接埋め込むことができる洗練された点群検索空間が生成されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Efficient Scale-Permuted Backbone with Learned Resource Distribution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_1.html">
      <font color="black">Efficient Scale-Permuted Backbone with Learned Resource Distribution</font>
    </a>
  </h2>
  <font color="black">
[概要]スケールメリット-順列モデルは、ネットワーク全体にわたるリソース分散を学習することでさらに改善できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: HRFA: High-Resolution Feature-based Attack -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_2.html">
      <font color="black">HRFA: High-Resolution Feature-based Attack</font>
    </a>
  </h2>
  <font color="black">
[概要]実験では、オブジェクトの分類と顔の検証タスクをbigganとstyleganで攻撃することにより、hrfaの有効性を検証します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-21">
        <br><font color="black">2020-01-21</font>
      </time>
    </span>
</section>
<!-- paper0: Once-for-All Adversarial Training: In-Situ Tradeoff between Robustness
  and Accuracy for Free -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_3.html">
      <font color="black">Once-for-All Adversarial Training: In-Situ Tradeoff between Robustness
  and Accuracy for Free</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]精度の高いトレーニング（オート麦）は、革新的なモデル（条件付きトレーニングフレームワーク）に基づいて構築されています。トレーニングされたモデルは、パフォーマンスを低下させることなく1つのモデルで学習できます。再トレーニングやアンサンブルなしで、オート麦は同様またはさらに優れています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: A Wigner-Eckart Theorem for Group Equivariant Convolution Kernels -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_4.html">
      <font color="black">A Wigner-Eckart Theorem for Group Equivariant Convolution Kernels</font>
    </a>
  </h2>
  <font color="black">
[概要] g-操作可能なカーネルはgcnnsで動作することができました。これらのモデルは、一般にg-操作可能なカーネルで畳み込みを実行するものとして理解できます。これらの作業は、gが任意のコンパクトグループであるという実際に関連するケースにそのような特性を提供します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-21">
        <br><font color="black">2020-10-21</font>
      </time>
    </span>
</section>
<!-- paper0: SOLOv2: Dynamic, Faster and Stronger -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_5.html">
      <font color="black">SOLOv2: Dynamic, Faster and Stronger</font>
    </a>
  </h2>
  <font color="black">
[概要]単純なインスタンスセグメンテーションシステムを示し、速度と精度の両方で最先端の方法でいくつかのマトリックスを上回りました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-23">
        <br><font color="black">2020-03-23</font>
      </time>
    </span>
</section>
<!-- paper0: Class-Conditional Defense GAN Against End-to-End Speech Attacks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_6.html">
      <font color="black">Class-Conditional Defense GAN Against End-to-End Speech Attacks</font>
    </a>
  </h2>
  <font color="black">
[概要]特定の入力信号を自動エンコードする代わりに、提案されたアプローチは機能しません。代わりに、1d信号と元の位相情報をテストします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning for Distinguishing Normal versus Abnormal Chest
  Radiographs and Generalization to Unseen Diseases -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_7.html">
      <font color="black">Deep Learning for Distinguishing Normal versus Abnormal Chest
  Radiographs and Generalization to Unseen Diseases</font>
    </a>
  </h2>
  <font color="black">
[概要] cxrの発見は、いくつかのaiシステムの主な焦点となっています。これらの結果は、aiを安全に使用して、これまでに見られなかった異常が存在するケースにフラグを立てることができるかどうかを評価するための重要なステップです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention
  and Spatial Memory -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_8.html">
      <font color="black">Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention
  and Spatial Memory</font>
    </a>
  </h2>
  <font color="black">
[概要]ナビゲーションシステムはグーグルストリートビューに基づいています。それは口頭のナビゲーション指示に基づいて視覚的なガイダンスを提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-04">
        <br><font color="black">2019-10-04</font>
      </time>
    </span>
</section>
<!-- paper0: Self-Supervised Shadow Removal -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_9.html">
      <font color="black">Self-Supervised Shadow Removal</font>
    </a>
  </h2>
  <font color="black">
[概要]シャドウとシャドウのペアを必要としません-無料の画像、代わりに自己監視に依存し、画像にシャドウを削除および追加するための深いモデルを共同で学習します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Future Urban Scenes Generation Through Vehicles Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_10.html">
      <font color="black">Future Urban Scenes Generation Through Vehicles Synthesis</font>
    </a>
  </h2>
  <font color="black">
[概要]オブジェクトごとの新しいビュー合成アプローチを活用します。同じ入力からマルチモーダルな方法で、多様で現実的な未来のセットを作成することを想像できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br><font color="black">2020-07-01</font>
      </time>
    </span>
</section>
<!-- paper0: Color Visual Illusions: A Statistics-based Computational Model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_11.html">
      <font color="black">Color Visual Illusions: A Statistics-based Computational Model</font>
    </a>
  </h2>
  <font color="black">
[概要]学習するデータセットが大きい場合にパッチの可能性を計算するツールを紹介します。このツールは、同じツールを逆に適用することで、自然画像に目の錯覚を生成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br><font color="black">2020-05-18</font>
      </time>
    </span>
</section>
<!-- paper0: StyleGAN2 Distillation for Feed-forward Image Manipulation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_12.html">
      <font color="black">StyleGAN2 Distillation for Feed-forward Image Manipulation</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]潜在コード拡張は一般的に画像の埋め込みに使用されます。結果のパイプラインは既存のガンの代替であり、ペアになっていないデータでトレーニングされ、私たちの方法を使用した生成の品質がstylegan2バックプロパゲーションに匹敵することを示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-07">
        <br><font color="black">2020-03-07</font>
      </time>
    </span>
</section>
<!-- paper0: DeepSVG: A Hierarchical Generative Network for Vector Graphics Animation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_13.html">
      <font color="black">DeepSVG: A Hierarchical Generative Network for Vector Graphics Animation</font>
    </a>
  </h2>
  <font color="black">
[概要]私たちのアーキテクチャは、形状自体をエンコードする低レベルのコマンドから高レベルの形状を効果的に解きほぐします。新しい大規模なデータセットをリリースすることにより、複雑なsvgアイコン生成のタスクを紹介します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-22">
        <br><font color="black">2020-07-22</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Example Games -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_14.html">
      <font color="black">Adversarial Example Games</font>
    </a>
  </h2>
  <font color="black">
[要約]この作業では、仮説クラス全体に転送可能な敵対的な例を作成するための理論的基盤を提供します。aegは、理論クラスにジェネレーターと分類子をトレーニングすることにより、敵対的な例を作成する新しい方法を提供します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br><font color="black">2020-07-01</font>
      </time>
    </span>
</section>
<!-- paper0: DeepCSR: A 3D Deep Learning Approach for Cortical Surface Reconstruction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_15.html">
      <font color="black">DeepCSR: A 3D Deep Learning Approach for Cortical Surface Reconstruction</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT] deepcsrは、画像をキャプチャする高解像度で皮質表面を効率的に再構築および分析します。deepcsrはまた、ハイパーカラム機能を使用して、脳テンプレート空間内のポイントの詳細な表面表現を予測します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Homography Estimation with Convolutional Neural Networks Under
  Conditions of Variance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_16.html">
      <font color="black">Homography Estimation with Convolutional Neural Networks Under
  Conditions of Variance</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT] cnnは、ノイズに対してより堅牢になるようにトレーニングできますが、ノイズのない場合の精度はわずかです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-02">
        <br><font color="black">2020-10-02</font>
      </time>
    </span>
</section>
<!-- paper0: Progressive Multi-Scale Residual Network for Single Image
  Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_17.html">
      <font color="black">Progressive Multi-Scale Residual Network for Single Image
  Super-Resolution</font>
    </a>
  </h2>
  <font color="black">
[概要]この論文は、制限されたパラメータで特徴を修正することにより、単一画像の超解像問題のためのプログレッシブマルチスケール残余ネットワーク（pmrn）を提案します。特徴活用の組み合わせは、非線形性を導入するために再帰的に定義されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-19">
        <br><font color="black">2020-07-19</font>
      </time>
    </span>
</section>
<!-- paper0: Rotated Binary Neural Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_18.html">
      <font color="black">Rotated Binary Neural Network</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]バイナリモデルはgithubで入手できます。ノルムギャップの補正以前の作品は、角度バイアスにほとんど触れずにギャップを補正することに焦点を当てていました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-28">
        <br><font color="black">2020-09-28</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Self-training Algorithm Based on Deep Learning for Optical
  Aerial Images Change Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_19.html">
      <font color="black">Unsupervised Self-training Algorithm Based on Deep Learning for Optical
  Aerial Images Change Detection</font>
    </a>
  </h2>
  <font color="black">
[概要]最終的な変更検出結果は、訓練を受けた学生ネットワークによって取得できます。アルゴリズムのプロセス全体は、手動でマークされたラベルのない教師なしプロセスです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-15">
        <br><font color="black">2020-10-15</font>
      </time>
    </span>
</section>
<!-- paper0: DiffGCN: Graph Convolutional Networks via Differential Operators and
  Algebraic Multigrid Pooling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_20.html">
      <font color="black">DiffGCN: Graph Convolutional Networks via Differential Operators and
  Algebraic Multigrid Pooling</font>
    </a>
  </h2>
  <font color="black">
[概要]この作業では、グラフの畳み込み、プーリング、およびアンレッティングの新しいアプローチを提案します。これらは、有限差分とプラザマルチグリッドフレームワークから着想を得ています。これらの方法は、標準の畳み込みニューラルネットワークと比較されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-07">
        <br><font color="black">2020-06-07</font>
      </time>
    </span>
</section>
<!-- paper0: Defense-guided Transferable Adversarial Attacks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_21.html">
      <font color="black">Defense-guided Transferable Adversarial Attacks</font>
    </a>
  </h2>
  <font color="black">
[概要]これらのタイプは未知のモデルへの転送が困難です。これらには攻撃が困難な敵対的な例が含まれます。これらのタイプは転送性が低く提案されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: AEGIS: A real-time multimodal augmented reality computer vision based
  system to assist facial expression recognition for individuals with autism
  spectrum disorder -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_22.html">
      <font color="black">AEGIS: A real-time multimodal augmented reality computer vision based
  system to assist facial expression recognition for individuals with autism
  spectrum disorder</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]拡張現実システムは、コンピュータービジョンと深い畳み込みニューラルネットワーク（cnn）の使用を組み合わせたものです。システムはリアルタイムで実行され、最小限のセットアップで簡単に使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Rethinking pooling in graph neural networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_23.html">
      <font color="black">Rethinking pooling in graph neural networks</font>
    </a>
  </h2>
  <font color="black">
[概要]ほとんどのアプローチは、グラフプーリングをクラスター割り当て問題として定式化します。gnnsを制限する代わりに、畳み込み層とそれに続くプーリング層の相互作用を研究します。一般的な信念とは対照的に、ローカルプーリングはgnnsの成功に責任を負いません。関連性があり広く使用されているベンチマーク</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: AMPA-Net: Optimization-Inspired Attention Neural Network for Deep
  Compressed Sensing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_24.html">
      <font color="black">AMPA-Net: Optimization-Inspired Attention Neural Network for Deep
  Compressed Sensing</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT] amp-netとampa-netは、4つの標準的なcs再構成ベンチマークデータセットにあります。ネットワークは、amp-ampの頭脳の発案-新しいシステムと呼ばれます。これは初めてのamp-amp-です。限られたシステムを使用することができました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-14">
        <br><font color="black">2020-10-14</font>
      </time>
    </span>
</section>
<!-- paper0: PLOP: Probabilistic poLynomial Objects trajectory Planning for
  autonomous driving -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_25.html">
      <font color="black">PLOP: Probabilistic poLynomial Objects trajectory Planning for
  autonomous driving</font>
    </a>
  </h2>
  <font color="black">
[概要]エゴビークルのナビゲーションコマンドによって条件付けられた条件付き模倣学習アルゴリズムに依存しています。私たちのアプローチは最小限の効率で、センサーのみに依存しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-09">
        <br><font color="black">2020-03-09</font>
      </time>
    </span>
</section>
<!-- paper0: Castle in the Sky: Dynamic Sky Replacement and Harmonization in Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_26.html">
      <font color="black">Castle in the Sky: Dynamic Sky Replacement and Harmonization in Videos</font>
    </a>
  </h2>
  <font color="black">
[概要]私たちの方法は完全にビジョンベースであり、キャプチャデバイスに要件はありません。オンラインまたはオフラインの処理シナリオに適切に適用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Spatio-temporal Features for Generalized Detection of Deepfake Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_27.html">
      <font color="black">Spatio-temporal Features for Generalized Detection of Deepfake Videos</font>
    </a>
  </h2>
  <font color="black">
[概要]私たちのアプローチは、ローカル空間をキャプチャします-既存のシーケンスエンコーダーがそれに無関心である間、ディープフェイクビデオの時間的関係と不整合</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Blind Video Temporal Consistency via Deep Video Prior -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_28.html">
      <font color="black">Blind Video Temporal Consistency via Deep Video Prior</font>
    </a>
  </h2>
  <font color="black">
[概要]ブラインドビデオの時間的一貫性のための斬新で一般的なアプローチを提示します。私たちの方法は、事前にディープビデオを使用してビデオで畳み込みネットワークをトレーニングすることで実現できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: DPD-InfoGAN: Differentially Private Distributed InfoGAN -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_29.html">
      <font color="black">DPD-InfoGAN: Differentially Private Distributed InfoGAN</font>
    </a>
  </h2>
  <font color="black">
[概要]デフォルトのガンは、生成する画像のタイプを制御できません。これは、infoganのモデルの複雑さが高いためです。この特定の特定の問題は、問題の原因です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Monocular Dense Depth from Events -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_30.html">
      <font color="black">Learning Monocular Dense Depth from Events</font>
    </a>
  </h2>
  <font color="black">
[概要]システムはセンサーの組み合わせを使用して予測を行います。単眼球を使用して詳細な予測を作成するためにも使用できます。ただし、これまでに示されていない標準でテストされています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-16">
        <br><font color="black">2020-10-16</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Occupancy Function from Point Clouds for Surface Reconstruction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_31.html">
      <font color="black">Learning Occupancy Function from Point Clouds for Surface Reconstruction</font>
    </a>
  </h2>
  <font color="black">
[概要]点群深層学習アーキテクチャである点畳み込みニューラルネットワーク（pcnn）は、学習モデルを構築するように設計されています。このメソッドは、点群データの幾何学的関数をネイティブに取得し、点順列にあります。実験では状態を示しています。シェイプネットデータセットを使用して再構築するための最新のパフォーマンス</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Fingerprint Orientation Estimation: Challenges and Opportunities -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_32.html">
      <font color="black">Fingerprint Orientation Estimation: Challenges and Opportunities</font>
    </a>
  </h2>
  <font color="black">
[概要]指紋の数は限られており、生涯にわたって変化しません。敵にリークされると、生涯にわたってリークします。この調査論文では、さまざまなセキュリティモデルと指紋テンプレート保護技術を確認します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: On the Power of Deep but Naive Partial Label Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_33.html">
      <font color="black">On the Power of Deep but Naive Partial Label Learning</font>
    </a>
  </h2>
  <font color="black">
[要約]現在の方法の大部分は、ラベルの明確化または平均化戦略のいずれかを使用しています。これは、最も古くて最も単純なpll方法の隠れた力を説明しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: High resolution weakly supervised localization architectures for medical
  images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_34.html">
      <font color="black">High resolution weakly supervised localization architectures for medical
  images</font>
    </a>
  </h2>
  <font color="black">
[概要]これは、モデルの特徴マップの量によるものです。メインツールであるピラミッドローカリゼーションネットワーク（pylon）は、0と比較してnihの胸部X線14データセットで0.62の平均ポイントローカリゼーション精度を達成しました。 。従来のカムモデルの場合は45</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Bandwidth-Adaptive Feature Sharing for Cooperative LIDAR Object
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_35.html">
      <font color="black">Bandwidth-Adaptive Feature Sharing for Cooperative LIDAR Object
  Detection</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]ドライバーの安全性は、そのようなシステムの堅牢性、信頼性、およびスカラーに直接依存します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Convolutional Autoencoders for Human Motion Infilling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_36.html">
      <font color="black">Convolutional Autoencoders for Human Motion Infilling</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]モーション入力は、間に欠落しているギャップを埋めることを目的としています。モデルは、長さが異なる可能性のある任意の数のギャップを埋めることができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Face Hallucination Using Split-Attention in Split-Attention Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_37.html">
      <font color="black">Face Hallucination Using Split-Attention in Split-Attention Network</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]畳み込みニューラルネットワーク（cnns）を顔の幻覚にうまく適用して、hr画像とlr画像の間の複雑な線形マッピングをモデル化します。提案されたアプローチは一貫して顔の幻覚の再構成性能を大幅に改善します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Cell Segmentation and Tracking using CNN-Based Distance Predictions and
  a Graph-Based Matching Strategy -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_38.html">
      <font color="black">Cell Segmentation and Tracking using CNN-Based Distance Predictions and
  a Graph-Based Matching Strategy</font>
    </a>
  </h2>
  <font color="black">
[概要]モーションベースの追跡方法を使用して、隣接する距離を予測できます。セルの追跡に加えて、参加者-トラトラトラトラトラトラトラトラトラトラトラトラトラトラトラトラトラトラトラトラトラトレートされたトラックを見つけることができます。ツールは距離マップに基づくツールを使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-03">
        <br><font color="black">2020-04-03</font>
      </time>
    </span>
</section>
<!-- paper0: Task-Adaptive Feature Transformer for Few-Shot Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_39.html">
      <font color="black">Task-Adaptive Feature Transformer for Few-Shot Segmentation</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT] taft、taftは、タスク（特定の高レベルの機能）を一連のタスクに線形変換します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Body Shape Privacy in Images: Understanding Privacy and Preventing
  Automatic Shape Extraction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_40.html">
      <font color="black">Body Shape Privacy in Images: Understanding Privacy and Preventing
  Automatic Shape Extraction</font>
    </a>
  </h2>
  <font color="black">
[概要]モデルを使用して、さまざまな体型をモデル化できます。服を脱いだ体のリアルな描写は、非常にプライベートであると見なされます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-05-27">
        <br><font color="black">2019-05-27</font>
      </time>
    </span>
</section>
<!-- paper0: Convolutional Neural Network and decision support in medical imaging:
  case study of the recognition of blood cell subtypes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_41.html">
      <font color="black">Convolutional Neural Network and decision support in medical imaging:
  case study of the recognition of blood cell subtypes</font>
    </a>
  </h2>
  <font color="black">
[概要]研究者は、特定のタイプの深層学習、つまり、4つの血球タイプ（好中球、好酸球、リンパ球、単球）の画像認識に畳み込みニューラルネットワーク（cnnsまたはconvnets）を使用しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-19">
        <br><font color="black">2019-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Asynchronous Edge Learning using Cloned Knowledge Distillation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_42.html">
      <font color="black">Asynchronous Edge Learning using Cloned Knowledge Distillation</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT] flシステムは、ユーザーの迅速な流入と流出を使用すると同時に、複数のユーザーのネットワーク遅延によるボトルネックを最小限に抑えます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Super-Human Performance in Online Low-latency Recognition of
  Conversational Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_43.html">
      <font color="black">Super-Human Performance in Online Low-latency Recognition of
  Conversational Speech</font>
    </a>
  </h2>
  <font color="black">
[概要]システムは、複数の注意ベースのエンコーダーデコーダーネットワークを使用します。新しい低遅延の増分情報アプローチを使用します。システムは、カリフォルニア大学の研究者によって開発されました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-07">
        <br><font color="black">2020-10-07</font>
      </time>
    </span>
</section>
<!-- paper0: Novel View Synthesis from only a 6-DoF Camera Pose by Two-stage Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_44.html">
      <font color="black">Novel View Synthesis from only a 6-DoF Camera Pose by Two-stage Networks</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]新しいモデルを使用してシーンの画像を作成できますが、それに対処する作品はほとんどありません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Selecting Data Augmentation for Simulating Interventions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_45.html">
      <font color="black">Selecting Data Augmentation for Simulating Interventions</font>
    </a>
  </h2>
  <font color="black">
[概要]観測されたドメインと実際のタスクラベルの間の疑似相関が疑似相関を弱める可能性があると主張します。データを不適切に拡張すると、パフォーマンスが大幅に低下する可能性があることを示します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-04">
        <br><font color="black">2020-05-04</font>
      </time>
    </span>
</section>
<!-- paper0: Deep convolutional embedding for digitized painting clustering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_46.html">
      <font color="black">Deep convolutional embedding for digitized painting clustering</font>
    </a>
  </h2>
  <font color="black">
[要約]これらの問題に対処するために、デジタル化された絵画クラスタリングに深い畳み込み埋め込みモデルを使用することを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-19">
        <br><font color="black">2020-03-19</font>
      </time>
    </span>
</section>
<!-- paper0: TLGAN: document Text Localization using Generative Adversarial Nets -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_47.html">
      <font color="black">TLGAN: document Text Localization using Generative Adversarial Nets</font>
    </a>
  </h2>
  <font color="black">
[概要]テキストローカリゼーションzativeadversarial net（tlgan）は、デジタル画像からテキストローカリゼーションを実行するディープニューラルネットワークです。tlganは、テストデータに対して99. 83％の精度と99. 64％のリコールを達成しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Effective training of deep convolutional neural networks for
  hyperspectral image classification through artificial labeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_48.html">
      <font color="black">Effective training of deep convolutional neural networks for
  hyperspectral image classification through artificial labeling</font>
    </a>
  </h2>
  <font color="black">
[概要]転移学習アプローチは、特定のデータセットの2番目の要件を緩和するために使用できます。特定の画像タイプやニューラルネットワークアーキテクチャに制限されることなく、分類精度を向上させるのに非常に効果的です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-12">
        <br><font color="black">2019-09-12</font>
      </time>
    </span>
</section>
<!-- paper0: An explainable deep vision system for animal classification and
  detection in trail-camera images with automatic post-deployment retraining -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_49.html">
      <font color="black">An explainable deep vision system for animal classification and
  detection in trail-camera images with automatic post-deployment retraining</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]鳥の検出システムは、93％の感度、92％の特異度、68％の平均交差率を達成します。システムは、フロート画像を検出し、再トレーニング手順をトリガーするための新しい手法も導入します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Visual link retrieval and knowledge discovery in painting datasets -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_50.html">
      <font color="black">Visual link retrieval and knowledge discovery in painting datasets</font>
    </a>
  </h2>
  <font color="black">
[概要]視覚芸術の方法は、さまざまな芸術家や絵画学校の絵画間の類似関係を見つけることです。この方法では、深い畳み込みニューラルネットワークを使用して特徴抽出を実行し、完全に監視されていない最近傍メカニズムを使用してデジタル化された絵画間のリンクを取得します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-18">
        <br><font color="black">2020-03-18</font>
      </time>
    </span>
</section>
<!-- paper0: Iterative Network for Image Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_51.html">
      <font color="black">Iterative Network for Image Super-Resolution</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT] cnnベースのメソッドは、一般に、低解像度の画像を、高度なネットワーク構造と損失関数を備えた対応する高解像度バージョンにマッピングします。これらのメソッドメソッドメソッドは、ネットワーク構造と損失関数を組み合わせて、印象的なパフォーマンスを示します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-20">
        <br><font color="black">2020-05-20</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Loss for Test-Time Augmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_52.html">
      <font color="black">Learning Loss for Test-Time Augmentation</font>
    </a>
  </h2>
  <font color="black">
[概要]最近のデータ拡張方法のほとんどは、トレーニングフェーズでのデータセットの拡張に重点を置いています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: COVID-19-CT-CXR: a freely accessible and weakly labeled chest X-ray and
  CT image collection on COVID-19 from biomedical literature -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_53.html">
      <font color="black">COVID-19-CT-CXR: a freely accessible and weakly labeled chest X-ray and
  CT image collection on COVID-19 from biomedical literature</font>
    </a>
  </h2>
  <font color="black">
[概要]データセット、コード、およびdlモデルはwwwで公開されています。インフルエンザ.covid-19-ct-cxrおよびct画像は、pubmed中央オープンアクセス（pmc-oa）サブセットから自動的に抽出されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-11">
        <br><font color="black">2020-06-11</font>
      </time>
    </span>
</section>
<!-- paper0: Attention Deep Model with Multi-Scale Deep Supervision for Person
  Re-Identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_54.html">
      <font color="black">Attention Deep Model with Multi-Scale Deep Supervision for Person
  Re-Identification</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]多くの最先端のpreidメソッドは、注意ベースまたはマルチスケールの特徴学習ディープモデルです。特徴学習ブロックを特徴抽出ディープネットワークに埋め込むモデルの多く。これらには、逆注意ブロックと小説が含まれます。バックボーンネットワークをトレーニングするためのアーキテクチャオペレーターのマルチスケールレイヤー</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-23">
        <br><font color="black">2019-11-23</font>
      </time>
    </span>
</section>
<!-- paper0: SEG-MAT: 3D Shape Segmentation Using Medial Axis Transform -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_55.html">
      <font color="black">SEG-MAT: 3D Shape Segmentation Using Medial Axis Transform</font>
    </a>
  </h2>
  <font color="black">
[概要] 3D形状セグメンテーションの既存の方法は複雑な技術に悩まされています。新しい方法はシンプルで原理的です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Hierarchical Patch VAE-GAN: Generating Diverse Videos from a Single
  Sample -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_56.html">
      <font color="black">Hierarchical Patch VAE-GAN: Generating Diverse Videos from a Single
  Sample</font>
    </a>
  </h2>
  <font color="black">
[概要]新しい `patch --gan &#39;アプローチを使用して、多様な画像を作成できます。これらには、トレーニング時に1つのサンプルのみが与えられた画像が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-22">
        <br><font color="black">2020-06-22</font>
      </time>
    </span>
</section>
<!-- paper0: Correlation Filter for UAV-Based Aerial Tracking: A Review and
  Experimental Evaluation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_57.html">
      <font color="black">Correlation Filter for UAV-Based Aerial Tracking: A Review and
  Experimental Evaluation</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]風の状態、UAVの機械構造の振動、限られた計算リソースに基づいて、オンボードの追跡方法はすべて重要です。これは、20の最先端のdcfベースのトラッカーが整然としていることに基づいています。彼らの革新によると</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-13">
        <br><font color="black">2020-10-13</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Dual Semantic Relations with Graph Attention for Image-Text
  Matching -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_58.html">
      <font color="black">Learning Dual Semantic Relations with Graph Attention for Image-Text
  Matching</font>
    </a>
  </h2>
  <font color="black">
[概要]主要な課題は、統一された視覚的および文学的表現を学習することです。地域の特徴とグローバルな特徴の共同学習の欠如は、地域の特徴がグローバルな文脈との接触を失う原因になります。二重意味関係注意ネットワーク（dsran）は注意を高めることを目的としています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarially-learned Inference via an Ensemble of Discrete Undirected
  Graphical Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_59.html">
      <font color="black">Adversarially-learned Inference via an Ensemble of Discrete Undirected
  Graphical Models</font>
    </a>
  </h2>
  <font color="black">
[概要]たとえば、sanchezモデルは、リスク最小化を使用してトレーニングできます。代わりに、suv-ag theo敵対的トレーニングフレームワークを提案します。これにより、sanchezモデル（agms）の調整可能な大きなアンサンブルが生成されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-09">
        <br><font color="black">2020-07-09</font>
      </time>
    </span>
</section>
<!-- paper0: Beyond accuracy: quantifying trial-by-trial behaviour of CNNs and humans
  by measuring error consistency -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_60.html">
      <font color="black">Beyond accuracy: quantifying trial-by-trial behaviour of CNNs and humans
  by measuring error consistency</font>
    </a>
  </h2>
  <font color="black">
[概要]試行の使用-試行エラーの一貫性、2つの決定が同じ入力でエラーを起こすかどうかを測定するための定量分析.cnnsと人間のオブザーバーの間の一貫性は、偶然だけで予想できるものを少し上回っています------人間がとcnnsはおそらく非常に異なる戦略を実装しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-30">
        <br><font color="black">2020-06-30</font>
      </time>
    </span>
</section>
<!-- paper0: Towards human performance on automatic motion tracking of infant
  spontaneous movements -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_61.html">
      <font color="black">Towards human performance on automatic motion tracking of infant
  spontaneous movements</font>
    </a>
  </h2>
  <font color="black">
[概要]ネットワークの精度レベルは、推定されたキーポイント位置と人間の専門家の注釈との間の偏差として評価されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-12">
        <br><font color="black">2020-10-12</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Analysis of CNN-based Spatio-temporal Representations for Action
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_62.html">
      <font color="black">Deep Analysis of CNN-based Spatio-temporal Representations for Action
  Recognition</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]詳細分析では、cnnの統合分析を実施し、300を超えるアクションビデオモデルを含む大規模分析に向けて努力しています。これは、最近のアクションモデルが学習できるように見えるという事実に基づいています。データ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Auto-Augment via Augmentation-Wise Weight Sharing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_63.html">
      <font color="black">Improving Auto-Augment via Augmentation-Wise Weight Sharing</font>
    </a>
  </h2>
  <font color="black">
[概要]自動拡張検索の重要なコンポーネントは、特定の拡張ポリシーの評価プロセスです。これは、報酬を返すために使用され、通常は数千回実行されます。効率を達成するために、多くの人が速度のために評価の信頼性を犠牲にすることを選択します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Representation Sharing for Fast Object Detector Search and Beyond -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_64.html">
      <font color="black">Representation Sharing for Fast Object Detector Search and Beyond</font>
    </a>
  </h2>
  <font color="black">
[概要] rpnを持たない一段検出器の場合、強力なサブネットワークがより要求されます。設計された検索空間に対処するために、表現共有（repshare）と呼ばれる新しい検索アルゴリズムが提案され、効果的に識別されます。定義された変換の最良の組み合わせ。私たちの流行検出器は、msで46.4 apを達成します-coco（シングルスケールテストの下で）、状態ベースの検出器を上回ります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Residual Force Control for Agile Human Behavior Imitation and Extended
  Motion Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_65.html">
      <font color="black">Residual Force Control for Agile Human Behavior Imitation and Extended
  Motion Synthesis</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT] rfcベースのポリシーは、ダイナミクスの不一致を補正し、参照モーションをより適切に模倣するために、ヒューマノイドに残留力を適用することを学習します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-12">
        <br><font color="black">2020-06-12</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Sort Image Sequences via Accumulated Temporal Differences -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_66.html">
      <font color="black">Learning to Sort Image Sequences via Accumulated Temporal Differences</font>
    </a>
  </h2>
  <font color="black">
[要約]提案されたアプローチは、標準により最先端の方法を上回っています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Malaria detection from RBC images using shallow Convolutional Neural
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_67.html">
      <font color="black">Malaria detection from RBC images using shallow Convolutional Neural
  Networks</font>
    </a>
  </h2>
  <font color="black">
[概要]これは、アルゴリズムの商用展開に大きな利点をもたらす可能性があります。これは、アフリカの貧しい国やインド亜大陸の一部で使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Fine-tuned Pre-trained Mask R-CNN Models for Surface Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_68.html">
      <font color="black">Fine-tuned Pre-trained Mask R-CNN Models for Surface Object Detection</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]モデルは、cocoデータセットと15、188のセグメント化された路面アノテーションタグを使用していました。結果は、かなりの数の偽陰性のカウントを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: 3D Meta-Registration: Learning to Learn Registration of 3D Point Clouds -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_69.html">
      <font color="black">3D Meta-Registration: Learning to Learn Registration of 3D Point Clouds</font>
    </a>
  </h2>
  <font color="black">
[概要]提案された3D登録モデルは3Dメタ登録と呼ばれます。新しい3D登録タスクに迅速に適応して一般化することができます。提案されたモデルは2つのモジュールで構成されています：3D登録学習者と3D登録ポイントクラウド</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: The Detection of Thoracic Abnormalities ChestX-Det10 Challenge Results -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_70.html">
      <font color="black">The Detection of Thoracic Abnormalities ChestX-Det10 Challenge Results</font>
    </a>
  </h2>
  <font color="black">
[概要]チャレンジは2つのラウンドに分かれています。これはdeepwiseailabによって手配されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: Neural Enhancement in Content Delivery Systems: The State-of-the-Art and
  Future Directions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_71.html">
      <font color="black">Neural Enhancement in Content Delivery Systems: The State-of-the-Art and
  Future Directions</font>
    </a>
  </h2>
  <font color="black">
[概要]高速応答時間と高い視覚品質を実現するための主要コンポーネントとしてニューラルエンハンスメントを採用した高品質のコンテンツ配信システムの状態を調査します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-12">
        <br><font color="black">2020-10-12</font>
      </time>
    </span>
</section>
<!-- paper0: Robust Quantization: One Model to Rule Them All -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_72.html">
      <font color="black">Robust Quantization: One Model to Rule Them All</font>
    </a>
  </h2>
  <font color="black">
[要約]この方法は、広範囲の量子化プロセスに対してモデルに「固有のロバスト性」を提供します。これは、システムが個々のイメージネットモデルと互換性がないという事実に基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-18">
        <br><font color="black">2020-02-18</font>
      </time>
    </span>
</section>
<!-- paper0: mEBAL: A Multimodal Database for Eye Blink Detection and Attention Level
  Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_73.html">
      <font color="black">mEBAL: A Multimodal Database for Eye Blink Detection and Attention Level
  Estimation</font>
    </a>
  </h2>
  <font color="black">
[概要]まばたきデータベースは認知活動に関連しています。まばたきの自動検出器は多くのタスクで提案されています。提案されたmebalは、取得センサーとサンプルの観点から既存のデータベースを改善します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-09">
        <br><font color="black">2020-06-09</font>
      </time>
    </span>
</section>
<!-- paper0: Restoring Negative Information in Few-Shot Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_74.html">
      <font color="black">Restoring Negative Information in Few-Shot Object Detection</font>
    </a>
  </h2>
  <font color="black">
[概要]最近のいくつかの進歩-ショット学習は主に画像分類に焦点を当てています。この論文では、オブジェクトの検出に焦点を当てています。ネガは空間学習を埋め込むために不可欠です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: QISTA-Net: DNN Architecture to Solve $\ell_q$-norm Minimization Problem
  and Image Compressed Sensing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_75.html">
      <font color="black">QISTA-Net: DNN Architecture to Solve $\ell_q$-norm Minimization Problem
  and Image Compressed Sensing</font>
    </a>
  </h2>
  <font color="black">
[概要]新しい方法は、高密度信号再構成問題を解決するためにqista --net --s-と呼ばれます。これは、qista --sと呼ばれるネットワークアーキテクチャに基づいています。これは、再構成のパフォーマンスが依然としてほとんどの状態を上回っていることを意味します。アート自然画像再構成方法</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: GAN based Unsupervised Segmentation: Should We Match the Exact Number of
  Objects -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_76.html">
      <font color="black">GAN based Unsupervised Segmentation: Should We Match the Exact Number of
  Objects</font>
    </a>
  </h2>
  <font color="black">
[概要]簡単なアイデアは、教師なし合成問題として教師あり厳密化タスクにアプローチすることです。調査によると、強度画像はサイクルを使用して注釈ドメインに転送できます-一貫した敵対的学習</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: FasterRCNN Monitoring of Road Damages: Competition and Deployment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_77.html">
      <font color="black">FasterRCNN Monitoring of Road Damages: Competition and Deployment</font>
    </a>
  </h2>
  <font color="black">
[概要]効率的なインフラストラクチャの重要な前提条件は、非常に大きな構造の状態を監視することです。これは、管理者がメンテナンス操作を最適化するのに役立ちます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Two-Stream Consensus Network for Weakly-Supervised Temporal Action
  Localization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_78.html">
      <font color="black">Two-Stream Consensus Network for Weakly-Supervised Temporal Action
  Localization</font>
    </a>
  </h2>
  <font color="black">
[概要]提案されたtscnは、反復的な改良トレーニング方法を特徴としています。フレームレベルの疑似グラウンドトゥルースは繰り返し更新され、効果的なモデルトレーニングと誤検知アクション提案の排除のためのフレームレベルのインスタンスを提供するために使用されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Meta-Learning with Context-Agnostic Initialisations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_79.html">
      <font color="black">Meta-Learning with Context-Agnostic Initialisations</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]これにより、ファインの初期化が生成されます-コンテキスト-agversとタスク-generalizedの両方であるターゲットに適応します。たとえば、コンテキスト-agnalmeta-学習がそれぞれの場合に結果を改善することを示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-29">
        <br><font color="black">2020-07-29</font>
      </time>
    </span>
</section>
<!-- paper0: EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational
  Reasoning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_80.html">
      <font color="black">EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational
  Reasoning</font>
    </a>
  </h2>
  <font color="black">
[概要]状況の効果的な理解とインタラクティブエージェントの正確な軌道予測は、ポジショニングと計画において重要な役割を果たします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-31">
        <br><font color="black">2020-03-31</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Graph-Based Priors for Generalized Zero-Shot Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_81.html">
      <font color="black">Learning Graph-Based Priors for Generalized Zero-Shot Learning</font>
    </a>
  </h2>
  <font color="black">
[概要]これは、クラスラベルに関するサイド情報を活用することで実現されます。gzslは、見えないクラスからサンプルを生成するために使用されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Fine-grained Synthesis of Unrestricted Adversarial Examples -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CV/paper_82.html">
      <font color="black">Fine-grained Synthesis of Unrestricted Adversarial Examples</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]私たちのアプローチは、分類、セマンティックセグメンテーション、およびオブジェクト検出モデルに対するターゲットおよび非ターゲットの無制限の攻撃に使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-20">
        <br><font color="black">2019-11-20</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: CDEvalSumm: An Empirical Study of Cross-Dataset Evaluation for Neural
  Summarization Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_0.html">
      <font color="black">CDEvalSumm: An Empirical Study of Cross-Dataset Evaluation for Neural
  Summarization Systems</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]初期評価方法はドメイン内設定に制限されています。これはさまざまなデータセットの分析に基づいています。データセットの詳細な分析はさまざまなタイプのテストを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-11">
        <br><font color="black">2020-10-11</font>
      </time>
    </span>
</section>
<!-- paper0: GeDi: Generative Discriminator Guided Sequence Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_1.html">
      <font color="black">GeDi: Generative Discriminator Guided Sequence Generation</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT] gediを使用すると、新しいトピックをゼロで制御可能に生成できます。キーワードだけからショットできます。これにより、キーワードよりもはるかに少ない5つの質問を制御可能に生成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-14">
        <br><font color="black">2020-09-14</font>
      </time>
    </span>
</section>
<!-- paper0: slimIPL: Language-Model-Free Iterative Pseudo-Labeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_2.html">
      <font color="black">slimIPL: Language-Model-Free Iterative Pseudo-Labeling</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]反復疑似ラベリング（ipl）は、モデルが学習するときに反復的に再生成された疑似ラベルを使用します。asrのパフォーマンスをさらに向上させることが示されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Cross Copy Network for Dialogue Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_3.html">
      <font color="black">Cross Copy Network for Dialogue Generation</font>
    </a>
  </h2>
  <font color="black">
[概要]ダイアログロジックは、特定のドメインに関する重要な情報を伝達します。テキストの流暢さと正確さは、モデルトレーニングの主要な指標として機能することがよくあります。ダイアログロジックは、多くの場合無視されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: A Technical Report: BUT Speech Translation Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_4.html">
      <font color="black">A Technical Report: BUT Speech Translation Systems</font>
    </a>
  </h2>
  <font color="black">
[概要]システムは英語とドイツ語のオフライン音声翻訳システムです。ただし、asr理論を翻訳すると、オラクルの入力テキストと比較して大幅な劣化が見られます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Confidence Estimation for Attention-based Sequence-to-sequence Models
  for Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_5.html">
      <font color="black">Confidence Estimation for Attention-based Sequence-to-sequence Models
  for Speech Recognition</font>
    </a>
  </h2>
  <font color="black">
[概要]信頼度推定モジュール（cem）と呼ばれる新しいシステムは、既存のasrモデルに基づいています。新しい方法では、モデルの信頼度としてデコーダーのソフトマックス確率を使用します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Sentimental LIAR: Extended Corpus and Deep Learning Models for Fake
  Claim Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_6.html">
      <font color="black">Sentimental LIAR: Extended Corpus and Deep Learning Models for Fake
  Claim Classification</font>
    </a>
  </h2>
  <font color="black">
[概要]ソーシャルメディアの監視されていない性質により、虚偽の情報や偽のニュースを広めることが容易になりました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-01">
        <br><font color="black">2020-09-01</font>
      </time>
    </span>
</section>
<!-- paper0: Exploit Multiple Reference Graphs for Semi-supervised Relation
  Extraction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_7.html">
      <font color="black">Exploit Multiple Reference Graphs for Semi-supervised Relation
  Extraction</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]ラベルなしの方法は、この問題を解決するのに役立ちます。しかし、全体的に優れた比較関数を見つけるのは困難です。これらには、ラベルなしのサンプルの観察が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Evaluating Factuality in Generation with Dependency-level Entailment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_8.html">
      <font color="black">Evaluating Factuality in Generation with Dependency-level Entailment</font>
    </a>
  </h2>
  <font color="black">
[要約]研究では、文学的含意システムを使用して事実上の誤りを特定できるかどうかを調査しましたが、これらは世代フィルタリングとは異なる問題を解決するように訓練されており、世代のどの部分が事実ではないかを特定しません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-12">
        <br><font color="black">2020-10-12</font>
      </time>
    </span>
</section>
<!-- paper0: Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention
  and Spatial Memory -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_9.html">
      <font color="black">Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention
  and Spatial Memory</font>
    </a>
  </h2>
  <font color="black">
[概要]ナビゲーションシステムはグーグルストリートビューに基づいています。それは口頭のナビゲーション指示に基づいて視覚的なガイダンスを提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-04">
        <br><font color="black">2019-10-04</font>
      </time>
    </span>
</section>
<!-- paper0: Kwame: A Bilingual AI Teaching Assistant for Online SuaCode Courses -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_10.html">
      <font color="black">Kwame: A Bilingual AI Teaching Assistant for Online SuaCode Courses</font>
    </a>
  </h2>
  <font color="black">
[概要]システムは質問を使用します-過去のコホートでのコースのクイズと学生の質問から作成された回答ペア</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Recipes for Safety in Open-domain Chatbots -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_11.html">
      <font color="black">Recipes for Safety in Open-domain Chatbots</font>
    </a>
  </h2>
  <font color="black">
[概要]これらの問題を軽減するためにさまざまな方法を調査します。これらの方法を比較する実験を行い、新しい技術が内部にあることを発見します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-14">
        <br><font color="black">2020-10-14</font>
      </time>
    </span>
</section>
<!-- paper0: Rethinking Evaluation in ASR: Are Our Models Robust Enough? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_12.html">
      <font color="black">Rethinking Evaluation in ASR: Are Our Models Robust Enough?</font>
    </a>
  </h2>
  <font color="black">
[要約]ベンチマークに関する調査結果は、通常、一般化に基づいて評価されます。調査によると、十分な数のベンチマークセットを使用すると、ベンチマークの平均ワードエラー率（wer）のパフォーマンスが、実際のデータセットでのパフォーマンスの優れたプロキシになります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: NU-GAN: High resolution neural upsampling with GAN -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_13.html">
      <font color="black">NU-GAN: High resolution neural upsampling with GAN</font>
    </a>
  </h2>
  <font color="black">
[概要] nu-ganは、テキスト内の個別のコンポーネントとしてオーディオのアップサンプリングを解決することに飛躍します-最大解像度。テクニックに加えて、nuganはgansを使用したオーディオ生成のテクニックを活用します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Knowledge Distillation for BERT Unsupervised Domain Adaptation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_14.html">
      <font color="black">Knowledge Distillation for BERT Unsupervised Domain Adaptation</font>
    </a>
  </h2>
  <font color="black">
[概要]モデルは、さまざまなトピックでトレーニングされています。ドメインシフトの問題に対して優れたパフォーマンスを示します。この方法は、敵対的な識別ドメイン適応（adda）フレームワークと知識蒸留を組み合わせたものです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: wav2vec 2.0: A Framework for Self-Supervised Learning of Speech
  Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_15.html">
      <font color="black">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech
  Representations</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT] wav2vec 2. 0は、潜在空間での音声入力をマスクします。これは、3年前の350万ケースのラベル付きデータの量子化で定義された対照的なタスクを解決します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-20">
        <br><font color="black">2020-06-20</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Training for Aspect-Based Sentiment Analysis with BERT -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_16.html">
      <font color="black">Adversarial Training for Aspect-Based Sentiment Analysis with BERT</font>
    </a>
  </h2>
  <font color="black">
[概要]これらの例は実際の文ではありませんが、ニューラルネットワークをより堅牢にすることができる正則化手法として機能することが示されています。これらのタスクを使用した後、敵対的トレーニングを利用するためのバート敵対的トレーニング（バット）と呼ばれる新しいアーキテクチャを提案しますアブサで</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-30">
        <br><font color="black">2020-01-30</font>
      </time>
    </span>
</section>
<!-- paper0: ConVEx: Data-Efficient and Few-Shot Slot Labeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_17.html">
      <font color="black">ConVEx: Data-Efficient and Few-Shot Slot Labeling</font>
    </a>
  </h2>
  <font color="black">
[概要] redditデータの状態トレーニングの使用とredditを報告します。さまざまなタイプの事前トレーニングタスクの改善を報告します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Rewriting Meaningful Sentences via Conditional BERT Sampling and an
  application on fooling text classifiers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_18.html">
      <font color="black">Rewriting Meaningful Sentences via Conditional BERT Sampling and an
  application on fooling text classifiers</font>
    </a>
  </h2>
  <font color="black">
[概要]複数の方法で元の文を効率的に書き換えるために、paraphrasesamplerと呼ばれる新しいサンプリング方法を設計します。新しい基準では、その単語レベルと文レベルの変更が可能であり、意味的類似性と2次元で独立して調整できます。文法的な品質</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Detecting and Exorcising Statistical Demons from Language Models with
  Anti-Models of Negative Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_19.html">
      <font color="black">Detecting and Exorcising Statistical Demons from Language Models with
  Anti-Models of Negative Data</font>
    </a>
  </h2>
  <font color="black">
[概要] lstmsからn-gram信号を削除する方法により、ネガティブデータと言われます。これは、モデルがネガティブデータに一般化できることを意味し、標準的な自己監視が一般化しすぎていることを示します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: On the Effects of Using word2vec Representations in Neural Networks for
  Dialogue Act Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_20.html">
      <font color="black">On the Effects of Using word2vec Representations in Neural Networks for
  Dialogue Act Recognition</font>
    </a>
  </h2>
  <font color="black">
[概要]提案されたアプローチは言語間で一貫しており、英語の最先端の結果に匹敵します。しかし、これはさらに驚くべきことです。また、標準のword2vecem-寝具は貴重な情報をもたらさないようです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Chatbot Interaction with Artificial Intelligence: Human Data
  Augmentation with T5 and Language Transformer Ensemble for Text
  Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_21.html">
      <font color="black">Chatbot Interaction with Artificial Intelligence: Human Data
  Augmentation with T5 and Language Transformer Ensemble for Text
  Classification</font>
    </a>
  </h2>
  <font color="black">
[概要]インテリジェントシステムは、人工的な言い換えによって人間と情報を補強します。より古典的、注意、言語変換のためのトレーニングデータの大規模なセットを作成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-12">
        <br><font color="black">2020-10-12</font>
      </time>
    </span>
</section>
<!-- paper0: Continuous Speech Separation with Conformer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_22.html">
      <font color="black">Continuous Speech Separation with Conformer</font>
    </a>
  </h2>
  <font color="black">
[概要]より厳密な分離モデルは、混合音声から単一の話者信号を抽出します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: PARENTing via Model-Agnostic Reinforcement Learning to Correct
  Pathological Behaviors in Data-to-Text Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_23.html">
      <font color="black">PARENTing via Model-Agnostic Reinforcement Learning to Correct
  Pathological Behaviors in Data-to-Text Generation</font>
    </a>
  </h2>
  <font color="black">
[要約]この作業では、古典的な方法の省略の上に構築します。それらは、モデル-親メトリックに依存するag theoフレームワークが、幻覚と省略の両方を減らすのに効率的であることを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-21">
        <br><font color="black">2020-10-21</font>
      </time>
    </span>
</section>
<!-- paper0: Improving BERT Performance for Aspect-Based Sentiment Analysis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_24.html">
      <font color="black">Improving BERT Performance for Aspect-Based Sentiment Analysis</font>
    </a>
  </h2>
  <font color="black">
[要約]レビューには、感情のタイプと複製ターゲットの調査が含まれます。これには、バート市場でのさらなるテストが必要です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: German's Next Language Model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_25.html">
      <font color="black">German's Next Language Model</font>
    </a>
  </h2>
  <font color="black">
[概要]ベースモデルとラージサイズの両方のモデルについて、一連のドキュメントトレーニングと固有表現抽出（ner）タスク全体でsotaパフォーマンスを達成することができました。既存のモデルに対するベンチマークを使用して、これらのモデルがドイツの最高のモデルであることを示します。日付</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-21">
        <br><font color="black">2020-10-21</font>
      </time>
    </span>
</section>
<!-- paper0: WHALETRANS: E2E WHisper to nAturaL spEech conversion using modified
  TRANSformer network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_26.html">
      <font color="black">WHALETRANS: E2E WHisper to nAturaL spEech conversion using modified
  TRANSformer network</font>
    </a>
  </h2>
  <font color="black">
[概要]ささやきから自然に変換された音声フォルマントの確率分布は、地面に似ています-真理分布。これまでに、ささやきから自然に変換されたフォルマントが見つかりました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-20">
        <br><font color="black">2020-04-20</font>
      </time>
    </span>
</section>
<!-- paper0: MAM: Masked Acoustic Modeling for End-to-End Speech-to-Text Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_27.html">
      <font color="black">MAM: Masked Acoustic Modeling for End-to-End Speech-to-Text Translation</font>
    </a>
  </h2>
  <font color="black">
[概要]マスクされたアコースティックコード言語（mam）と呼ばれる手法を監視できます。また、注釈のないものも含め、あらゆる音響信号に対して事前トレーニングを初めて使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Real-Time Execution of Large-scale Language Models on Mobile -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_28.html">
      <font color="black">Real-Time Execution of Large-scale Language Models on Mobile</font>
    </a>
  </h2>
  <font color="black">
[概要]最初のドキュメントを提案します-実際のニューラルアーキテクチャ拡張フレームワークを認識します。モバイルデバイスでリアルタイムモデルに高精度で適合するようにプログラムできます。いくつかのよく知られたベンチマークでモデルを評価します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-15">
        <br><font color="black">2020-09-15</font>
      </time>
    </span>
</section>
<!-- paper0: Calibrated Language Model Fine-Tuning for In- and Out-of-Distribution
  Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_29.html">
      <font color="black">Calibrated Language Model Fine-Tuning for In- and Out-of-Distribution
  Data</font>
    </a>
  </h2>
  <font color="black">
[概要]私たちの実験は、提案された方法が既存のキャリブレーション方法よりも優れていることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: LNMap: Departures from Isomorphic Assumption in Bilingual Lexicon
  Induction Through Non-Linear Mapping in Latent Space -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_30.html">
      <font color="black">LNMap: Departures from Isomorphic Assumption in Bilingual Lexicon
  Induction Through Non-Linear Mapping in Latent Space</font>
    </a>
  </h2>
  <font color="black">
[要約]最近のいくつかの研究では、密接に関連する言語でも一般的には当てはまらないという仮定を批判しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-28">
        <br><font color="black">2020-04-28</font>
      </time>
    </span>
</section>
<!-- paper0: Not all parameters are born equal: Attention is mostly what you need -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_31.html">
      <font color="black">Not all parameters are born equal: Attention is mostly what you need</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]埋め込み、注意、初期ニューラルネットワーク（ffn）は重要です。これらも重要であり、モデル内で同じ機能を果たします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Parallel Interactive Networks for Multi-Domain Dialogue State Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_32.html">
      <font color="black">Parallel Interactive Networks for Multi-Domain Dialogue State Generation</font>
    </a>
  </h2>
  <font color="black">
[概要]これらの依存関係を組み込むことはmdstの設計にとって重要であると主張し、これらの依存関係をモデル化するための並列対話型ネットワーク（ピン）を提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-16">
        <br><font color="black">2020-09-16</font>
      </time>
    </span>
</section>
<!-- paper0: AI-lead Court Debate Case Investigation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_33.html">
      <font color="black">AI-lead Court Debate Case Investigation</font>
    </a>
  </h2>
  <font color="black">
[概要]私たちのモデルは、事前定義された知識を通じて裁判官の質問意図を学ぶことができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Stronger Transformers for Neural Multi-Hop Question Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_34.html">
      <font color="black">Stronger Transformers for Neural Multi-Hop Question Generation</font>
    </a>
  </h2>
  <font color="black">
[概要]以前の作業では、グラフベースのモデルの重要性が強調されていましたが、より複雑なマルチホップ質問の生成が可能なシステムの開発への関心が高まっています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: EIGEN: Event Influence GENeration using Pre-trained Language Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_35.html">
      <font color="black">EIGEN: Event Influence GENeration using Pre-trained Language Models</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT] eigenは、事前にトレーニングされた言語モデルを使用してイベントの質問を生成する方法です。この方法を使用して、3,000を超える言語モデルを活用します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: N-ODE Transformer: A Depth-Adaptive Variant of the Transformer Using
  Neural Ordinary Differential Equations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_36.html">
      <font color="black">N-ODE Transformer: A Depth-Adaptive Variant of the Transformer Using
  Neural Ordinary Differential Equations</font>
    </a>
  </h2>
  <font color="black">
[概要]私たちの目標は、その深さ-適応性が変圧器の特定の既知の理論的制限を克服するのに役立つかどうかを調査することです。しかし、十分に、パリティ問題の本質的に非局所的な部分の救済策を提供しないことがわかります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: An Industry Evaluation of Embedding-based Entity Alignment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_37.html">
      <font color="black">An Industry Evaluation of Embedding-based Entity Alignment</font>
    </a>
  </h2>
  <font color="black">
[概要]調査によると、サイズとバイアスが異なるシードマッピングが調査されています。結果は、シードマッピングの影響がどのように調査されているかについての洞察を提供します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: STAR: A Schema-Guided Dialog Dataset for Transfer Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_38.html">
      <font color="black">STAR: A Schema-Guided Dialog Dataset for Transfer Learning</font>
    </a>
  </h2>
  <font color="black">
[概要]これらのモデルの有効性を、特にゼロショット研究で実証しました。このモデルは、星と同じ品質のデータセットを収集するように設計されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Compositional Generalization via Semantic Tagging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_39.html">
      <font color="black">Compositional Generalization via Semantic Tagging</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]デコードは、シーケンスモデルからシーケンスモデルへの表現力と一般性を維持する新しいモデルです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Incorporate Semantic Structures into Machine Translation Evaluation via
  UCCA -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_40.html">
      <font color="black">Incorporate Semantic Structures into Machine Translation Evaluation via
  UCCA</font>
    </a>
  </h2>
  <font color="black">
[概要]機械翻訳では、1つの原文のすべての適切な翻訳に特定の単語またはフレーズが表示されていることがわかります。これらの単語は重要な意味情報を伝える傾向があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-17">
        <br><font color="black">2020-10-17</font>
      </time>
    </span>
</section>
<!-- paper0: A Disentangled Adversarial Neural Topic Model for Separating Opinions
  from Plots in User Reviews -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_41.html">
      <font color="black">A Disentangled Adversarial Neural Topic Model for Separating Opinions
  from Plots in User Reviews</font>
    </a>
  </h2>
  <font color="black">
[要約]これらのアプローチは重要な結果を達成しました。しかし、潜在的なトピックを解きほぐす方法についてはほとんどライターが行われていません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Investigating the True Performance of Transformers in Low-Resource
  Languages: A Case Study in Automatic Corpus Creation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_42.html">
      <font color="black">Investigating the True Performance of Transformers in Low-Resource
  Languages: A Case Study in Automatic Corpus Creation</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT] newsph --nliは、低リソースのフィリピン語のベンチマークリソースリソースです。ハードベンチマークデータセットの欠如、およびそれらを作成する可能性とコストに基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Fine-Tune Longformer for Jointly Predicting Rumor Stance and Veracity -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_43.html">
      <font color="black">Fine-Tune Longformer for Jointly Predicting Rumor Stance and Veracity</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]ソーシャルメディアプラットフォームと使用の増加により、データが大量に利用可能になりました。このような大規模なデータを手動で処理する方法は、コストと時間がかかります。このようなコンテンツを自動的に処理して存在を確認することに注目が集まっています。噂の</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Distilling Dense Representations for Ranking using Tightly-Coupled
  Teachers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_44.html">
      <font color="black">Distilling Dense Representations for Ranking using Tightly-Coupled
  Teachers</font>
    </a>
  </h2>
  <font color="black">
[概要]関連性スコアを計算するためのコルベールの表現力豊かなmaxsim演算子からの知識を単純な内積に抽出します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Bilinear Fusion of Commonsense Knowledge with Attention-Based NLI Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_45.html">
      <font color="black">Bilinear Fusion of Commonsense Knowledge with Attention-Based NLI Models</font>
    </a>
  </h2>
  <font color="black">
[概要]既存の外部知識を組み込んだメソッドは、語彙レベルの知識に限定されています。欠如モデルは、現実世界の常識知識を使用します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: FAQ-based Question Answering via Knowledge Anchors -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_46.html">
      <font color="black">FAQ-based Question Answering via Knowledge Anchors</font>
    </a>
  </h2>
  <font color="black">
[概要]実世界のqaシステムでは、よくある質問（faq）ベースのqaは通常、実用的で効果的な適合です。このペーパーでは、faqベースのqaの新しい知識アンカーベースの質問応答（kaqa）フレームワークを提案します。質問を理解し、より適切な回答を取得する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-14">
        <br><font color="black">2019-11-14</font>
      </time>
    </span>
</section>
<!-- paper0: Method of noun phrase detection in Ukrainian texts -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_47.html">
      <font color="black">Method of noun phrase detection in Ukrainian texts</font>
    </a>
  </h2>
  <font color="black">
[概要]ウクライナ語のテキストでの自然言語フレーズの検索はまだ初期段階です。名詞句の検出の複雑な方法が計算されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Utterance-level Dialogue Understanding: An Empirical Study -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_48.html">
      <font color="black">Utterance-level Dialogue Understanding: An Empirical Study</font>
    </a>
  </h2>
  <font color="black">
[要約]完全な発話-レベルの理解には、多くの場合、コンテキストの理解が必要です。これらのアプローチの多くは、効果的な理解のためのコンテキストを説明します。これらの洞察は、より効果的な対話理解モデルを刺激することができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-29">
        <br><font color="black">2020-09-29</font>
      </time>
    </span>
</section>
<!-- paper0: Noise Robust TTS for Low Resource Speakers using Pre-trained Model and
  Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_49.html">
      <font color="black">Noise Robust TTS for Low Resource Speakers using Pre-trained Model and
  Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">
[概要]音声合成では、低品質で低リソースの音声データを使用します。モデルは、クリーンなデータとノイズの多い拡張データの両方で事前トレーニングされています。このモデルは、新しいスピーカーのクリーンな音声を合成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-26">
        <br><font color="black">2020-05-26</font>
      </time>
    </span>
</section>
<!-- paper0: BERT Loses Patience: Fast and Robust Inference with Early Exit -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_50.html">
      <font color="black">BERT Loses Patience: Fast and Robust Inference with Early Exit</font>
    </a>
  </h2>
  <font color="black">
[要約]このアプローチは、内部分類器をplmの各層と結合します。内部分類器の中間予測が事前に定義されたステップ数の間変更されない場合、可能性を停止します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-07">
        <br><font color="black">2020-06-07</font>
      </time>
    </span>
</section>
<!-- paper0: Similarity Analysis of Self-Supervised Speech Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_51.html">
      <font color="black">Similarity Analysis of Self-Supervised Speech Representations</font>
    </a>
  </h2>
  <font color="black">
[概要]有用な表現を学習するために多くのアルゴリズムが提案されています。これらには、幅広い音声タスクへの広範なアプリケーションが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Developing Real-time Streaming Transformer Transducer for Speech
  Recognition on Large-scale Dataset -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_52.html">
      <font color="black">Developing Real-time Streaming Transformer Transducer for Speech
  Recognition on Large-scale Dataset</font>
    </a>
  </h2>
  <font color="black">
[概要]トランスフォーマー-xlとチャンク-ワイズストリーミング処理のアイデアを組み合わせて、ストリーミング可能なトランスフォーマートランスデューサーモデルを作成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: An Analysis of Simple Data Augmentation for Named Entity Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_53.html">
      <font color="black">An Analysis of Simple Data Augmentation for Named Entity Recognition</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]名前付きエンティティ認識のデータ拡張はデータ拡張に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Contextualize Knowledge Bases with Transformer for End-to-end
  Task-Oriented Dialogue Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_54.html">
      <font color="black">Contextualize Knowledge Bases with Transformer for End-to-end
  Task-Oriented Dialogue Systems</font>
    </a>
  </h2>
  <font color="black">
[要約]調査によると、知識ジェネレーターを組み合わせて対話システムに組み込む方法について、より慎重にする必要があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-12">
        <br><font color="black">2020-10-12</font>
      </time>
    </span>
</section>
<!-- paper0: Reducing Unintended Identity Bias in Russian Hate Speech Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_55.html">
      <font color="black">Reducing Unintended Identity Bias in Russian Hate Speech Detection</font>
    </a>
  </h2>
  <font color="black">
[概要]ヘイトスピーチは、脅迫や差別の環境を作り出し、現実世界の暴力を扇動することさえあります。ヘイトスピーチは、一部の単語に対する偏見の存在として、これらのモデルの一般的な問題です。これらのモデルには、言語モデルを使用したトレーニングデータの生成が含まれます。保護されたアイデンティティに関連する用語や単語をコンテキストとして使用する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: CUNI Systems for the Unsupervised and Very Low Resource Translation Task
  in WMT20 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_56.html">
      <font color="black">CUNI Systems for the Unsupervised and Very Low Resource Translation Task
  in WMT20</font>
    </a>
  </h2>
  <font color="black">
[概要]合成データのトレーニングと関連する言語ペアの事前トレーニングを実験しました。低リソースシステムは、ドイツ語からの転移学習に依存していました。チェコ語のパラレルデータ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Streamlining Cross-Document Coreference Resolution: Evaluation and
  Modeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_57.html">
      <font color="black">Streamlining Cross-Document Coreference Resolution: Evaluation and
  Modeling</font>
    </a>
  </h2>
  <font color="black">
[概要]私たちの主な貢献は、生のテキストのみへのアクセスを想定する実用的な評価方法を提案することです。金の言及を想定する代わりに、シングルトン予測を無視し、CD共参照解決の典型的なターゲット設定に対処します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: BERT for Joint Multichannel Speech Dereverberation with Spatial-aware
  Tasks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_58.html">
      <font color="black">BERT for Joint Multichannel Speech Dereverberation with Spatial-aware
  Tasks</font>
    </a>
  </h2>
  <font color="black">
[要約]提案された方法は、シーケンスからシーケンスへのマッピング問題として関連するタスクに対処します。これは、さまざまなフロントエンド音声強調タスクに十分一般的です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-21">
        <br><font color="black">2020-10-21</font>
      </time>
    </span>
</section>
<!-- paper0: Incorporating Stylistic Lexical Preferences in Generative Language
  Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_59.html">
      <font color="black">Incorporating Stylistic Lexical Preferences in Generative Language
  Models</font>
    </a>
  </h2>
  <font color="black">
[要約]提案されたアプローチは、特定のターゲット著者の語彙スタイルに沿ったテキストを生成する可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Fully Bilingual Deep Language Modeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_60.html">
      <font color="black">Towards Fully Bilingual Deep Language Modeling</font>
    </a>
  </h2>
  <font color="black">
[概要]トレーニング前のデータを収集し、フィンランド語の英語のバイリンガルbertモデルを作成し、対応する単一言語モデルの評価に使用されるデータセットでそのパフォーマンスを評価します。単一言語モデルと同等のパフォーマンスを実現</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: XOR QA: Cross-lingual Open-Retrieval Question Answering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_61.html">
      <font color="black">XOR QA: Cross-lingual Open-Retrieval Question Answering</font>
    </a>
  </h2>
  <font color="black">
[概要] xorqaは、多言語の質問応答のための新しい手法の開発を容易にする挑戦的なタスクです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-dimensional Style Transfer for Partially Annotated Data using
  Language Models as Discriminators -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_62.html">
      <font color="black">Multi-dimensional Style Transfer for Partially Annotated Data using
  Language Models as Discriminators</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]複数のスタイルモデルの分析は可能ですが、特にスタイルの寸法が互いに完全に独立していない場合は、コンテンツが失われます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Gender Prediction Based on Vietnamese Names with Machine Learning
  Techniques -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_63.html">
      <font color="black">Gender Prediction Based on Vietnamese Names with Machine Learning
  Techniques</font>
    </a>
  </h2>
  <font color="black">
[概要]データセットは、性別に基づいて26,000を超えるフルネームで構成されています。このデータセットは、サイトの26,000を超えるフルネームに基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-21">
        <br><font color="black">2020-10-21</font>
      </time>
    </span>
</section>
<!-- paper0: Self-alignment Pre-training for Biomedical Entity Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/cs.CL/paper_64.html">
      <font color="black">Self-alignment Pre-training for Biomedical Entity Representations</font>
    </a>
  </h2>
  <font color="black">
[概要]私たちはどこで学んでいるのかを知る必要があります、と私たちは言います</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: ADL-MVDR: All deep learning MVDR beamformer for target speech separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_0.html">
      <font color="black">ADL-MVDR: All deep learning MVDR beamformer for target speech separation</font>
    </a>
  </h2>
  <font color="black">
[要約]提案された方法は、音声への影響の影響を減らすのに役立つ可能性があります。方法方法方法は、音声認識を改善するために使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-16">
        <br><font color="black">2020-08-16</font>
      </time>
    </span>
</section>
<!-- paper0: Confidence Estimation for Attention-based Sequence-to-sequence Models
  for Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_1.html">
      <font color="black">Confidence Estimation for Attention-based Sequence-to-sequence Models
  for Speech Recognition</font>
    </a>
  </h2>
  <font color="black">
[概要]信頼度推定モジュール（cem）と呼ばれる新しいシステムは、既存のasrモデルに基づいています。新しい方法では、モデルの信頼度としてデコーダーのソフトマックス確率を使用します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-End Classification of Reverberant Rooms using DNNs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_2.html">
      <font color="black">End-to-End Classification of Reverberant Rooms using DNNs</font>
    </a>
  </h2>
  <font color="black">
[要約]残響音声による研究は、深層学習が音声に対する空気の影響を使用して、録音された部屋の観点から録音を分類する方法を示唆しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-12-21">
        <br><font color="black">2018-12-21</font>
      </time>
    </span>
</section>
<!-- paper0: Microsoft Speaker Diarization System for the VoxCeleb Speaker
  Recognition Challenge 2020 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_3.html">
      <font color="black">Microsoft Speaker Diarization System for the VoxCeleb Speaker
  Recognition Challenge 2020</font>
    </a>
  </h2>
  <font color="black">
[概要]最初に、実際のマルチトーカー録音を処理する際の問題に対処するためのシステム設計について説明します。voxsrcchallenge2020によって提供されるデータセットには、YouTubeからの実際のマルチトークセルオーディオセットが含まれています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Class-Conditional Defense GAN Against End-to-End Speech Attacks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_4.html">
      <font color="black">Class-Conditional Defense GAN Against End-to-End Speech Attacks</font>
    </a>
  </h2>
  <font color="black">
[概要]特定の入力信号を自動エンコードする代わりに、提案されたアプローチは機能しません。代わりに、1d信号と元の位相情報をテストします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: All for One and One for All: Improving Music Separation by Bridging
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_5.html">
      <font color="black">All for One and One for All: Improving Music Separation by Bridging
  Networks</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT] mdlとclは、多くの既存のdnnベースの分離方法に簡単に適用できます。これらは、トレーニング中にのみ使用され、結論ステップに影響を与えない単なる損失関数です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-08">
        <br><font color="black">2020-10-08</font>
      </time>
    </span>
</section>
<!-- paper0: Mood Classification Using Listening Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_6.html">
      <font color="black">Mood Classification Using Listening Data</font>
    </a>
  </h2>
  <font color="black">
[概要]百万曲のデータセットに加えて、百万曲のデータセットのサブセットをコンパイルします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Backdoor Attack against Speaker Verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_7.html">
      <font color="black">Backdoor Attack against Speaker Verification</font>
    </a>
  </h2>
  <font color="black">
[概要]トレーニングデータをポイズニングすることで、話者検証モデルに感染するための隠しバックドアを挿入できることを実証しました。感染したモデルは良性のサンプルで正常に動作しますが、攻撃者が指定した未登録のトリガーは検証に合格します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Noise-Robust Adaptation Control for Supervised Acoustic System
  Identification Exploiting A Noise Dictionary -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_8.html">
      <font color="black">Noise-Robust Adaptation Control for Supervised Acoustic System
  Identification Exploiting A Noise Dictionary</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]提案されたアルゴリズムは、干渉ノイズ信号の顕著なスペクトルを利用します。提案されたアルゴリズムは、新しい機械学習ベースのノイズアプローチを表しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-03">
        <br><font color="black">2020-07-03</font>
      </time>
    </span>
</section>
<!-- paper0: Rethinking Evaluation in ASR: Are Our Models Robust Enough? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_9.html">
      <font color="black">Rethinking Evaluation in ASR: Are Our Models Robust Enough?</font>
    </a>
  </h2>
  <font color="black">
[要約]ベンチマークに関する調査結果は、通常、一般化に基づいて評価されます。調査によると、十分な数のベンチマークセットを使用すると、ベンチマークの平均ワードエラー率（wer）のパフォーマンスが、実際のデータセットでのパフォーマンスの優れたプロキシになります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Parallel Tacotron: Non-Autoregressive and Controllable TTS -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_10.html">
      <font color="black">Parallel Tacotron: Non-Autoregressive and Controllable TTS</font>
    </a>
  </h2>
  <font color="black">
[概要]自己回帰ニューラルオートエンコーダーを使用すると、テキストの1つから多くのマッピングの性質が音声合成の問題に緩和されます。また、自然性が向上し、自然性が向上します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: NU-GAN: High resolution neural upsampling with GAN -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_11.html">
      <font color="black">NU-GAN: High resolution neural upsampling with GAN</font>
    </a>
  </h2>
  <font color="black">
[概要] nu-ganは、テキスト内の個別のコンポーネントとしてオーディオのアップサンプリングを解決することに飛躍します-最大解像度。テクニックに加えて、nuganはgansを使用したオーディオ生成のテクニックを活用します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: The HUAWEI Speaker Diarisation System for the VoxCeleb Speaker
  Diarisation Challenge -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_12.html">
      <font color="black">The HUAWEI Speaker Diarisation System for the VoxCeleb Speaker
  Diarisation Challenge</font>
    </a>
  </h2>
  <font color="black">
[概要]話者ダイアリゼーションシステムに有害なBGMやノイズを除去するための新しいシステムが採用されています。提案されたシステムは、voxceleb話者認識チャレンジ2020のダイアリゼーションタスクのベースライン方法と比較して大幅な改善をもたらします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: wav2vec 2.0: A Framework for Self-Supervised Learning of Speech
  Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_13.html">
      <font color="black">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech
  Representations</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT] wav2vec 2. 0は、潜在空間での音声入力をマスクします。これは、3年前の350万ケースのラベル付きデータの量子化で定義された対照的なタスクを解決します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-20">
        <br><font color="black">2020-06-20</font>
      </time>
    </span>
</section>
<!-- paper0: A multilingual approach to joint Speech and Accent Recognition with
  DNN-HMM framework -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_14.html">
      <font color="black">A multilingual approach to joint Speech and Accent Recognition with
  DNN-HMM framework</font>
    </a>
  </h2>
  <font color="black">
[概要]英語の音声を認識するための多言語アプローチを提案します。次に、それらをマージして、多言語音声認識システムをトレーニングします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Graph Attention Networks for Speaker Verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_15.html">
      <font color="black">Graph Attention Networks for Speaker Verification</font>
    </a>
  </h2>
  <font color="black">
[概要]カリフォルニア大学の研究者は、新しい方法を提案しました。彼らは、注意を引くためのグラフの使用に関連していると言います。これには、話者の埋め込みを異なる発話から分離することが含まれます。結果は、平均的な等しいエラー率の改善と一致しています。コサイン類似性バックエンドを20％上回っています-テスト時間の増加なしで終了</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Transformer based unsupervised pre-training for acoustic representation
  learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_16.html">
      <font color="black">Transformer based unsupervised pre-training for acoustic representation
  learning</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]音響タスクには、音声感情認識、音声イベント検出、音声翻訳が含まれます。テストは、3種類の音響タスクで実施されました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-29">
        <br><font color="black">2020-07-29</font>
      </time>
    </span>
</section>
<!-- paper0: Momentum Contrast Speaker Representation Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_17.html">
      <font color="black">Momentum Contrast Speaker Representation Learning</font>
    </a>
  </h2>
  <font color="black">【概要】学習メカニズムの一形態として、voxceleb（mocovox）の運動量コントラストを提案する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: How Similar or Different Is Rakugo Speech Synthesizer to Professional
  Performers? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_18.html">
      <font color="black">How Similar or Different Is Rakugo Speech Synthesizer to Professional
  Performers?</font>
    </a>
  </h2>
  <font color="black">
[概要]研究は国際大学によって実施されました。結果は、本物の面白い音声合成に到達するための重要なステップを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Continuous Speech Separation with Conformer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_19.html">
      <font color="black">Continuous Speech Separation with Conformer</font>
    </a>
  </h2>
  <font color="black">
[概要]より厳密な分離モデルは、混合音声から単一の話者信号を抽出します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: A Qualitative Analysis of Haptic Feedback in Music Focused Exercises -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_20.html">
      <font color="black">A Qualitative Analysis of Haptic Feedback in Music Focused Exercises</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]ハプティックスはデジタル機器におけるハプティックスの役割であることがわかりました。ハプティックスは4つの別々のフィードバック段階を探索することができました。ハプティックスはデジタル機器（dmi）にとって最も効果的なツールです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: A Framework for Contrastive and Generative Learning of Audio
  Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_21.html">
      <font color="black">A Framework for Contrastive and Generative Learning of Audio
  Representations</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]自作の対照学習は、音声信号とそのさまざまな拡張バージョンをマッピングすることです。システムは、完全に監視された方法と比較して、グラウンドトゥルースラベルにアクセスして、かなりのタスクを実行します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: DBNET: DOA-driven beamforming network for end-to-end farfield sound
  source separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_22.html">
      <font color="black">DBNET: DOA-driven beamforming network for end-to-end farfield sound
  source separation</font>
    </a>
  </h2>
  <font color="black">
[概要]分離された音声信号とターゲット音声信号の間の距離に基づく損失関数を使用してdbnetをトレーニングすることを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Perceptual Loss based Speech Denoising with an ensemble of Audio Pattern
  Recognition and Self-Supervised Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_23.html">
      <font color="black">Perceptual Loss based Speech Denoising with an ensemble of Audio Pattern
  Recognition and Self-Supervised Models</font>
    </a>
  </h2>
  <font color="black">
[概要]知覚損失の概念に基づいて構築された知覚アンサンブル正則化損失（perl）と呼ばれるフレームワークを紹介します。最初に、vctoisoisデマンドと呼ばれる人気のある拡張ベンチマークでコンフォーマートランスフォーマーネットワークを使用して強力なベースライン（w / o perl）を構築します。モデル（perl --ae）は、音響イベントモデル（audiosetを利用）のみを使用して、主要な知覚メトリックで最先端の方法を上回ります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: WHALETRANS: E2E WHisper to nAturaL spEech conversion using modified
  TRANSformer network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_24.html">
      <font color="black">WHALETRANS: E2E WHisper to nAturaL spEech conversion using modified
  TRANSformer network</font>
    </a>
  </h2>
  <font color="black">
[概要]ささやきから自然に変換された音声フォルマントの確率分布は、地面に似ています-真理分布。これまでに、ささやきから自然に変換されたフォルマントが見つかりました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-20">
        <br><font color="black">2020-04-20</font>
      </time>
    </span>
</section>
<!-- paper0: DiDiSpeech: A Large Scale Mandarin Speech Corpus -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_25.html">
      <font color="black">DiDiSpeech: A Large Scale Mandarin Speech Corpus</font>
    </a>
  </h2>
  <font color="black">
[概要]コーパスは、6000人の話者からの約800時間の音声データと対応するテキストで構成されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: Transcription Is All You Need: Learning to Separate Musical Mixtures
  with Score as Supervision -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_26.html">
      <font color="black">Transcription Is All You Need: Learning to Separate Musical Mixtures
  with Score as Supervision</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]調査によると、スコア情報の使用は衛星の弱いラベルよりも優れており、敵対的な構造は分離と入力の両方のパフォーマンスの向上につながります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Position-Agnostic Multi-Microphone Speech Dereverberation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_27.html">
      <font color="black">Position-Agnostic Multi-Microphone Speech Dereverberation</font>
    </a>
  </h2>
  <font color="black">
[概要]私たちのアプローチは、ディープセットアプリの最近の進歩を利用して、残響ログスペクトルを強化するアーキテクチャを設計します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Dual-Signal Transformation LSTM Network for Real-Time Noise Suppression -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_28.html">
      <font color="black">Dual-Signal Transformation LSTM Network for Real-Time Noise Suppression</font>
    </a>
  </h2>
  <font color="black">
[概要]この方法は、短時間セオドア変換（stft）と、100万未満のパラメーターを使用したスタックネットワークアプローチで学習した分析と合成の基礎を組み合わせたものです。ネットワークは、実際の処理（1フレームイン、1フレームアウト）そして競争力のある結果に達する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-15">
        <br><font color="black">2020-05-15</font>
      </time>
    </span>
</section>
<!-- paper0: Robust Audio-Based Vehicle Counting in Low-to-Moderate Traffic Flow -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_29.html">
      <font color="black">Robust Audio-Based Vehicle Counting in Low-to-Moderate Traffic Flow</font>
    </a>
  </h2>
  <font color="black">
[概要]この方法は、交通監視データセットでトレーニングおよびテストされています。これには、1421ドルの範囲が含まれます。マイクを通過する車両です。この方法は、新しい高周波電力機能によって開発されました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: The NTU-AISG Text-to-speech System for Blizzard Challenge 2020 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_30.html">
      <font color="black">The NTU-AISG Text-to-speech System for Blizzard Challenge 2020</font>
    </a>
  </h2>
  <font color="black">
[概要]今年の課題は、リソースの制約が少ないttsシステムを構築することです。上海語の方言は、外部の北京語データを使用して、エンドツーエンドエンドツーエンドの音響モデルとウェーブネットボコーダーの両方をトレーニングします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Low-Resource StarGAN Voice Conversion using Weight Adaptive
  Instance Normalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_31.html">
      <font color="black">Towards Low-Resource StarGAN Voice Conversion using Weight Adaptive
  Instance Normalization</font>
    </a>
  </h2>
  <font color="black">
[概要]この作業では、モデルのデータ効率の向上を目指しています。実験は、2つの低リソース状況で109人のスピーカーを使用して実施されます。自然性と類似性の両方について、提案されたモデルはベースラインメソッドよりも優れています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Listening to 10 People Simultaneously: An Efficient Permutation
  Invariant Training of Audio Source Separation Using Sinkhorn's Algorithm -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_32.html">
      <font color="black">Towards Listening to 10 People Simultaneously: An Efficient Permutation
  Invariant Training of Audio Source Separation Using Sinkhorn's Algorithm</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]ライターは、シンクピットを使用して単一チャネル混合物を10個のソースに分解するニューラルネットワークモデルをトレーニングする実験を実施しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-channel target speech extraction with channel decorrelation and
  target speaker adaptation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_33.html">
      <font color="black">Multi-channel target speech extraction with channel decorrelation and
  target speaker adaptation</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]音声抽出のend-to％の調査はまだ制限されています。最初の調査は、並列チャネルでターゲット音声適応層を使用することです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: Analysis of the BUT Diarization System for VoxConverse Challenge -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_34.html">
      <font color="black">Analysis of the BUT Diarization System for VoxConverse Challenge</font>
    </a>
  </h2>
  <font color="black">
[概要]私たちのシステムは、一次メトリック（ダイアリゼーションエラー率）に関してチャレンジで2番目に、二次メトリックに従って最初にスコアを付けました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: AISHELL-3: A Multi-speaker Mandarin TTS Corpus and the Baselines -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_35.html">
      <font color="black">AISHELL-3: A Multi-speaker Mandarin TTS Corpus and the Baselines</font>
    </a>
  </h2>
  <font color="black">
[概要]コーパスには約85時間の感情が含まれています-ニュートラルな録音。これらの録音は218人の中国語のネイティブスピーカーによって話されています。システムはタコトロンの拡張です-2</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Robust Text-Dependent Speaker Verification via Character-Level
  Information Preservation for the SdSV Challenge 2020 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_36.html">
      <font color="black">Robust Text-Dependent Speaker Verification via Character-Level
  Information Preservation for the SdSV Challenge 2020</font>
    </a>
  </h2>
  <font color="black">
[ABSTRACT]タスク1は、テキストに依存する話者検証タスクです。これは、text --to-33333333 text --33333333333333333333333textに基づいています。このメソッドはテストシステムのテスターによって開発されました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Neural Network-based Acoustic Vehicle Counting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_37.html">
      <font color="black">Neural Network-based Acoustic Vehicle Counting</font>
    </a>
  </h2>
  <font color="black">
[要約]結果は、nnベースの距離エラーが以前に提案されたサポート利他的をはるかに上回っていることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Noise Robust TTS for Low Resource Speakers using Pre-trained Model and
  Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_38.html">
      <font color="black">Noise Robust TTS for Low Resource Speakers using Pre-trained Model and
  Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">
[概要]音声合成では、低品質で低リソースの音声データを使用します。モデルは、クリーンなデータとノイズの多い拡張データの両方で事前トレーニングされています。このモデルは、新しいスピーカーのクリーンな音声を合成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-26">
        <br><font color="black">2020-05-26</font>
      </time>
    </span>
</section>
<!-- paper0: Urban Sound Classification : striving towards a fair comparison -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_39.html">
      <font color="black">Urban Sound Classification : striving towards a fair comparison</font>
    </a>
  </h2>
  <font color="black">
[概要]都市の騒音公害は大都市で懸念が高まっています。これにより、都市では依然として懸念されている騒音公害を監視できます。コードはgithubリポジトリで入手でき、再現性が向上しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Similarity Analysis of Self-Supervised Speech Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_40.html">
      <font color="black">Similarity Analysis of Self-Supervised Speech Representations</font>
    </a>
  </h2>
  <font color="black">
[概要]有用な表現を学習するために多くのアルゴリズムが提案されています。これらには、幅広い音声タスクへの広範なアプリケーションが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: CycleGAN-VC3: Examining and Improving CycleGAN-VCs for Mel-spectrogram
  Conversion -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_41.html">
      <font color="black">CycleGAN-VC3: Examining and Improving CycleGAN-VCs for Mel-spectrogram
  Conversion</font>
    </a>
  </h2>
  <font color="black">
[概要] cycle-一貫した敵対的ネットワーク（cyclegan）とcyclegan-vc2は、ベンチマーク手法として広く使用されています。それらの問題に対処するために、cyclegan-vc / vc2からmel-スペクトログラムへの変換の適用性を調べました。サイクルガン-時間の過度の使用-周波数適応正規化（tfan）</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Representation Learning for Speaker Recognition via
  Contrastive Equilibrium Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_42.html">
      <font color="black">Unsupervised Representation Learning for Speaker Recognition via
  Contrastive Equilibrium Learning</font>
    </a>
  </h2>
  <font color="black">
[要約]研究は、話者の識別可能性を維持する必要があることを示しています。また、対照的な類似性損失関数が一緒に使用されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Prediction of Object Geometry from Acoustic Scattering Using
  Convolutional Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_43.html">
      <font color="black">Prediction of Object Geometry from Acoustic Scattering Using
  Convolutional Neural Networks</font>
    </a>
  </h2>
  <font color="black">
[概要]本研究では、散乱特徴からオブジェクトの彫刻を推測する方法を提案します。シミュレーションの完全なセットをサンプリングして、複数のデータセットを生成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-21">
        <br><font color="black">2020-10-21</font>
      </time>
    </span>
</section>
<!-- paper0: Self-training and Pre-training are Complementary for Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_44.html">
      <font color="black">Self-training and Pre-training are Complementary for Speech Recognition</font>
    </a>
  </h2>
  <font color="black">
[概要] libriからのわずか10分のラベル付きデータの使用-軽いだけでなく、53k時間のラベルなしデータも効果的です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Neural Kalman Filtering for Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_45.html">
      <font color="black">Neural Kalman Filtering for Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">
[要約]提案された方法は、音声強調のための新しいシステムの構築を支援するために使用できます。また、ニューラルカルマンフィルター処理（nkf）にも使用できます。また、ニューラルカルマンフィルター処理（nkf&#39;s）でも採用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-28">
        <br><font color="black">2020-07-28</font>
      </time>
    </span>
</section>
<!-- paper0: Muse: Multi-modal target speaker extraction with visual cues -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_46.html">
      <font color="black">Muse: Multi-modal target speaker extraction with visual cues</font>
    </a>
  </h2>
  <font color="black">
[要約]スピーチは通常、税引前です。平均して、それは通常、事前に記録されています..museは、si-sdrおよびpesqに関して他の競合ベースラインよりも優れていますが、優れたターゲットポイントも示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-15">
        <br><font color="black">2020-10-15</font>
      </time>
    </span>
</section>
<!-- paper0: Compositional embedding models for speaker identification and
  diarization with simultaneous speech from 2+ speakers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_47.html">
      <font color="black">Compositional embedding models for speaker identification and
  diarization with simultaneous speech from 2+ speakers</font>
    </a>
  </h2>
  <font color="black">
[要約]提案された方法は、構成的埋め込みに基づいています。それは、異なる話者からの音声を分離する関数fを含みます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: BERT for Joint Multichannel Speech Dereverberation with Spatial-aware
  Tasks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_48.html">
      <font color="black">BERT for Joint Multichannel Speech Dereverberation with Spatial-aware
  Tasks</font>
    </a>
  </h2>
  <font color="black">
[要約]提案された方法は、シーケンスからシーケンスへのマッピング問題として関連するタスクに対処します。これは、さまざまなフロントエンド音声強調タスクに十分一般的です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-21">
        <br><font color="black">2020-10-21</font>
      </time>
    </span>
</section>
<!-- paper0: LaSAFT: Latent Source Attentive Frequency Transformation for Conditioned
  Source Separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_49.html">
      <font color="black">LaSAFT: Latent Source Attentive Frequency Transformation for Conditioned
  Source Separation</font>
    </a>
  </h2>
  <font color="black">
[概要]このホワイトペーパーの目的は、ftブロックを拡張してマルチソースタスクに適合させることです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: Neural Audio Fingerprint for High-specific Audio Retrieval based on
  Contrastive Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-23/eess.AS/paper_50.html">
      <font color="black">Neural Audio Fingerprint for High-specific Audio Retrieval based on
  Contrastive Learning</font>
    </a>
  </h2>
  <font color="black">
[概要]この作業では、オーディオの短い単位セグメントから低次元を生成します。このフィンガープリントは、高速の最大内積検索です。トレーニングでは、システムは10分の1の小さいストレージを使用します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
