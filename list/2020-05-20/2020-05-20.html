<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-05-20の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Improving Accent Conversion with Reference Encoder and End-To-End
  Text-To-Speech -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.SD/paper_0.html">
      Improving Accent Conversion with Reference Encoder and End-To-End
  Text-To-Speech
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、提案された手法を適用すると、統合システムが音響的アイデンティティ（平均意見スコアの30 $ \％$の相対的な増加）とネイティブアクセント（68 $ \％$の相対的な好み）を大幅に向上させ、非ネイティブスピーカー。これは、ネイティブリファレンスから抽出された音響機能と言語情報によって動機付けられます。これは、従来の音声後部図（PPG）を補完するものであるため、PPGのみに基づいてベースラインシステムを改善する機能として連結できます。アクセント変換（AC）は、スピーカーの声の音色を維持しながら、非ネイティブスピーカーのアクセントをネイティブアクセントに変換します。 
[要約]新しい論文で、アクセント変換の改善を提案します。彼らは、音声の質とアクセントを改善することが可能であると言います
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Competitive Wakeup Scheme for Distributed Devices -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.SD/paper_1.html">
      Competitive Wakeup Scheme for Distributed Devices
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、ユーザーオリエンテーションは、最適なデバイスを決定するために支援されます。このために、マイクのエネルギーを受信するための精巧に設計されたキャリブレーション方法を使用して、競合するウェイクアップスキームがこの論文で提案されます。この問題を解決する唯一の方法は、同じワイヤレスローカルエリアネットワーク（WLAN）内のデバイスは、同じスコアリングルールに基づいてウェイクアップを競合します。 
[要約]この問題を解決する唯一の方法は、同じワイヤレスローカルエリアネットワーク（wlan）内のすべてのデバイスを同じスコアルールに基づいてウェイクアップを競合させることです
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: SpeechBERT: An Audio-and-text Jointly Learned Language Model for
  End-to-end Spoken Question Answering -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.SD/paper_2.html">
      SpeechBERT: An Audio-and-text Jointly Learned Language Model for
  End-to-end Spoken Question Answering
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さまざまなテキスト処理タスクで非常に成功したBERTモデルから学び、音声とテキストを組み合わせて学習したSpeechBERTモデルを提案しました。このモデルは、次のようなデータセットに関する次のテキスト質問応答（TQA）モデルを使用して、ASRをカスケードする従来のアプローチよりも優れています。 ASRがエラーを生成する前に、エンドツーエンドモデルがオーディオデータから情報を抽出できることが示されているため、応答スパンのASRエラー。音声言語理解タスクのさまざまなエンドツーエンドモデルが最近検討されていますが、この論文はおそらく、エンドツーエンドの音声による質問応答（SQA）の非常に難しい課題に挑戦する最初の既知の試みです。 
[要約]成功したbertモデルの一部として、音声と質問のスピーチベルトモデルを提案します。提案されたエンドツーエンドをアンサンブルする一方で、十分に準備された効果もありました。成功したモデルからの学習に加えて、これらは成功した
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-25">
        <br>2019-10-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Quaternion Neural Networks for Multi-channel Distant Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.SD/paper_3.html">
      Quaternion Neural Networks for Multi-channel Distant Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      連結されたマルチチャネル音声信号でトレーニングされたクォータニオン長期短期記憶ニューラルネットワーク（QLSTM）が、マルチチャネル遠隔音声認識の2つの異なるタスクで同等の実数値LSTMよりも優れていることを示します。自動音声認識（ASR）、遠方のASRはノイズと残響のために挑戦的なままです。四元数代数は標準のドット積をハミルトンのものに置き換え、要素間の依存関係をモデル化するシンプルでエレガントな方法を提供します。 
[要約]この論文では、これらの構造内および構造内依存性をクォータニオンニューラルネットワークでキャプチャすることを提案しています。クォータニオンレイヤーは、時間領域での長い依存関係を学習できる反復的なやりがいのあるニューラルネットワークと組み合わされます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Sams-Net: A Sliced Attention-based Neural Network for Music Source
  Separation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.SD/paper_4.html">
      Sams-Net: A Sliced Attention-based Neural Network for Music Source
  Separation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      MUSDB18データセットの実験結果は、パラメーターが少ない提案された方法が、最先端のDNNベースの方法のほとんどを上回っていることを示しています。これにより、マルチヘッドアテンションメカニズムとのスペクトル機能の相互作用が可能になり、並列計算が容易になり、 LSTMおよびCNNに比べてそれぞれ受容野が大きくなります。スペクトログラムまたは波形を入力とする畳み込みニューラルネットワーク（CNN）または長期短期記憶（LSTM）ベースのモデルは、深層学習ベースのオーディオソース分離によく使用されます。 
[ABSTRACT]この論文では、音楽ソース分離タスクのために、スペクトログラムドメインでスライスされた注意ベースのニューラルネットワーク（sams-net）を提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-12">
        <br>2019-09-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Acoustic Echo Cancellation by Combining Adaptive Digital Filter and
  Recurrent Neural Network -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.SD/paper_5.html">
      Acoustic Echo Cancellation by Combining Adaptive Digital Filter and
  Recurrent Neural Network
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      いくつかの非線形処理方法はすでに提起されていますが、それらは複雑で抑制に非効率的であり、音声オーディオに損傷を与えます。この論文では、適応フィルターとニューラルネットワークを組み合わせたフュージョンスキームがAECに提案されています。 。一般的な方法と比較した実験が行われ、提案された組み合わせスキームの有効性と優位性が検証されます。 
[ABSTRACT] aecは、適応フィルターとニューラルネットワークを組み合わせたフュージョン方式で提案されています。提案された組み合わせ方式は、人間の耳で使用でき、コミュニケーションを煩わしくする可能性があります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Deep Architecture Enhancing Robustness to Noise, Adversarial Attacks,
  and Cross-corpus Setting for Speech Emotion Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.SD/paper_6.html">
      Deep Architecture Enhancing Robustness to Noise, Adversarial Attacks,
  and Cross-corpus Setting for Speech Emotion Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      （1）ノイズ、（2）敵対的攻撃、（3）クロスコーパス設定に対するデータ拡張と組み合わせたアーキテクチャを包括的に評価します。また、堅牢性をさらに向上させるために、ネットワークアーキテクチャによるデータ拡張を提案します。音声感情認識システム（SER）は、トレーニングデータとテストデータが同じように分散されている場合に高精度を実現できますが、この仮定は実際には頻繁に違反され、SERシステムのパフォーマンスは予期しないデータシフトに対して急降下します。 
[ABSTRACT]正確なserの堅牢なモデルの設計は困難であり、実用的なアプリケーションでの使用が制限されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Meta-Learning for Short Utterance Speaker Recognition with Imbalance
  Length Pairs -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.SD/paper_7.html">
      Meta-Learning for Short Utterance Speaker Recognition with Imbalance
  Length Pairs
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ただし、既存の話者認識モデルは、このような短い発話では十分に機能しません。提案されたモデルを目に見えない話者の識別についても検証します。これにより、既存のアプローチよりも大幅にパフォーマンスが向上します。さらに、特定のクラスのエピソードは、目に見えないクラスの判別的な埋め込みを学習するには不十分な場合があります。さらに、トレーニングセットのクラスセット全体に対してサポートとクエリセットの両方を分類するようにモデルを強制します。 
[ABSTRACT]既存の話者認識モデルは短い発話ではパフォーマンスが低下します。既存のモデル認識モデルにはそのような発話はありません。モデルは既存の最新モデルより優れています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-06">
        <br>2020-04-06
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Iterative Pseudo-Labeling for Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.SD/paper_8.html">
      Iterative Pseudo-Labeling for Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      また、さまざまなコーパスでトレーニングされた言語モデルがIPLが追加のテキストを効果的に利用できることを示す効果についても調査します。次に、両方の標準のLibrispeechテストセットで最先端の単語エラー率を達成することにより、IPLの有効性を示します低リソース設定。IPLの主要コンポーネントである、言語モデルを使用したデコードとデータ拡張を研究します。 
[ABSTRACT]反復擬似ラベリング（ipl）は半教師付きアルゴリズムです。言語モデルとデータ拡張を使用してシステムを作成します。システムは追加のテキストを効果的に利用できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Toward Automated Classroom Observation: Multimodal Machine Learning to
  Estimate CLASS Positive Climate and Negative Climate -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.SD/paper_9.html">
      Toward Automated Classroom Observation: Multimodal Machine Learning to
  Estimate CLASS Positive Climate and Negative Climate
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ACORNは、畳み込みニューラルネットワークを使用して、スペクトルオーディオ機能、教師と生徒の顔、各画像フレームのピクセルを分析し、時間的畳み込みネットワークを使用してこの情報を経時的に統合します。オーディオビジュアルACORNのPCおよびNC予測には、ピアソン相関があります。 $ 0.55 $と$ 0.63 $、UVAトッドラーデータセットのエキスパートCLASSコーダーによって提供されたグラウンドトゥルーススコア（$ n = 300 $ 15分のビデオセグメントの相互検証）、および純粋な聴覚ACORNが$ 0.36の相関でPCとNCを予測METデータセットの$および$ 0.41 $（$ n = 2000 $ビデオセグメントのテストセット）。これらの数値は、人間のコーダーのコーダー間信頼性に似ています。 
[ABSTRACT] acornは、畳み込みニューラルネットワークを使用してデータの特徴を分析します。これらには、教師と生徒の顔、および各画像フレームのピクセルが含まれます。これらのデータは、人間のコーダーのコーダー間信頼性に似ています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Joint NN-Supported Multichannel Reduction of Acoustic Echo,
  Reverberation and Noise -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.SD/paper_10.html">
      Joint NN-Supported Multichannel Reduction of Acoustic Echo,
  Reverberation and Noise
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さまざまな状況でスマートスピーカーを使用して取得した音響エコー、残響、ノイズの実際の録音でシステムを評価します。音響エコー、残響、ノイズの同時低減の問題を考慮します。反復ブロック座標上昇アルゴリズムを開発して、すべてのフィルターを更新します。 
[要約]線形エコーキャンセレーションと残響除去後のターゲット信号と残差信号をモデル化することを提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-20">
        <br>2019-11-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Atss-Net: Target Speaker Separation via Attention-based Neural Network -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.SD/paper_11.html">
      Atss-Net: Target Speaker Separation via Attention-based Neural Network
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これにより、CNN-LSTMアーキテクチャーと比較して、ネットワークは各機能間の相関を並列で計算し、浅いレイヤーを使用してより多くの機能を抽出できます。実験結果は、Atss-NetがVoiceFilterよりも優れたパフォーマンスをもたらすことを示しています。パラメータの半分..さらに、提案されたモデルは、音声強調で有望なパフォーマンスも示します。 
[要約]このホワイトペーパーでは、タスクのスペクトログラムドメインで注意ベースのニューラルネットワーク（atss-net）を提案します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Unsupervised Cross-Domain Speech-to-Speech Conversion with
  Time-Frequency Consistency -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.SD/paper_12.html">
      Unsupervised Cross-Domain Speech-to-Speech Conversion with
  Time-Frequency Consistency
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      近年、生成的敵対的ネットワーク（GAN）ベースのモデルは、教師なしの音声から音声への変換にうまく適用されています。マグニチュードスペクトログラムの豊富なコンパクトな調和ビューは、これらのモデルをオーディオデータでトレーニングするのに適した選択肢と見なされています。生成されたマグニチュードスペクトログラムに一貫性がない可能性があるという問題。これは、完全なスペクトログラムが自然に聞こえる音声波形を持つような位相を見つけるために必要です。この作業では、この問題にアプローチするときに、スペクトログラムの一貫性を促進する条件を提案します。敵対的な訓練手順。 
[要約] librispeechコーパスの研究者は、tf整合性でトレーニングされたモデルが、知覚的に優れた音声品質を音声に変換することを示しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-15">
        <br>2020-05-15
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Augmenting Generative Adversarial Networks for Speech Emotion
  Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.SD/paper_13.html">
      Augmenting Generative Adversarial Networks for Speech Emotion
  Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      提案されたフレームワークの有効性を示すために、（i）合成特徴ベクトル、（ii）合成特徴によるトレーニングデータの拡張、（iii）圧縮表現でのエンコードされた特徴に関するSERの結果を示します。フレームワークは、効果的に圧縮された感情表現を学習できるだけでなく、コーパス内およびコーパス間の評価のパフォーマンス向上に役立つ合成サンプルを生成することもできます。この作業では、混合データ拡張スキームを利用してGANを拡張するフレームワークを提案します。特徴の学習と生成。 
[要約]提案されたシステムは、音声感情認識を向上させるために使用できます。ただし、新しいシステムで開発できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Enhancing Monotonic Multihead Attention for Streaming ASR -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.SD/paper_14.html">
      Enhancing Monotonic Multihead Attention for Streaming ASR
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最後に、安定したストリーミング推論を保証するヘッド同期ビーム検索デコードを提案します。各MAヘッドのチャンクごとの注意はマルチヘッド対応に拡張されます。さらに、境界検出のためにヘッド間のコンセンサスを改善し、遅延検出を防ぐために冗長ヘッドをプルーニングすることを提案します。そのようなヘッドによって引き起こされるトークンの生成。 
[ABSTRACT]トレーニング中にヘッドの一部をマスクして、ヘッドドロップの正規化を提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Sequential Multi-Frame Neural Beamforming for Speech Separation and
  Enhancement -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.SD/paper_15.html">
      Sequential Multi-Frame Neural Beamforming for Speech Separation and
  Enhancement
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、ビーム形成の定式化にコンテキストフレームを追加することで結果を大幅に改善するマルチフレームビーム形成方法を紹介します。最良の方法は、一連の3つの神経分離とマルチフレーム時間不変空間ビーム形成段階を利用し、 4つのやりがいのある反響のある音声強調および分離タスク全体で、スケール不変の信号対雑音比が2.75 dB向上し、強力なベースラインでの差分ワードエラー率が14.2％減少します。ウィンドウサイズの影響を広範囲に評価および分析します。 、ブロックサイズ、およびこれらのメソッドのマルチフレームコンテキストサイズ。 
[要約]私たちの最良の方法は、3つの神経分離とマルチフレームサイズのシーケンス、およびマルチフレームフレームフレーム形成段階を使用します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-18">
        <br>2019-11-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Lite Microphone Array Beamforming Scheme with Maximum Signal-to-Noise
  Ratio Filter -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.SD/paper_16.html">
      A Lite Microphone Array Beamforming Scheme with Maximum Signal-to-Noise
  Ratio Filter
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの実験は、他の広く使用されているアルゴリズムと比較すると、提案されたアルゴリズムは信号対干渉およびノイズ比（SINR）のゲインが高いことを示しています。このホワイトペーパーでは、最大信号対ノイズ比（ SNR）フィルターは、ビームフォーミングの複雑さを軽減するために提案されています。空間領域の情報を利用できるため、指向性の乱れを抑制して音声の品質を向上させるために、マイクロフォンアレイのビームフォーミングがよく使用されます。 
[ABSTRACT]最大snrフィルターは、音源定位（ssl）の推定到来方向（doa）を使用して作成されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Distilling Knowledge from Ensembles of Acoustic Models for Joint
  CTC-Attention End-to-End Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.SD/paper_17.html">
      Distilling Knowledge from Ensembles of Acoustic Models for Joint
  CTC-Attention End-to-End Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      TIMIT音素認識タスクの選択されたトレーニング手順の下でこれらの戦略を評価し、これらの戦略の有望なエラー率を一般的なベースラインと比較して観察しました。知識の蒸留は、幅広いパフォーマンスを維持しながら既存のディープラーニングモデルを圧縮するために広く使用されていますアプリケーションの範囲..このホワイトペーパーでは、マルチティーチャー蒸留法をctc-atentionエンドツーエンドASRシステムに拡張することを提案します。 
[要約]音響モデルのアンサンブルからの蒸留は、最近、認識性能の向上に有望な結果を示しています。このようにして、スピーチプレスの関連メトリックに向けて学生を直接蒸留して最適化します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Anomalous sound detection based on interpolation deep neural network -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.SD/paper_18.html">
      Anomalous sound detection based on interpolation deep neural network
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、提案されたアプローチが、特に非定常機械音に対して、標準AUCスコアに基づいて27％の改善を達成したことを示しました。力が弱まる中、産業機器のメンテナンスを行う省力化自動異音検知技術の需要が高まっています。 
[ABSTRACT]従来のアプローチは、オートエンコーダの再構成エラーに基づいて異常を検出します。スペクトログラムの複数のフレームを使用する代わりに、削除されたフレームの補間を出力として予測します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Novel Fusion of Attention and Sequence to Sequence Autoencoders to
  Predict Sleepiness From Speech -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.SD/paper_19.html">
      A Novel Fusion of Attention and Sequence to Sequence Autoencoders to
  Predict Sleepiness From Speech
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      両方のオートエンコーダーから学習した表現を評価し、早期の融合を行って、それらの間の可能な相補性を確認します。次に、時間依存周波数ベクトルと見なされるこれらのスペクトログラムで反復オートエンコーダーをトレーニングします。人間の視覚システムと機械翻訳の分野での最近の発展により、オーディオファイルから完全に教師なしの表現学習を行うために、自動エンコーダーに注意ベースの反復シーケンスを導入します。 
[ABSTRACT]私たちの表現では、オートエンコーダの特定の完全に接続されたレイヤーのアクティベーションを抽出します。また、これらのオーディオインスタンスの学習された機能を表すオートエンコーダのアクティベーションをトレーニングします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-15">
        <br>2020-05-15
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Active Learning for Coreference Resolution using Discrete Annotation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_0.html">
      Active Learning for Coreference Resolution using Discrete Annotation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      既存のベンチマーク相互参照データセットを使用した実験では、この追加の質問からの信号が、人間による注釈の1時間あたりのパフォーマンスの大幅な向上につながることを示しています。コードはhttps://github.com/belindal/discrete-active-learningで公開されています。 -coref ..この簡単な変更を、ラベルを付ける例を選択するための新規の言及クラスタリングアルゴリズムと組み合わせると、注釈バジェットあたりのパフォーマンスの点ではるかに効率的です。 
[ABSTRACT]アップグレードは、アノテーション予算ごとに取得されるパフォーマンスの点でより効率的です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-28">
        <br>2020-04-28
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving Accent Conversion with Reference Encoder and End-To-End
  Text-To-Speech -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_1.html">
      Improving Accent Conversion with Reference Encoder and End-To-End
  Text-To-Speech
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      まず、変換段階では参照音声が利用できないと想定しているため、ネイティブ音声でトレーニングされたエンドツーエンドのテキスト音声変換システムを使用して、ネイティブ参照音声を生成します。アクセント変換（AC）は、話者の声の音色を維持しながら、非ネイティブスピーカーのアクセントをネイティブアクセントに変換します。変換された音声の品質とアクセントを改善するために、マルチソース情報を利用できるリファレンスエンコーダーを導入します。 
[要約]新しい論文で、アクセント変換の改善を提案します。彼らは、音声の質とアクセントを改善することが可能であると言います
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Human Instruction-Following with Deep Reinforcement Learning via
  Transfer-Learning from Text -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_2.html">
      Human Instruction-Following with Deep Reinforcement Learning via
  Transfer-Learning from Text
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      その結果、ディープRLを使用した命令追跡には、通常、テンプレートから（環境シミュレーターによって）生成された言語が含まれます。これは、実際のユーザーの多様な表現またはあいまいな表現を反映しません。訓練されたテキストベースの言語モデル（BERT）、エージェントが自然な3Dシミュレートされた部屋で他のオブジェクトと比較して日常のオブジェクトを識別して配置する必要があるタスクで、合成テンプレートコマンドから指定された自然な指示への大幅なチャンスゼロショット転送私たちのアプローチは、人間のユーザーとやり取りするための深いRLベースのシステムをトレーニングするための一般的なレシピであり、注目すべき最近の成功の2つの研究方向間のギャップを埋めます。エージェント中心の運動行動とテキストベースの表現学習です。 
[ABSTRACT]新しいトレーニング方法は、深いrlを持つ人々のネットワークに基づいています。これらは、3D人間の指示などのモデルシステムに基づいています。これらの詳細な手順は、これらのタイプの接続をテストするように設計されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Generative Adversarial Training Data Adaptation for Very Low-resource
  Automatic Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_3.html">
      Generative Adversarial Training Data Adaptation for Very Low-resource
  Automatic Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この目的のために、CycleGANベースの非並列音声変換技術を使用して、テストスピーカーの音声に近いラベル付きトレーニングデータを偽造します。言語文化の遺産を保護するために、絶滅危惧言語の音声データを転記してアーカイブすることが重要です自動音声認識（ASR）は、このプロセスを容易にする強力なツールです。2つの低リソースコーパス（アイヌとムボシ）でこのスピーカー適応アプローチを評価しました。 
[要約]このスピーカーのスパース性の問題を軽減するために、トレーニング音声データ全体を変換して、テストスピーカーのように聞こえるようにすることを提案します。このスピーカー適応プロセスを、2つの低リソースコーパス、ainuおよびmboshiに基づいて評価しました。これら2つのコーパスを使用した教師なし適応および多言語トレーニングを含む従来の方法
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: SpeechBERT: An Audio-and-text Jointly Learned Language Model for
  End-to-end Spoken Question Answering -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_4.html">
      SpeechBERT: An Audio-and-text Jointly Learned Language Model for
  End-to-end Spoken Question Answering
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      エンドツーエンドのSQAの可能性に加えて、SpeechBERTは、他の多くの音声言語理解タスクでも、多くのテキスト処理タスクのBERTと同様に考慮することができます。エンドツーエンドモデルは、ASRがエラーを生成する前にオーディオデータから情報を抽出できることが示されたため、応答スパンのASRエラーを含むデータセットの応答（TQA）モデル。提案されたエンドツーエンドモデルを拡張する場合カスケードアーキテクチャにより、さらに優れたパフォーマンスが達成されました。 
[要約]成功したbertモデルの一部として、音声と質問のスピーチベルトモデルを提案します。提案されたエンドツーエンドをアンサンブルする一方で、十分に準備された効果もありました。成功したモデルからの学習に加えて、これらは成功した
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-25">
        <br>2019-10-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Cross-lingual Transfer Learning for Dialogue Act Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_5.html">
      Cross-lingual Transfer Learning for Dialogue Act Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      したがって、提案されたアプローチは、十分な数の手動注釈から利益を得ることができない特定のタスクへの自動DA認識の適用性を研究します。さらに、スピーカーのターンの埋め込みを計算するためにCNNとマルチヘッドの自己注意の両方を比較し、リソースが少ない状況では、転送された情報のすべてのソースを組み合わせることで最良の結果が得られます。このアーキテクチャの重要なコンポーネントは自動翻訳モジュールです。この制限は、外国語と翻訳された単語のシーケンスを同じモデルにスタックすることで対処されます。 
[要約]目的は、標準の英語のdaコーパスでトレーニングされたモデルを、ドイツ語とフランス語の2つの他の言語に転送することです。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Quantifying the Uncertainty of Precision Estimates for Rule based Text
  Classifiers -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_6.html">
      Quantifying the Uncertainty of Precision Estimates for Rule based Text
  Classifiers
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      証拠のデンプスターシェーファー理論を適用することにより、バイナリ分類子のセットを単一のマルチラベル分類子に組み合わせることができます。重要な部分文字列の有無を使用して分類の決定を行うルールベースの分類子には、自然な精度の不確実性を定量化するメカニズム。各確率変数の平均値は、その分割を誘導するドキュメントが提示されたときの分類子の精度の推定です。 
[ABSTRACT]バイナリ分類子の場合、重要な洞察は、ドキュメントによって引き起こされた部分文字列セットの分離をベルヌーイ確率変数として扱うことです。これらの手段は、望ましいまたは期待される分類子の精度と比較できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Assertion Detection in Multi-Label Clinical Text using Scope
  Localization -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_7.html">
      Assertion Detection in Multi-Label Clinical Text using Scope
  Localization
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      臨床領域のマルチラベル文（テキスト）は、患者ケア中のシナリオの豊富な説明に起因します。したがって、畳み込みニューラルネットワーク（CNN）アーキテクチャを開発して、複数のラベルとそのスコープを単一ステージでローカライズしました。そして、私たちのモデルは、マルチラベルの臨床テキストで最先端の手法よりも少なくとも12％優れていることを示しています。アサーション検出の最先端の手法は、ほとんどの場合、単一の設定でこのタスクに対処します。文ごとのアサーションラベル（テキスト）。 
[ABSTRACT] cnnのマルチラベル主導主導主導主導のセンテンスを使用しない状態。文ごとに1つのアサーションラベルの設定でメソッドを使用できました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Retrieving and Highlighting Action with Spatiotemporal Reference -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_8.html">
      Retrieving and Highlighting Action with Spatiotemporal Reference
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験を通じて、モデルがさまざまなアクションを条件とするさまざまなマップを生成することを示します。この場合、従来の視覚的推論方法は、単一の決定論的な顕著性マップを表示するだけです。キャプションの名詞と動詞に関連するローカルの埋め込みを生成します。また、このモデルは、MSR-VTTデータセットでアライメントなしのベースラインに対する検索の再現率を2〜3％改善します。 
[ABSTRACT]私たちの仕事は、トリミングされていないビデオ設定でアクションが発生する場所とタイミングを視覚化する、アクションハイライトという新しいタスクを引き受けます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Good-Enough Compositional Data Augmentation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_9.html">
      Good-Enough Compositional Data Augmentation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      プロトコルはモデルにとらわれず、さまざまなタスクに役立ちます。n-gram言語モデルに適用すると、複数の言語の小さなコーパスで複雑さを約1％削減できます。ニューラルシーケンス間モデルに適用すると、 SCANデータセットからの診断タスクではエラー率が87％、セマンティック解析タスクでは16％。 
[ABSTRACT]それは神経シーケンス-to-シーケンスモデルに適用されます。スキャンデータセットからの診断タスクのエラーを最大87％削減します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-21">
        <br>2019-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Adversarial Alignment of Multilingual Models for Extracting Temporal
  Expressions from Text -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_10.html">
      Adversarial Alignment of Multilingual Models for Extracting Temporal
  Expressions from Text
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このペーパーでは、テキストから時間表現を抽出するための多言語手法を探索し、埋め込みスペースを1つの共通スペースに揃えるための敵対的なトレーニングを調査します。時間タグ付けは依然としてルールベースのシステムによって支配されていますが、神経時間タガー..しかし、それらはすべて単一言語設定に焦点を当てています。 
[ABSTRACT]多言語モデルは目に見えない言語にも転送できます。これらのクロスリンガル転送実験で設定された新しい最先端の技術
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: The Effect of Moderation on Online Mental Health Conversations -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_11.html">
      The Effect of Moderation on Online Mental Health Conversations
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この作業では、メンタルヘルスのモバイルアプリケーションでホストされている7,000の会話から200,000のメッセージにわたって発生する自然な実験を活用して、オンラインのメンタルヘルスのディスカッションに対するモデレーションの影響を評価しました。グループのメンタルヘルスのディスカッションへの参加が改善につながることがわかりました心理的観点から、そしてこれらの改善はモデレートされた会話でより大きかった。モデレーションはオンラインの談話の質を向上させることができるが、オンラインのメンタルヘルスの会話への影響についての理解が不足している。 
[ABSTRACT]オンラインメンタルヘルスコミュニティは、セラピストやサポートグループとのセッションに代わる、スケーラブルで簡単にアクセスできる代替手段を提供します。モデレーションにより、オンライン談話の質を向上させることができますが、オンライン会話への影響については理解できていません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Bayesian Subspace HMM for the Zerospeech 2020 Challenge -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_12.html">
      Bayesian Subspace HMM for the Zerospeech 2020 Challenge
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      SHMMは、各ユニットをHMMとしてモデル化します。そのパラメータは、音声の変動性をモデル化するようにトレーニングされた全パラメータ空間の低次元サブスペースにあるように制約されます。このシステムでは、ユニットにベイズ部分空間隠れマルコフモデル（SHMM）を使用します。発見..私たちのシステムは、非常に低い単位ビットレートを維持しながら、人間が評価した文字エラー率のベースラインと比較できます。 
[ABSTRACT]私たちのシステムは、人間が評価した文字エラー率のベースラインと比較して好意的です。ユニットの発見には、隠れマルコフモデル（shmm）も使用します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Modeling relation paths for knowledge base completion via joint
  adversarial training -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_13.html">
      Modeling relation paths for knowledge base completion via joint
  adversarial training
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、この方法が既存のパス情報ベースのアプローチよりも優れていることを示しています。モデルの各サブモジュールは適切に解釈できるため、モデルは多数の関係学習タスクに適用できます。関係とマルチホップを処理することによりパスを2つの異なる入力ソースとして使用する場合、2つのダウンストリームコンポーネントで共有される機能エクストラクターを使用します（つまり、
[ABSTRACT]ほとんどの既存のkbcメソッドは、ナレッジベース（kb）を特定のセマンティックスペースに埋め込むことに焦点を当てています。代わりに、単一の関係間の関係を探索する新しい方法。空間的注意ネットワーク（hans）を使用して、マルチホップパスで重要な関係を選択します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-10-14">
        <br>2018-10-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Closing the Gap: Joint De-Identification and Concept Extraction in the
  Clinical Domain -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_14.html">
      Closing the Gap: Joint De-Identification and Concept Extraction in the
  Clinical Domain
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちは、英語（匿名化の場合は96.1％F1、概念抽出の場合は88.9％F1）とスペイン語（概念抽出の場合は91.4％F1）のベンチマークデータセットに新しい最先端の技術を設定しました。臨床ドメインで自然言語処理を利用するには、匿名化、つまりテキスト内の個人情報の匿名化。特に、プライバシーに敏感な情報へのアクセスが制限されたスタックモデルとマルチタスクモデルを提案します。 
[ABSTRACT] de-識別タスクとダウンストリームタスクは分離されているだけです。ただし、この研究では、識別解除が他のタスクに与える影響を調査していません。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Grounding Conversations with Improvised Dialogues -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_15.html">
      Grounding Conversations with Improvised Dialogues
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      現代の対話システムは、共通の基盤を構築するように明示的にトレーニングされていないため、コミュニケーションのこの重要な側面を見落としています。私たちは、26,000以上のイエスとターンのコーパスを収集し、即興の対話からそれらを書き起こし、ブートストラップされた分類子を介して、より大きく、まばらに配置された映画のスクリプトの対話コーパスから抽出します。 
[ABSTRACT]私たちは26,000以上のコーパスを収集していますはい-そして対話と協力するようになります。チャットで共通の基盤を構築する方法を知る必要があります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-20">
        <br>2020-04-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Vector-quantized neural networks for acoustic unit discovery in the
  ZeroSpeech 2020 challenge -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_16.html">
      Vector-quantized neural networks for acoustic unit discovery in the
  ZeroSpeech 2020 challenge
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      2番目のモデルは、ベクトル量子化とコントラスト予測コーディング（VQ-CPC）を組み合わせたものです。ABX電話識別テストでは、どちらのモデルも、2019年および2020年の課題へのすべての提出物を上回り、30％以上の相対的な改善が見られます。将来の音響単位を予測することにより、音声の表現を学びます。 
[ABSTRACT]コンセプトは、将来の音響単位を予測することによって音声の表現を学習することです。vq-vaeは音声を離散的な表現にエンコードし、そこから自動波形が再構築されます。両方のモデルが、2019および2020の課題へのすべての提出よりも優れています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Zero-Resource Cross-Domain Named Entity Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_17.html">
      Zero-Resource Cross-Domain Named Entity Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最後に、実験結果は、モデルが強力な教師なしクロスドメインシーケンスラベリングモデルよりも優れていることを示しており、モデルのパフォーマンスは、豊富なリソースを活用する最先端のモデルのパフォーマンスに近いものです。トークンが名前付きエンティティであるかどうかを検出する新しい目的関数を追加して、タスク学習（MTL）。したがって、外部リソースを使用しないクロスドメインNERモデルを提案します。 
[ABSTRACT]最初に、トークンがエンティティに名前が付けられているかどうかを検出する新しい目的関数を追加して、マルチタスク学習（mtl）を導入します。次に、実験結果から、モデルが強力な教師なしクロスドメインシーケンスラベル付けモデルより優れていることがわかります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-14">
        <br>2020-02-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Iterative Pseudo-Labeling for Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_18.html">
      Iterative Pseudo-Labeling for Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      次に、標準と低リソースの両方の設定でLibrispeechテストセットの最先端の単語エラー率を達成することにより、IPLの有効性を示します。また、さまざまなコーパスでトレーニングされた言語モデルがIPLを示すようにトレーニングした効果も調査します。追加のテキストを効果的に利用できます。最後に、Librispeechトレーニングトランスクリプションと重複しない新しい大きなドメイン内テキストコーパスをリリースして、低リソースの半教師付きASR 
[ABSTRACT]反復擬似ラベリング（ipl）の研究を促進します。半教師付きアルゴリズムです。言語モデルとデータ拡張を使用してシステムを作成します。システムは追加のテキストを効果的に利用できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Embeddings as representation for symbolic music -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_19.html">
      Embeddings as representation for symbolic music
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これを行うには、結果の埋め込みをt-SNE手法を使用して投影で視覚化します。このホワイトペーパーでは、埋め込みを実験して、データセットの3つの異なるバリエーションからの音符を表し、モデルが有用な音楽パターンをキャプチャできるかどうかを分析します。音楽的な意味を含む方法で音楽をエンコードできる表現手法は、メロディーの生成やより高品質のハーモニーなどのコンピューター音楽タスク用にトレーニングされたモデルの結果を改善します。 
[ABSTRACT]自然言語処理の分野は、単語や文章の意味を捉える方法を見つけるために多くの作業を行ってきました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Predicting Strategic Behavior from Free Text -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_20.html">
      Predicting Strategic Behavior from Free Text
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このペーパーは、これら2つの研究を結びつけることを目的としています。これは、Web上での膨大なオンラインテキストコミュニケーションのために非常にタイムリーで重要であると考えています。メッセージングとアクションの間の接続は、Web検索や感情分析などのWebアプリケーションの両方にとって基本です。 、そして経済学に。私たちは、クラウドソーシングを介して、常識的な性格属性を個人が書いたフリーテキストに帰属させ、トランスダクティブ学習を使用して、これらの属性に基づいてワンショットゲームでこれらの個人が取る行動を予測することによって問題に取り組みます。 
[要約]経済学の文献は、ゲームでの重要な決定とマルチエージェントの遭遇へのメッセージング間の接続に焦点を当てています。私たちは、群衆を介して常識的な人格属性を帰属させることによって問題に取り組みます-個人によって書かれたフリーテキストを調達する
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-06">
        <br>2020-04-06
      </time>
    </span>
  </h3>
</article>
<!-- paper0: On the Choice of Auxiliary Languages for Improved Sequence Tagging -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_21.html">
      On the Choice of Auxiliary Languages for Improved Sequence Tagging
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最近の研究では、関連言語からの埋め込みにより、単一言語モデルの場合でも、シーケンスタグ付けのパフォーマンスが向上することが示されています。さらに、注意ベースのメタ埋め込みにより、異なる言語からの事前トレーニング済み埋め込みを効果的に組み合わせて、シーケンスタグ付けを行い、新しい状態を設定できることが示されています。 5つの言語での品詞タグ付けの最新の結果。この分析ペーパーでは、言語の距離に基づいて最適な補助言語を予測できるかどうかを調査し、最も関連性の高い言語が常に最適であるとは限らないことを示します。補助言語。 
[ABSTRACT]最適な補助言語は、距離、距離、距離に基づいて予測できます。最も関連する言語が常に最適であるとは限りません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Staying True to Your Word: (How) Can Attention Become Explanation? -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_22.html">
      Staying True to Your Word: (How) Can Attention Become Explanation?
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらの問題に対する対策を単語レベルの目標の形で提案します。この調査結果は、反復モデルの忠実な解釈を提供するための注意の信頼性を提供します。シーケンス分類タスクにおける再帰ネットワーク。後者の側面は、近年、最も注目されている話題になりました。特に、Jain and Wallace、2019の研究で顕著です。 Wiegreffe and Pinter、2019。
[ABSTRACT]アテンションメカニズムが具体的に証明されずに、リンボに行き詰まっている。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Enhancing lexical-based approach with external knowledge for Vietnamese
  multiple-choice machine reading comprehension -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_23.html">
      Enhancing lexical-based approach with external knowledge for Vietnamese
  multiple-choice machine reading comprehension
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちが提案する手法は、61.81 \％の精度を達成します。これは、最良のベースラインモデルよりも5.51 \％高くなります。また、データセットで人間のパフォーマンスを測定したところ、人間とモデルのパフォーマンスの間に大きなギャップがあることがわかりました。さらに、語彙ベースのMRC手法を提案します。これは、意味論的類似性測定値と外部知識ソースを利用して、質問を分析し、所定のテキストから回答を抽出します。 
[ABSTRACT]このタスクの高品質のベンチマークデータセットが不足しているのは、高品質のタスクが不足していることが原因です。このテキストは、通常、小学校の生徒の読解力を教えるために使用されます。データセットは、研究目的で当社のウェブサイトで無料で利用できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-16">
        <br>2020-01-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: ScenarioSA: A Large Scale Conversational Database for Interactive
  Sentiment Analysis -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_24.html">
      ScenarioSA: A Large Scale Conversational Database for Interactive
  Sentiment Analysis
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      対話型感情分析は、感情分析問題の新たな、しかし挑戦的なサブタスクです。既存の感情データセットと比較すると、ScenarioSA（1）は幅広いシナリオをカバーしています。 （2）2人の話者間の相互作用について説明します。 （3）会話の過程での各スピーカーの感情の進化を反映します。既存の感情分析アプローチは、人々間の相互作用をモデル化するには不十分です。 
[要旨] 2、214マルチターンの英会話に手動でラベルを付けます。既存のデータセットはインタラクティブなデータセットの欠如に基づいています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-07-12">
        <br>2019-07-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Table Search Using a Deep Contextualized Language Model -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_25.html">
      Table Search Using a Deep Contextualized Language Model
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      BERTの構造と入力長の制限を考慮してテーブルコンテンツをエンコードする方法を調査します。また、テーブル検索に関する以前の文献の特徴を組み込み、それらをBERTと共同でトレーニングするアプローチを提案します。複数の事前トレーニングタスクと大規模トレーニングコーパスのメリット、事前トレーニング済みモデルは、複雑な構文上の単語の関係をキャプチャできます。 
[ABSTRACT]事前トレーニング済みのモデルは、複雑な構文上の単語の関係をキャプチャできます。公開データセットでの実験では、最善のアプローチが以前の状態よりも優れていることを示しています-最先端の方法
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Enhancing Monotonic Multihead Attention for Streaming ASR -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_26.html">
      Enhancing Monotonic Multihead Attention for Streaming ASR
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      各MAヘッドのチャンク単位の注意は、マルチヘッド対応に拡張されています。最後に、安定したストリーミング推論を保証するために、ヘッド同期ビーム検索デコードを提案します。ハード単調注意をトランスフォーマーベースの自動に拡張することにより、単調マルチヘッド注意（MMA）を調査します。オンラインストリーミングアプリケーション用の音声認識（ASR）。 
[ABSTRACT]トレーニング中にヘッドの一部をマスクして、ヘッドドロップの正規化を提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Neural Entity Linking on Technical Service Tickets -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_27.html">
      Neural Entity Linking on Technical Service Tickets
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最後に、さまざまなBERTベースのアーキテクチャを比較し、単純な文単位のエンコーディング（Bi-Encoder）が実際に高速かつ効率的な検索を提供することを示します。ウィキペディアなど-ラベルが不足し、テキストが低品質で、用語が非常にドメイン固有である実用的なビジネスユースケースへの移行。エンティティのリンク、既知のエンティティへのテキストの言及のマッピングのタスクは、最近、文脈化ニューラルネットワーク。 
[ABSTRACT]ウィキペディアなどのデータセットからの結果は、それらを使用できることを示しています。ただし、結果はビジネス用途に移行する可能性が高いことを示しています。大規模なコーパスでの転移学習の利点が実証されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-15">
        <br>2020-05-15
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multi-task Learning for Low-resource Second Language Acquisition
  Modeling -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_28.html">
      Multi-task Learning for Low-resource Second Language Acquisition
  Modeling
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このアイデアにインスパイアされて、本稿では、マルチタスク学習によって異なる言語学習データセット間の潜在的な共通パターンを学習し、さらに、低リソースシナリオでの予測パフォーマンスの向上に適用される、新しいSLAモデリング手法を提案します。広範な実験は、提案された方法が低リソースシナリオで最先端のベースラインよりもはるかに優れていることを示しています。その一方で、非リソースシナリオでもわずかに改善が得られます。 
[ABSTRACT]言語言語言語-学習システムは新しい方法で使用できます。提案された方法は、低水準のリソースシナリオにおける最先端のベースラインよりもはるかに優れたパフォーマンスを発揮します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-25">
        <br>2019-08-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Comparing Transformers and RNNs on predicting human sentence processing
  data -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_29.html">
      Comparing Transformers and RNNs on predicting human sentence processing
  data
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      訓練された言語モデルを使用して、いくつかの読書実験で使用される刺激の驚き値を計算し、混合線形モデリングを使用して、驚きが人間の読書努力の測定をどの程度説明できるかを測定します。分析では、トランスフォーマーがRNNを認知モデルとして上回っていることを示しています。自己ペースの読書時間とN400強度を説明するが、視線追跡実験からの注視時間は説明しない。 
[ABSTRACT]トランスフォーマーアーキテクチャは、多くの自然言語処理タスクでリカレントニューラルネットワークよりも優れていることが示されています。人間の言語処理をモデル化する能力についてはほとんど知られていません。トランスフォーマーとrnnベースの言語モデルの両方をトレーニングし、人間のモデルとしてのパフォーマンスを比較します文処理
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Matching Questions and Answers in Dialogues from Online Forums -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_30.html">
      Matching Questions and Answers in Dialogues from Online Forums
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      Ubuntuデータセットなどの既存の対話データセットはQAマッチングタスクに適していないため、1,000のラベルが付けられた対話を含むデータセットをさらに作成し、提案されたモデルが、特にマッチングに関して、最先端およびその他の強力なベースラインよりも優れていることを示します長距離QAペア..各非質問ターンとその候補質問との間のトレーニング済みモデルによって計算されたスコアが与えられると、貪欲マッチング戦略が最終予測に使用されます。会話の2つのターン間の質問と回答のマッチングは、対話構造を分析する最初のステップですが、対話システムのトレーニングにも役立ちます。 
[要約] qaマッチングモデルは、相互注意と呼ばれる2つの同時注意メカニズムによって距離情報と対話履歴の両方を考慮します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Schema2QA: Answering Complex Queries on the Structured Web with a Neural
  Model -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_31.html">
      Schema2QA: Answering Complex Queries on the Structured Web with a Neural
  Model
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      レストランのドメインでは、Schema2QAは、クラウドソーシングされた多様な一連の質問で最高の商業アシスタントより21％優れています。また、既存の仮想アシスタントによく寄せられる質問に対しても同等の精度を実現しています。Schema2QAを2つの異なるSchema.orgドメイン、レストラン、これらのドメインにSchema.orgメタデータを持つすべてのWebサイトは、Schema2QAを使用して、Webサイト固有のスキルを自動的に構築できます。 
[ABSTRACT] schema2qaは、データベーススキーマのコンパウンドからaqとスキルを構築できるオープンソースのツールキットです。このツールキットでは、各フィールドに数個の手動の注釈が必要です。レストランからホテルのドメインにアシスタントが学習することを示しています、クラウドソーシングされた質問に対して、手作業なしで64％の精度を達成できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-16">
        <br>2020-01-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Classical linear logic, cobordisms and categorial grammars -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_32.html">
      Classical linear logic, cobordisms and categorial grammars
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これにより、単語cobordismsのタイピングシステムとして線形論理を使用できるようになります。このようなオブジェクトを{\ it word cobordisms}と呼びます。古典的な乗法線形論理に基づくカテゴリ文法を提案します。 
[ABSTRACT]これは、抽象的なccの文法の拡張と見なすことができます。「functional」などの単語は「monoidal」および「機能的に多様」と呼ばれます。ただし、これにより、acgの具体的かつ直感的な表現が得られます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-10">
        <br>2019-11-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: ISeeU2: Visually Interpretable ICU mortality prediction using deep
  learning and free-text medical notes -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_33.html">
      ISeeU2: Visually Interpretable ICU mortality prediction using deep
  learning and free-text medical notes
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この研究では、生の看護メモを使用して死亡率を予測するMIMIC-IIIでトレーニングされたディープラーニングモデルと、単語の重要性に関する視覚的な説明を示します。一方、ディープラーニングは、医療データを活用して臨床実践にプラスの影響を与える可能性を秘めています死亡率予測を含む診断と予測。私たちのモデルはROCが0.8629（+/- 0.0058）に達し、従来のSAPS-IIスコアよりも優れており、同様のディープラーニングアプローチと比較すると解釈性が向上しています。 
[要約]予測を生成するときに、強力なディープラーニングモデルが健全な医学知識に裏付けられた相関関係に参加するかどうかの問題は未解決のままです。信頼性を高め、臨床医によるaiの使用を促進するために、より多くの解釈ツールが必要です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Fast, Simpler and More Accurate Hybrid ASR Systems Using Wordpieces -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_34.html">
      Fast, Simpler and More Accurate Hybrid ASR Systems Using Wordpieces
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、モデリング単位としてワードピースを使用すると、精度を失うことなくより大きなストライドを使用できるため、ランタイム効率が大幅に向上します。次に、CTCトレーニングと組み合わせてモデリング単位としてワードピースを使用すると、従来のフレームベースに比べてエンジニアリングパイプラインを大幅に簡略化できることを示しますすべてのGMMブートストラップ、ディシジョンツリー構築、および強制整列ステップを除外しながら、クロスエントロピートレーニングを行いますが、非常に競争力のある語誤り率を達成しています。2つの内部\ emph {VideoASR}データセットでこれらの結果をさらに確認します。融合言語としての英語や、膠着言語であるトルコ語に似ています。 
[要旨]単語をモデリングユニットとして使用し、ctcトレーニングと組み合わせると、従来のフレームベースのクロスシンプソントレーニングと比較して、エンジニアリングパイプラインを大幅に簡略化できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Recurrent Chunking Mechanisms for Long-Text Machine Reading
  Comprehension -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_35.html">
      Recurrent Chunking Mechanisms for Long-Text Machine Reading
  Comprehension
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      3つのMRCデータセット（CoQA、QuAC、およびTriviaQA）の実験は、提案された反復チャンクメカニズムの有効性を示しています。完全な回答を含む可能性が高いセグメントを取得すると同時に、グラウンドトゥルースの回答に十分なコンテキストを提供できます。結果として、それらは正しい回答スパンをカバーできない、またはその周りに不十分なコンテキストを保持しないセグメントを形成する可能性があり、パフォーマンスを大幅に低下させます。さらに、クロスセグメント情報を必要とする質問に答える能力が低下します。 。 
[ABSTRACT]最先端のモデルは、事前トレーニング済みのトランスモデルを使用する傾向があります。これらは、gグラムをエンコードできるという事実に基づいています。クロスパー情報を必要とする質問には答えられません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-16">
        <br>2020-05-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Functorial Language Games for Question Answering -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_36.html">
      Functorial Language Games for Question Answering
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      自然言語処理におけるゲーム理論の語用論と質問応答への応用を用いて、ウィトゲンシュタインの言語ゲームのいくつかのカテゴリー調査を提示します。 
[ABSTRACT]ウィットゲンシュタインの言語-ゲーム、ゲームへの応用-理論的実用論と質問-自然言語処理への回答について、いくつかのカテゴリー調査を提示します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_37.html">
      A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらの課題に対処するために、最初に、同じ料理の異なるレシピの命令間のペアワイズアライメントを学習する教師なしアライメントアルゴリズムを使用します。次に、グラフアルゴリズムを使用して、同じ料理の複数のテキストレシピと複数のビデオレシピ間のジョイントアライメントを導出します。 。豊富な常識情報を備えた4,262皿のレシピ間の150Kのペアワイズアライメントを含むMicrosoft Researchマルチモーダルアライメントレシピコーパスをリリースします。 
[ABSTRACT]ウェブでは、テキストとビデオのレシピが部分的に重複しています。レシピは、手順の順序や食材の使用方法が異なります。ビデオの手順は、うるさく、テキストの手順よりもはるかに多くの情報を含む傾向があります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Investigations on Phoneme-Based End-To-End Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/cs.CL/paper_38.html">
      Investigations on Phoneme-Based End-To-End Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      Switchboard 300hで実験を行い、音素ベースのモデルが書記素ベースのモデルと競合することを示すことができます。これらは、コンテキストのない単一の音素（〜40ラベル）、または1つの出力ラベルで複数の音素をまとめることができます。さらなる実験では、同音異義語（同じ発音の異なる単語）を区別できるように、音素セットを補助単位で拡張します。 
[要約]書記素ベースと音素ベースの出力ラベルを体系的に比較します。これらには、音素ベースの注意ラベルが含まれます。この目的のために、音素ベースのBPEラベルを導入します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Improved Noisy Student Training for Automatic Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_0.html">
      Improved Noisy Student Training for Automatic Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これにより、LibriSpeech 100h（4.74％/ 12.20％）およびLibriSpeech（1.9％/ 4.1％）で達成された以前の最先端のクリーン/ノイズテストWERを改善することができます。これにより、 LibriSpeechのクリーンな100hサブセットを監視対象セットとして、残り（860h）をラベルなしセットとしてのみ使用することにより、クリーン/ノイズの多いLibriSpeechテストセットでワードエラー率（WER）4.2％/ 8.6％を取得する方法。セルフトレーニングの反復の間に生成されたデータをフィルターにかけ、バランスを取り、増強します。 
[ABSTRACT]騒々しい学生のトレーニングは自動セルフトレーニング方式です。写真セッションの間に生成されたデータをフィルタリング、バランス調整、および拡張できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving Accent Conversion with Reference Encoder and End-To-End
  Text-To-Speech -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_1.html">
      Improving Accent Conversion with Reference Encoder and End-To-End
  Text-To-Speech
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      アクセント変換（AC）は、ネイティブではない話者のアクセントを、話者の声の音色を維持しながら、ネイティブなアクセントに変換します。それらは、PPGのみに基づいてベースラインシステムを改善する機能として連結できます。最初に、変換段階で参照音声が利用できないと想定し、エンドツーエンドのテキスト音声変換システムを採用します。ネイティブ音声でトレーニングされ、ネイティブ参照音声を生成します。 
[要約]新しい論文で、アクセント変換の改善を提案します。彼らは、音声の質とアクセントを改善することが可能であると言います
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Generative Adversarial Training Data Adaptation for Very Low-resource
  Automatic Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_2.html">
      Generative Adversarial Training Data Adaptation for Very Low-resource
  Automatic Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この目的のために、CycleGANベースの非並列音声変換技術を利用して、テストスピーカーの音声に近いラベル付きトレーニングデータを偽造します。このスピーカー適応アプローチを、アイヌとムボシの2つの低リソースコーパスで評価しました。この作業では、この話者のスパース性の問題を軽減するために、この話者用の高精度ASRシステムを開発するために、トレーニング音声データ全体を変換し、テスト話者のように聞こえるようにすることを提案します。 
[要約]このスピーカーのスパース性の問題を軽減するために、トレーニング音声データ全体を変換して、テストスピーカーのように聞こえるようにすることを提案します。このスピーカー適応プロセスを、2つの低リソースコーパス、ainuおよびmboshiに基づいて評価しました。これら2つのコーパスを使用した教師なし適応および多言語トレーニングを含む従来の方法
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Competitive Wakeup Scheme for Distributed Devices -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_3.html">
      Competitive Wakeup Scheme for Distributed Devices
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      すべてのデバイスで同じウェイクアップワードが使用されている場合、すべてのデバイスが応答します。さらに、ユーザーオリエンテーションは、最適なデバイスを決定するのに役立ちます。ウェイクアップは、マンマシンインタラクションの主流スキームである音声インタラクションの主要機能です。 （HMI）スマートホーム用のアプリケーション。 
[要約]この問題を解決する唯一の方法は、同じワイヤレスローカルエリアネットワーク（wlan）内のすべてのデバイスを同じスコアルールに基づいてウェイクアップを競合させることです
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Robust Beam Search for Encoder-Decoder Attention Based Speech
  Recognition without Length Bias -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_4.html">
      Robust Beam Search for Encoder-Decoder Attention Based Speech
  Recognition without Length Bias
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      再解釈された確率をビームプルーニングと一緒に適用することにより、得られた最終確率は、異なる長さの出力シーケンス間の信頼できる比較を可能にするロバストなモデル修正につながります。LibriSpeechコーパスの実験的検証は、提案されたアプローチがヒューリスティックスまたは追加の調整作業..ヒューリスティックスは適切なモデリングの改良ではないことを示しています。これにより、ビームサイズが大幅に増加すると、パフォーマンスが大幅に低下します。 
[要約]ヒューリスティックベースの検索が問題を緩和するために適用されました。これらのほとんどはヒューリスティックベースであり、かなりの調整が必要です。提案されたアプローチは、ヒューリスティックなしで長さバイアスの問題を解決します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: SpeechBERT: An Audio-and-text Jointly Learned Language Model for
  End-to-end Spoken Question Answering -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_5.html">
      SpeechBERT: An Audio-and-text Jointly Learned Language Model for
  End-to-end Spoken Question Answering
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さまざまなテキスト処理タスクで非常に成功したBERTモデルから学び、音声とテキストを組み合わせて学習したSpeechBERTモデルを提案しました。エンドツーエンドのSQAの可能性に加えて、SpeechBERTは他の多くの場合にも考慮できます。多くのテキスト処理タスクのBERTと同じように、音声言語理解タスク。このモデルは、エンドツーエンドモデルによりASRがエラーを生成する前に、オーディオデータから情報を抽出できることが示されました。 
[要約]成功したbertモデルの一部として、音声と質問のスピーチベルトモデルを提案します。提案されたエンドツーエンドをアンサンブルする一方で、十分に準備された効果もありました。成功したモデルからの学習に加えて、これらは成功した
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-25">
        <br>2019-10-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Quaternion Neural Networks for Multi-channel Distant Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_6.html">
      Quaternion Neural Networks for Multi-channel Distant Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      クォータニオン代数は標準のドット積をハミルトンのものに置き換えます。これにより、要素間の依存関係をモデル化するシンプルでエレガントな方法を提供します。この問題を軽減する一般的なアプローチは、さまざまな場所から音響シーンをキャプチャする複数のマイクを録音デバイスに装備することですパースペクティブ..自動音声認識（ASR）の大幅な進歩にもかかわらず、遠くのASRはノイズと残響のために挑戦的なままです。 
[要約]この論文では、これらの構造内および構造内依存性をクォータニオンニューラルネットワークでキャプチャすることを提案しています。クォータニオンレイヤーは、時間領域での長い依存関係を学習できる反復的なやりがいのあるニューラルネットワークと組み合わされます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Should we hard-code the recurrence concept or learn it instead ?
  Exploring the Transformer architecture for Audio-Visual Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_7.html">
      Should we hard-code the recurrence concept or learn it instead ?
  Exploring the Transformer architecture for Audio-Visual Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      トランスフォーマーもクロスモーダル単調整列を学習しますが、LSTMモデルと同じ視覚収束の問題に悩まされており、機械学習における主要なモダリティ問題のより深い調査を求めています。オーディオとビジュアルの音声融合戦略AV Alignはは、挑戦的なLRS2データセットでのオーディオビジュアル音声認識（AVSR）のパフォーマンスの大幅な向上を示しました。2つの方法を比較し、それぞれの長所と短所について詳しく説明します。 
[要旨]音声レベルは、ノイズレベルに応じて7％から30％の範囲です。2つの方法を比較して結果をテストします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: The Privacy ZEBRA: Zero Evidence Biometric Recognition Assessment -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_8.html">
      The Privacy ZEBRA: Zero Evidence Biometric Recognition Assessment
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ZEBRAフレームワークは音声アプリケーションを念頭に置いて設計されていますが、生体認証情報保護基準への組み込みの候補であり、音声と生体認証を超えてさえ、アプリケーションのプライバシーの研究に容易に拡張できます。このペーパーは、この方向への最初のステップを示します。 ：メトリック..それらは、人口に対する特定の保護手段によって与えられるプライバシー保護の平均レベルと、個人に対する最悪の場合のプライバシー開示を測定します。 
[ABSTRACT]私たちはゼロエビデンスバイオメトリック認識評価（zebra）フレームワークを導入し、2つの新しいプライバシーメトリックを提案します。このペーパーでは、音声プライバシーの課題の範囲内でのプライバシー保護評価への応用を示しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Sams-Net: A Sliced Attention-based Neural Network for Music Source
  Separation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_9.html">
      Sams-Net: A Sliced Attention-based Neural Network for Music Source
  Separation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      MUSDB18データセットの実験結果は、パラメータが少ない提案された方法が、最先端のDNNベースの方法のほとんどを上回っていることを示しています。これにより、マルチヘッドアテンションメカニズムとのスペクトル機能の相互作用が可能になり、並列計算が容易になり、 LSTMとCNNに比べて受容フィールドがそれぞれ大きくなっています。この論文では、音楽ソース分離タスクのために、スペクトログラムドメインでスライス注意ベースのニューラルネットワーク（Sams-Net）を提案します。 
[ABSTRACT]この論文では、音楽ソース分離タスクのために、スペクトログラムドメインでスライスされた注意ベースのニューラルネットワーク（sams-net）を提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-12">
        <br>2019-09-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Bayesian Subspace HMM for the Zerospeech 2020 Challenge -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_10.html">
      Bayesian Subspace HMM for the Zerospeech 2020 Challenge
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このペーパーでは、参加者が注釈なし音声から潜在表現を発見し、それらの表現を使用して音声合成を実行する必要があるZerospeech 2020チャレンジへの提出について説明します。合成品質は、ユニット品質のプロキシメトリックとして使用されます。 SHMMは、各ユニットをHMMとしてモデル化します。そのパラメーターは、音声の変動性をモデル化するようにトレーニングされたパラメータースペース全体の低次元サブスペースにあるように制約されます。私たちのシステムは、人間が評価した文字エラー率のベースラインと比較しながら、単位ビットレートが大幅に低くなります。 
[ABSTRACT]私たちのシステムは、人間が評価した文字エラー率のベースラインと比較して好意的です。ユニットの発見には、隠れマルコフモデル（shmm）も使用します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Acoustic Echo Cancellation by Combining Adaptive Digital Filter and
  Recurrent Neural Network -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_11.html">
      Acoustic Echo Cancellation by Combining Adaptive Digital Filter and
  Recurrent Neural Network
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ニューラルネットワークは、このような残留エコーを抑制するために精巧に設計およびトレーニングされています。一部の非線形処理方法は既に提起されていますが、抑制には複雑で非効率的であり、音声に損傷を与えます。このホワイトペーパーでは、 AECには、適応フィルターとニューラルネットワークを組み合わせた融合方式が提案されています。 
[ABSTRACT] aecは、適応フィルターとニューラルネットワークを組み合わせたフュージョン方式で提案されています。提案された組み合わせ方式は、人間の耳で使用でき、コミュニケーションを煩わしくする可能性があります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Deep Architecture Enhancing Robustness to Noise, Adversarial Attacks,
  and Cross-corpus Setting for Speech Emotion Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_12.html">
      Deep Architecture Enhancing Robustness to Noise, Adversarial Attacks,
  and Cross-corpus Setting for Speech Emotion Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      （1）ノイズ、（2）敵対的攻撃、および（3）クロスコーパス設定に対するデータ拡張と組み合わせたアーキテクチャを包括的に評価します。また、堅牢性をさらに向上させるために、ネットワークアーキテクチャによるデータ拡張を提案します。このホワイトペーパーでは、 DenseNet、LSTM、およびHighway Networkを融合して、ノイズに強い強力な識別機能を学習する、より深いニューラルネットワークアーキテクチャを提案します。 
[ABSTRACT]正確なserの堅牢なモデルの設計は困難であり、実用的なアプリケーションでの使用が制限されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: GEV Beamforming Supported by DOA-based Masks Generated on Pairs of
  Microphones -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_13.html">
      GEV Beamforming Supported by DOA-based Masks Generated on Pairs of
  Microphones
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このペーパーで紹介するソリューションは、間隔と音響環境条件が異なるマイクのペアでニューラルネットワークをトレーニングし、このネットワークを使用して、任意の形状のアレイを形成するすべてのマイクのペアから時間周波数マスクを推定することです。結果は、提案されたアプローチが、市販のハードウェアに対応するさまざまなマイクアレイジオメトリで、SDRを平均で4.78 dBから7.69 dBに改善することを示しています。遠くの音声処理は、特にカクテルパーティーの効果を扱う場合、難しい作業です。 
[ABSTRACT]音声認識は、信号対歪み比（sdr）を改善するために必要です。ただし、このタイプのアプローチでは、ニューラルネットワークを事前にトレーニングする必要があります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Meta-Learning for Short Utterance Speaker Recognition with Imbalance
  Length Pairs -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_14.html">
      Meta-Learning for Short Utterance Speaker Recognition with Imbalance
  Length Pairs
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、特定のエピソードのクラスのみを最適化することは、目に見えないクラスの識別的埋め込みを学習するには不十分な場合があるため、トレーニングセットのクラスセット全体に対してサポートとクエリセットの両方を分類するようにモデルを強制します。実用的な設定では、話者認識システムは、短い発話を与えられた話者を識別する必要がありますが、登録発話は比較的長い可能性があります。また、目に見えない話者を識別するための提案モデルを検証します。これにより、既存のアプローチよりも大幅にパフォーマンスが向上します。 。 
[ABSTRACT]既存の話者認識モデルは短い発話ではパフォーマンスが低下します。既存のモデル認識モデルにはそのような発話はありません。モデルは既存の最新モデルより優れています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-06">
        <br>2020-04-06
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Vector-quantized neural networks for acoustic unit discovery in the
  ZeroSpeech 2020 challenge -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_15.html">
      Vector-quantized neural networks for acoustic unit discovery in the
  ZeroSpeech 2020 challenge
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      2番目のモデルは、ベクトル量子化とコントラスト予測コーディング（VQ-CPC）を組み合わせたものです。プローブ実験では、ベクトル量子化が効果的なボトルネックであることを示し、モデルに話者情報を破棄させます。発見されたユニットは、ダウンストリームの音声変換タスクでも競合的に動作します。 
[ABSTRACT]コンセプトは、将来の音響単位を予測することによって音声の表現を学習することです。vq-vaeは音声を離散的な表現にエンコードし、そこから自動波形が再構築されます。両方のモデルが、2019および2020の課題へのすべての提出よりも優れています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Iterative Pseudo-Labeling for Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_16.html">
      Iterative Pseudo-Labeling for Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      次に、標準と低リソースの両方の設定でLibrispeechテストセットの最先端の単語エラー率を達成することにより、IPLの有効性を示します。半教師付きアルゴリズムである反復擬似ラベル付け（IPL）を研究します。これは、音響モデルの進化に伴って、ラベルなしデータに対して疑似ラベル付けの複数の反復を効率的に実行します。また、IPLが追加のテキストを効果的に利用できることを示すために、さまざまなコーパスでトレーニングされた言語モデルの効果を研究します。 
[ABSTRACT]反復擬似ラベリング（ipl）は半教師付きアルゴリズムです。言語モデルとデータ拡張を使用してシステムを作成します。システムは追加のテキストを効果的に利用できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Transferring Source Style in Non-Parallel Voice Conversion -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_17.html">
      Transferring Source Style in Non-Parallel Voice Conversion
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      客観的評価と主観的リスニングテストは、変換された音声の発話の自然さおよび話者の類似性の点で、提案されたVCアプローチの優位性を示します。提案されたアプローチのソーススタイルの転送可能性を示すために実験も行われます。感情と強調）、スピーカーが意図的に追加した要素が含まれている可能性があり、変換中も保持する必要があります。 
[ABSTRACT]ほとんどのvcアプローチは、発話スタイルのモデリングを無視します。ただし、発話スタイルを変換元の音声から変換された音声に変換できる必要があります。テストは、ソース-提案されたアプローチの転送可能性を示すためにも行われます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Toward Automated Classroom Observation: Multimodal Machine Learning to
  Estimate CLASS Positive Climate and Negative Climate -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_18.html">
      Toward Automated Classroom Observation: Multimodal Machine Learning to
  Estimate CLASS Positive Climate and Negative Climate
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最後に、グラフたたみ込みネットワークを使用して、PCが特に弱い/強い場合の特定の瞬間（45〜90秒のクリップ）の予測に向けて、早期の段階（AUC = $ 0.70 $）を作成します。視聴覚ACORNのPCおよびNC予測には、ピアソン相関が$ 0.55 $です。 UVA ToddlerデータセットのエキスパートCLASSコーダーによって提供されたグラウンドトゥルーススコア（$ n = 300 $ 15分のビデオセグメントの相互検証）と$ 0.63 $、および純粋に聴覚ACORNは$ 0.36 $の相関でPCとNCを予測し、 METデータセットの$ 0.41 $（$ n = 2000 $ビデオセグメントのテストセット）。これらの数値は、人間のコーダーのコーダー間の信頼性に似ています。 
[ABSTRACT] acornは、畳み込みニューラルネットワークを使用してデータの特徴を分析します。これらには、教師と生徒の顔、および各画像フレームのピクセルが含まれます。これらのデータは、人間のコーダーのコーダー間信頼性に似ています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Joint NN-Supported Multichannel Reduction of Acoustic Echo,
  Reverberation and Noise -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_19.html">
      Joint NN-Supported Multichannel Reduction of Acoustic Echo,
  Reverberation and Noise
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      提案されたアプローチは、個々のアプローチのカスケード、およびターゲット信号と残差信号のスペクトルモデルに依存しないジョイント削減アプローチの全体的な歪みの面で優れています。音響エコー、残響、ノイズの実際の録音でシステムを評価しますさまざまな状況でスマートスピーカーを使って取得します。マルチチャネルガウスモデリングフレームワークを使用して線形エコーキャンセレーションと残響除去後のターゲット信号と残差信号をモデル化し、ニューラルネットワークを使用してそれらのスペクトルを共同で表すことを提案します。 
[要約]線形エコーキャンセレーションと残響除去後のターゲット信号と残差信号をモデル化することを提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-20">
        <br>2019-11-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Atss-Net: Target Speaker Separation via Attention-based Neural Network -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_20.html">
      Atss-Net: Target Speaker Separation via Attention-based Neural Network
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これにより、CNN-LSTMアーキテクチャーと比較して、ネットワークは各機能間の相関を並列で計算し、浅いレイヤーを使用してより多くの機能を抽出できます。実験結果は、Atss-NetがVoiceFilterよりも優れたパフォーマンスをもたらすことを示しています。パラメータの半分..さらに、提案されたモデルは、音声強調で有望なパフォーマンスも示します。 
[要約]このホワイトペーパーでは、タスクのスペクトログラムドメインで注意ベースのニューラルネットワーク（atss-net）を提案します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Unsupervised Cross-Domain Speech-to-Speech Conversion with
  Time-Frequency Consistency -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_21.html">
      Unsupervised Cross-Domain Speech-to-Speech Conversion with
  Time-Frequency Consistency
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この手順では、生成されたマグニチュードスペクトログラムに一貫性がない可能性があるという問題が発生します。これは、完全なスペクトログラムに自然な音声波形が含まれるような位相を見つけるために必要です。男性の声を翻訳するタスクに対するアプローチを示します。近年、ジェネレーティブアドバタリアルネットワーク（GAN）ベースのモデルが、教師なしの音声から音声への変換に適用されています。マグニチュードスペクトログラムの豊富なコンパクトな調和ビューは、これらのモデルをオーディオデータでトレーニングするための選択肢。 
[要約] librispeechコーパスの研究者は、tf整合性でトレーニングされたモデルが、知覚的に優れた音声品質を音声に変換することを示しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-15">
        <br>2020-05-15
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Augmenting Generative Adversarial Networks for Speech Emotion
  Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_22.html">
      Augmenting Generative Adversarial Networks for Speech Emotion
  Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの結果は、提案されたフレームワークが圧縮された感情表現を効果的に学習できるだけでなく、コーパス内およびコーパス間の評価のパフォーマンス向上に役立つ合成サンプルを生成できることを示しています。提案されたフレームワークの有効性を示すために、SERの結果を提示します。 （i）合成特徴ベクトル、（ii）合成特徴によるトレーニングデータの拡張、（iii）圧縮表現での特徴の符号化。生成的敵対的ネットワーク（GAN）は、感情的属性の学習と新しいデータサンプルの生成に可能性を示しています。 
[要約]提案されたシステムは、音声感情認識を向上させるために使用できます。ただし、新しいシステムで開発できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br>2020-05-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Enhancing Monotonic Multihead Attention for Streaming ASR -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_23.html">
      Enhancing Monotonic Multihead Attention for Streaming ASR
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、境界検出のためにヘッド間のコンセンサスを改善し、そのようなヘッドによって引き起こされる遅延トークン生成を防ぐために冗長ヘッドをプルーニングすることを提案します。最後に、安定したストリーミング推論を保証するためにヘッド同期ビーム検索デコードを提案します。 MAヘッドは、単純な実装との連携を学びます。 
[ABSTRACT]トレーニング中にヘッドの一部をマスクして、ヘッドドロップの正規化を提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Sequential Multi-Frame Neural Beamforming for Speech Separation and
  Enhancement -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_24.html">
      Sequential Multi-Frame Neural Beamforming for Speech Separation and
  Enhancement
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらのメソッドのウィンドウサイズ、ブロックサイズ、およびマルチフレームコンテキストサイズの影響を広範囲に評価および分析します。この作業では、ニューラルネットワークベースのスペクトル分離とビーム形成ベースの空間分離を交互に行う順次ニューラルビームフォーミングを紹介します。一連の3つのニューラル分離とマルチフレーム時間不変空間ビームフォーミングステージを利用し、スケール不変信号対雑音比の平均が2.75 dB向上し、強力なベースラインでの差分ワードエラーレートが14.2％減少したことを示します。 4つのやりがいのある残響の音声強調と分離タスクにまたがります。 
[要約]私たちの最良の方法は、3つの神経分離とマルチフレームサイズのシーケンス、およびマルチフレームフレームフレーム形成段階を使用します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-18">
        <br>2019-11-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Lite Microphone Array Beamforming Scheme with Maximum Signal-to-Noise
  Ratio Filter -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_25.html">
      A Lite Microphone Array Beamforming Scheme with Maximum Signal-to-Noise
  Ratio Filter
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このホワイトペーパーでは、ビームフォーミングの複雑さを軽減するために、最大信号対ノイズ比（SNR）フィルターを使用する簡潔なビームフォーミングスキームを提案します。私たちの実験では、他の広く使用されているアルゴリズムと比較して、提案されたアルゴリズムがより高い信号ゲインを得ることが示されています。対干渉およびノイズ比（SINR）。ただし、マイクの数が増えると、複雑さが増します。 
[ABSTRACT]最大snrフィルターは、音源定位（ssl）の推定到来方向（doa）を使用して作成されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Distilling Knowledge from Ensembles of Acoustic Models for Joint
  CTC-Attention End-to-End Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_26.html">
      Distilling Knowledge from Ensembles of Acoustic Models for Joint
  CTC-Attention End-to-End Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      TIMIT音素認識タスクの選択されたトレーニング手順の下でこれらの戦略を評価し、共通のベースラインと比較してこれらの戦略の有望なエラー率を観察しました。両方の背後にある中心的な直観は、エラー率メトリックを単に選択するのではなく、教師の選択に統合することです観測された損失に焦点を当てます。知識の蒸留は、幅広いアプリケーションでパフォーマンスを維持しながら、既存のディープラーニングモデルを圧縮するために広く使用されています。 
[要約]音響モデルのアンサンブルからの蒸留は、最近、認識性能の向上に有望な結果を示しています。このようにして、スピーチプレスの関連メトリックに向けて学生を直接蒸留して最適化します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Anomalous sound detection based on interpolation deep neural network -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_27.html">
      Anomalous sound detection based on interpolation deep neural network
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この問題を解決するために、モデルは、中心フレームが入力として削除されたスペクトログラムの複数のフレームを利用し、削除されたフレームの補間を出力として予測する異常検出へのアプローチを提案します。実験結果では、提案されたアプローチは、標準のAUCスコアに基づいて、特に非定常機械音に対して27％の改善を達成しました。提案されたアプローチは、エッジフレームを予測するのではなく、再構成エラーを異常と一致させます。 
[ABSTRACT]従来のアプローチは、オートエンコーダの再構成エラーに基づいて異常を検出します。スペクトログラムの複数のフレームを使用する代わりに、削除されたフレームの補間を出力として予測します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A New Training Pipeline for an Improved Neural Transducer -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_28.html">
      A New Training Pipeline for an Improved Neural Transducer
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      出力ラベルトポロジーをさらに一般化して、RNN-T、RNA、およびCTCをカバーします。変換器モデルは、注意モデルよりも長いシーケンスでより一般化することがわかります。 ％相対WER。 
[ABSTRACT]元のトレーニング基準を完全な周辺化と比較します。出力ラベルのドッキングをさらに一般化して、rnn-t、rna、ctcをカバーします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Fast, Simpler and More Accurate Hybrid ASR Systems Using Wordpieces -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_29.html">
      Fast, Simpler and More Accurate Hybrid ASR Systems Using Wordpieces
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      次に、CTCトレーニングと組み合わせたモデリングユニットとしてワードピースを使用すると、GMMのブートストラップ、ディシジョンツリーの構築、フォースアライメントのステップをすべて除外することで、従来のフレームベースのクロスエントロピートレーニングと比較して、エンジニアリングパイプラインを大幅に簡略化でき、しかも非常に高い競争力のある単語エラー率..この作業では、まず、広く使用されているLibriSpeechベンチマークで、トランスフォーマーベースのコンテキスト依存の接続主義時間分類（CTC）システムが最先端の結果を生成することを示します。 2つの内部\ emph {VideoASR}データセットでこれらの結果を確認します。ドイツ語（融合言語としての英語に似ています）とトルコ語（膠着言語）です。 
[要旨]単語をモデリングユニットとして使用し、ctcトレーニングと組み合わせると、従来のフレームベースのクロスシンプソントレーニングと比較して、エンジニアリングパイプラインを大幅に簡略化できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Novel Fusion of Attention and Sequence to Sequence Autoencoders to
  Predict Sleepiness From Speech -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_30.html">
      A Novel Fusion of Attention and Sequence to Sequence Autoencoders to
  Predict Sleepiness From Speech
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      次に、これらのスペクトログラムで反復オートエンコーダをトレーニングします。これらのスペクトログラムは、時間依存の周波数ベクトルと見なされます。両方のオートエンコーダから学習した表現を評価し、早期の融合を行って、それらの間の可能な相補性を確認します。その後、対応するオーディオインスタンスのスペクトログラムの学習機能を表す、自動エンコーダーの完全に接続された特定のレイヤー。 
[ABSTRACT]私たちの表現では、オートエンコーダの特定の完全に接続されたレイヤーのアクティベーションを抽出します。また、これらのオーディオインスタンスの学習された機能を表すオートエンコーダのアクティベーションをトレーニングします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-15">
        <br>2020-05-15
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Investigations on Phoneme-Based End-To-End Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/eess.AS/paper_31.html">
      Investigations on Phoneme-Based End-To-End Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらは、コンテキストのない単一の音素（〜40ラベル）にすることも、1つの出力ラベルに複数の音素を組み合わせて音素ベースのサブワードを取得することもできます。これにより、非常にシンプルで効率的なデコードアルゴリズムが可能になります。スイッチボード300hで実験を行いますそして、音素ベースのモデルが書記素ベースのモデルと競合することを示すことができます。 
[要約]書記素ベースと音素ベースの出力ラベルを体系的に比較します。これらには、音素ベースの注意ラベルが含まれます。この目的のために、音素ベースのBPEラベルを導入します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Thiazoline-related TRPA1 agonist odorants orchestrate survival fate in mice -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/biorxiv.physiology/paper_0.html">
      Thiazoline-related TRPA1 agonist odorants orchestrate survival fate in mice
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      したがって、強い恐怖の知覚は、低体温/低代謝に基づく特殊な生命保護代謝を誘発する可能性があります。ただし、そのような現象とそのインデューサーはまだ解明されていません。低体温療法は心肺停止後の脳を保護します。全脳マッピングと化学発生的活性化は、tFOの感覚的表現が脳幹Sp5 / NSTから中脳PBN経路を介して生存運命を調整することを明らかにしました。 
[要約]チアゾリン関連の恐怖のにおい（tfos）はtrpa1アゴニストであり、マウスに強い先天性恐怖を誘発します。全身性低体温症のような冬眠の誘発、脳へのグルコース取り込みの促進、有酸素代謝の抑制
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Model-Based Approach for Pulse Selection from Electrodermal Activity -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-05-20/biorxiv.physiology/paper_1.html">
      A Model-Based Approach for Pulse Selection from Electrodermal Activity
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      しきい値を超えてスクリーニングし、4つのモデルすべてをフィッティングすることにより、逆ガウスのような構造を反映するより重いテールを選択し、適合度分析でパルス選択を検証しました。重要性：パラダイムの堅牢性と生理学のEDAをより近くに移動痛み、不安、抑うつ、睡眠などのさまざまな条件での交感神経活動の臨床マーカーとしての役割を果たします。これらの4つのモデルは、尾腺の動作が異なり、汗腺の生理学をさまざまな程度に反映しています。 
[要約]さまざまなセンサーからのデータでテストされた4つのモデルのコホート。一定の周波数しきい値を想定しているため、ノイズからパルスを分離できました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br>2020-05-19
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
