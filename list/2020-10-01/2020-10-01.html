<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-10-01の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Efficiently Trainable Text-to-Speech System Based on Deep Convolutional
  Networks with Guided Attention -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.SD/paper_0.html">
      <font color="black">Efficiently Trainable Text-to-Speech System Based on Deep Convolutional
  Networks with Guided Attention</font>
    </a>
  </h2>
  <font color="black">私たちの実験では、提案されたDeep Convolutional TTSは、2つのGPUを搭載した通常のゲーミングPCを使用して、一晩（15時間）十分にトレーニングされましたが、合成音声の品質はほぼ許容範囲内でした。このペーパーの目的は、 CNNのみに基づく代替ニューラルTTSは、トレーニングのこれらの経済的コストを軽減します。このペーパーでは、反復ユニットを使用せずに、深い畳み込みニューラルネットワーク（CNN）に基づく新しいテキスト読み上げ（TTS）手法について説明します。 
[ABSTRACT]リカレントニューラルネットワーク（rnn）は、最近、連続データをモデル化するための標準的な手法になりました。最近の研究では、cnnベースのシーケンス合成はcnnベースの手法よりもはるかに高速であることが示されています。提案された深い畳み込みttsは一晩で十分にトレーニングされました。 、2つのGPUを搭載した通常のゲーミングPCを使用</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2017-10-24">
        <br><font color="black">2017-10-24</font>
      </time>
    </span>
</section>
<!-- paper0: Consonant gemination in Italian: the nasal and liquid case -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.SD/paper_1.html">
      <font color="black">Consonant gemination in Italian: the nasal and liquid case</font>
    </a>
  </h2>
  <font color="black">この論文（残りのすべての子音をカバーする2つのセットの最初のもの）は、鼻音と液体に対処します。そのコンパニオンペーパーは破擦音と摩擦音を扱っています。175-185）は、イタリア語での長子音の主な音響的手がかりは閉鎖期間であり、周波数とエネルギー領域のパラメータは長子音の影響をあまり受けなかったことを示しました。鼻と液体の結果は、特に、イタリア語での長子音の主な音響キューは本質的に持続的であり、摩擦音の持続時間の延長に対応します。 
[概要]この論文は、すべての鼻音をカバーする一連の停止の最初のものです。また、母音と瓶前の母音の長さの間の逆相関を示しています。これは、シングルトンとジェミネートの単語セットを組み合わせた場合にも存在します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-19">
        <br><font color="black">2020-04-19</font>
      </time>
    </span>
</section>
<!-- paper0: CRNNs for Urban Sound Tagging with spatiotemporal context -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.SD/paper_2.html">
      <font color="black">CRNNs for Urban Sound Tagging with spatiotemporal context</font>
    </a>
  </h2>
  <font color="black">コードは、https：//github.com/multitel-ai/urban-sound-taggingのGitHubリポジトリで入手できます。このタスクは、時空間コンテキストを使用した階層型マルチラベルアーバンサウンドタグ付けに焦点を当てています。このペーパーでは、参加に使用したCRNNについて説明します。 DCASE2020チャレンジのタスク5で。 
[概要]このタスクは、時空間コンテキストを使用したマルチラベルの都市音タグ付けの改革に焦点を当てています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-24">
        <br><font color="black">2020-08-24</font>
      </time>
    </span>
</section>
<!-- paper0: Enhancing Monotonic Multihead Attention for Streaming ASR -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.SD/paper_3.html">
      <font color="black">Enhancing Monotonic Multihead Attention for Streaming ASR</font>
    </a>
  </h2>
  <font color="black">ただし、すべてのMAヘッドがナイーブな実装でアライメントを学習するわけではありません。最後に、安定したストリーミング推論を保証するために、ヘッド同期ビーム検索デコードを提案します。ハード単調注意を拡張することにより、単調マルチヘッド注意（MMA）を調査します。オンラインストリーミングアプリケーション用のTransformerベースの自動音声認識（ASR）へ
[概要]トレーニング中に頭の一部をマスクすることにより、頭のドロップの正規化を提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br><font color="black">2020-05-19</font>
      </time>
    </span>
</section>
<!-- paper0: Rethinking Evaluation Methodology for Audio-to-Score Alignment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.SD/paper_4.html">
      <font color="black">Rethinking Evaluation Methodology for Audio-to-Score Alignment</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、オーディオとスコアのアラインメントの正確で正式な定義を提供します。アラインメントの概念は直感的に理解できますが、この精度により、オーディオとスコアのアラインメントアルゴリズムの評価に関する新しい洞察が得られます。洞察、オーディオとスコアの調整のための新しい評価指標を紹介します。 
[概要]配置の概念は直感的に把握できますが、この精度により新しい洞察が得られます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Transfer Learning from Speech Synthesis to Voice Conversion with
  Non-Parallel Training Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.SD/paper_5.html">
      <font color="black">Transfer Learning from Speech Synthesis to Voice Conversion with
  Non-Parallel Training Data</font>
    </a>
  </h2>
  <font color="black">音声変換システムでは、エンコーダーは入力としてテキストではなく音声を受け取りますが、デコーダーは機能的にTTSデコーダーと似ています。スピーカーの埋め込みでデコーダーを調整すると、システムは任意の非並列データでトレーニングできます。 -任意の音声変換..このペーパーでは、TTS-VC転送学習と呼ばれる、テキスト読み上げ（TTS）合成システムから学習することによって音声変換（VC）システムを構築するための新しいフレームワークを紹介します。 
[概要]最初に、シーケンス-から-シーケンス-デコーダー-デコーダーアーキテクチャを備えた多人数音声合成システムを開発します。システムでは、エンコーダーはテキストの代わりに音声を入力として受け取り、デコーダーはデータをマッピングしてターゲットを作成します音響機能。提案されたアプローチは、2つの競合する音声変換ベースラインを一貫して上回っています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: ESPnet-ST: All-in-One Speech Translation Toolkit -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.SD/paper_6.html">
      <font color="black">ESPnet-ST: All-in-One Speech Translation Toolkit</font>
    </a>
  </h2>
  <font color="black">ツールキットはhttps://github.com/espnet/espnetで公開されています。データの前処理、特徴抽出、トレーニング、さまざまなベンチマークデータセットのパイプラインのデコードを含むオールインワンレシピを提供しています。私たちの再現可能な結果は、現在の最先端のパフォーマンスに匹敵するか、それを上回ることさえできます。これらの事前トレーニング済みモデルはダウンロード可能です。 
[ABSTRACT] espnet --stは、us-mexico音声処理ツールキットの新しいプロジェクトです。自動音声認識、機械翻訳、およびテキスト読み上げ機能を統合または新たに実装します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br><font color="black">2020-04-21</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-End Spoken Language Understanding Without Full Transcripts -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.SD/paper_7.html">
      <font color="black">End-to-End Spoken Language Understanding Without Full Transcripts</font>
    </a>
  </h2>
  <font color="black">ATISコーパスでの音声からエンティティへの実験では、CTCモデルと注意モデルの両方で、エンティティ以外の単語をスキップする優れた能力が示されました。エンティティのみでトレーニングした場合と完全なトランスクリプトでトレーニングした場合では、劣化はほとんどありませんでした。 、これらのシステムは、エンティティラベルとエンティティ値を表す単語の両方を正しく認識する必要があります。また、エンティティが発話の発話順序に必ずしも関連しない順序であるシナリオについても検討しました。 
[概要] 2種類の音声エンティティが音声認識に適合しています。これらのモデルは、単語ごとのトランスクリプトなしで、意味エンティティの注釈のみでトレーニングできます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Embedded Emotions -- A Data Driven Approach to Learn Transferable
  Feature Representations from Raw Speech Input for Emotion Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.SD/paper_8.html">
      <font color="black">Embedded Emotions -- A Data Driven Approach to Learn Transferable
  Feature Representations from Raw Speech Input for Emotion Recognition</font>
    </a>
  </h2>
  <font color="black">音響機能は、開発セットでクラス最高の結果を達成しましたが、ベースラインシステムと比較すると、チャレンジのテストセットでパフォーマンスが大幅に低下しました。この論文では、大きなテキストから学んだ知識を伝達することの適用性を調査します。自動感情認識のタスクへのオーディオコーパス..ただし、テキストフォームから抽出された特徴は、両方のセットで有望な結果を示しており、重み付けされていない平均リコールが5.7パーセントポイントだけ公式ベースラインを上回っています。 
[概要]今年は、今年のインタースピーチ比較高齢者感情サブチャレンジに参加しています。目標は、話者の感情に関して高齢者の話された物語を分類することです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Transfer Learning from Monolingual ASR to Transcription-free
  Cross-lingual Voice Conversion -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.SD/paper_9.html">
      <font color="black">Transfer Learning from Monolingual ASR to Transcription-free
  Cross-lingual Voice Conversion</font>
    </a>
  </h2>
  <font color="black">この論文では、コンテンツの不一致の問題に対処するために、モノリン-gualASRからcross-lingualVCへの知識の伝達に焦点を当てます。カスケードASR-TTS法と比較すると、提案された方法はMOSドロップを大幅に削減します。 -トゥイーンイントラリンガル変換とクロスリンガル変換..クロスリンガル音声変換（VC）は、ソーススピーカーとターゲットスピーカーが異なる言語で話しているときに、同じコンテンツでターゲット音声を合成することを目的としたタスクです。 
[概要]音声変換チャレンジ2020データセットでこれをテストします。スピーカーに依存する変換モデルがゼロショットベースラインを上回り、音声品質で3.83と3.54のmosを達成していることを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Consonant gemination in Italian: the affricate and fricative case -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.SD/paper_10.html">
      <font color="black">Consonant gemination in Italian: the affricate and fricative case</font>
    </a>
  </h2>
  <font color="black">この効果は、組み合わせたセットで強化され、異なる音素間の持続的な補償がリズミカルな構造を維持するのに役立つ可能性があるという仮説を確認しました。停止、鼻音、および液体は、停止の閉鎖期間と鼻音および液体の子音期間が最も多く形成されることを示しました長子音への顕著な音響的手がかり..結果は、イタリア語の破裂音は、単音としての発声位置ではなく、それらの長子音の形でのみ現れる可能性があるという仮説を支持します。 
[概要]他の瓶のカテゴリーの分析結果は、それらの妥当性を確認しました。発見は、破擦音が母音間子音の位置ではシングルトンとしてではなく、長子音の形でのみ現れる可能性があるという理論を支持します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-19">
        <br><font color="black">2020-04-19</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Dissected 3D CNNs: Temporal Skip Connections for Efficient Online Video
  Processing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.IV/paper_0.html">
      <font color="black">Dissected 3D CNNs: Temporal Skip Connections for Efficient Online Video
  Processing</font>
    </a>
  </h2>
  <font color="black">これらの欠点に対処するために、ネットワークの中間ボリュームが分析され、将来の計算のために深度（時間）次元に伝播され、オンライン操作での計算数が大幅に削減される、分析された3D CNNを提案します。最後に、3DCNNは柔軟性を制限する固定の時間入力サイズでの使用に制約されています。3Dカーネル（3D CNN）を備えた畳み込みニューラルネットワークは、ビデオフレーム内の時空間特徴の抽出における優位性により、現在、ビデオ認識タスクで最先端の結果を達成しています。 
[概要]最先端の結果を超える成功した3dcnnアーキテクチャが数多くあります。これらは、成功した3Dモデルのほぼ最も成功した例です。これらには、3D視覚認識とresnetモデルの分析バージョンが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: FAN: Frequency Aggregation Network for Real Image Super-resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.IV/paper_1.html">
      <font color="black">FAN: Frequency Aggregation Network for Real Image Super-resolution</font>
    </a>
  </h2>
  <font color="black">私たちは、FANがAIM 2020チャレンジの実際の画像の超解像タスクでうまく機能することを検証するために、定量的および定性的に広範な実験を行います。リリースされた最終結果に対して、私たちのチームSR-IMは、PSNRが31.1735、SSIMが0.8728で、X4トラックで4位を獲得しました。 
[ABSTRACT] sisrは、ディープラーニングの開発で大きな進歩を遂げました。これには、hr画像を復元するために、より高密度のマップを抽出することが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Password-conditioned Anonymization and Deanonymization with Face
  Identity Transformers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.IV/paper_2.html">
      <font color="black">Password-conditioned Anonymization and Deanonymization with Face
  Identity Transformers</font>
    </a>
  </h2>
  <font color="black">広範な実験により、私たちのアプローチは、既存の匿名化アプローチと比較してプライバシーを犠牲にすることなく、マルチモーダルパスワード条件付き顔の匿名化と匿名化を可能にすることが示されています。自動フォトリアリスティックなパスワードベースの匿名化と人間の顔の匿名化を可能にする新しい顔IDトランスフォーマーを提案します。ビジュアルデータに表示されます。私たちの顔IDトランスフォーマーは、（1）匿名化後に顔ID情報を削除し、（2）正しいパスワードを指定すると元の顔を復元できるようにし、（3）間違ったものを返すようにトレーニングされています。フォトリアリスティック-間違ったパスワードが与えられた顔。 
[概要]私たちの顔IDトランスフォーマーは、顔ID情報を削除するようにトレーニングされています。正しいパスワードを指定すると、元の顔の復元が可能になります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-26">
        <br><font color="black">2019-11-26</font>
      </time>
    </span>
</section>
<!-- paper0: Fast versus conventional HAADF-STEM tomography: advantages and
  challenges -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.IV/paper_3.html">
      <font color="black">Fast versus conventional HAADF-STEM tomography: advantages and
  challenges</font>
    </a>
  </h2>
  <font color="black">これらの方法には、プロジェクションムービーを取得しながらホルダーを連続的に傾けることや、従来の手法と連続手法の利点を組み合わせたハイブリッドのインクリメンタル手法が含まれます。したがって、電子断層撮影法を使用すると、統計的に意味のある3Dデータを取得して調査することは簡単ではありません。長時間の取得に耐えられないサンプル、またはこの手法を使用してその場での3D特性評価を実行するサンプル。この論文では、さまざまな取得戦略を、さまざまな方法で取得した実験的な傾斜シリーズに基づいて、速度、分解能、電子量の観点から実験的に比較します。金属ナノ粒子。 
[概要]従来の電子断層撮影傾斜シリーズの取得は、実験の複雑さにもよりますが、簡単に1時間以上かかる場合があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: CAD Applications and Emerging Research Potential in Medical Imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.IV/paper_4.html">
      <font color="black">CAD Applications and Emerging Research Potential in Medical Imaging</font>
    </a>
  </h2>
  <font color="black">5つの重大な疾患の診断に使用されるコンピュータ断層撮影、磁気共鳴画像法、マンモグラフィ、および骨シンチグラフィーは、臨床的背景とともに議論されてきました。4つの画像診断法における上記の4つの段階の工学的側面、すなわち、画像の自動分類は、ヘルスケアのコストを下げる非常に重大な病気の予備スクリーニングにおける重要な役割。 
[概要]医用画像におけるcadの将来の範囲については、新しい論文で説明されています。この論文では、研究者や医療業界が手頃な価格の医療サービスを提供する機会を探るために、これらの工学的側面をレビューしています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: iSeeBetter: Spatio-temporal video super-resolution using recurrent
  generative back-projection networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.IV/paper_5.html">
      <font color="black">iSeeBetter: Spatio-temporal video super-resolution using recurrent
  generative back-projection networks</font>
    </a>
  </h2>
  <font color="black">ただし、SISRを各ビデオフレームに連続して適用すると、時間的一貫性が失われます。時間的に一貫した超解像ビデオをレンダリングする、ビデオ超解像（VSR）への新しいGANベースの時空間アプローチであるiSeeBetterを紹介します。ニューラルネットワーク（CNN）は、ピーク信号対雑音比（PSNR）や構造的類似性（SSIM）などの画質メトリックの点で、従来のアプローチよりも優れています。 
[ABSTRACT] iseebetterは、リカレントバックプロジェクションネットワークの概念をジェネレータとして使用して、現在のフレームと隣接するフレームから空間情報とタイミング情報を抽出します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-13">
        <br><font color="black">2020-06-13</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning-based Pipeline for Module Power Prediction from EL
  Measurements -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.IV/paper_6.html">
      <font color="black">Deep Learning-based Pipeline for Module Power Prediction from EL
  Measurements</font>
    </a>
  </h2>
  <font color="black">ここでは、主なタイプの欠陥として非アクティブ領域とクラックに焦点を当てます。ただし、単一モジュールの電力の低下はストリング全体のパフォーマンスに影響を与える可能性があるため、最大電力点での電力の知識も重要です。 。自動検査は、大規模な太陽光発電所の監視において重要な役割を果たします。 
[概要]これは通常、モジュールの取り外しまたは取り外しが必要な測定によって決定され、個々のモジュールの定期的な検査が実行不可能になります。したがって、モジュールの電力を決定することは困難です。モジュールの719個の電光測定の大規模なデータセットをコンパイルします。劣化のさまざまな段階、特にセルの亀裂や破損、および最大電力点での対応する電力</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Noise-free computational ghost imaging with pink noise speckle patterns -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.IV/paper_7.html">
      <font color="black">Noise-free computational ghost imaging with pink noise speckle patterns</font>
    </a>
  </h2>
  <font color="black">計算ゴーストイメージングは、単一ピクセル検出器を使用した2次相関測定によってオブジェクトを再構成します。結果は、標準のホワイトノイズの使用とも比較されます。極端なノイズの多い環境またはパターンの歪みの下で、この方法が優れていることを示します。従来の方法は失敗しますが、高品質の画像。 
[概要]通常、目的の画像を取得するためにアンサンブル平均を作成するには、多数のスペクトルパターンが必要です。次に、さまざまなノイズの存在下で、高い信号対ノイズ比の再構成画像を実験的に示しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Small Drone Classification with Light CNN and New Micro-Doppler
  Signature Extraction Method Based on A-SPC Technique -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.IV/paper_8.html">
      <font color="black">Small Drone Classification with Light CNN and New Micro-Doppler
  Signature Extraction Method Based on A-SPC Technique</font>
    </a>
  </h2>
  <font color="black">この手紙では、FMCWレーダーを使用して小型ドローンのMDS画像を抽出する新しい方法を提案します。小型ドローンの脅威が高まるにつれて、小型ドローンの検出だけでなく分類も重要になります。実験結果は、提案された方法により、全体の分類精度が10.00％向上したことを示しました。 
[概要]提案されたmds抽出法と提案された光cnnを使用して、合計分類精度が97. 14％で記録されました。この方法は、光畳み込みニューラルネットワーク（mds）を使用して提案されました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Real-Time Instrument Segmentation in Robotic Surgery using Auxiliary
  Supervised Deep Adversarial Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.IV/paper_9.html">
      <font color="black">Real-Time Instrument Segmentation in Robotic Surgery using Auxiliary
  Supervised Deep Adversarial Learning</font>
    </a>
  </h2>
  <font color="black">また、補助損失と敵対的損失を組み合わせてセグメンテーションモデルを正規化する新しい方法を紹介します。補助ブランチとメインブランチからのさまざまな次元とチャネルの機能マップを融合する多重解像度機能融合モジュール（MFF）を提案します。手術シーンの正確で効率的なセグメンテーションは、器具の識別と追跡に役立つだけでなく、操作されているさまざまな組織と器具に関するコンテキスト情報も提供します。 
[概要]実際の外科医は、高度な高度なナビゲーションシステムを開発しました。mffは、補助ブランチとメインブランチからのさまざまな次元とチャネルの機能マップを融合する多重解像度機能融合モジュールです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-22">
        <br><font color="black">2020-07-22</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-focus Image Fusion for Visual Sensor Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.IV/paper_10.html">
      <font color="black">Multi-focus Image Fusion for Visual Sensor Networks</font>
    </a>
  </h2>
  <font color="black">いくつかの画像の実験結果は、他のDCTベースの手法と比較して融合画像の主観的および客観的品質の両方の観点から提案されたアルゴリズムの改善を示しています。この論文では、DCTでの多焦点画像の融合のための効率的なアルゴリズムドメインが提案されます。ソース画像の対応するブロックの修正ラプラシアン（SML）の合計がコントラスト基準として使用され、SMLの値が大きいブロックが出力画像に吸収されます。 
[概要] dctを使用した画像の分析はそれほど複雑ではなく、時間の節約になります。dctは写真とビデオの標準に基づいているため、vsnアプリケーションにより適しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-28">
        <br><font color="black">2020-09-28</font>
      </time>
    </span>
</section>
<!-- paper0: FMCW SAR with New Synthesis Method Based on A-SPC Technique -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.IV/paper_11.html">
      <font color="black">FMCW SAR with New Synthesis Method Based on A-SPC Technique</font>
    </a>
  </h2>
  <font color="black">次に、従来法と提案法の両方でSAR画像を合成し、提案法の性能を実証した。提案法はSAR画像の品質を向上させることができる。この手紙はSARの抽出のための新しい方法を提案する。 FMCWレーダーによる画像。 
[概要]手紙は、sar画像を抽出するための新しい方法を提案しています。この方法は、fmcwレーダーによって提案されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Detecting and Counting Pistachios based on Deep Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.IV/paper_12.html">
      <font color="black">Detecting and Counting Pistachios based on Deep Learning</font>
    </a>
  </h2>
  <font color="black">最初の段階では、ビデオフレーム内のピスタチオを検出するための3つの異なるバックボーンを備えた完全畳み込みオブジェクト検出器であるRetinaNetをトレーニングしました。ピスタチオの新しいデータセットを導入して共有しました。これには、全長167の6つのビデオが含まれます。秒と3927ラベルのピスタチオ..第2段階では、ビデオで口を開けたピスタチオと口を閉じたピスタチオをカウントする新しい方法を紹介します。 
[概要]この論文は、コンピュータビジョンでピスタチオの種類を数える新しいシステムを提案することを目的としています。輸送ライン上を移動および転がるピスタチオは、一部のフレームでは口が閉じ、他のフレームでは口が開いているように見える場合があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-08">
        <br><font color="black">2020-05-08</font>
      </time>
    </span>
</section>
<!-- paper0: Knee Injury Detection using MRI with Efficiently-Layered Network (ELNet) -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.IV/paper_13.html">
      <font color="black">Knee Injury Detection using MRI with Efficiently-Layered Network (ELNet)</font>
    </a>
  </h2>
  <font color="black">さらに、トレーニング中にローカリゼーション情報がないにもかかわらず、膝の涙を見つけるモデルの機能を示します。筋骨格（MSK）放射線科医の増え続ける作業負荷にうまく対処するために、患者のトリアージ用の自動化ツールが真のニーズになりつつあります。 、病理学的症例の読み取りの遅延を減らします。過去のアプローチとは異なり、転送学習アプローチを使用する代わりに、ELNetを最初からトレーニングします。 
[ABSTRACT]磁気ネットワーク（elnet）は畳み込みニューラルネットワーク（cnn）アーキテクチャです。提案された方法は定量的および定性的に検証されます。単一のイメージングスタックを入力として使用しながら、最先端のmrnetと比較して遜色ありません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-06">
        <br><font color="black">2020-05-06</font>
      </time>
    </span>
</section>
<!-- paper0: Enhanced Standard Compatible Image Compression Framework based on
  Auxiliary Codec Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.IV/paper_14.html">
      <font color="black">Enhanced Standard Compatible Image Compression Framework based on
  Auxiliary Codec Networks</font>
    </a>
  </h2>
  <font color="black">したがって、コンパクトな表現と後処理ネットワークを効果的かつ最適に学習できます。ただし、これらのアプローチは既存のコーデックと互換性がないか、コーディング効率を高めるのに最適ではありません。画像圧縮パフォーマンスを向上させるために、最近のディープニューラルネットワークベース研究は、学習可能なコーデック、後処理ネットワーク、およびコンパクトな表現ネットワークの3つのカテゴリに分類できます。 
[概要]学習可能なコーデックは、従来の圧縮モジュールを超えたエンドツーエンドの学習用に設計されています。コンパクトな表現ネットワークは、入力画像の容量を減らしてビットレートをカットすると同時に、デコードされた画像</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Image-adaptive 3D Lookup Tables for High Performance Photo
  Enhancement in Real-time -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.IV/paper_15.html">
      <font color="black">Learning Image-adaptive 3D Lookup Tables for High Performance Photo
  Enhancement in Real-time</font>
    </a>
  </h2>
  <font color="black">私たちは、初めて、ペアワイズ学習またはペアなし学習を使用して、注釈付きデータから3DLUTを学習することを提案します。マルチベースの3DLUTと小さな畳み込みニューラルネットワーク（CNN）をエンドツーエンドで同時に学習します。 .. 3D LUTは、写真の色とトーンを操作するために広く使用されていますが、通常は手動で調整し、カメライメージングパイプラインまたは写真編集ツールで修正します。 
[概要] 3d lutは写真の色や色調を操作するために広く使用されていますが、通常は手動で調整し、カメライメージングパイプラインや写真編集ツールで修正します。たとえば、学習した3dlutは画像です-柔軟な写真の強調に適応します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Driver Anomaly Detection: A Dataset and Contrastive Learning Approach -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.IV/paper_16.html">
      <font color="black">Driver Anomaly Detection: A Dataset and Contrastive Learning Approach</font>
    </a>
  </h2>
  <font color="black">このタスクでは、新しいビデオベースのベンチマークであるDriver Anomaly Detection（DAD）データセットを紹介します。このデータセットには、通常の運転ビデオとトレーニングセット内の一連の異常アクションが含まれています。この方法はテストセットで0.9673AUCに達します。異常検出タスクに対する対照的な学習アプローチの有効性を示しています。DADデータセットのテストセットには、通常の運転から除外する必要のある、目に見えない異常なアクションがあります。 
[概要]お父さんのデータセットのテストセットには、通常の運転から除外する必要のある目に見えない異常なアクションがあります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Where Does Trust Break Down? A Quantitative Trust Analysis of Deep
  Neural Networks via Trust Matrix and Conditional Trust Densities -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_0.html">
      <font color="black">Where Does Trust Break Down? A Quantitative Trust Analysis of Deep
  Neural Networks via Trust Matrix and Conditional Trust Densities</font>
    </a>
  </h2>
  <font color="black">より具体的には、信頼マトリックスは、特定のアクターとオラクルの回答シナリオで予想される質問と回答の信頼を定義し、ディープニューラルネットワークの信頼性を向上させるために対処する必要がある信頼性の低い領域をすばやく特定できるようにします。提案された信頼マトリックスは計算が簡単で、人間が解釈でき、著者の知る限りでは、アクターオラクルの回答レベルで信頼を研究する最初の人です。特定のディープニューラルの信頼がどこで崩壊するかについて、より深く、より詳細な洞察を提供します。一連の質問が与えられたネットワーク。 
[概要]新しい調査によると、信頼マトリックスは、既存の一連の信頼定量化メトリックに加えて役立つ可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Dissected 3D CNNs: Temporal Skip Connections for Efficient Online Video
  Processing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_1.html">
      <font color="black">Dissected 3D CNNs: Temporal Skip Connections for Efficient Online Video
  Processing</font>
    </a>
  </h2>
  <font color="black">これらの欠点に対処するために、ネットワークの中間ボリュームが分析され、将来の計算のために深度（時間）次元に伝播され、オンライン操作での計算数が大幅に削減される、分析された3DCNNを提案します。3Dを使用した畳み込みニューラルネットワークカーネル（3D CNN）は現在、ビデオフレーム内の時空間特徴の抽出における優位性により、ビデオ認識タスクで最先端の結果を達成しています。最後に、3D CNNは、柔軟性を制限する固定の時間入力サイズで使用するように制約されています。 
[概要]最先端の結果を超える成功した3dcnnアーキテクチャが数多くあります。これらは、成功した3Dモデルのほぼ最も成功した例です。これらには、3D視覚認識とresnetモデルの分析バージョンが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Position-based Scaled Gradient for Model Quantization and Sparse
  Training -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_2.html">
      <font color="black">Position-based Scaled Gradient for Model Quantization and Sparse
  Training</font>
    </a>
  </h2>
  <font color="black">次に、重みベクトルの正則化として機能するPSGが、量子化やスパーストレーニングなどのモデル圧縮ドメインで非常に役立つことを経験的に示します。PSGは、完全精度モデルとその圧縮されたモデルの重み分布間のギャップを減らします。これにより、リソースの可用性に応じて、非圧縮モードまたは圧縮モードとしてモデルを多目的に展開できます。 
[ABSTRACT] psgは、フルプレシジョンモデルとその圧縮されたモデルの重量効果の間のギャップを減らします。提案されたpsgは、psgdと呼ばれるgdの標準スケールに適用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-22">
        <br><font color="black">2020-05-22</font>
      </time>
    </span>
</section>
<!-- paper0: Knowledge Fusion Transformers for Video Action Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_3.html">
      <font color="black">Knowledge Fusion Transformers for Video Action Recognition</font>
    </a>
  </h2>
  <font color="black">事前トレーニングをほとんどまたはまったく使用せずに、1つのストリームネットワークのみを使用することで、現在の最先端技術に近いパフォーマンスへの道を開くことができることを示します。事前トレーニングが少なくて済みます。当社のアーキテクチャは、UCF-101およびCharadesデータセットでトレーニングおよび評価されており、最先端技術と競合しています。 
[概要] 3Dインセプションベースの空間でアクション知識を融合するための自己注意ベースの機能エンハンサーを提示します-分類されることを目的としたビデオクリップの時間的コンテキスト</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-29">
        <br><font color="black">2020-09-29</font>
      </time>
    </span>
</section>
<!-- paper0: Asymmetric Contextual Modulation for Infrared Small Target Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_4.html">
      <font color="black">Asymmetric Contextual Modulation for Infrared Small Target Detection</font>
    </a>
  </h2>
  <font color="black">アブレーション研究と最先端の方法との比較を報告します。この方法では、アプローチのパフォーマンスが大幅に向上します。トップダウンのグローバルコンテキストフィードバックに加えて、小さなターゲットをより適切に強調するために、ボトムアップの変調経路を補足します。高レベルのセマンティクスと微妙な低レベルの詳細を交換するためのポイントごとのチャネルの注意に基づいています。データセットとコードはオンラインで入手できます。 
[概要]データセットとコードはオンラインで入手できます。データセットはオンラインで入手できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: FAN: Frequency Aggregation Network for Real Image Super-resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_5.html">
      <font color="black">FAN: Frequency Aggregation Network for Real Image Super-resolution</font>
    </a>
  </h2>
  <font color="black">私たちは、FANがAIM 2020チャレンジの実際の画像の超解像タスクでうまく機能することを検証するために、定量的および定性的に広範な実験を行います。次に、これらの残差の密な特徴マップを適応的に集約して、詳細とテクスチャが強化されたHR画像を復元します。 、複雑な本物の劣化を伴う実世界のLR画像を復元することは依然として課題です。 
[ABSTRACT] sisrは、ディープラーニングの開発で大きな進歩を遂げました。これには、hr画像を復元するために、より高密度のマップを抽出することが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Tracking-by-Trackers with a Distilled and Reinforced Model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_6.html">
      <font color="black">Tracking-by-Trackers with a Distilled and Reinforced Model</font>
    </a>
  </h2>
  <font color="black">広範な検証により、提案されたアルゴリズムがリアルタイムの最先端のトラッカーと競合することが示されています。学習後、学生は最終的に（i）非常に高速なシングルショットトラッカー、（ii）を備えたトラッカーを構築するために使用できます。シンプルで効果的なオンライン適応メカニズム、（iii）他のトラッカーの融合を実行するトラッカー。最初のメカニズムでは、他のトラッカーの追跡知識を転送および圧縮できます。 
[概要]新しい調査では、オフラインとオンラインで他のビジュアルトラッカーを活用する新しい追跡方法が提案されています。最初の調査では、他のトラッカーの追跡知識を転送および圧縮できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-08">
        <br><font color="black">2020-07-08</font>
      </time>
    </span>
</section>
<!-- paper0: MAP-Net: Multi Attending Path Neural Network for Building Footprint
  Extraction from Remote Sensed Imagery -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_7.html">
      <font color="black">MAP-Net: Multi Attending Path Neural Network for Building Footprint
  Extraction from Remote Sensed Imagery</font>
    </a>
  </h2>
  <font color="black">MAP-Netは、各ステージが徐々に生成され、固定解像度で高レベルの意味的特徴を抽出するマルチパラレルパスを通じて、空間ローカリゼーションで保存されたマルチスケール特徴を学習します。この論文では、新しいマルチアテンダントパスニューラルネットワーク（MAP-Net）を提案します。マルチスケールの建物のフットプリントと正確な境界を正確に抽出するために..さらに、CNNによって抽出された特徴は、それぞれのフィールドのサイズによって制限される常に部分的であり、テクスチャの低い大規模な建物は、抽出時に常に不連続で穴が開いています。 
[ABSTRACT]既存の畳み込みニューラルネットワーク（cnn）ベースの建物抽出方法は、cnnの繰り返しのプーリング操作中に、cnnフィーチャマップの空間情報が失われるため、小さな建物を検出できないと不満を漏らしています。不正確なセグメンテーションエッジがあります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-26">
        <br><font color="black">2019-10-26</font>
      </time>
    </span>
</section>
<!-- paper0: Uncertainty Estimation and Sample Selection for Crowd Counting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_8.html">
      <font color="black">Uncertainty Estimation and Sample Selection for Crowd Counting</font>
    </a>
  </h2>
  <font color="black">1つのドメインでトレーニングされたネットワークからの予測の密度と不確実性を利用して、対象のターゲットドメインから有益な画像を選択し、人間の注釈を取得するサンプル選択戦略を示します。経験的に、UCF-QNRFデータセットでトレーニングされたネットワークはターゲットドメインからのラベル付けされたトレーニングサンプルの17 $ \％$のみを使用して、NWPUデータセットとShanghaitechデータセットで以前の最先端の結果のパフォーマンスを超えるように適合させることができます。サンプル選択戦略が大幅に削減されることを示しますソースドメインでトレーニングされたカウントネットワークをターゲットドメインに適合させるために必要な、ターゲットドメインからのラベル付きデータの量。 
[ABSTRACT]クラウドパスは、ガウス分布を使用してクラウドパスを予測するために使用されます。この方法により、データを新しいドメインに適応させるために必要な人間による注釈の労力を削減できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Password-conditioned Anonymization and Deanonymization with Face
  Identity Transformers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_9.html">
      <font color="black">Password-conditioned Anonymization and Deanonymization with Face
  Identity Transformers</font>
    </a>
  </h2>
  <font color="black">ただし、キャプチャされた画像/ビデオにプライバシーに配慮した情報（顔のIDなど）が含まれている可能性があるため、社会的な懸念も高まっています。顔IDトランスフォーマーは、（1）匿名化後に顔ID情報を削除するようにトレーニングされています。正しいパスワードを指定すると元の顔を復元でき、（3）間違ったパスワードを指定すると、写真のようにリアルな間違った顔を返すことができます。カメラは私たちの日常生活で普及しており、多くの便利なシステムを構築できます。サービスアプリケーション用のスマートカメラやホームロボットなどのコンピュータービジョンテクノロジー。 
[概要]私たちの顔IDトランスフォーマーは、顔ID情報を削除するようにトレーニングされています。正しいパスワードを指定すると、元の顔の復元が可能になります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-26">
        <br><font color="black">2019-11-26</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Adaptive Semantic Segmentation by Progressive Feature Refinement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_10.html">
      <font color="black">Towards Adaptive Semantic Segmentation by Progressive Feature Refinement</font>
    </a>
  </h2>
  <font color="black">具体的には、まずソースドメイン画像とターゲットドメイン画像の多段階中間特徴マップを位置合わせし、次にドメイン分類器を採用してセグメンテーション出力を識別します。実験結果により、提案した方法の効率を現状と比較して検証します。アートメソッド..コンピュータビジョンの基本的なタスクの1つとして、セマンティックセグメンテーションは実際のアプリケーションで重要な役割を果たします。 
[概要]ディープラーニングモデルは畳み込みネットワークを開発しましたが、実際のシナリオでは依然として課題に直面しています。プログレッシブ機能の改良とドメインの敵対的学習が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Ask-n-Learn: Active Learning via Reliable Gradient Representations for
  Image Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_11.html">
      <font color="black">Ask-n-Learn: Active Learning via Reliable Gradient Representations for
  Image Classification</font>
    </a>
  </h2>
  <font color="black">さらに重要なことに、信頼性の高い勾配埋め込みを取得するための予測キャリブレーションの使用を提唱し、疑似ラベル付け中の確証バイアスの影響を軽減するためのデータ拡張戦略を提案します。広く使用されているにもかかわらず、実際には、パフォーマンスはキャリブレーションされていない不確実性、データの探索と活用の間の不十分なトレードオフ、確証バイアスの存在などを含む多くの要因。既存のアクティブな学習では、不確実性と多様性に基づくさまざまなヒューリスティックを使用してクエリサンプルを選択します。 
[概要]現在、被験者をテストするためのさまざまな方法を開発しています。これらは予測が難しく、これらのモデルを取り除くことは困難です。これらの課題に対処するために、「ask --n--learn」を提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Object Detection from Captions via Textual Scene Attributes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_12.html">
      <font color="black">Learning Object Detection from Captions via Textual Scene Attributes</font>
    </a>
  </h2>
  <font color="black">結果として得られるモデルが、いくつかの挑戦的なオブジェクト検出データセットで最先端の結果を達成し、最近のアプローチを上回っていることを経験的に示します。つまり、テキストは、最近文献に記載されているように、画像のシーンを表します。アノテーターはオブジェクトとその境界ボックスにラベルを付ける必要があるため、収集が困難な大きな注釈付きデータセットを必要とする、コンピュータービジョンの基本的なタスク。 
[概要]この作品では、キャプションには、オブジェクトのカテゴリやそれらの関係など、画像に関するより豊富な情報が含まれていると主張します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Self-Adaptive Training: beyond Empirical Risk Minimization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_13.html">
      <font color="black">Self-Adaptive Training: beyond Empirical Risk Minimization</font>
    </a>
  </h2>
  <font color="black">コードはhttps://github.com/LayneH/self-adaptive-trainingでリリースされます。自己適応型トレーニングのエラー容量曲線を評価します。テストエラーは単調に減少しています。ただし、このようなデータの標準的な経験的リスク最小化（ERM）は、ノイズを簡単にオーバーフィットする可能性があるため、パフォーマンスが最適ではありません。 
[ABSTRACT] self-適応トレーニングは、さまざまなレベルのノイズの下でermの一般化を大幅に改善し、オーバートンの問題を軽減します。これは、、 em、ラベルノイズ、および分布サンプルの外で破損したデータから堅牢に学習するために重要です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-24">
        <br><font color="black">2020-02-24</font>
      </time>
    </span>
</section>
<!-- paper0: Finding It at Another Side: A Viewpoint-Adapted Matching Encoder for
  Change Captioning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_14.html">
      <font color="black">Finding It at Another Side: A Viewpoint-Adapted Matching Encoder for
  Change Captioning</font>
    </a>
  </h2>
  <font color="black">さらに、人間の注意の好みをさらにシミュレートし、言語評価の報酬で注意を直接微調整するための新しい強化学習プロセスを提案します。広範な実験結果は、私たちの方法が最先端のアプローチを大幅に上回っていることを示していますSpot-the-DiffデータセットとCLEVR-Changeデータセットの両方のマージン..ほとんどの既存の方法は、視点の変更などの注意散漫が存在しない場合に、この問題を差異判断として扱います。 
[概要]ほとんどの既存の方法は、この問題を、視点の変更などの気を散らすものが存在しない差異判断として扱います。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Joint Visual-Temporal Embedding for Unsupervised Learning of Actions in
  Untrimmed Sequences -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_15.html">
      <font color="black">Joint Visual-Temporal Embedding for Unsupervised Learning of Actions in
  Untrimmed Sequences</font>
    </a>
  </h2>
  <font color="black">この目的のために、予測U-Netアーキテクチャに基づく視覚的埋め込みと時間的連続関数を組み合わせます。提案された方法は、3つの標準ベンチマークデータセット、朝食アクション、INRIA YouTube教育ビデオ、および50サラダで評価されます。提案されたアプローチは、連続するビデオフレームに存在する視覚的手がかりから意味のある視覚的および時間的埋め込みを提供することができ、アクションの監視されていない時間的セグメンテーションのタスクに適していること。 
[概要]提案された方法は、予測u-netアーキテクチャに基づいています。これは、3つの標準ベンチマークデータセット、朝食アクション、inria youtubeビデオ、および50個のサラダで評価されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-29">
        <br><font color="black">2020-01-29</font>
      </time>
    </span>
</section>
<!-- paper0: Benchmark for Anonymous Video Analytics -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_16.html">
      <font color="black">Benchmark for Anonymous Video Analytics</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、オーディエンスのローカリゼーションとカウント、およびオーディエンスの人口統計のタスクを評価する、デジタルの家庭外オーディエンス測定の最初のベンチマークを提案します。ただし、オーディエンス測定ソリューションへの関心は高まっていますが、一般的に受け入れられているベンチマークは存在しません。それらのパフォーマンスを評価します。ベンチマークを使用して、GPUおよびCPUに最適化された推論を備えた4つのハードウェアプラットフォームでの8つのオープンソースアルゴリズムと、ローカリゼーション、カウント、経過時間に関する2つの市販のソリューションの詳細な比較を示します。と性別の推定。 
[概要]ベンチマークは、新しいビデオデータセットと一連のパフォーマンス測定値で構成されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: A robustness measure for singular point and index estimation in
  discretized orientation and vector fields -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_17.html">
      <font color="black">A robustness measure for singular point and index estimation in
  discretized orientation and vector fields</font>
    </a>
  </h2>
  <font color="black">提示されたロバスト性測定は、離散化されたベクトル場の欠陥の不確実性の定量化への道を開きます。欠陥の周りの典型的なベクトル場パターンをサンプリングすると、ロバスト性はテンプレートパスの長さとともに増加しますが、ベクトルにノイズが存在する場合はそれほど増加しません。離散化されたベクトル場の特異点またはトポロジー欠陥の識別は、宇宙マイクロ波背景の分極から液晶、指紋認識、生物医学イメージングに至るまで、さまざまな領域で発生します。 
[概要]これは欠陥とそのサットンチャージによるものですが、動きがしきい値を超えて変化するとすぐに不連続に変化します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Encode the Unseen: Predictive Video Hashing for Scalable Mid-Stream
  Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_18.html">
      <font color="black">Encode the Unseen: Predictive Video Hashing for Scalable Mid-Stream
  Retrieval</font>
    </a>
  </h2>
  <font color="black">FCVIDとActivityNetでの実験は、このタスクの実現可能性を示しています。この厳しい状況で検索を実行するために、（1）クエリで欠落しているビデオコンテンツを説明するために、予測と増分の両方であるバイナリエンコーダーに基づくアプローチを提案します。時間と（2）ストリーミング全体で繰り返され、継続的に進化するクエリに対応します。このペーパーでは、コンピュータービジョンの新しい問題であるミッドストリームビデオ間検索に取り組みます。 
[概要]再生中の動画に類似した動画コンテンツをデータベースで検索します。これにより、ユーザーは新しい動画コンテンツを検索できます。これは、動画動画ストレージの最初の検索です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: iSeeBetter: Spatio-temporal video super-resolution using recurrent
  generative back-projection networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_19.html">
      <font color="black">iSeeBetter: Spatio-temporal video super-resolution using recurrent
  generative back-projection networks</font>
    </a>
  </h2>
  <font color="black">iSeeBetterを紹介します。これは、時間的に一貫した超解像ビデオをレンダリングする、ビデオ超解像（VSR）への新しいGANベースの時空間アプローチです。最近、学習ベースのモデルにより、単一画像超解像（SISR）のパフォーマンスが向上しました。 ..畳み込みニューラルネットワーク（CNN）は、ピーク信号対ノイズ比（PSNR）や構造的類似性（SSIM）などの画質メトリックの点で従来のアプローチよりも優れています。 
[ABSTRACT] iseebetterは、リカレントバックプロジェクションネットワークの概念をジェネレータとして使用して、現在のフレームと隣接するフレームから空間情報とタイミング情報を抽出します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-13">
        <br><font color="black">2020-06-13</font>
      </time>
    </span>
</section>
<!-- paper0: CoKe: Localized Contrastive Learning for Robust Keypoint Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_20.html">
      <font color="black">CoKe: Localized Contrastive Learning for Robust Keypoint Detection</font>
    </a>
  </h2>
  <font color="black">特に、CoKeは、オブジェクトが部分的に遮られ、さまざまなデータセット（PASCAL3D +、MPII、ObjectNet3D）で関連する作業を上回っている場合に非常に優れたパフォーマンスを発揮します。特に、機能バンクメカニズムを導入し、キーポイント機能と非キーポイント機能のルールを更新します。ローカルのあいまいさに対して正確で堅牢なローカルキーポイント検出器の学習を可能にします。キーポイント検出への今日の最も一般的なアプローチは、すべてのキーポイントの全体的な表現を学習します。 
[ABSTRACT]キーポイント検出により、誤検出を防ぐことができます-ローカルのあいまいさによる陽性検出</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-29">
        <br><font color="black">2020-09-29</font>
      </time>
    </span>
</section>
<!-- paper0: Pyramidal Edge-maps and Attention based Guided Thermal Super-resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_21.html">
      <font color="black">Pyramidal Edge-maps and Attention based Guided Thermal Super-resolution</font>
    </a>
  </h2>
  <font color="black">提案されたネットワークには2つのサブネットワークがあります。最初のサブネットワークは低解像度の熱画像を超解像し、2番目のサブネットワークは拡大する知覚スケールで可視画像からエッジマップを取得し、それらを超解像サブネットワークに統合します。注意ベースの融合の助けを借りてネットワーク..マルチレベルエッジの抽出と統合により、超解像ネットワークはテクスチャからオブジェクトレベルの情報を段階的に処理し、入力画像間で重複するエッジをより簡単に識別できます。 
[ABSTRACT]超解像ネットワークは超解像を使用してテクスチャからオブジェクトレベルの情報を段階的に処理します。これは、画像間に重大なテクスチャの不一致があり、ぼやけやゴーストのアーティファクトとして現れることを意味します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-13">
        <br><font color="black">2020-03-13</font>
      </time>
    </span>
</section>
<!-- paper0: PESAO: Psychophysical Experimental Setup for Active Observers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_22.html">
      <font color="black">PESAO: Psychophysical Experimental Setup for Active Observers</font>
    </a>
  </h2>
  <font color="black">多くの研究は人間のパフォーマンスを調査しますが、通常、2Dで描かれた線画を使用し、アクティブなオブザーバーは関与しません。目標は、人間の被験者（アクティブなオブザーバー）を念頭に置いて、さまざまなアクティブな知覚タスクの実験セットアップを構築することでした。頭と視線を追跡する方法..私たちのインスタンス化では、400cm x 300cmの領域にまたがり、120Hzの周波数でアクティブな観察者を追跡できます。 
[ABSTRACT] pesaoは、3Dワールドでのアクティブな視覚的観測を調査するために設計されています。これは、米国で開発されたpesaoによって開発されました。pesaoは、120Hzの周波数でアクティブな観測者を追跡できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-15">
        <br><font color="black">2020-09-15</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning-based Pipeline for Module Power Prediction from EL
  Measurements -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_23.html">
      <font color="black">Deep Learning-based Pipeline for Module Power Prediction from EL
  Measurements</font>
    </a>
  </h2>
  <font color="black">次に、ディープラーニングを使用して、パフォーマンスが大幅に向上するモデル（7.3 +/- 2.7Wまたは3.2 +/- 6.5％）をトレーニングできることを示します。ここでは、主なタイプとして非アクティブ領域と亀裂に焦点を当てます。欠陥..したがって、他の研究者が私たちの結果と比較できるように、データセット、コード、およびトレーニング済みモデルを公開します。 
[概要]これは通常、モジュールの取り外しまたは取り外しが必要な測定によって決定され、個々のモジュールの定期的な検査が実行不可能になります。したがって、モジュールの電力を決定することは困難です。モジュールの719個の電光測定の大規模なデータセットをコンパイルします。劣化のさまざまな段階、特にセルの亀裂や破損、および最大電力点での対応する電力</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Single Image Reflection Removal with Physically-Based Training Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_24.html">
      <font color="black">Single Image Reflection Removal with Physically-Based Training Images</font>
    </a>
  </h2>
  <font color="black">分離をより適切にガイドするために、反射をバックトラックするためのモジュール、バックトラックネットワーク（BT-net）を追加で検討します。これにより、ガラス/レンズの複雑なゴースト、減衰、ぼやけ、焦点ぼけの影響が除去されます。物理的にシミュレートされたトレーニングデータは、さまざまな実際の反射画像で検証され、最先端の技術と比較して視覚的に快適で数値的な利点を示します。この論文では、物理ベースのレンダリングを使用して、必要なトレーニング画像とそれに対応するトレーニング画像を忠実に合成します。ネットワーク構造と損失期間が提案されています。 
[概要]多数のトレーニング画像ペアがさまざまな方法で作成されましたが、物理ベースの構造からは離れています。既存のrgbd / rgb画像を使用して必要なメッシュを推定し、メッシュ、ガラス、の間の光輸送を物理的にシミュレートしました。トレーニングデータを合成するためのパストレーシング付きレンズ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-26">
        <br><font color="black">2019-04-26</font>
      </time>
    </span>
</section>
<!-- paper0: Flood severity mapping from Volunteered Geographic Information by
  interpreting water level from images containing people: a case study of
  Hurricane Harvey -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_25.html">
      <font color="black">Flood severity mapping from Volunteered Geographic Information by
  interpreting water level from images containing people: a case study of
  Hurricane Harvey</font>
    </a>
  </h2>
  <font color="black">洪水の重大度は、画像の解釈に基づいて抽出することもできます。場所のあるソーシャルメディアの投稿は、ボランタリー地理情報（VGI）と呼ばれることが多く、そのようなイベントの空間パターンを明らかにすることができます。単に投稿を洪水として分類するだけではありません。関連するかどうかにかかわらず、新しいデータソースとしての
[ABSTRACT]ソーシャルメディアなどのより詳細な情報は、洪水監視のためのリアルタイム情報を提供できます。最近の研究は、テキストに加えて画像を分析することによる詳細な投稿の抽出に焦点を当てています。具体的には、洪水の深刻度は、画像の解釈に基づいて抽出することもできます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-21">
        <br><font color="black">2020-06-21</font>
      </time>
    </span>
</section>
<!-- paper0: Representation Learning on Visual-Symbolic Graphs for Video
  Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_26.html">
      <font color="black">Representation Learning on Visual-Symbolic Graphs for Video
  Understanding</font>
    </a>
  </h2>
  <font color="black">この豊かな視覚的および意味論的コンテキストをキャプチャするために、2つのグラフを使用することを提案します：（1）ノードがアクターとオブジェクトに対応し、エッジがさまざまなタイプの相互作用をエンコードする属性付き時空間視覚グラフ、および（2）モデル化するシンボリックグラフセマンティックな関係..Charadesデータセットでの時間的アクションのローカリゼーションなど、挑戦的なビデオ理解タスクの実験は、提案された方法が最先端のパフォーマンスにつながることを示しています。私たちのモデルは、ノードとエッジが同じタイプの場合、エッジの重みが固定されたグラフを操作し、シンボリックグラフを使用しません。 
[要約]提案された方法は、意味関係をモデル化する視覚的および象徴的なネットワークを含みます。これらには、属性付きの空間が含まれます。ノードがアクターとオブジェクトに対応するタイミング視覚グラフです。また、人々が対話できるネットワークを作成するためにも使用されます。それら</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-05-17">
        <br><font color="black">2019-05-17</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Semi-Supervised Multi-Domain Tracking -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_27.html">
      <font color="black">Adversarial Semi-Supervised Multi-Domain Tracking</font>
    </a>
  </h2>
  <font color="black">しかし、完全に共有されたアーキテクチャでは、いくつかの新しい機能は特定のドメインでのみ有用であり、学習された機能表現の一般化を減らします。ドメイン不変の機能とドメイン固有の機能を分離するための半監視学習スキームを提案します。敵対的学習、それらの間の相互排除を促進し、ラベルのないリザーバーを使用して共有機能を強化するための自己監視学習を活用します。マルチドメイン学習用のニューラルネットワークは、共有と共同学習によって異なるドメインからの情報の効果的な組み合わせを強化しますパラメータ。 
[概要]マルチドメイントラッカーの共有レイヤーの新しい機能は、目に見えない動画の追跡に不可欠です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: MeshWalker: Deep Mesh Understanding by Random Walks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_28.html">
      <font color="black">MeshWalker: Deep Mesh Understanding by Random Walks</font>
    </a>
  </h2>
  <font color="black">さらに、非常に少数の例でも学習に十分です。各ウォークは頂点のリストとして編成され、何らかの方法でメッシュに規則性を課します。深層学習用の3D形状を表現するほとんどの試みは、ボリュームグリッドに焦点を当てています。マルチビュー画像と点群。 
[概要]この論文では、コンピュータグラフィックスで最も一般的な3D形状の表現を見ていきます。これは、メッシュウォーカーと呼ばれる、特定のメッシュから直接形状を学習するための非常に異なるアプローチを示唆しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-09">
        <br><font color="black">2020-06-09</font>
      </time>
    </span>
</section>
<!-- paper0: Restoring Spatially-Heterogeneous Distortions using Mixture of Experts
  Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_29.html">
      <font color="black">Restoring Spatially-Heterogeneous Distortions using Mixture of Experts
  Network</font>
    </a>
  </h2>
  <font color="black">マルチタスク学習に動機付けられて、共通表現と歪み固有の表現の両方を学習する複数のパスを持つようにネットワークを設計します。このモデルは実際の歪みを復元するのに効果的であり、この方法が管理用に設計された他のモデルよりも優れていることを実験的に検証します。単一の歪みと複数の歪みの両方..さらに、マルチ歪みの画像を効果的に復元するために、専門家ネットワークの混合を提案します。 
[概要]組み合わせの別のポイントで、空間的に不均一な歪みデータセットを導入します。複数の破損が各画像の異なる場所に適用されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Real-Time Instrument Segmentation in Robotic Surgery using Auxiliary
  Supervised Deep Adversarial Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_30.html">
      <font color="black">Real-Time Instrument Segmentation in Robotic Surgery using Auxiliary
  Supervised Deep Adversarial Learning</font>
    </a>
  </h2>
  <font color="black">補助およびメインブランチからの異なる次元およびチャネルの特徴マップを融合するための多重解像度特徴融合モジュール（MFF）を提案します。また、補助損失と敵対的損失を組み合わせてセグメンテーションモデルを正規化する新しい方法を紹介します。この目的のために、商用ロボットシステムから取得した高解像度ビデオから手術器具をセグメント化するための軽量カスケード畳み込み神経ネットワーク（CNN）を開発しました。 
[概要]実際の外科医は、高度な高度なナビゲーションシステムを開発しました。mffは、補助ブランチとメインブランチからのさまざまな次元とチャネルの機能マップを融合する多重解像度機能融合モジュールです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-22">
        <br><font color="black">2020-07-22</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Auto-Augment via Augmentation-Wise Weight Sharing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_31.html">
      <font color="black">Improving Auto-Augment via Augmentation-Wise Weight Sharing</font>
    </a>
  </h2>
  <font color="black">包括的な分析により、有効性と効率の点でこのアプローチの優位性が検証されます。拡張ポリシーの自動検索に関する最近の進歩により、さまざまなタスクのパフォーマンスが大幅に向上しました。これにより、拡張に基づいた強力で効率的なプロキシタスクを設計するようになりました。賢明な方法で迅速かつ正確な評価プロセスを形成するためのワイズウェイトシェアリング（AWS）。 
[ABSTRACT]自動引数検索の重要なコンポーネントは、報酬を返すために使用される特定の拡張ポリシーの評価プロセスです。効率を達成するために、多くの人が評価の信頼性を犠牲にして速度を上げることを選択します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-focus Image Fusion for Visual Sensor Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_32.html">
      <font color="black">Multi-focus Image Fusion for Visual Sensor Networks</font>
    </a>
  </h2>
  <font color="black">離散コサイン変換（DCT）に基づく画像融合法は、DCTに基づく画像とビデオの標準ではそれほど複雑でなく、時間を節約できるため、VSNアプリケーションにより適しています。ソース画像の対応するブロックの修正ラプラシアン（SML）の合計はコントラスト基準として使用され、SMLの値が大きいブロックが出力画像に吸収されます。いくつかの画像の実験結果は、他のDCTベースと比較して融合画像の主観的および客観的品質の両方の点で提案されたアルゴリズムの改善を示しています。テクニック。 
[概要] dctを使用した画像の分析はそれほど複雑ではなく、時間の節約になります。dctは写真とビデオの標準に基づいているため、vsnアプリケーションにより適しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-28">
        <br><font color="black">2020-09-28</font>
      </time>
    </span>
</section>
<!-- paper0: Labeling of Multilingual Breast MRI Reports -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_33.html">
      <font color="black">Labeling of Multilingual Breast MRI Reports</font>
    </a>
  </h2>
  <font color="black">私たちの提案する方法は、臨床現場で直面する実際的な課題を克服し、従来のアプローチと比較して、医療レポートからラベルを抽出する際のパフォーマンスが向上することを示しています。医療レポートは、臨床試験を通じて患者の状態を記録するための不可欠な媒体です。貴重な情報が含まれています。これを抽出して、臨床ツールの開発に必要な大きなラベル付きデータセットを生成できます。 
[概要]データセットには、大きなラベル付きフォーマットを作成するために抽出できる貴重な情報が含まれています。データセットは、臨床ツールの開発に必要です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-06">
        <br><font color="black">2020-07-06</font>
      </time>
    </span>
</section>
<!-- paper0: In-bed Pressure-based Pose Estimation using Image Space Representation
  Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_34.html">
      <font color="black">In-bed Pressure-based Pose Estimation using Image Space Representation
  Learning</font>
    </a>
  </h2>
  <font color="black">私たちのモデルは、あいまいな圧力マップを、既存のポーズ推定方法の共通の入力ドメインに類似した形状と構造を含む画像に変換します。損失関数の組み合わせを使用して、手動で注釈を付けた公開圧力マップデータセットでメソッドをトレーニングおよびテストします。この論文では、あいまいな圧力データから身体の部分を正確に特定できる新しいエンドツーエンドのフレームワークを提示することで、この課題に対処します。 
[概要]私たちの方法は、既成のポーズ推定器に深いトレーニング可能なニューラルネットワークを装備するというアイデアを活用しています。このニューラルネットワークは、圧力データを前処理して、後続のポーズ推定のために準備します。その結果、モデルは不明確なものを再構築できます体の部分。これにより、ポーズ推定者はポーズを正確かつ確実に推定できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-21">
        <br><font color="black">2019-08-21</font>
      </time>
    </span>
</section>
<!-- paper0: AttendNets: Tiny Deep Image Recognition Neural Networks for the Edge via
  Visual Attention Condensers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_35.html">
      <font color="black">AttendNets: Tiny Deep Image Recognition Neural Networks for the Edge via
  Visual Attention Condensers</font>
    </a>
  </h2>
  <font color="black">より具体的には、AttendNetsは、視覚的注意コンデンサーに基づく深い自己注意アーキテクチャを備えています。これは、最近導入されたスタンドアロンの注意コンデンサーを拡張して、空間チャネルの選択的注意を改善します。ImageNet$ _ {50} $ベンチマークデータセットの実験結果デバイス上の画像認識のタスクは、最高の精度を達成しながら効率を高めるように設計された研究文献のいくつかのディープニューラルネットワークと比較して、AttendNetのアーキテクチャと計算の複雑さが大幅に低いことを示しました（最小のAttendNetは$ \ sim $ 7.2％高い精度を達成しますが、 MobileNet-V1よりも$ \ sim $ 3 $ \ times $少ない乗算加算操作、$ \ sim $ 4.17 $ \ times $少ないパラメーター、および$ \ sim $ 16.7 $ \ times $少ない重みメモリ要件）。さらに、AttendNetには独自の機能があります。機械駆動の設計探索戦略によって達成された、機械設計のマクロアーキテクチャおよびマイクロアーキテクチャの設計。 
[要約]研究は、低シムシムシム時間ディープニューラルネットワークを示しています。これらは、デバイス上の画像認識用に設計されています。結果は、これらの有望な結果に基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: The Utility of Decorrelating Colour Spaces in Vector Quantised
  Variational Autoencoders -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_36.html">
      <font color="black">The Utility of Decorrelating Colour Spaces in Vector Quantised
  Variational Autoencoders</font>
    </a>
  </h2>
  <font color="black">VQ-VAEモデルの色表現を解きほぐすために、トレーニングされたネットワークの有限の埋め込み空間を調べました。RGBからCIE L * a * b *まで（合計5つの色空間が考慮されました）。この目的のために、トレーニングを行いました。入力が1つの色空間の画像であり、その出力が別の色空間であるVQ-VAEのいくつかのインスタンス、たとえばこの記事の
[ABSTRACT]では、構造化された表現を学習するネットワークを実施するために、単純な準監視なしのタスクである色空間変換を提案します。 ..合計5色（cqを含む）が異なる色で分析されました。さらに、5色が考慮されました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Joint Contrastive Learning with Infinite Possibilities -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_37.html">
      <font color="black">Joint Contrastive Learning with Infinite Possibilities</font>
    </a>
  </h2>
  <font color="black">JCLは多くのコンピュータビジョンアプリケーションで実際に効果的ですが、理論的にはJCLの動作を管理する特定のメカニズムも明らかにします。提案された定式化には、各インスタンス固有のクラス内の類似性を強く支持する先天的な機関が含まれているため、個別のインスタンス間で識別可能な特徴を検索する場合に有利です。JCLは、無限の数のクエリとキーのペアの同時学習を暗黙的に含み、不変の特徴を検索するときに厳しい制約を課します。 
[ABSTRACT]コードはgithubで公開されています。共同のcom / caiqi-学習</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Pruning Filter in Filter -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_38.html">
      <font color="black">Pruning Filter in Filter</font>
    </a>
  </h2>
  <font color="black">既存の剪定方法は、フィルター剪定（FP）と重み剪定（WP）の2つのカテゴリにグループ化できます。具体的には、フィルター$ F \ in \ mathbb {R} ^ {C \ times K \ times K} $を次のように扱います。 $ K \ times K $ストライプ、つまり$ 1 \ times 1 $は$ \ in \ mathbb {R} ^ {C} $をフィルター処理し、フィルター全体ではなくストライプをプルーニングすることで、PFFは従来のFPよりも細かい粒度を実現します。ハードウェアにやさしい..最近のいくつかの研究では、プルーニングされたアーキテクチャが継承された重要な重みよりも重要であることが示されているため、単一のフィルター、つまりフィルタースケルトンのアーキテクチャも重要であると主張します。 
[ABSTRACT] fpと重みの剪定は、新しい剪定方法の例です。この方法は、明確な精度なしでcifar-10およびimagenetデータセットで最先端の剪定率を達成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Bilateral Asymmetry Guided Counterfactual Generating Network for
  Mammogram Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_39.html">
      <font color="black">Bilateral Asymmetry Guided Counterfactual Generating Network for
  Mammogram Classification</font>
    </a>
  </h2>
  <font color="black">私たちが提案するモデルは、主にジェネレーター敵対的ネットワークと\ emph {予測フィードバックメカニズム}で構成され、それらは共同で最適化され、相互にプロンプトを出します。一方、後者は分類損失の監視によって反事実的生成を支援します。両側画像のそのような事前分布を伴う因果モデルでは、反事実生成の2つの最適化目標を取得します。これは、新しく提案された反事実生成ネットワークを介して達成できます。 
[ABSTRACT]病変領域を特定するために、画像に病変がない場合に特徴がどのように振る舞うかという問題を調査できます。両側画像のそのような事前分布を伴う因果モデルを構築することにより、次の2つの目標を取得します。反事実</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-channel Deep 3D Face Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_40.html">
      <font color="black">Multi-channel Deep 3D Face Recognition</font>
    </a>
  </h2>
  <font color="black">3D顔データに基づく顔認識のためのマルチチャネルディープ3D顔ネットワークを提案します。実験結果はまた、9チャネル画像がコンフォーマルマップに基づいて平面に平坦化された場合、ネットワークがより優れていることを明確に示しています。正射投影..ピースワイズ線形三角形メッシュ構造に基づいて3D面の幾何学的情報を計算し、3Dから2D平面までの色とともに幾何学的情報をコンフォーマルに平坦化して、最先端のディープCNNアーキテクチャを活用します。 。 
[概要]ネットワークは、2つの3D 3D顔テクスチャ画像に基づく3D認識に基づいています。vgg顔の画像を使用して事前トレーニングし、生成されたマルチチャネル顔画像を微調整することができます。結果また、正射影と比較して、等角写像に基づいて9チャンネル画像を平面に平坦化すると、ネットワークのパフォーマンスが大幅に向上することも明確に示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Feature Learning for Event Data: Direct vs Inverse Problem
  Formulation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_41.html">
      <font color="black">Unsupervised Feature Learning for Event Data: Direct vs Inverse Problem
  Formulation</font>
    </a>
  </h2>
  <font color="black">各アプローチの主な利点を特定して示します。ローカルイベントデータ（時空間で記述されたイベントのローカルボリューム）からの教師なし特徴学習について、2つの一般的な問題の定式化のパフォーマンスを分析します。経験的な結果は、イベントデータからの表現学習のための両方のアプローチの利点を強調しています。 
[概要] 2つの一般的な問題の解釈のパフォーマンスを分析します。ローカルイベントデータからの教師なし特徴学習。直接および逆は教師なし特徴学習用です。これらは、最先端の方法との比較を提供します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Learning from a tiny dataset of manual annotations: a teacher/student
  approach for surgical phase recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_42.html">
      <font color="black">Learning from a tiny dataset of manual annotations: a teacher/student
  approach for surgical phase recognition</font>
    </a>
  </h2>
  <font color="black">利用可能な注釈付きの記録がほとんどないオフラインとオンラインの両方の手術段階認識の場合、この新しい教師/学生戦略は、注釈なしのデータを効率的に活用することにより、貴重なパフォーマンスの向上を提供します。この場合、教師は新しいCNN-biLSTM-CRFアーキテクチャを備えています。オフライン推論専用に設計されています。教師/学生タイプのアプローチを提案します。教師と呼ばれる強力な予測子が、グラウンドトゥルース注釈付きビデオの小さなデータセットで事前にトレーニングされ、より大きなデータセットの合成注釈を生成します。学生-から学びます。 
[概要]私たちの仕事は、注釈付きデータの量が少ないシナリオでの手術段階認識の学習の問題に直面しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-11-30">
        <br><font color="black">2018-11-30</font>
      </time>
    </span>
</section>
<!-- paper0: Monocular Differentiable Rendering for Self-Supervised 3D Object
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_43.html">
      <font color="black">Monocular Differentiable Rendering for Self-Supervised 3D Object
  Detection</font>
    </a>
  </h2>
  <font color="black">実験は、高価な3DグラウンドトゥルースラベルまたはLiDAR情報の代わりに、ノイズの多い単眼深度と微分可能なレンダリングを効果的に使用できることを示しています。KITTI3Dオブジェクト検出データセットを使用して、メソッドの精度を評価します。このメソッドは3Dを予測します。微分可能なレンダリングと、事前にトレーニングされた単眼深度推定ネットワークから導出された自己監視対象を使用した、画像内の各オブジェクトの位置とメッシュ。 
[概要]キティ3Dオブジェクト検出データセットを使用すると、オブジェクトを正確に正確に識別できます。画像追跡キティデータセットを使用して、メソッドの精度を評価します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Attention-Aware Noisy Label Learning for Image Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_44.html">
      <font color="black">Attention-Aware Noisy Label Learning for Image Classification</font>
    </a>
  </h2>
  <font color="black">各ユニットは、さまざまな外乱がより正確にモデル化されるように、画像のサブセットの特定のノイズ分布を学習することが期待されます。ただし、これらのサンプルには、誤ったラベルが含まれる傾向があります（つまり、この論文では、注意を意識したノイズラベル学習アプローチ（$ A ^ 2NL $）は、潜在的なラベルノイズのあるデータセットでトレーニングされたネットワークの識別能力を向上させるために提案されています。
[要約]大量のラベル付きビジュアルデータを取得する最も安価な方法は、ユーザーが指定したラベルのあるWebサイトからクロールすることです。 、flickrなど。これらにはエラー（ノイズの多いラベル）が含まれ、ネットワークパフォーマンスが大幅に低下します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Decoupling Representation Learning from Reinforcement Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_45.html">
      <font color="black">Decoupling Representation Learning from Reinforcement Learning</font>
    </a>
  </h2>
  <font color="black">また、複数の環境からのデータでマルチタスクエンコーダーをトレーニングし、さまざまなダウンストリームRLタスクへの一般化を示します。実験はDeepMind Control、DeepMind Lab、およびAtariの視覚的に多様なRLベンチマークにまたがり、完全なコードはhttps：//で入手できます。 github.com/astooke/rlpyt/tree/master/rlpyt/ul ..オンラインRL実験では、ATCのみを使用してエンコーダーをトレーニングすると、ほとんどの環境でエンドツーエンドRLよりも優れていることがわかります。 
[ABSTRACT]拡張時間コントラスト（atc）と呼ばれる新しい教師なし学習（ul）タスクは、画像拡張の下でコントラスト損失を使用して、観測値のペアを短い時間差で関連付けるように畳み込みエンコーダーをトレーニングします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-14">
        <br><font color="black">2020-09-14</font>
      </time>
    </span>
</section>
<!-- paper0: Detecting and Counting Pistachios based on Deep Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_46.html">
      <font color="black">Detecting and Counting Pistachios based on Deep Learning</font>
    </a>
  </h2>
  <font color="black">ピスタチオの新しいデータセットを紹介し、共有しました。これには、全長167秒の6つの動画と、3927のラベルが付いたピスタチオが含まれます。ピスタチオは、殻の形状に基づいて口を開けたものと閉じたものの2つのカテゴリに分類される栄養価の高いナッツです。 -口..第2段階では、ビデオで口を開けたピスタチオと口を閉じたピスタチオを数える新しい方法を紹介します。 
[概要]この論文は、コンピュータビジョンでピスタチオの種類を数える新しいシステムを提案することを目的としています。輸送ライン上を移動および転がるピスタチオは、一部のフレームでは口が閉じ、他のフレームでは口が開いているように見える場合があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-08">
        <br><font color="black">2020-05-08</font>
      </time>
    </span>
</section>
<!-- paper0: Knee Injury Detection using MRI with Efficiently-Layered Network (ELNet) -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_47.html">
      <font color="black">Knee Injury Detection using MRI with Efficiently-Layered Network (ELNet)</font>
    </a>
  </h2>
  <font color="black">モデルのコードはhttps://github.com/mxtsai/ELNetで提供されています。最後に、提案されたモデルは非常に軽量（$ &lt;$ 1MB）であるため、実際の臨床設定でのトレーニングと展開が簡単です。提案された方法は、定量的および定性的に検証され、入力として単一のイメージングスタック（アキシャルまたはコロナル）を使用しながら、最先端のMRNetと比較して有利です。 
[ABSTRACT]磁気ネットワーク（elnet）は畳み込みニューラルネットワーク（cnn）アーキテクチャです。提案された方法は定量的および定性的に検証されます。単一のイメージングスタックを入力として使用しながら、最先端のmrnetと比較して遜色ありません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-06">
        <br><font color="black">2020-05-06</font>
      </time>
    </span>
</section>
<!-- paper0: Enhanced Standard Compatible Image Compression Framework based on
  Auxiliary Codec Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_48.html">
      <font color="black">Enhanced Standard Compatible Image Compression Framework based on
  Auxiliary Codec Networks</font>
    </a>
  </h2>
  <font color="black">したがって、コンパクトな表現と後処理ネットワークを効果的かつ最適に学習することができます。本論文では、補助コーデックネットワーク（ACN）に基づく新しい標準互換画像圧縮フレームワークを提案します。具体的には、最適な学習を達成することは困難です。コーデックの考慮が不正確であるため、コンパクトな表現ネットワークを使用した以前の研究。 
[概要]学習可能なコーデックは、従来の圧縮モジュールを超えたエンドツーエンドの学習用に設計されています。コンパクトな表現ネットワークは、入力画像の容量を減らしてビットレートをカットすると同時に、デコードされた画像</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Evaluating the Disentanglement of Deep Generative Models through
  Manifold Topology -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_49.html">
      <font color="black">Evaluating the Disentanglement of Deep Generative Models through
  Manifold Topology</font>
    </a>
  </h2>
  <font color="black">ただし、解きほぐしの測定は困難で一貫性がなく、アドホック外部モデルに依存するか、特定のデータセットに固有であることがよくあります。これに対処するために、トポロジの類似性を測定することにより、生成モデルのみを使用する解きほぐしを定量化する方法を示します。学習した表現における条件付き部分多様体の例..私たちの方法の有効性と適用可能性を説明するために、複数のデータセットにわたるいくつかの最先端モデルを経験的に評価します。 
[概要]解きほぐしの測定は困難であり、アドホック外部モデルに依存するか、特定のデータセットに固有であることがよくあります。この方法では、教師なしバージョンと教師ありバージョンの両方が表示されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-05">
        <br><font color="black">2020-06-05</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Image-adaptive 3D Lookup Tables for High Performance Photo
  Enhancement in Real-time -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_50.html">
      <font color="black">Learning Image-adaptive 3D Lookup Tables for High Performance Photo
  Enhancement in Real-time</font>
    </a>
  </h2>
  <font color="black">私たちは、初めて、ペアワイズ学習またはペアなし学習を使用して、注釈付きデータから3D LUTを学習することを提案します。この論文では、高速で堅牢な写真を実現するために、画像適応型3次元ルックアップテーブル（3D LUT）を学習します。エンハンスメント..さらに重要なことに、学習した3D LUTは、柔軟な写真エンハンスメントのために画像に適応します。 
[概要] 3d lutは写真の色や色調を操作するために広く使用されていますが、通常は手動で調整し、カメライメージングパイプラインや写真編集ツールで修正します。たとえば、学習した3dlutは画像です-柔軟な写真の強調に適応します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Toward Privacy and Utility Preserving Image Representation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_51.html">
      <font color="black">Toward Privacy and Utility Preserving Image Representation</font>
    </a>
  </h2>
  <font color="black">顔画像のプライバシー保護メカニズムとしてのAIAの有効性を実証するために、公開されているデータセットで実験を行いました。AIAは最初に生成モデルを使用して画像表現を作成し、次に敵対的学習を使用して学習した画像表現を強化してプライバシーを保護し、特定のタスクのユーティリティ..ただし、最適なタスクユーティリティを維持しながら、画像を保護する問題にはあまり注意が払われていません。 
[概要]性別や人種などの識別可能な情報の痕跡を削除するために画像を混乱させることにより、個人のプライバシーを保護するための複数の方法が提案されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Meshing Point Clouds with Predicted Intrinsic-Extrinsic Ratio Guidance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_52.html">
      <font color="black">Meshing Point Clouds with Predicted Intrinsic-Extrinsic Ratio Guidance</font>
    </a>
  </h2>
  <font color="black">私たちの方法は、詳細を保持し、あいまいな構造を処理できるだけでなく、合成データと実際のデータの実験によって、見えないカテゴリへの強力な一般化可能性を備えていることを示します。特に、どの点のトリプレットが面を形成するかを予測します。代わりに、既存のポイントに接続情報を追加するだけで、入力ポイントクラウドを可能な限り活用します。 
[概要]コードはwwwで入手できます。 github。 com / colin97 / point2mesh。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-17">
        <br><font color="black">2020-07-17</font>
      </time>
    </span>
</section>
<!-- paper0: 3D Dense Geometry-Guided Facial Expression Synthesis by Adversarial
  Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_53.html">
      <font color="black">3D Dense Geometry-Guided Facial Expression Synthesis by Adversarial
  Learning</font>
    </a>
  </h2>
  <font color="black">公開されている2つの挑戦的な表情ベンチマークであるAffectNetとRaFDについて、定量的評価と定性的評価の両方を広範囲に実行しました。このデータセットを利用して、敵対的学習による新しい深度一貫性の損失を最小限に抑えます（生成された顔画像のグラウンドトゥルース深度マップがないことに注意してください）。 ）およびディスクリミネーターでの合成データの深度カテゴリカル損失。この目的のために、最先端の3D再構成モデルを使用して深度を推定し、大規模なRGB-を作成することを提案します。手動のデータクリーンアッププロセス後の深度データセット。 
[概要]式操作のために3D高密度（深さおよび表面法線）情報を活用するための新しいganフレームワークを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Demographic Influences on Contemporary Art with Unsupervised Style
  Embeddings -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_54.html">
      <font color="black">Demographic Influences on Contemporary Art with Unsupervised Style
  Embeddings</font>
    </a>
  </h2>
  <font color="black">一方では視覚スタイルと、他方では社会的近接性、性別、国籍との間に関連性は見られません。このアートは、スタイルとジャンルの点で分類されていませんが、監視された分析にはあまり適していませんが、データソースには小説が付属しています同様に斬新な方法で視覚的コンテンツを組み立てるのに役立つ可能性のある情報。計算アート分析は、分類タスクへの依存を通じて、アートワークが必要な注釈ですでに適切にソートされている優先履歴データセットを持っています。 
[ABSTRACT]現代アートワークの新しいデータセットであるcontempartは、contempartとcontempartのコラボレーションです。contempartは、現代アートワークのみのマルチモーダルデータセットです。データセットは、世界中のデータを含むネットワークからのデータに基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Efficient Kernel Transfer in Knowledge Distillation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_55.html">
      <font color="black">Efficient Kernel Transfer in Knowledge Distillation</font>
    </a>
  </h2>
  <font color="black">最近、中間層からの情報もより良い蒸留のために採用されています。ニューラルネットワークの各層の出力は、元の画像にカーネル関数を適用することによって生成された新しい特徴空間と見なすことができます。したがって、転送することを提案します。蒸留用の教師モデルから学生モデルまでの対応するカーネル行列（つまり、グラム行列）。 
[概要]初期の作品では、教師モデルの最終出力のみが、学生モデルのトレーニングを支援するためのソフトラベルとして使用されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Temporal Context Aggregation for Video Retrieval with Contrastive
  Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_56.html">
      <font color="black">Temporal Context Aggregation for Video Retrieval with Contrastive
  Learning</font>
    </a>
  </h2>
  <font color="black">CC_WEB_VIDEO、FIVR-200K、EVVEなどの複数のビデオ検索タスクで広範な実験が行われます。この論文では、長距離の時間情報を組み込んだビデオ表現学習フレームワークであるTCA（Temporal Context Aggregation for Video Retrieval）を提案します。自己注意メカニズムを使用したフレームレベルの機能間で..提案された方法は、ビデオレベルの機能を備えた最先端の方法に比べて大幅なパフォーマンス上の利点（FIVR-200Kで約17％mAP）を示し、競争力のある結果をもたらしますフレームレベルの機能と比較して、推論時間が22倍高速です。 
[概要]提案された方法は、大幅なパフォーマンス上の利点を示しています（fivrで約17％のマップ-200k）。フレームレベルの機能と比較して、22倍速い尤度時間で競争力のある結果を提供します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-04">
        <br><font color="black">2020-08-04</font>
      </time>
    </span>
</section>
<!-- paper0: Teacher-Critical Training Strategies for Image Captioning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_57.html">
      <font color="black">Teacher-Critical Training Strategies for Image Captioning</font>
    </a>
  </h2>
  <font color="black">ベンチマークMSCOCOデータセットで広く採用されているいくつかのキャプションモデルの実験的評価は、提案されたTCTSが、両方のトレーニング段階でほとんどの評価指標、特にBleuスコアとRouge-Lスコアを包括的に強化することを示しています。教師モデルから効果的に学習するために、Teacher-を提案します。キャプションモデルのより良い学習プロセスを促進するためのXEトレーニングとRLトレーニングの両方のクリティカルトレーニング戦略（TCTS）。教師モデルは、ベースラインキャプションモデルにグラウンドトゥルース画像属性を組み込むことによって構築されます。 
[概要]通常採用されているトレーニング戦略は、xeとrlのトレーニングでミスアライメントに悩まされています。これらはxeとxeの間のミスアライメントですが、多くの場合成功しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: S3K: Self-Supervised Semantic Keypoints for Robotic Manipulation via
  Multi-View Consistency -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_58.html">
      <font color="black">S3K: Self-Supervised Semantic Keypoints for Robotic Manipulation via
  Multi-View Consistency</font>
    </a>
  </h2>
  <font color="black">これらの問題は、これらのモデルの幾何学的構造の欠如から生じると主張します。プラグとソケットの把握と嵌合..この作業では、視覚的表現としてセマンティック3Dキーポイントを提唱し、半教師ありトレーニングの目的を提示します。最小限の監視で1〜5ミリメートルの精度にトレーニングされるインスタンスまたはカテゴリレベルのキーポイント。 
[要約]視覚表現学習への多くの既存のアプローチは、汎用のトレーニング基準を利用しています。 。再構築によって..アディダスなどの多くの既存のモデルは、詳細をキャプチャできません-センシングなどの特定のオブジェクトの精密タスクに必要な詳細</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: ContourCNN: convolutional neural network for contour data classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_59.html">
      <font color="black">ContourCNN: convolutional neural network for contour data classification</font>
    </a>
  </h2>
  <font color="black">等高線はまばらに表されることがよくあります。優先プーリングレイヤーは、残りの部分を変更せずに、大きさが小さいフィーチャをプールします。等高線は、閉じた形状を表す点の円形シーケンスです。 
[ABSTRACT]輪郭はまばらに表現されることが多いこれらの輪郭はまばらに表現されることが多い</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-20">
        <br><font color="black">2020-09-20</font>
      </time>
    </span>
</section>
<!-- paper0: Driver Anomaly Detection: A Dataset and Contrastive Learning Approach -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_60.html">
      <font color="black">Driver Anomaly Detection: A Dataset and Contrastive Learning Approach</font>
    </a>
  </h2>
  <font color="black">DADデータセットのテストセットには、通常の運転から除外する必要のある目に見えない異常なアクションがあります。注意散漫なドライバーは危険を予測できない可能性が高く、自動車事故につながります。私たちの方法は0.9673AUCに達します。テストセット。異常検出タスクに対する対照的な学習アプローチの有効性を示しています。 
[概要]お父さんのデータセットのテストセットには、通常の運転から除外する必要のある目に見えない異常なアクションがあります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Texture-aware Multi-resolution Image Inpainting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_61.html">
      <font color="black">Texture-aware Multi-resolution Image Inpainting</font>
    </a>
  </h2>
  <font color="black">具体的には、トレーニングスキーマは、4つの連続するジェネレーターのパラメーターを最適化して、高解像度のジェネレーターが低解像度のジェネレーターによって生成された修復された画像を活用するようにします。これは主に、トレーニングアプローチと損失関数によるものです。きめの細かいテクスチャを復元するために、生成されたテクスチャとグラウンドトゥルーステクスチャの違いを最小限に抑える、新しいLBPベースの損失関数。 
[ABSTRACT]ガンは高解像度の画像でトレーニングするのが難しく、モデルが不安定になり、パフォーマンスが低下します。これらのモデルは大量のコンピューティングリソースを必要とし、リアルなテクスチャの詳細を復元できない場合があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: SceneGen: Generative Contextual Scene Augmentation using Scene Graph
  Priors -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CV/paper_62.html">
      <font color="black">SceneGen: Generative Contextual Scene Augmentation using Scene Graph
  Priors</font>
    </a>
  </h2>
  <font color="black">SceneGenは、意味的にセグメント化されたシーンを入力として受け取り、仮想コンテンツを配置するための位置および方向の確率マップを出力します。この問題を動機として、この論文では、既存のシーン内の仮想オブジェクトの位置と方向を予測する生成コンテキスト拡張フレームワークであるSceneGenを紹介します。 。空間コンピューティングエクスペリエンスは、ユーザーの実際の環境によって制約されます。 
[ABSTRACT]仮想オブジェクトには、幾何学的な競合が回避され、他のオブジェクトとの機能的でもっともらしい関係がターゲット環境で維持されるコンテキストアプローチが必要です。プロジェクトは、仮想オブジェクトの位置と方向を予測する一連の空間シーン生成に基づいています。既存のシーン</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-25">
        <br><font color="black">2020-09-25</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Measuring Systematic Generalization in Neural Proof Generation with
  Transformers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_0.html">
      <font color="black">Measuring Systematic Generalization in Neural Proof Generation with
  Transformers</font>
    </a>
  </h2>
  <font color="black">さらに、TLMは、前向き連鎖の証明と比較して、後向き連鎖の証明を使用してより一般化できることを発見しましたが、前向き連鎖の証明を生成するのは簡単です。この結果は、トランスフォーマーが効率的であるが解釈できない推論戦略を持っていることを示唆しています。内部的に..トレーニングされたシーケンスよりも長いシーケンスで評価された場合、証明の生成と推論で長さの一般化の問題が観察されます。 
[ABSTRACT] tlmsは、より長い証明にさらされた後、一般化のパフォーマンスを向上させます。証明を生成するようにトレーニングされていないモデルは、より長い証明に基づく問題への一般化に優れていることがわかりました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: BERT for Monolingual and Cross-Lingual Reverse Dictionary -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_1.html">
      <font color="black">BERT for Monolingual and Cross-Lingual Reverse Dictionary</font>
    </a>
  </h2>
  <font color="black">それでも、Multilingual BERT（mBERT）を使用することで、1つのサブワードを埋め込んだ言語間逆辞書を効率的に実行でき、言語間の調整は必要ありません。以前のモデルでは、2つの異なる単語の埋め込みを維持し、これらの調整を学習する必要があります。埋め込み..コードはhttps://github.com/yhcc/BertForRD.gitで公開されています。 
[ABSTRACT] bertは、この特定のタスクのターゲット単語を生成するためのツールです。パラレルコーパスなしでクロスリンガルリバース辞書を実行するためにも使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Rethinking Attention with Performers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_2.html">
      <font color="black">Rethinking Attention with Performers</font>
    </a>
  </h2>
  <font color="black">パフォーマーは、通常のトランスフォーマーと完全に互換性があり、強力な理論的保証を備えた線形アーキテクチャです。アテンションマトリックスの偏りのない、またはほぼ偏りのない推定、均一な収束、低い推定分散。ランダム機能アプローチ（FAVOR +）は、スケーラブルなカーネルメソッドに独立して関心がある可能性があります。他の検討された効率的なスパースおよびデンスアテンションメソッドとの競合結果を示し、パフォーマーが活用する新しいアテンションラーニングパラダイムの有効性を示します。 
[ABSTRACT]実行者は、正の線形ランダム特徴アプローチ（favor）を介して新しい高速注意を使用します。これは、スケーラブルなカーネル法にとって独立した関心事である可能性があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Tackling Online Abuse: A Survey of Automated Abuse Detection Methods -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_3.html">
      <font color="black">Tackling Online Abuse: A Survey of Automated Abuse Detection Methods</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、これまでに提案された方法の包括的な調査を提示し、この分野をさらに発展させるためのプラットフォームを提供します。出現する主な傾向について説明し、残っている課題を強調し、考えられる解決策の概要を示します。倫理と説明性のガイドラインを提案します。個人に対するそのような虐待の心理的影響は、深刻で持続する可能性があります。 
[概要]オンラインでの虐待の被害者は、オンラインプラットフォームで嫌がらせ、人種差別、個人攻撃、その他の種類の虐待に直面しています。過去数年間、自然言語処理の分野で自動虐待検出に向けた多大な研究努力が行われてきました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-13">
        <br><font color="black">2019-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Development of Word Embeddings for Uzbek Language -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_4.html">
      <font color="black">Development of Word Embeddings for Uzbek Language</font>
    </a>
  </h2>
  <font color="black">開発された単語の埋め込みは、多くの自然言語処理のダウンストリームタスクで使用できます。私たちの作業の結果は、で開発された高品質のWebクロールコーパスを使用して、word2vec、GloVe、およびfastTextアルゴリズムでトレーニングされた最初の公開された単語ベクトルのセットです。 -house ..このペーパーでは、Uzbek言語のCyrillicバリアントの単語埋め込みを開発するプロセスを共有します。 
[概要]私たちの仕事の結果は、最初に公開された単語のゲル化のセットです。それらは、word2vec、glove、およびfasttextアルゴリズムでトレーニングされています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Entropia: A Family of Entropy-Based Conformance Checking Measures for
  Process Mining -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_5.html">
      <font color="black">Entropia: A Family of Entropy-Based Conformance Checking Measures for
  Process Mining</font>
    </a>
  </h2>
  <font color="black">定義上、メジャーは有用なプロパティを備えており、多くの場合、迅速に計算できます。プロセスモデルは、ログの一部ではない多くのトレースをエンコードしない場合、検出されたログに関して「良好な」精度を持ち、ログからのトレースのほとんどをエンコードする場合、「良好な」再現率。この測定により、ITシステムによって実行され、イベントログに記録されるトレースから自動的に検出されたプロセスモデルの古典的な非決定論的および確率論的精度と再現率の品質基準を定量化できます。 
[要約]メジャーは、古典的な非決定論的および確率的精度を定量化することを可能にします。それらはエンピアと呼ばれ、メジャーはエンベルと呼ばれ、迅速に分析できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-21">
        <br><font color="black">2020-08-21</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Hard Retrieval Cross Attention for Transformer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_6.html">
      <font color="black">Learning Hard Retrieval Cross Attention for Transformer</font>
    </a>
  </h2>
  <font color="black">その結果、当社のハード検索アテンションメカニズムは、クロスアテンションネットワークに使用する場合に幅広い機械翻訳タスクで競争力を発揮しながら、長いシーケンスと短いシーケンスの両方でスケーリングされたドット積アテンションを経験的に加速できます。出席するシーケンスを絞ると、すべてのトークンではなく、文中の1つのトークンのみに注意を向けるハード検索注意を学習することにより、スケーリングされた内積注意の計算を簡略化します。ハード注意メカニズムは1つの位置にのみ注意を向けるため、注意確率と標準の内積注意の値シーケンスとの間の行列乗算は、単純で効率的な検索操作に置き換えることができます。 
[概要]マルチヘッドアテンションネットワークは、スケーリングされたドット積アテンション機能を並行して実行し、さまざまな表現からの情報に共同で対応することでモデルを強化します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: DocBank: A Benchmark Dataset for Document Layout Analysis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_7.html">
      <font color="black">DocBank: A Benchmark Dataset for Document Layout Analysis</font>
    </a>
  </h2>
  <font color="black">ドキュメントレイアウト分析は通常、キャプチャに不可欠なテキスト情報を無視しながらドキュメントを理解するためにコンピュータビジョンモデルに依存しています。一方、視覚情報とテキスト情報の両方を備えた高品質のラベル付きデータセットはまだ不十分です。この論文では、\ textbf {DocBank }、ドキュメントレイアウト分析用のきめ細かいトークンレベルの注釈付きの500Kドキュメントページを含むベンチマークデータセット。 
[概要]レポートは、arxivで入手可能な「ラテックス」レベルのドキュメントからの弱い監督の下で、シンプルで効果的な方法を使用して編集されました。 com.further分析は、結果がまだ不十分であることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-01">
        <br><font color="black">2020-06-01</font>
      </time>
    </span>
</section>
<!-- paper0: Cross-lingual Spoken Language Understanding with Regularized
  Representation Alignment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_8.html">
      <font color="black">Cross-lingual Spoken Language Understanding with Regularized
  Representation Alignment</font>
    </a>
  </h2>
  <font color="black">クロスリンガルの話し言葉理解タスクの実験は、私たちのモデルが、数ショットとゼロショットの両方のシナリオで現在の最先端の方法よりも優れていること、およびわずか3 \の数ショット設定でトレーニングされたモデルを示しています。ターゲット言語トレーニングデータの％は、すべてのトレーニングデータを使用した教師ありトレーニングと同等のパフォーマンスを達成します。音声言語理解システムの現在のクロスリンガルモデルの有望な結果にもかかわらず、それらの間の不完全なクロスリンガル表現の調整に依然として苦しんでいます。ソース言語とターゲット言語。これにより、パフォーマンスが最適ではなくなります。次に、敵対的なトレーニングを活用して潜在変数を解きほぐすことにより、潜在変数モデルを正規化します（Liu et al。、2019）。 
[ABSTRACT]たとえば、言語間で単語レベル、音声パフォーマンスをさらに調整するモデルを提案します。また、言語の規則的な表現を提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Bridging Information-Seeking Human Gaze and Machine Reading
  Comprehension -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_9.html">
      <font color="black">Bridging Information-Seeking Human Gaze and Machine Reading
  Comprehension</font>
    </a>
  </h2>
  <font color="black">このアプローチが、最先端の読解モデルの英語での多肢選択式質問応答のパフォーマンス向上につながることを示します。この作業では、読解中の人間の視線が、与えられた読解質問にどのように条件付けられるかを分析します。 、およびこの信号が機械の読解に有益であるかどうか。この目的のために、多肢選択式の読解タスクに従事する多数の参加者を含む新しい視線追跡データセットを収集します。 
[概要]私たちは、自動読解をより人間的なものにすることを提案します-その人間の情報を模倣することによって-読解中の読解行動を求めます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: SemEval-2020 Task 12: Multilingual Offensive Language Identification in
  Social Media (OffensEval 2020) -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_10.html">
      <font color="black">SemEval-2020 Task 12: Multilingual Offensive Language Identification in
  Social Media (OffensEval 2020)</font>
    </a>
  </h2>
  <font color="black">タスクには、サブタスクAの英語、アラビア語、デンマーク語、ギリシャ語、トルコ語の5つの言語が含まれていました。合計528チームがタスクに参加するためにサインアップし、145チームが評価期間中にシステムを提出し、70チームがシステム説明ペーパーを提出しました。さらに、英語にはサブタスクBとCも含まれていました。OffensEval2020は、SemEval-2020で最も人気のあるタスクの1つであり、すべてのサブタスクとすべての言語で多数の参加者を魅了しました。 
[概要]タスクには、2019年の攻撃で3つのサブタスクが含まれていました。タスクには、タスクの一部であった3つのサブタスクが含まれています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-12">
        <br><font color="black">2020-06-12</font>
      </time>
    </span>
</section>
<!-- paper0: A Vietnamese Dataset for Evaluating Machine Reading Comprehension -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_11.html">
      <font color="black">A Vietnamese Dataset for Evaluating Machine Reading Comprehension</font>
    </a>
  </h2>
  <font color="black">その結果、人間とデータセットの最高のモデルパフォーマンスとの大きな違いは、将来の研究を通じてViQuADの改善を探ることができることを示しています。さらに、英語と中国語で最先端のMRCメソッドの実験を行っています。 ViQuADの最初の実験モデル。これは、他のモデルと比較されます。私たちのデータセットは、研究コミュニティがベトナムのMRCの課題を克服することを奨励するために無料で利用できます。 
[概要]データセットは、23,000を超える人間で構成されています-174のベトナムの記事の5、109のパッセージに基づく回答ペア</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Generation of lyrics lines conditioned on music audio clips -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_12.html">
      <font color="black">Generation of lyrics lines conditioned on music audio clips</font>
    </a>
  </h2>
  <font color="black">このモデルは、スペクトログラム変分オートエンコーダー（VAE）とテキストVAEで構成されています。バイモーダルニューラルネットワークモデルは、任意の短いオーディオクリップを条件とする線の生成を学習します。自動評価と人間による評価の両方で、特定のオーディオクリップに一致する感情的な影響。 
[概要]バイモーダルニューラルネットワークモデルは、任意の短いオーディオクリップを条件とする線の生成を学習します。自動評価と人間による評価の両方で、モデルの有効性が実証されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Ethically Collecting Multi-Modal Spontaneous Conversations with People
  that have Cognitive Impairments -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_13.html">
      <font color="black">Ethically Collecting Multi-Modal Spontaneous Conversations with People
  that have Cognitive Impairments</font>
    </a>
  </h2>
  <font color="black">このデータを必要とする研究者は、一般に、脆弱な参加者との協力に関する倫理的および法的問題に不慣れです。さらに、標準の録音機器は安全ではないため、機密データのキャプチャには使用しないでください。音声対話システム（Amazonなど）を作成するためAlexaまたはGoogleアシスタント）認知障害のある人にとってよりアクセスしやすく、自然にインタラクティブであるため、適切なデータを取得できる必要があります。 
[概要]新しいシステムは、機密データを安全にキャプチャ、転送、交換するように設計されています。このガイドと安全な記録システムにより、研究者は倫理的措置を確認および改善できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Stronger Baselines for Grammatical Error Correction Using Pretrained
  Encoder-Decoder Model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_14.html">
      <font color="black">Stronger Baselines for Grammatical Error Correction Using Pretrained
  Encoder-Decoder Model</font>
    </a>
  </h2>
  <font color="black">単一言語および多言語のBARTモデルがGECで高いパフォーマンスを達成し、その結果の1つが英語のGECでの現在の強力な結果に匹敵することがわかりました。GECにこの一般的な事前トレーニング済みモデルを使用すると、時間のかかる事前トレーニングを排除できます。 ..この研究では、GECの一般的な事前トレーニング済みエンコーダー-デコーダーモデルとして、双方向および自動回帰トランスフォーマー（BART）の有用性を探ります。 
[概要] gecにこの汎用モデルを使用すると、時間のかかる事前トレーニングを排除できます。これらの調査はgithubで公開されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-24">
        <br><font color="black">2020-05-24</font>
      </time>
    </span>
</section>
<!-- paper0: Enhancing Monotonic Multihead Attention for Streaming ASR -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_15.html">
      <font color="black">Enhancing Monotonic Multihead Attention for Streaming ASR</font>
    </a>
  </h2>
  <font color="black">最後に、安定したストリーミング推論を保証するために、ヘッド同期ビーム検索デコードを提案します。各MAヘッドに対するチャンクワイズの注意をマルチヘッドの対応物に拡張します。さらに、境界検出のためのヘッド間のコンセンサスを改善し、遅延を防ぐために冗長ヘッドを整理することを提案します。そのようなヘッドによって引き起こされるトークンの生成。 
[概要]トレーニング中に頭の一部をマスクして、ヘッドドロップの正則化を提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br><font color="black">2020-05-19</font>
      </time>
    </span>
</section>
<!-- paper0: Towards a Multi-modal, Multi-task Learning based Pre-training Framework
  for Document Representation Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_16.html">
      <font color="black">Towards a Multi-modal, Multi-task Learning based Pre-training Framework
  for Document Representation Learning</font>
    </a>
  </h2>
  <font color="black">ドキュメント分類、ドキュメント情報抽出、ドキュメント検索など、さまざまな実際のドキュメントタスクに対する事前トレーニングフレームワークの適用性を紹介します。現在の制限と作業の次のステップについて説明します。このペーパーでは、私たちは、一般的なドキュメント表現を学習するために、自己監視および監視された事前トレーニングタスクの組み合わせを利用するマルチタスク学習ベースのフレームワークを提案します。 
[概要]ネットワークアーキテクチャと事前トレーニングタスクは、テキスト、レイアウト、画像の次元全体でマルチモーダルドキュメント情報を組み込むように設計されています。徹底的な実験を行って、フレームワークのさまざまなアブレーションとパフォーマンスを比較します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Dilated Convolutional Attention Network for Medical Code Assignment from
  Clinical Text -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_17.html">
      <font color="black">Dilated Convolutional Attention Network for Medical Code Assignment from
  Clinical Text</font>
    </a>
  </h2>
  <font color="black">拡張畳み込みを採用して、拡張サイズとともに指数関数的に増加する受容野を持つ複雑な医療パターンをキャプチャします。臨床テキストから医療コードを予測する医療コード割り当ては、インテリジェント医療情報システムの基本的なタスクです。ただし、最近の高度なニューラルアーキテクチャフラット畳み込みまたはマルチチャネル機能の連結では、テキストシーケンス内の順次因果制約を無視し、特に長期の順次依存を伴う長い臨床ノートの場合、意味のある臨床テキスト表現を学習しない可能性があります。 
[概要]自然言語でのディープモデルの出現により、自動割り当て方法の開発が促進されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Can Automatic Post-Editing Improve NMT? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_18.html">
      <font color="black">Can Automatic Post-Editing Improve NMT?</font>
    </a>
  </h2>
  <font color="black">この新しいコーパスは、CC BY-NC-SA 4.0ライセンスの下でhttps://github.com/shamilcm/pedraでリリースされます。適切な監督が不足しているため、APEモデルのNMT翻訳の改善が不十分であると仮定します。 、APEモデルのトレーニングは、限られた人間の編集後のデータと組み合わせた大規模な人工体に大きく依存してきました。 
[概要]類人猿モデルのトレーニングは、限られた人間の投稿編集データと組み合わされた大規模な人工コーパスに大きく依存してきました。人間の投稿のより大きなコーパスをコンパイルします-英語からドイツ語への編集</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Neural RST-based Evaluation of Discourse Coherence -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_19.html">
      <font color="black">Neural RST-based Evaluation of Discourse Coherence</font>
    </a>
  </h2>
  <font color="black">これを、ツリー再帰ニューラルモデル、つまりRST-Recursiveを通じて示します。これは、最先端のRSTパーサーによって生成されたテキストのRST機能を利用します。このペーパーでは、Rhetorical Structure Theory（RST）ツリーと関係の有用性を評価します。談話の一貫性評価において..さらに、RST-Recursiveを単独で展開すると、62％少ないパラメーターで競争力のある精度を実現します。 
[概要]このベンチマークで新しい最先端の精度を達成できることを示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: ESPnet-ST: All-in-One Speech Translation Toolkit -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_20.html">
      <font color="black">ESPnet-ST: All-in-One Speech Translation Toolkit</font>
    </a>
  </h2>
  <font color="black">ツールキットはhttps://github.com/espnet/espnetで公開されています。データの前処理、特徴抽出、トレーニング、さまざまなベンチマークデータセットのパイプラインのデコードを含むオールインワンレシピを提供しています。単一のフレームワークで音声から音声への変換システムを迅速に開発するために設計されたESPnet-STを紹介します。 
[ABSTRACT] espnet --stは、us-mexico音声処理ツールキットの新しいプロジェクトです。自動音声認識、機械翻訳、およびテキスト読み上げ機能を統合または新たに実装します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br><font color="black">2020-04-21</font>
      </time>
    </span>
</section>
<!-- paper0: Labeling of Multilingual Breast MRI Reports -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_21.html">
      <font color="black">Labeling of Multilingual Breast MRI Reports</font>
    </a>
  </h2>
  <font color="black">私たちの提案する方法は、臨床現場で直面する実際的な課題を克服し、従来のアプローチと比較して、医療レポートからラベルを抽出する際のパフォーマンスが向上することを示しています。医療レポートは、臨床試験を通じて患者の状態を記録するための不可欠な媒体です。医療レポートの多くは正規化されていない形式で保存され、訓練を受けた人間のアノテーター（通常は医師）が各症例を手動で評価してラベルを付ける必要があるため、費用と時間がかかります。 
[概要]データセットには、大きなラベル付きフォーマットを作成するために抽出できる貴重な情報が含まれています。データセットは、臨床ツールの開発に必要です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-06">
        <br><font color="black">2020-07-06</font>
      </time>
    </span>
</section>
<!-- paper0: Data Readiness for Natural Language Processing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_22.html">
      <font color="black">Data Readiness for Natural Language Processing</font>
    </a>
  </h2>
  <font color="black">このドキュメントの内容は、公的部門と民間部門の両方の組織や企業がビジネスプロセスでデータを使用するのを支援する応用研究機関としての私たちの仕事で遭遇した実際的な課題とよくある質問に基づいています。このドキュメントは、機械学習と自然言語処理のコンテキストでのデータの準備に関するものです。自動分析方法を容易にするために、組織がデータを識別、利用可能、検証、および準備する方法について説明します。 
[概要]この情報は、米国を拠点とする組織の助けを借りて書かれました。組織がどのように識別、利用可能、検証、誤解を招く可能性があるかを説明しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-04">
        <br><font color="black">2020-09-04</font>
      </time>
    </span>
</section>
<!-- paper0: Recurrent Inference in Text Editing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_23.html">
      <font color="black">Recurrent Inference in Text Editing</font>
    </a>
  </h2>
  <font color="black">部分的に編集されたテキストをエンコードする各反復で、Recurrenceは潜在的な表現をデコードし、短い固定長のアクションを生成し、アクションを適用して1回の編集を完了します。さまざまな難しさのあるこれらのタスクの広範な実験は、Recurrenceが改善を達成することを示しています従来の推論方法よりも..包括的な比較のために、3種類のテキスト編集タスクを紹介します。算術演算子の復元（AOR）、算術方程式の簡略化（AES）、算術方程式の修正（AEC）です。 
[概要]これは、新しいテキスト編集ツールであるrecurrenceに基づいており、編集アクションを繰り返し実行し、問題領域を大幅に狭めます。さらに、3種類のテキスト編集タスクを紹介します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-26">
        <br><font color="black">2020-09-26</font>
      </time>
    </span>
</section>
<!-- paper0: TaxiNLI: Taking a Ride up the NLU Hill -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_24.html">
      <font color="black">TaxiNLI: Taking a Ride up the NLU Hill</font>
    </a>
  </h2>
  <font color="black">TAXINLIでのさまざまな実験を通じて、特定の分類学的カテゴリではSOTAニューラルモデルがほぼ完全な精度を達成しているのに対し、以前のモデルを大幅に上回っているカテゴリもあります。NLIの例にはさまざまな言語、論理、および推論現象では、訓練されたシステムによってどの特定の概念が学習され、どこで強力な一般化を達成できるかについては不明なままです。事前訓練されたトランスベースのニューラルアーキテクチャは、自然言語推論で一貫して最先端のパフォーマンスを達成しています。 （NLI）タスク。 
[概要] mnliデータセットからの10kの例を含む新しいデータセットであるtaxinliを紹介します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-End Spoken Language Understanding Without Full Transcripts -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_25.html">
      <font color="black">End-to-End Spoken Language Understanding Without Full Transcripts</font>
    </a>
  </h2>
  <font color="black">ATISコーパスでのスピーチからエンティティへの実験では、CTCモデルと注意モデルの両方で、エンティティ以外の単語をスキップする優れた能力が示されました。エンティティのみでトレーニングした場合と完全なトランスクリプトでトレーニングした場合では、劣化はほとんどありませんでした。このようなモデルのトレーニングは、次のように非常に役立ちます。データ収集のコストを大幅に削減できます。私たちの実験には音声入力が含まれるため、これらのシステムはエンティティラベルとエンティティ値を表す単語の両方を正しく認識する必要があります。 
[概要] 2種類の音声エンティティが音声認識に適合しています。これらのモデルは、単語ごとのトランスクリプトなしで、意味エンティティの注釈のみでトレーニングできます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Point-of-Interest Type Inference from Social Media Text -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_26.html">
      <font color="black">Point-of-Interest Type Inference from Social Media Text</font>
    </a>
  </h2>
  <font color="black">ツイートからセマンティックな場所情報を予測する機能は、レコメンデーションシステム、パーソナライズサービス、文化地理学に応用できます。分類器をトレーニングして、ツイートが送信された場所のタイプを予測し、8つのクラスで43.67のマクロF1に到達して明らかにします。それぞれのタイプの場所に関連付けられた言語マーカー..初めて、ソーシャルメディアテキストと、それが投稿された場所のタイプ（公園、レストラン、その他の場所）との関係を調査します。 
[ABSTRACT]初めて、ソーシャルメディアのテキストとそれが投稿された場所のタイプとの関係がわかりました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Tasty Burgers, Soggy Fries: Probing Aspect Robustness in Aspect-Based
  Sentiment Analysis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_27.html">
      <font color="black">Tasty Burgers, Soggy Fries: Probing Aspect Robustness in Aspect-Based
  Sentiment Analysis</font>
    </a>
  </h2>
  <font color="black">この問題を解決するために、ABSAテストセットを強化するためのシンプルで効果的なアプローチを開発します。SemEval2014データセットに基づいて、ABSAモデルのアスペクトロバスト性の包括的なプローブとしてアスペクトロバストネステストセット（ARTS）を構築します。 ARTSでは、9つのABSAモデルの堅牢性を分析し、驚くべきことに、それらの精度が最大69.73％低下することを確認しています。 
[ABSTRACT]既存のabsaテストセットを使用して、モデルが感情と非ターゲットの側面を区別できるかどうかを調べることができます。テストはwwwで入手できます。 github。 com</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-16">
        <br><font color="black">2020-09-16</font>
      </time>
    </span>
</section>
<!-- paper0: KPQA: A Metric for Generative Question Answering Using Keyphrase Weights -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_28.html">
      <font color="black">KPQA: A Metric for Generative Question Answering Using Keyphrase Weights</font>
    </a>
  </h2>
  <font color="black">この問題を軽減するために、GenQAの正しさを評価するための新しいメトリックを提案します。提案されたメトリックは、さまざまなデータセットの既存のメトリックよりも人間の判断との相関が大幅に高いことを示しています。生成的質問回答（GenQA）システムの自動評価では、回答は自由形式であるため、生成された回答の正しさを評価することは困難です。 
[概要]新しい指標は、キーフレーズ予測を介して各トークンに異なる重みを決定します。新しい指標は、指標が人間の判断と相関していないことを示しています。これは、既存の指標の正しさを見つける機会がないためである可能性があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-01">
        <br><font color="black">2020-05-01</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Improved Model Design for Authorship Identification: A Survey on
  Writing Style Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_29.html">
      <font color="black">Towards Improved Model Design for Authorship Identification: A Survey on
  Writing Style Understanding</font>
    </a>
  </h2>
  <font color="black">まず、両方のタスクの現在の研究状況に関する調査結果を説明し、著者関連タスクの既存の成果と問題を要約します。言語スタイルに大きく依存する著者識別タスクは、常に自然言語の重要な部分です。理解（NLU）研究..言語スタイルの理解に基づく他のタスクは、深い学習方法の恩恵を受けますが、これらの方法は、多くの著者ベースのタスクで従来の機械学習方法と同様に動作していません。 
[概要]ディープラーニング手法が従来の機械ベースのソリューションと同様に機能しないのはこれが初めてです。私たちは、スタイルの理解を書くことに関連するベースのタスクやその他の課題に基づいて調査を行います。次に、スタイルに関連するタスク全般の優れたメソッドについて説明します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Contextual Knowledge Selection and Embedding towards Enhanced
  Pre-Trained Language Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_30.html">
      <font color="black">Contextual Knowledge Selection and Embedding towards Enhanced
  Pre-Trained Language Models</font>
    </a>
  </h2>
  <font color="black">パフォーマンスの向上に加えて、DKPLMで動的に選択された知識は、テキスト関連の知識のセマンティクスを従来のPLMよりも解釈しやすい形式で記述できます。この論文では、DKPLMという名前の新しいフレームワークを提案し、知識コンテキストを動的に選択して埋め込みます。 PLMのテキストコンテキストに変換します。これにより、入力テキストと一致しないKGの冗長であいまいな知識の影響を回避できます。実験結果は、DKPLMが一般的な知識駆動型NLPタスクのさまざまなベースラインを上回っていることを示しており、動的知識を利用することの有効性を示しています。言語理解のための文脈。 
[要約] plmsはkgの静的サブグラフを埋め込みます（「知識コンテキスト」）.dkplmsは、典型的な知識主導のnlpタスクのさまざまなベースラインを上回ります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-29">
        <br><font color="black">2020-09-29</font>
      </time>
    </span>
</section>
<!-- paper0: More Data, More Relations, More Context and More Openness: A Review and
  Outlook for Relation Extraction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_31.html">
      <font color="black">More Data, More Relations, More Context and More Openness: A Review and
  Outlook for Relation Extraction</font>
    </a>
  </h2>
  <font color="black">この論文では、既存のRE手法を振り返り、現在直面している主要な課題を分析し、より強力なREへの有望な方向性を示します。これらの事実をテキストから抽出するために、人々は関係抽出（RE）に取り組んできました。年..私たちの見解がこの分野を前進させ、地域社会でのさらなる努力を刺激することを願っています。 
[概要]私たちは何年にもわたって関係抽出（re）に取り組んできました。reからの「もっと」が必要です：より強力なシステム。私たちの見解がこの分野を前進させ、より多くの努力を刺激することを願っています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-07">
        <br><font color="black">2020-04-07</font>
      </time>
    </span>
</section>
<!-- paper0: LEBANONUPRISING: a thorough study of Lebanese tweets -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_32.html">
      <font color="black">LEBANONUPRISING: a thorough study of Lebanese tweets</font>
    </a>
  </h2>
  <font color="black">10月17日、レバノンは革命の始まりを目撃しました。 LebanonUprisingハッシュタグがTwitterで口コミで広まりました。データセットに手動で注釈を付けて、適合率と再現率の指標を測定し、さまざまなアルゴリズムを比較しました。この論文では、レバノンアラビア語で話されたツイートの感情分析調査を実施しました。さまざまな機械学習アルゴリズムを使用したLebanonUprisingハッシュタグ。 
[概要]ツイートのデータセットは10月18日から21日の間に収集されました。データセットは、適合率と再現率の指標を測定するために手動で注釈が付けられました。予測しようとした2つの感情は、「皮肉」と「面白い」感情でした。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: TextDecepter: Hard Label Black Box Attack on Text Classifiers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_33.html">
      <font color="black">TextDecepter: Hard Label Black Box Attack on Text Classifiers</font>
    </a>
  </h2>
  <font color="black">これらの敵対的な例の生成は、モデルをより堅牢にし、これらのモデルの基礎となる意思決定の洞察として提供するのに役立ちます。このような攻撃シナリオは、感情などのセキュリティに敏感なアプリケーションに使用される実際のブラックボックスモデルに適用できます。分析と毒性成分の検出。このホワイトペーパーでは、自然言語処理（NLP）分類子に対するハードラベルブラックボックス攻撃の新しいアプローチを紹介します。モデル情報は開示されておらず、攻撃者はモデルにクエリを実行するだけで、自信を持って分類子の最終決定を得ることができません。関係するクラスのスコア。 
[概要]これらの敵対的な例の生成は、モデルをより堅牢にするのに役立ちます。テキストデータは本質的に離散的であるため、これらの方法はテキストに直接関連していません。この論文では、ハードラベルブラックボックス攻撃に対する新しいアプローチを紹介します。自然言語処理（nlp）分類子</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-16">
        <br><font color="black">2020-08-16</font>
      </time>
    </span>
</section>
<!-- paper0: Multiple Word Embeddings for Increased Diversity of Representation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_34.html">
      <font color="black">Multiple Word Embeddings for Increased Diversity of Representation</font>
    </a>
  </h2>
  <font color="black">この連結手法が多くのタスク、データセット、モデルタイプで機能することを示します。TensorFlowとPyTorchの両方でモデルのオープンソース実装を提供します。単語の表現を強化するために、事前にトレーニングされた複数の埋め込みを連結します。 
[概要]単語の表現を強化するために、事前にトレーニングされた複数のモデルの埋め込みを連結します。事前にトレーニングされたさまざまなモデル間の表現の多様性が、この手法が機能する理由の原動力であることがわかります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: On Romanization for Model Transfer Between Scripts in Neural Machine
  Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_35.html">
      <font color="black">On Romanization for Model Transfer Between Scripts in Neural Machine
  Translation</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、ローマ字化は情報の損失を伴うため、単純な語彙転送方法よりも常に優れているとは限りませんが、異なるスクリプトを使用して関連する言語間の転送を改善できることを示しています。最後に、ローマ字化をターゲット側に拡張し、これが成功する可能性があることを示します単純なローマ字表記解除モデルと組み合わせた場合の戦略..転送学習は、低リソースのマシン変換の品質を向上させるための一般的な戦略です。 
[概要] 2つのロマゼーションツールを比較したところ、翻訳品質に影響を与える情報損失の程度が異なることがわかりました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Taming the Expressiveness and Programmability of Graph Analytical
  Queries -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_36.html">
      <font color="black">Taming the Expressiveness and Programmability of Graph Analytical
  Queries</font>
    </a>
  </h2>
  <font color="black">これに動機付けられて、3つのプリミティブ演算子Filter、LocAl、PuSHにちなんで名付けられた\ flash DSLを提案します。実験結果は、\ flashの表現力と、満足のいくランタイムを実現する複雑なアルゴリズムをプログラミングする能力を示しています。コード生成に基づく\ flashの実装。代表的なクエリを使用して、ネイティブC ++コードおよび既存のDSLと比較します。 
[概要]これは分析クエリの分析に基づいています。これらは10年間の観察に基づいています。代表的なアルゴリズムを使用して、さまざまな言語と比較します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-20">
        <br><font color="black">2020-04-20</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Low Compute Language Modeling with In-Domain Embedding
  Initialisation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_37.html">
      <font color="black">Improving Low Compute Language Modeling with In-Domain Embedding
  Initialisation</font>
    </a>
  </h2>
  <font color="black">生物医学データやテクニカルサポートなどの多くのNLPアプリケーションには、ドメイン内データのトークンが1,000万〜1億個あり、そこから学習するための計算リソースが限られています。その過程で、入力と出力の埋め込みを結び付ける標準的な規則がドメイン内データでトレーニングされた埋め込みで初期化するときに、混乱を改善しないでください。このシナリオで言語モデルをどのようにトレーニングする必要がありますか？ 
[概要]英語でのターゲット設定では、ドメインデータを使用して入力埋め込みを初期化およびフリーズすると、言語モデルのパフォーマンスが向上することを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-29">
        <br><font color="black">2020-09-29</font>
      </time>
    </span>
</section>
<!-- paper0: A Tale of Two Linkings: Dynamically Gating between Schema Linking and
  Structural Linking for Text-to-SQL Parsing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_38.html">
      <font color="black">A Tale of Two Linkings: Dynamically Gating between Schema Linking and
  Structural Linking for Text-to-SQL Parsing</font>
    </a>
  </h2>
  <font color="black">分析によると、私たちの方法は、複雑なSQLクエリを生成するときにモデル出力の構造を強化し、説明可能な予測を提供するのに役立ちます。直感的に、これら2つのリンクプロセスの効果は、生成されるエンティティに基づいて変化するため、動的に選択することを提案します。ゲーティングメカニズムを使用します。提案された方法を2つのグラフニューラルネットワークベースのセマンティックパーサーとBERT表現と統合すると、困難なSpiderデータセットの解析精度が大幅に向上することがわかります。 
[概要]提案された方法は、スパイダーデータセットのツールとして宣伝されています。成功を予測するためにこの方法が使用されたのはこれが初めてである可能性があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Temporal Random Indexing of Context Vectors Applied to Event Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_39.html">
      <font color="black">Temporal Random Indexing of Context Vectors Applied to Event Detection</font>
    </a>
  </h2>
  <font color="black">また、問題の単語に関連するイベントを提案するために、クエリ単語と他の単語との意味関係を追跡するために、単語コーパスのサイズが対数線形であるアルゴリズムを提案します。 RI表現のクラスにつながるランダム化されたエントリの数に確率分布を課す効果。最後に、「iPhone」という単語に関連するツイートに対して提案されたアルゴリズムを使用して、新しいRI表現のシミュレーションを実行し、結果を提示します。 
[ABSTRACT] 1つの一般的な方法-ホット表現は直線的に成長します。ランダム化されたエントリの数に確率分布を課す効果を利用する新しいri表現を提案します。ツイート用に提案されたアルゴリズムを使用して、新しいri表現のシミュレーションを実行します。 「iphone」という言葉に関連する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-28">
        <br><font color="black">2020-08-28</font>
      </time>
    </span>
</section>
<!-- paper0: Explainable Natural Language Reasoning via Conceptual Unification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_40.html">
      <font color="black">Explainable Natural Language Reasoning via Conceptual Unification</font>
    </a>
  </h2>
  <font color="black">WorldtreeコーパスとARCチャレンジに関する経験的評価の結果、次の結論が得られました。（1）質問応答モデルは、回答予測に関する明示的なトレーニングを必要とせずに、競合するニューラルベースラインおよびマルチホップベースラインよりも優れています。 （2）説明抽出として使用すると、提案されたモデルはトランスフォーマーのパフォーマンスを大幅に向上させ、ワールドツリーコーパスで最先端の結果をもたらします。 （3）類推的推論とアブダクション的推論は、健全な説明的推論を達成するために非常に補完的です。これは、統一パターンがパフォーマンスと解釈可能性に与える影響を示す機能です。段階的概念統一が教師なし質問応答に効果的であることを示します。最先端のトランスフォーマーと組み合わせた説明抽出器として..推論プロセスは、2つの主要なアーキテクチャコンポーネントの相互作用を通じて計算された、説明の統一力と妥当性の概念によって導かれます。（a）類推推論モデル説明のコーパスで統一パターンを活用することにより、説明の事実をランク付けします。 （b）概念の抽象化とその後の統合によって実現される、最良の説明の検索を実行するアブダクション推論モデル。 
[要約]推論プロセスは、アイデアの統一力と説明の妥当性によって導かれます。疑う余地のないフレームワークの実験的評価により、回答予測に関する明示的なトレーニングを必要とせずに、マルチホップベースラインを上回る説明が得られました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: RDSGAN: Rank-based Distant Supervision Relation Extraction with
  Generative Adversarial Framework -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_41.html">
      <font color="black">RDSGAN: Rank-based Distant Supervision Relation Extraction with
  Generative Adversarial Framework</font>
    </a>
  </h2>
  <font color="black">私たちのフレームワークは、敵対的なトレーニングを介して真陽性インスタンスの分布を学習するためのソフトアテンションとハード決定を組み合わせ、偽陽性問題に対処するランクベースの遠隔監視を介して分布に準拠する有効なインスタンスを選択します。実験結果は、フレームワークの優位性を示しています。強力なベースラインを超えています。削除されたインスタンスに含まれる有用な情報が失われる原因となりますが、誤ってラベル付けされたインスタンスをポジティブセットから削除するという難しい決定が提案されています。 
[概要] wimph wis wi-fi、wi-fi、およびwiki wi wi wi-fiは、注意メカニズムでノイズを除去することが提案されていますが、重みがゼロでないため、ノイズの多いデータを排除できません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: TAVAT: Token-Aware Virtual Adversarial Training for Language
  Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/cs.CL/paper_42.html">
      <font color="black">TAVAT: Token-Aware Virtual Adversarial Training for Language
  Understanding</font>
    </a>
  </h2>
  <font color="black">きめ細かい摂動を作成するために、トークン対応の仮想敵対トレーニング方法を提案します。摂動をより適切に初期化するためにトークンレベルの累積摂動語彙を導入し、これらの摂動を適切に制約するためにトークンレベルの正規化ボールを使用します。私たちの方法は、さまざまなタスクでBERTやALBERTなどの事前トレーニング済みモデルのパフォーマンスを大幅に向上させます。 
[ABSTRACT]仮想敵対トレーニングが自然言語処理フィールドに導入されました。テキストは離散的であり、勾配によって直接摂動されることはありません。提案された方法は、接着剤ベンチマークのスコアを改善します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-30">
        <br><font color="black">2020-04-30</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: Efficiently Trainable Text-to-Speech System Based on Deep Convolutional
  Networks with Guided Attention -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.AS/paper_0.html">
      <font color="black">Efficiently Trainable Text-to-Speech System Based on Deep Convolutional
  Networks with Guided Attention</font>
    </a>
  </h2>
  <font color="black">この論文の目的は、CNNのみに基づく代替ニューラルTTSがこれらのトレーニングの経済的コストを軽減することを示すことです。私たちの実験では、提案されたディープ畳み込みTTSは、装備された通常のゲーミングPCを使用して、一晩（15時間）十分にトレーニングされました。合成音声の品質はほぼ許容範囲内でしたが、2つのGPU ..リカレントニューラルネットワーク（RNN）は、最近、シーケンシャルデータをモデル化するための標準的な手法になり、この手法は一部の最先端のニューラルTTS手法で使用されています。 
[ABSTRACT]リカレントニューラルネットワーク（rnn）は、最近、連続データをモデル化するための標準的な手法になりました。最近の研究では、cnnベースのシーケンス合成はcnnベースの手法よりもはるかに高速であることが示されています。提案された深い畳み込みttsは一晩で十分にトレーニングされました。 、2つのGPUを搭載した通常のゲーミングPCを使用</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2017-10-24">
        <br><font color="black">2017-10-24</font>
      </time>
    </span>
</section>
<!-- paper0: Consonant gemination in Italian: the nasal and liquid case -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.AS/paper_1.html">
      <font color="black">Consonant gemination in Italian: the nasal and liquid case</font>
    </a>
  </h2>
  <font color="black">175-185）は、イタリア語での長子音の主な音響キューは閉鎖期間であり、周波数とエネルギー領域のパラメータは長子音の影響をあまり受けないことを示しました。30、pp ..この論文-残りのすべてをカバーする2つのセットの最初のもの子音-鼻音と液体に対処します。そのコンパニオンペーパーは破擦音と摩擦音を扱っています。 
[概要]この論文は、すべての鼻音をカバーする一連の停止の最初のものです。また、母音と瓶前の母音の長さの間の逆相関を示しています。これは、シングルトンとジェミネートの単語セットを組み合わせた場合にも存在します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-19">
        <br><font color="black">2020-04-19</font>
      </time>
    </span>
</section>
<!-- paper0: CRNNs for Urban Sound Tagging with spatiotemporal context -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.AS/paper_2.html">
      <font color="black">CRNNs for Urban Sound Tagging with spatiotemporal context</font>
    </a>
  </h2>
  <font color="black">コードは、https：//github.com/multitel-ai/urban-sound-taggingのGitHubリポジトリで入手できます。このタスクは、時空間コンテキストを使用した階層型マルチラベルアーバンサウンドタグ付けに焦点を当てています。このペーパーでは、参加に使用したCRNNについて説明します。 DCASE2020チャレンジのタスク5で。 
[概要]このタスクは、時空間コンテキストを使用したマルチラベルの都市音タグ付けの改革に焦点を当てています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-24">
        <br><font color="black">2020-08-24</font>
      </time>
    </span>
</section>
<!-- paper0: Enhancing Monotonic Multihead Attention for Streaming ASR -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.AS/paper_3.html">
      <font color="black">Enhancing Monotonic Multihead Attention for Streaming ASR</font>
    </a>
  </h2>
  <font color="black">各MAヘッドに対するチャンクワイズの注意は、マルチヘッドの対応物に拡張されます。最後に、安定したストリーミング推論を保証するために、ヘッド同期ビーム検索デコードを提案します。さらに、境界検出のためのヘッド間のコンセンサスを改善し、遅延を防ぐために、冗長ヘッドを整理することを提案します。そのようなヘッドによって引き起こされるトークンの生成。 
[概要]トレーニング中に頭の一部をマスクして、ヘッドドロップの正則化を提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br><font color="black">2020-05-19</font>
      </time>
    </span>
</section>
<!-- paper0: Rethinking Evaluation Methodology for Audio-to-Score Alignment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.AS/paper_4.html">
      <font color="black">Rethinking Evaluation Methodology for Audio-to-Score Alignment</font>
    </a>
  </h2>
  <font color="black">これらの洞察に動機付けられて、オーディオとスコアのアラインメントの新しい評価指標を導入します。アラインメントの概念は直感的に理解できますが、この精度により、オーディオとスコアのアラインメントアルゴリズムの評価に関する新しい洞察が得られます。 KernScoresとMAESTROのパフォーマンスのペアから導出されたアライメント評価データセットでは、いくつかの古典的なアライメントアルゴリズムで新しいメトリックと標準メトリックの動作を研究します。 
[概要]配置の概念は直感的に把握できますが、この精度により新しい洞察が得られます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Transfer Learning from Speech Synthesis to Voice Conversion with
  Non-Parallel Training Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.AS/paper_5.html">
      <font color="black">Transfer Learning from Speech Synthesis to Voice Conversion with
  Non-Parallel Training Data</font>
    </a>
  </h2>
  <font color="black">音声変換システムでは、エンコーダーはテキストではなく音声を入力として受け取りますが、デコーダーは機能的にTTSデコーダーと似ています。スピーカーの埋め込みでデコーダーを調整すると、システムは任意の非並列データでトレーニングできます。 -任意の音声変換..音声変換トレーニング中に、テキストと音声から音声への合成および音声変換ネットワークの両方をそれぞれ提示します。 
[概要]最初に、シーケンス-から-シーケンス-デコーダー-デコーダーアーキテクチャを備えた多人数音声合成システムを開発します。システムでは、エンコーダーはテキストの代わりに音声を入力として受け取り、デコーダーはデータをマッピングしてターゲットを作成します音響機能。提案されたアプローチは、2つの競合する音声変換ベースラインを一貫して上回っています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: ESPnet-ST: All-in-One Speech Translation Toolkit -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.AS/paper_6.html">
      <font color="black">ESPnet-ST: All-in-One Speech Translation Toolkit</font>
    </a>
  </h2>
  <font color="black">このツールキットはhttps://github.com/espnet/espnetで公開されています。ESPnet-STは、エンドツーエンドの音声処理ツールキットであるESPnet内の新しいプロジェクトであり、自動音声認識、機械翻訳、音声翻訳のためのテキスト読み上げ機能..単一のフレームワークで音声読み上げシステムを迅速に開発するために設計されたESPnet-STを紹介します。 
[ABSTRACT] espnet --stは、us-mexico音声処理ツールキットの新しいプロジェクトです。自動音声認識、機械翻訳、およびテキスト読み上げ機能を統合または新たに実装します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br><font color="black">2020-04-21</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-End Spoken Language Understanding Without Full Transcripts -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.AS/paper_7.html">
      <font color="black">End-to-End Spoken Language Understanding Without Full Transcripts</font>
    </a>
  </h2>
  <font color="black">ATISコーパスでのスピーチからエンティティへの実験では、CTCモデルと注意モデルの両方で、エンティティ以外の単語をスキップする優れた能力が示されました。エンティティのみでトレーニングした場合と完全なトランスクリプトでトレーニングした場合では、劣化はほとんどありませんでした。エンティティは、発話の発話順序に必ずしも関連しない順序になっています。実験に音声入力が含まれる場合、これらのシステムは、エンティティラベルとエンティティ値を表す単語の両方を正しく認識する必要があります。 
[概要] 2種類の音声エンティティが音声認識に適合しています。これらのモデルは、単語ごとのトランスクリプトなしで、意味エンティティの注釈のみでトレーニングできます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Embedded Emotions -- A Data Driven Approach to Learn Transferable
  Feature Representations from Raw Speech Input for Emotion Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.AS/paper_8.html">
      <font color="black">Embedded Emotions -- A Data Driven Approach to Learn Transferable
  Feature Representations from Raw Speech Input for Emotion Recognition</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、学習した特徴表現を話し言葉から感情を分類するために効果的に適用できることを示しています。音声信号から抽出された特徴のパフォーマンスは、トランスクリプトから抽出されたものほど一貫していないことがわかりました。論文では、大きなテキストと音声のコーパスから学習した知識を自動感情認識のタスクに転送することの適用性を調査します。 
[概要]今年は、今年のインタースピーチ比較高齢者感情サブチャレンジに参加しています。目標は、話者の感情に関して高齢者の話された物語を分類することです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: DCASENET: A joint pre-trained deep neural network for detecting and
  classifying acoustic scenes and events -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.AS/paper_9.html">
      <font color="black">DCASENET: A joint pre-trained deep neural network for detecting and
  classifying acoustic scenes and events</font>
    </a>
  </h2>
  <font color="black">コードと事前トレーニング済みのDcaseNetウェイトは、https：//github.com/Jungjee/DcaseNetで入手できます。主な目標は、3つのタスクすべての事前トレーニング済みモデルとして機能できるジョイントシステムを構築することです。 3つのデータセットを使用して、提案された統合アーキテクチャであるDcaseNet自体を、競争力のある結果が得られる任意のタスクに直接使用できること、またはターゲットタスクに合わせてさらに微調整できることを示します。 
[ABSTRACT]提案された統合ディープニューラルネットワークは、3つの関連タスクを実行できます。これらには、音響シーン分類、オーディオタグ付け、およびサウンドイベント検出が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Transfer Learning from Monolingual ASR to Transcription-free
  Cross-lingual Voice Conversion -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.AS/paper_10.html">
      <font color="black">Transfer Learning from Monolingual ASR to Transcription-free
  Cross-lingual Voice Conversion</font>
    </a>
  </h2>
  <font color="black">クロスリンガル音声変換（VC）は、ソーススピーカーとターゲットスピーカーが異なる言語で話しているときに、同じコンテンツのターゲットボイスを合成することを目的としたタスクです。外国語音声の文字起こしや言語固有の知識がなくても、クロスリンガルVCに対応できます。 .. Voice Conversion Challenge 2020データセットでこれを実験し、話者に依存する変換モデルがゼロショットベースラインを上回り、言語間変換の音声品質と話者の類似性で3.83と3.54のMOSを達成することを示します。 
[概要]音声変換チャレンジ2020データセットでこれをテストします。スピーカーに依存する変換モデルがゼロショットベースラインを上回り、音声品質で3.83と3.54のmosを達成していることを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-30">
        <br><font color="black">2020-09-30</font>
      </time>
    </span>
</section>
<!-- paper0: Consonant gemination in Italian: the affricate and fricative case -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.AS/paper_11.html">
      <font color="black">Consonant gemination in Italian: the affricate and fricative case</font>
    </a>
  </h2>
  <font color="black">子音と子音前の母音の長さの逆相関は、両方の子音カテゴリ、および別々に検討した場合のシングルトンとジェミネートの両方の単語セットに存在しました。摩擦音と子音の結果により、上記の結果が確認されました。長子音は本質的に持続的であり、摩擦性長子音の子音持続時間の延長と、長子音ジェミネートの閉鎖持続時間の延長に対応します。 。 
[概要]他の瓶のカテゴリーの分析結果は、それらの妥当性を確認しました。発見は、破擦音が母音間子音の位置ではシングルトンとしてではなく、長子音の形でのみ現れる可能性があるという理論を支持します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-19">
        <br><font color="black">2020-04-19</font>
      </time>
    </span>
</section>
<!-- paper0: Tdcgan: Temporal Dilated Convolutional Generative Adversarial Network
  for End-to-end Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-01/eess.AS/paper_12.html">
      <font color="black">Tdcgan: Temporal Dilated Convolutional Generative Adversarial Network
  for End-to-end Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">さらに、以前のGANベースの方法と比較して、提案されたTDCGANはパラメータの数を大幅に減らすことができました。この論文では、従来の音声強調システムの位相情報を無視することによって引き起こされるパフォーマンスの低下にさらに対処するために、エンドツーエンドベースの音声強調アーキテクチャにおける時間的拡張畳み込み生成的敵対的ネットワーク（TDCGAN）..予想通り、この作業は、正規化としてのSNRペナルティ項目が強化音声のSNRの改善に$ L1 $よりも効果的であることも示しました。 。 
[ABSTRACT]初めて、深さ方向の畳み込みを伴う時間的拡張畳み込みネットワークをgan構造に導入しました。結果は、私たちのベースの方法が最先端のベースの音声強調よりも優れていることをさらに示しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
