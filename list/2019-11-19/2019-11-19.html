<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
<title>Akari-2019-11-19の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
  <div class="header-logo">
    <a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
  </div>
</header>
<input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">

<div class="menubar">
<span class="bar"></span>
<span class="bar"></span>
<span class="bar"></span>
</div>

<ul>
<li><a id="home" href="../../index.html">Home</a></li>
<li><a id="about" href="../../teamAkariとは.html">About</a></li>
<li><a id="contact" href="../../contact.html">Contact</a></li>
<li><a id="contact" href="../../list/newest.html">New Papers</a></li>
<li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
</ul>

</label>
<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: A Spatial Sampling Approach to Wave Field Synthesis: PBAP and Huygens
  Arrays -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/cs.SD/paper_0.html">
      A Spatial Sampling Approach to Wave Field Synthesis: PBAP and Huygens
  Arrays
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      よく知られているベクトルベースの振幅パニング（VBAP）ファミリの技術を参照して、遠方界のラインアレイの場合を平面波ベースの角度パン（PBAP）と呼びます。音場をアンダーサンプリングすると、結果はVBAPの形式と見なされる可能性があり、VBAPは多角形PBAPアレイが多角形の境界に切り捨てられた制限ケースとしても得られる可能性があります。サンプリング理論を使用して、クロスオーバー周波数の選択を通知します。HAは、スピーカーアレイの背後にあるソースに対して非常に一般的であり、線形である必要がなくなり、ソースは遠方界に制限されなくなりました。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-18">
        <br>2019-11-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Temporal Convolution for Real-time Keyword Spotting on Mobile Devices -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/cs.SD/paper_1.html">
      Temporal Convolution for Real-time Keyword Spotting on Mobile Devices
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      低周波数領域と高周波数領域の両方を完全にキャプチャするためにディープアーキテクチャを必要とする2DコンボリューションベースのKWSアプローチのほとんどとは異なり、コンパクトなResNetアーキテクチャで時間コンボリューションを活用します。ディープラーニングの分野における最近の開発により、 Googleスピーチコマンドデータセットでは、Google Pixel 1で\ textbf {385x}以上の高速化を達成し、現状に比べて精度を上回るKWSシステムでの畳み込みニューラルネットワーク（CNN）の採用-最新モデル。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-08">
        <br>2019-04-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Maximizing Mutual Information for Tacotron -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/cs.SD/paper_2.html">
      Maximizing Mutual Information for Tacotron
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      比較的高い発話エラー率は、条件付き自己回帰モデルのローカル情報選好と、自己回帰モジュールのトレーニングステータスを主に説明するモデルの不適切なトレーニング目標に起因しますが、まれに条件モジュールのトレーニングステータスに起因します。結果は、我々の方法が発話エラー率を低減できることを示しています。相互情報量を最大化するトレーニング目標は、自己回帰モジュールと条件モジュール間の依存性のメトリックと見なすことができます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-30">
        <br>2019-08-30
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Fine-Grained Static Detection of Obfuscation Transforms Using
  Ensemble-Learning and Semantic Reasoning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/cs.CL/paper_0.html">
      Fine-Grained Static Detection of Obfuscation Transforms Using
  Ensemble-Learning and Semantic Reasoning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      構築の全体的な精度は最大100％です。そのために、スケーラブルで効率的なモデルのための機械学習手法の使用のベストプラクティスに関するいくつかの研究を提供します。実験結果と難読化ツールの評価によるとTigressおよびOLLVMとして、当社のモデルは最先端の難読化変換で最大91％の精度を備えています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-18">
        <br>2019-11-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Span Model for Open Information Extraction on Accurate Corpus -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/cs.CL/paper_1.html">
      Span Model for Open Information Extraction on Accurate Corpus
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      新しく導入されたモデルは、両方のベンチマーク評価データセットで新しい最先端のパフォーマンスを実現します。この作業では、まずトレーニングとテストセットの両方の側面からこの困難を軽減します。OpenIEシステムのほとんどは、コーパスが自動的に構築され、不正確なテストセットで評価されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-01-30">
        <br>2019-01-30
      </time>
    </span>
  </h3>
</article>
<!-- paper0: RSL19BD at DBDC4: Ensemble of Decision Tree-based and LSTM-based Models -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/cs.CL/paper_2.html">
      RSL19BD at DBDC4: Ensemble of Decision Tree-based and LSTM-based Models
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      LSTMベースのモデルは、アーキテクチャにいくつかの変更を加えたKTHのアプローチに従い、Convolutional Neural Network（CNN）を使用してテキスト特徴抽出を実行します。さらに、3つの内訳の確率分布の平均と分散を予測する代わりにラベルでは、各ラベルの確率を直接予測します。さらに、単一のブレークダウンラベルを対象にしてカテゴリクロスエントロピー損失を最小化する代わりに、3つのブレークダウンラベルの確率分布を対象として平均二乗誤差を最小化します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-05-06">
        <br>2019-05-06
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Graph Transformer for Graph-to-Sequence Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/cs.CL/paper_3.html">
      Graph Transformer for Graph-to-Sequence Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      具体的には、AMRからテキストへの生成について、LDC2015E86で27.4 BLEU、LDC2017T10で29.7 BLEUを達成し、最新の結果を最大2.2ポイント向上させます。構文ベースの翻訳タスクでは、モデルが確立します新しい単一モデルの最新のBLEUスコア、英語からドイツ語の21.3、英語からチェコ語の14.1。アンサンブルを含む既存の最良の結果を1 BLEU以上改善します。 to-sequence変換モデルは、グラフ表現学習にグラフニューラルネットワークを使用します。ここでは、構造情報がニューロンの受容野に反映されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-18">
        <br>2019-11-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multi-label Categorization of Accounts of Sexism using a Neural
  Framework -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/cs.CL/paper_4.html">
      Multi-label Categorization of Accounts of Sexism using a Neural
  Framework
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      BERTなどのモデルを使用して得られた文表現を、反復コンポーネントとオプションの畳み込みコンポーネントを含む柔軟な階層アーキテクチャを使用して、分布的および言語的な単語の埋め込みと組み合わせることができるこのマルチラベル分類のニューラルソリューションを開発します。かなりのマージンで従来の機械学習のベースラインと同様に学習します。性差別、女性と少女を大きな苦痛にさらす不公平は、露骨で微妙な方法で現れます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-10">
        <br>2019-10-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Dialog State Tracking with Reinforced Data Augmentation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/cs.CL/paper_5.html">
      Dialog State Tracking with Reinforced Data Augmentation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      具体的には、新しいコンテキストバンディットジェネレーターを導入して、特定のコンテキストに適した代替を選択することにより、新しい効果的なインスタンスを生成できるきめ細かい拡張ポリシーを学習します。注釈付きトレーニングの量と多様性の不足により、ニューラルダイアログステートトラッカーは一般的に制限されていますデータ.. WoZおよびMultiWoZ（レストラン）データセットの実験結果は、特に限られたトレーニングデータで、提案されたフレームワークが最新モデルよりもパフォーマンスを大幅に改善することを示しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-21">
        <br>2019-08-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Incorporating Word and Subword Units in Unsupervised Machine Translation
  Using Language Model Rescoring -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/cs.CL/paper_6.html">
      Incorporating Word and Subword Units in Unsupervised Machine Translation
  Using Language Model Rescoring
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、一連の前処理および後処理アプローチが適用され、最終的な翻訳の品質が向上します。ドイツ語とチェコ語のバイトペアエンコーディング（BPE）埋め込みを個別にトレーニングすることにより、言語の形態的豊かさの問題を解決することを提案します。このペーパーでは、ドイツ語からチェコ語へのWMT&#39;19ニュース共有タスクの監視なし機械翻訳トラックへのCAiREの提出について説明します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-16">
        <br>2019-08-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Dialogue Transformers -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/cs.CL/paper_7.html">
      Dialogue Transformers
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      デフォルトでは、RNNはシーケンス内のすべてのアイテムが完全なシーケンスのエンコーディングの生成に関連すると想定しますが、スピーカーが複数のトピックをインターリーブするため、単一の会話は複数の重複する談話セグメントで構成できます。現在のダイアログ状態のエンコーディングであり、ダイアログ履歴を選択的に無視または注意することに自然に適しています。TransformerEmbedding Dialogue（TED）ポリシーのパフォーマンスをLSTMおよびこの制限を克服するために特別に設計されたREDPと比較します。 RNNの。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-01">
        <br>2019-10-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multi-task Sentence Encoding Model for Semantic Retrieval in Question
  Answering Systems -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/cs.CL/paper_8.html">
      Multi-task Sentence Encoding Model for Semantic Retrieval in Question
  Answering Systems
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      質問応答（QA）システムは、ユーザーの質問に適切な応答を自動的に提供するために使用されます。実験は、既存の文照合モデルと比較した提案方法の優位性を示しています。 QAナレッジベースからの最も類似した質問。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-18">
        <br>2019-11-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Short Text Language Identification for Under Resourced Languages -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/cs.CL/paper_9.html">
      Short Text Language Identification for Under Resourced Languages
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このアルゴリズムは、南アフリカの言語に関する以前の作品のテストセットを使用した最近のアプローチや、類似言語の識別（DSL）共有タスクのデータセットと比較されます。LIDアプローチの評価と比較における残りの研究機会と差し迫った懸念についても説明します。 。論文は、リソース不足の言語に役立つショートテキスト言語識別（LID）の階層型ナイーブベイジアンおよび辞書ベースの分類器を提示します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-18">
        <br>2019-11-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: An Annotated Corpus of Reference Resolution for Interpreting Common
  Grounding -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/cs.CL/paper_10.html">
      An Annotated Corpus of Reference Resolution for Interpreting Common
  Grounding
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      シンプルで一般的なアノテーションスキーマに基づいて、既存のコーパスからキュレーションされた5,191のダイアログで合計40,172の参照表現を収集し、指示対象の解釈の複数の判断を加えました。ベースライン対話システムの共通基盤。注釈は非常に信頼性が高く、自然な程度の合理的な不一致を通じて共通基盤の複雑さを捉え、共通基盤戦略のより詳細で定量的な分析を可能にします。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-18">
        <br>2019-11-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Dense and Deep Sarcasm Detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/cs.CL/paper_11.html">
      Dense and Deep Sarcasm Detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最後に、ケーススタディを示します。このアプローチは、標準CNNが誤分類する明確な皮肉の追加の使用を正確に分類することをサポートします。私たちは、外部の情報をかなり活用する最新のアーキテクチャとアプローチを比較し、テキストのローカルな機能のみを使用して競争力のある結果を示します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-18">
        <br>2019-11-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End
  Speech Translation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/cs.CL/paper_12.html">
      Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End
  Speech Translation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、大規模なベンチマークデータセットでモデルがベースライン2.2 BLEUよりも優れていることを示しています。これらの問題に対処するために、サブネットの役割を維持しながら微調整ですべてのサブネットを再利用することでギャップを埋めるTandem Connectionist Encoding Network（TCEN）を提案しますさらに、音声エンコーダ出力とMTエンコーダ入力がセマンティック表現とシーケンス長に関して一貫していることを保証する2つのシンプルだが効果的な方法を提案します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-17">
        <br>2019-09-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Maximizing Mutual Information for Tacotron -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/cs.CL/paper_13.html">
      Maximizing Mutual Information for Tacotron
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      発話エラー率が比較的高いのは、条件付き自己回帰モデルのローカル情報の優先度と、自己回帰モジュールのトレーニングステータスを主に説明するが、条件モジュールのトレーニングステータスをほとんど説明しないモデルの不適切なトレーニング目標に起因します。 InfoGANにより、テキスト条件と予測された音響的特徴との間の相互情報を最大化して、CAR音声合成モデルのそれらの間の依存性を強化することを提案します。 HMMベースおよびNNベースのフレーム間回帰法では、単語の欠落や繰り返し、不完全な合成など、いくつかの合成エラーが発生しやすくなります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-30">
        <br>2019-08-30
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: A Spatial Sampling Approach to Wave Field Synthesis: PBAP and Huygens
  Arrays -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/eess.AS/paper_0.html">
      A Spatial Sampling Approach to Wave Field Synthesis: PBAP and Huygens
  Arrays
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      HAは、スピーカーアレイの背後にあるソースに対して非常に一般的であり、リニアである必要がなくなり、ソースは遠方界に制限されなくなりました。遠方界ラインアレイケースをPlanewave-Based Angle Panning（PBAP ）、よく知られているベクトルベースの振幅パニング（VBAP）ファミリのテクニックを参照して、そのいくつかはここでは特別なケースとして導き出されます：スピーカーが音場をアンダーサンプリングすると、結果はVBAPおよびVBAPのフォームと見なされる場合がありますまた、多角形の境界に切り捨てられた多角形PBAPアレイの制限ケースとして取得されます。空間サンプルは線形アレイ上にある必要はなく、Huygensアレイ（HA）と呼ばれる単純な空間オーディオシステムにつながります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-18">
        <br>2019-11-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Temporal Convolution for Real-time Keyword Spotting on Mobile Devices -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/eess.AS/paper_1.html">
      Temporal Convolution for Real-time Keyword Spotting on Mobile Devices
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      低周波数領域と高周波数領域の両方を完全にキャプチャするために深いアーキテクチャを必要とする2D畳み込みベースのKWSアプローチのほとんどとは異なり、コンパクトなResNetアーキテクチャで時間畳み込みを活用します。KWSシステムが直面する主な課題はトレードオフです残念ながら、モバイルデバイス上のKWSモデルの実際のレイテンシの定量的分析はほとんどありませんでした。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-08">
        <br>2019-04-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Learning to Have an Ear for Face Super-Resolution -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/eess.AS/paper_2.html">
      Learning to Have an Ear for Face Super-Resolution
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この目標に向けて、オーディオトラックから人物の顔に関する情報を抽出し、それを顔のポーズと色に関連する低解像度画像から抽出された情報と組み合わせるモデルとトレーニング手順を提案します..これらの2つの入力の組み合わせにより、顔の正しい属性をより適切にキャプチャする高解像度画像が得られることを実証します。特に、性別、年齢、アイデンティティなどの属性の回復にオーディオが役立つことを実験的に示します、したがって、画像再構成プロセスの正確性が向上します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-27">
        <br>2019-09-27
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End
  Speech Translation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/eess.AS/paper_3.html">
      Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End
  Speech Translation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、大規模なベンチマークデータセットでモデルがベースライン2.2 BLEUより優れていることを示しています。さらに、音声エンコーダ出力とMTエンコーダ入力がセマンティック表現とシーケンス長に関して一貫していることを保証する2つのシンプルだが効果的な方法を提案します。近年の話題であるエンドツーエンドの音声翻訳は、オーディオのセグメントをエンドツーエンドのモデルで特定の言語に翻訳することを目的としています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-17">
        <br>2019-09-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Maximizing Mutual Information for Tacotron -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-19/eess.AS/paper_4.html">
      Maximizing Mutual Information for Tacotron
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      相互情報量を最大化するトレーニング目標は、自己回帰モジュールと条件モジュール間の依存性のメトリックと見なすことができます。エンドツーエンドの音声合成方法は、すでに人間に近い品質パフォーマンスを達成しています。実験結果は、メソッドは、発話エラー率を減らすことができます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-30">
        <br>2019-08-30
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="">
      
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本日更新された論文はありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="">
        <br>
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
