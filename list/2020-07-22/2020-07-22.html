<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-07-22の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: SLNSpeech: solving extended speech separation problem by the help of
  sign language -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.SD/paper_0.html">
      <font color="black">SLNSpeech: solving extended speech separation problem by the help of
  sign language</font>
    </a>
  </h2>
  <font color="black">次に、3つのモダリティの自己教師あり学習用の一般的なディープラーニングネットワークを設計します。特に、手話の埋め込みと音声情報または視聴覚情報を併用して、音声分離タスクをより適切に解決します。音声分離タスクは、大きく分けることができます。オーディオのみの分離とオーディオとビジュアルの分離に移行します。その後、特徴抽出段階でスキップ接続を備えた改善されたU-Netを適用して、ソースオーディオ、手話の機能、および視覚機能から変換された混合スペクトログラム間の埋め込みを学習します。 
[要旨]障害者の実際のシナリオで音声分離技術を適用するために、手話ニュース音声という大規模なデータセットを導入します。3D残差たたみ込みネットワークを使用して手話の特徴を抽出し、事前トレーニング済みのvggnetモデルを使用して視覚的特徴</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Read and Follow Music in Complete Score Sheet Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.SD/paper_1.html">
      <font color="black">Learning to Read and Follow Music in Complete Score Sheet Images</font>
    </a>
  </h2>
  <font color="black">着信音声とスコアの特定の画像に基づいて、システムは音声と一致するページ内の最も可能性の高い位置を直接予測し、位置合わせの精度の面で現在の最先端の画像ベースのスコアのフォロワーよりも優れています。また、私たちの方法をOMRベースのアプローチと比較し、それがそのようなシステムの実行可能な代替手段になる可能性があることを経験的に示します。既存の作業では、OMRソフトウェアを使用してコンピューターで読み取り可能なスコア表現を取得するか、準備されたシート画像を使用します抜粋として、フルページの完全に未処理のシート画像で直接スコアリングを実行する最初のシステムを提案します。 
[要約]スコアの最初のシステムは、全ページに基づいており、完全に未処理のシート画像に基づいています。データに基づいており、このようなシステムの実行可能な代替案になる可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: SoK: The Faults in our ASRs: An Overview of Attacks against Automatic
  Speech Recognition and Speaker Identification Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.SD/paper_2.html">
      <font color="black">SoK: The Faults in our ASRs: An Overview of Attacks against Automatic
  Speech Recognition and Speaker Identification Systems</font>
    </a>
  </h2>
  <font color="black">その際、このスペースで適切な緩和策を提供するには、かなりの追加作業が必要であると主張します。ニューラルネットワークに基づく他のシステムと同様に、最近の研究では、音声および話者認識システムが、操作された入力を使用した攻撃に対して脆弱であることを示しています。これはまず、このスペースでの既存の研究を体系化し、コミュニティが将来の作業を評価できる分類法を提供することによって行います。 
[要約]音声およびスピーカーシステムの幅広い使用が、ニューラルネットワークの精度の向上によって可能になりました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Time-Frequency Scattering Accurately Models Auditory Similarities
  Between Instrumental Playing Techniques -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.SD/paper_3.html">
      <font color="black">Time-Frequency Scattering Accurately Models Auditory Similarities
  Between Instrumental Playing Techniques</font>
    </a>
  </h2>
  <font color="black">それらの応答を分析すると、音色の知覚は、楽器や演奏テクニックだけで提供されるよりも柔軟な分類法の範囲内で動作することがわかります。さらに、マージンが大きい最近傍（LMNN）メトリック学習アルゴリズムにより、クラスターグラフの三重項損失を最小限に抑えます。さらに、楽器、ミュート、テクニック全体の聴覚類似性のクラスターグラフを復元するための機械リスニングモデルを提案します。 
[ABSTRACT]音色知覚は、楽器や演奏テクニックだけで提供される分類よりも柔軟な分類内で動作します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Audio Adversarial Examples for Robust Hybrid CTC/Attention Speech
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.SD/paper_4.html">
      <font color="black">Audio Adversarial Examples for Robust Hybrid CTC/Attention Speech
  Recognition</font>
    </a>
  </h2>
  <font color="black">ハイブリッドCTC /アテンションASRシステムのアイデアに従って、この作業では、AAEを生成して両方のアプローチを結合してCTCアテンショングラディエント法を統合するアルゴリズムを提案します。自動音声認識（ASR）の最近の進歩により、エンドツーエンドシステム最先端のパフォーマンスを達成できます。評価は、ハイブリッドCTC /アテンションエンドツーエンドASRモデルをケーススタディとして2つの参照文とTEDlium v2音声認識タスクを使用して実行します。 
[要約]より深いニューラルネットワークに向かう傾向がありますが、それらのasrモデルはより複雑で、特別に細工されたノイズデータに対しても傾向があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Foley Music: Learning to Generate Music from Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.SD/paper_5.html">
      <font color="black">Foley Music: Learning to Generate Music from Videos</font>
    </a>
  </h2>
  <font color="black">さまざまな音楽パフォーマンスを含むビデオでモデルの効果を実証します。次に、市販の音楽シンセサイザーツールを使用して、MIDIイベントをリアルな音楽に変換できます。さらに重要なことに、MIDI表現は完全に解釈可能で透明ですにより、音楽編集を柔軟に行うことができます。 
[ABSTRACT]動画から音楽への生成に成功するための2つの主要な中間表現を特定します。動画と音声の録音からの本体のキーポイントは、動画から音楽へのキーポイントになります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video
  Parsing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.SD/paper_6.html">
      <font color="black">Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video
  Parsing</font>
    </a>
  </h2>
  <font color="black">具体的には、ユニモーダルとクロスモーダルの時間的コンテキストを同時に探索するための新しいハイブリッドアテンションネットワークを提案します。さらに、モダリティバイアスとノイズのあるラベルの問題を個別ガイド付き学習メカニズムとラベルスムージング技術でそれぞれ発見して軽減します。問題は、ビデオ内に描かれているシーンを完全に理解するために不可欠です。 
[要旨]これは、ビデオ内に描かれているシーンを完全に理解するために不可欠です。注意深い問題を開発します。これは、マルチモーダルな複数インスタンス学習として自然に構成できます-視覚的なビデオ解析。ビデオだけでも課題を達成できます-レベルの弱いラベル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Very Fast Keyword Spotting System with Real Time Factor below 0.01 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.SD/paper_7.html">
      <font color="black">Very Fast Keyword Spotting System with Real Time Factor below 0.01</font>
    </a>
  </h2>
  <font color="black">標準のトライフォンまたはいわゆる疑似モノフォンによる双方向フィードフォワードシーケンシャルメモリネットワーク（リカレントネットの代替）による時間とメモリ効率の高いモデリング、および音声フレームの完全なフォワードデコード（ルックバックの必要性は最小限）を示します。すべての最適化（尤度計算のGPUを含む）を適用すると、完全なシステムがRT係数が0.001に近いシングルパスで実行できることを示しています。提案されたスキームのいくつかのバリアントは、3つの大きなチェコのデータセット（ブロードキャスト、インターネット）で評価されます電話、合計17時間）とそのパフォーマンスは、検出エラートレードオフ（DET）図とリアルタイム（RT）要因によって比較されます。 
[要約]提案されたアーキテクチャは、データセット、ブロードキャスト、インターネット、電話、合計17時間に基づいています。これらには、信号処理と尤度計算、ビタビ復号、スポット候補検出、信頼度計算が含まれます。プロジェクトはプロジェクトの `kwsと呼ばれます」</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Score and Lyrics-Free Singing Voice Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.SD/paper_8.html">
      <font color="black">Score and Lyrics-Free Singing Voice Generation</font>
    </a>
  </h2>
  <font color="black">歌声の生成モデルは、主に「歌声の合成」、つまり、楽譜とテキストの歌詞を指定して歌声の波形を生成するというタスクに関係してきました。この作品では、小説でありながら挑戦的な代替手段である歌声を探ります。トレーニングと推論時間の両方で、事前に割り当てられたスコアと歌詞なしで生成します。さらに、生成的敵対ネットワークを使用してそのようなモデルを実装し、客観的および主観的に評価します。 
[ABSTRACT]事前に割り当てられたスコアと歌詞なしで歌声を生成します。この作品では、小説でありながら挑戦的な代替案を模索します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-26">
        <br><font color="black">2019-12-26</font>
      </time>
    </span>
</section>
<!-- paper0: MUSICNTWRK: data tools for music theory, analysis and composition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.SD/paper_9.html">
      <font color="black">MUSICNTWRK: data tools for music theory, analysis and composition</font>
    </a>
  </h2>
  <font color="black">ピッチクラスセットとリズミカルシーケンスの分類と操作のためのpythonライブラリーであるMUSICNTWRKのAPI、一般化された音楽とサウンドスペースでのネットワークの生成、音色認識のためのディープラーニングアルゴリズム、および任意のデータの音波処理を紹介します。ソフトウェアはGPL 3.0で自由に利用でき、www.musicntwrk.comからダウンロードするか、PyPiプロジェクトとしてインストールできます（pip install musicntwrk）。 
[要約]ソフトウェアはgpl 3で無料で入手できます。deep.itはwwwからダウンロードできます。 musicntwrk。またはpypiプロジェクトとしてインストール</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-03">
        <br><font color="black">2019-06-03</font>
      </time>
    </span>
</section>
<!-- paper0: Optimization of data-driven filterbank for automatic speaker
  verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.SD/paper_10.html">
      <font color="black">Optimization of data-driven filterbank for automatic speaker
  verification</font>
    </a>
  </h2>
  <font color="black">さまざまな分類子バックエンドを使用して、さまざまなコーパスで自動スピーカー検証（ASV）実験を行います。VoxCeleb1と一般的なi-vectorバックエンドを使用した実験では、MFCCを超える等エラー率（EER）が9.75％改善されています。 。私たちは、提案されたフィルターバンクが、一般的に使用されているメルフィルターバンクや既存のデータ駆動型フィルターバンクよりもスピーカーの識別力が高いことを示しています。 
[ABSTRACT]提案されたフィルターバンクは、melフィルターバンクよりもスピーカーの識別力が高くなります。新しい方法では、特定の音声データからフィルターパラメーターを最適化します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Guided multi-branch learning systems for DCASE 2020 Task 4 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.SD/paper_11.html">
      <font color="black">Guided multi-branch learning systems for DCASE 2020 Task 4</font>
    </a>
  </h2>
  <font color="black">したがって、さまざまな目的を追求し、データのさまざまな特性に焦点を当てた複数のブランチは、特徴エンコーダーが特徴空間をより適切にモデル化し、過剰適合を回避するのに役立ちます。実験結果は、MBLがモデルのパフォーマンスを向上させ、SSを使用することで、 SEDアンサンブルシステムのパフォーマンスを向上させます。サウンド分離（SS）とサウンドイベント検出（SED）を組み合わせるために、SEDシステムの結果を、SSシステムが出力する個別のソースを使用してトレーニングされたSS-SEDシステムと融合します。 
[ABSTRACT]このシステムは、dcase 2019タスク4の最初の場所のシステムに基づいています。不十分な-監視付きの監視フレームワーク-ベースの埋め込み-レベルの複数インスタンスの学習プールモジュールと、ガイド付き学習と呼ばれる半監視付き学習アプローチを採用しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: The impact of Audio input representations on neural network based music
  transcription -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.SD/paper_12.html">
      <font color="black">The impact of Audio input representations on neural network based music
  transcription</font>
    </a>
  </h2>
  <font color="black">私たちの実験では、Melスペクトログラムがコンパクトな表現であり、比較的高い音楽の文字起こし精度を維持しながら、周波数ビンの数を512のみに減らすことができることも示しています。結果から、文字起こし精度が$ 8.33 $％増加し、$ 9.39であることがわかりました。ニューラルネットワークの設計（単層が完全に接続されている）を変更せずに、適切な入力表現（スペクトログラムのSTFTウィンドウ長が4,096および2,048の周波数ビンの対数周波数スペクトログラム）を選択することで、エラーを$％削減できます。さまざまな入力表現がポリフォニックなマルチ楽器音楽の文字起こしに与える影響。 
[要約]スペクトログラム抽出ツールnnaudioを使用して、線形周波数スペクトログラム、メルスペクトログラム、および定数-q変換（cqt）の使用による影響を調査します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-25">
        <br><font color="black">2020-01-25</font>
      </time>
    </span>
</section>
<!-- paper0: Regression-based music emotion prediction using triplet neural networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.SD/paper_13.html">
      <font color="black">Regression-based music emotion prediction using triplet neural networks</font>
    </a>
  </h2>
  <font color="black">次に、これらの新しい表現を、サポートベクターマシンや勾配ブースティングマシンなどの回帰アルゴリズムの入力として使用します。これは、オーディオ機能のコンパクトな潜在空間表現を提供することに加えて、提案されたアプローチがベースラインよりも高いパフォーマンスを持っていることを示していますモデル.. DEAMデータセットの結果は、TNNを使用することにより、ベースラインモデル（TNNなし）に対して、価数予測が9％、覚醒予測が4％改善され、90％の特徴次元削減が達成されることを示しています。 
[ABSTRACT]意味のある表現を作成できるメカニズムを提案します。異なる空間システムと比較してパフォーマンスを向上させます。tnnメソッドは、主成分分析などの他のフォームよりも優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-25">
        <br><font color="black">2020-01-25</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Multi-branch and Multi-scale Attention Learning for Fine-Grained Visual
  Categorization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.IV/paper_0.html">
      <font color="black">Multi-branch and Multi-scale Attention Learning for Fine-Grained Visual
  Categorization</font>
    </a>
  </h2>
  <font color="black">取得されたオブジェクト画像には、オブジェクトの構造全体が含まれるだけでなく、詳細も含まれ、パーツ画像には多くの異なるスケールとより細かい特徴が含まれ、生画像には完全なオブジェクトが含まれます。 -to-end、一方、短い推論時間を提供します。3種類のトレーニング画像は、当社のマルチブランチネットワークによって監視されます。 
[ABSTRACT] ilsvrの年間チャンピオンを直接、きめの細かい視覚的分類（fgvc）タスクに適用しても、パフォーマンスは良好ではありません。このアプローチは、エンドツーエンドでトレーニングできますが、可能性のある時間は短いです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-20">
        <br><font color="black">2020-03-20</font>
      </time>
    </span>
</section>
<!-- paper0: Fully Automated Segmentation of the Left Ventricle in Magnetic Resonance
  Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.IV/paper_1.html">
      <font color="black">Fully Automated Segmentation of the Left Ventricle in Magnetic Resonance
  Images</font>
    </a>
  </h2>
  <font color="black">達成された精度は、公開された文献で報告された最高の精度よりも高くなっています。このホワイトペーパーでは、CNNベースのLVセグメンテーション方法を、開示されたコードとトレーニング済みのCNNモデルで再現しようと試みています。完全に自動化されたLVセグメンテーション方法も提案しました。勾配差分布（SDD）しきい値の選択に基づいて、再現されたCNNメソッドと比較します。 
[要約] cnnのlvセグメンテーションは、近年畳み込みニューラルネットワーク（cnn）に変更されました。提案された方法は95でした。自動心臓診断チャレンジ（acdc）のテストセットで44％ダイス。2つの比較されたcnnメソッドは90を達成しました。28％と87. 13％のサイコロスコア</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Shape and Viewpoint without Keypoints -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.IV/paper_2.html">
      <font color="black">Shape and Viewpoint without Keypoints</font>
    </a>
  </h2>
  <font color="black">私たちは、アプローチを教師なしカテゴリー固有メッシュ再構成（U-CMR）と呼び、CUB、Pascal 3D、および新しいWebスクレイピングデータセットについて定性的および定量的な結果を提示します。この論文における私たちの特別な貢献は、カメラの分布の表現です。 「camera-multiplex」と呼んでいます。プロジェクトページ：https://shubham-goel.github.io/ucmr 
[要旨]キーポイントアノテーションや3Dのない画像コレクションを使用して、オブジェクト全体のさまざまな形状やテクスチャを予測する方法を学ぶことができますグラウンドトゥルース。目的は、さまざまな学習されたカテゴリ-特定の事前条件で画像を生成する可能性がある形状、テクスチャ、カメラの視点を予測すること</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Enhancement of damaged-image prediction through Cahn-Hilliard Image
  Inpainting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.IV/paper_3.html">
      <font color="black">Enhancement of damaged-image prediction through Cahn-Hilliard Image
  Inpainting</font>
    </a>
  </h2>
  <font color="black">MNISTのトレーニングセットを使用して、高密度レイヤーに基づくニューラルネットワークをトレーニングし、その後、テストセットをさまざまなタイプと強度の損傷で汚染します。次に、カーンヒリアードを適用した場合と適用しない場合のニューラルネットワークの予測精度を比較します。損傷した画像のテストにフィルターを適用します。ここで使用されているベンチマークデータセットは、数字のバイナリイメージで構成されるMNISTデータセットです。 
[要約]修正されたカーン-ヒリアード方程式は、画像修復フィルターです。有限体積スキームを使用してコストを削減します。テストセットには、さまざまなタイプと強度の損傷が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: CyCNN: A Rotation Invariant CNN using Polar Mapping and Cylindrical
  Convolution Layers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.IV/paper_4.html">
      <font color="black">CyCNN: A Rotation Invariant CNN using Polar Mapping and Cylindrical
  Convolution Layers</font>
    </a>
  </h2>
  <font color="black">回転したMNIST、CIFAR-10、およびSVHNデータセットでの分類タスクのCyCNNおよび従来のCNNモデルを評価します。CyConvレイヤーは、畳み込みで境界ユニットの入力画像受容フィールドを垂直方向に拡張する円柱スライディングウィンドウ（CSW）メカニズムを利用します。層..トレーニング中にデータの増加がない場合、CyCNNは従来のCNNモデルと比較して分類精度を大幅に向上させることを示しています。 
[要約]この論文では、cycnnと呼ばれるディープcnnモデルを提案しています。これは、レイヤーの極座標マッピングを利用して回転を並進に変換します。画像のマッピングは、cyconvレイヤーを畳み込みレイヤーに引き延ばす畳み込みレイヤーです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Balance Scene Learning Mechanism for Offshore and Inshore Ship Detection
  in SAR Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.IV/paper_5.html">
      <font color="black">Balance Scene Learning Mechanism for Offshore and Inshore Ship Detection
  in SAR Images</font>
    </a>
  </h2>
  <font color="black">このレターは、SAR画像でのオフショアとインショアの両方の船舶検出のための新しいバランスシーン学習メカニズム（BSLM）を提案します。 
[ABSTRACT]この手紙は、sar画像でのオフショアとインショアの両方の船舶検出のための新しいバランスシーン学習メカニズムを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Limited-angle tomographic reconstruction of dense layered objects by
  dynamical machine learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.IV/paper_6.html">
      <font color="black">Limited-angle tomographic reconstruction of dense layered objects by
  dynamical machine learning</font>
    </a>
  </h2>
  <font color="black">ここでは、根本的に異なるアプローチを提示します。ここでは、複数の角度からの未加工画像のコレクションが、オブジェクト依存の前方散乱演算子によって駆動される動的システムと同様に表示されます。いくつかの定量的メトリックの包括的な比較を通じて、動的メソッドが向上することを示しますアーティファクトが少なく全体的な再構成の忠実度が向上した以前の静的アプローチに基づいて。したがって、イメージングの問題は非線形システム同定の問題になり、動的学習が再構成を正則化するのに適していることも示唆しています。 
[ABSTRACT]リカレントニューラルネットワーク（rnn）アーキテクチャには新しい分割があります-ビルディングブロックとして必要な数が少ないことを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Automated diagnosis of COVID-19 with limited posteroanterior chest X-ray
  images using fine-tuned deep neural networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.IV/paper_7.html">
      <font color="black">Automated diagnosis of COVID-19 with limited posteroanterior chest X-ray
  images using fine-tuned deep neural networks</font>
    </a>
  </h2>
  <font color="black">実験結果を考慮すると、各モデルのパフォーマンスはシナリオに依存します。ただし、NASNetLargeは他のアーキテクチャと比較してより良いスコアを表示し、他の最近提案されたアプローチとさらに比較されます。したがって、より堅牢で代替の診断手法が望ましいです。この記事では、モデル分類の基礎を示す視覚的な説明も追加しましたそしてCXR画像におけるCOVID-19の知覚。 
[要約] covid-19の現在の診断手順は、逆転写酵素連鎖反応（rt-pcr）ベースのアプローチに従います。この方法は、診断の初期段階でウイルスを特定する感度が低くなります。これらの患者が初めてよりよい治療と治療を達成するためにディープラーニング技術を使用して識別されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-23">
        <br><font color="black">2020-04-23</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Preset: Blending and Retouching Photos with Color Style Transfer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.IV/paper_8.html">
      <font color="black">Deep Preset: Blending and Retouching Photos with Color Style Transfer</font>
    </a>
  </h2>
  <font color="black">さらに、ディーププリセットと呼ばれるカラースタイルの転送を提案します。写真の知識がなくても、エンドユーザーは自分の写真を美化して、よく修正されたリファレンスと同じカラースタイルにすることを望んでいます。実験結果は、ディーププリセットがカラースタイルの以前の作品は、量的および質的に転送されます。 
[ABSTRACT]これにより、使いすぎて多くの色が作成されます。ポートレートなどのデリケートなケースではさらに悪化します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Maximum information states for coherent scattering measurements -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.IV/paper_9.html">
      <font color="black">Maximum information states for coherent scattering measurements</font>
    </a>
  </h2>
  <font color="black">私たちの分析は、究極の測定精度を提供するライトフィールドが、システムの散乱行列に基づいてフィッシャー情報を定量化するエルミート演算子の固有状態であることを明らかにしています。この概念を説明するために、これらの最大情報状態が位相または最適化されていない状態と比較して桁違いに改善された精度で無秩序な媒体によって隠されているオブジェクトの位置。これらの進歩は、どの光の状態が究極の測定精度を提供できるかという興味深い疑問を自然に引き起こします。 
[要約]量子研究は、観測可能なシステムツールの推定に使用されるライトフィールドの空間プロファイルを調整することにより、このような測定の精度が大幅に向上することを示しています。これは理論と矛盾し、これらの最大情報状態が位相または無秩序な媒体によって隠されているオブジェクトの位置</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-24">
        <br><font color="black">2020-02-24</font>
      </time>
    </span>
</section>
<!-- paper0: AinnoSeg: Panoramic Segmentation with High Perfomance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.IV/paper_10.html">
      <font color="black">AinnoSeg: Panoramic Segmentation with High Perfomance</font>
    </a>
  </h2>
  <font color="black">（b）損失関数を変更して、画像内の複数のオブジェクトの境界ピクセルを考慮できるようにします。（d）マルチスケールのトレーニングと推論を使用します。AinnoSeg、AinnoSegという名前のこれらすべての操作は、よく知られているデータセットADE20Kでのアートパフォーマンス。 
[ABSTRACT] cnnネットワークの開発により、パノラマタスクの開発に成功しました。ただし、現在のパノラマデータはコンテキストに関係しています。代わりに、画像の詳細が十分に処理されていません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: 3D-CVF: Generating Joint Camera and LiDAR Features Using Cross-View
  Spatial Feature Fusion for 3D Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.IV/paper_11.html">
      <font color="black">3D-CVF: Generating Joint Camera and LiDAR Features Using Cross-View
  Spatial Feature Fusion for 3D Object Detection</font>
    </a>
  </h2>
  <font color="black">まず、この方法では自動較正された投影を使用して、2Dカメラ機能を、鳥瞰図（BEV）ドメインのLiDAR機能に最も対応する滑らかな空間機能マップに変換します。次に、カメラとLiDAR機能の融合もカメラとLiDARの融合によって提示される課題の1つは、各モダリティから取得された空間特徴マップが、カメラと世界座標の大幅に異なるビューで表されることです。したがって、情報を失うことなく2つの異種機能マップを組み合わせるのは簡単な作業ではありません。 
[要約] 2つのモダリティを融合すると、3Dオブジェクト検出の精度と堅牢性が向上することが期待されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-27">
        <br><font color="black">2020-04-27</font>
      </time>
    </span>
</section>
<!-- paper0: Lightweight image super-resolution with enhanced CNN -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.IV/paper_12.html">
      <font color="black">Lightweight image super-resolution with enhanced CNN</font>
    </a>
  </h2>
  <font color="black">その後、RBはグローバルとローカルの機能を融合することにより、低頻度の機能を高頻度の機能に変換します。これは、IEEBが長期的な依存関係の問題に取り組む際に補完します。LESRCNNのコードは、https：// githubからアクセスできます。 .com / hellloxiaotian / LESRCNN ..取得された冗長な情報を削除するために、IEEBでは異種アーキテクチャが採用されています。 
[ABSTRACT]提案されたlesrcnnは、さまざまなスケールのモデルによって画像を取得できます。lesrcnnは低解像度の再構成機能を使用します。これらの変更は、メモリのメモリ能力を高めるために使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-08">
        <br><font color="black">2020-07-08</font>
      </time>
    </span>
</section>
<!-- paper0: Attention-Guided Generative Adversarial Network to Address Atypical
  Anatomy in Modality Transfer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.IV/paper_13.html">
      <font color="black">Attention-Guided Generative Adversarial Network to Address Atypical
  Anatomy in Modality Transfer</font>
    </a>
  </h2>
  <font color="black">質的分析は、注意GANが空間的に焦点を合わせた領域を使用して外れ値、複雑な解剖学領域または術後領域をより適切に処理できることを示しており、ほぼリアルタイムのMRのみの治療計画をサポートする強力な可能性を提供します。 15人の脳癌患者について、注意GANが既存のsynCTモデルを上回り、平均MAEが85.22 $ \ pm $ 12.08、232.41 $ \ pm $ 60.86、246.38 $ \ pm $ 42.67 Hounsfieldユニットで、synCTとCT-SIMの間で頭全体で達成されたことを示しています。骨と空気の領域それぞれ。この論文では、非定型の解剖学に対処するための入力としてT1強調MRI画像を使用して正確なsynCTを生成するために、新しい空間注意誘導生成敵対ネットワーク（注意GAN）モデルを提案します。 
[ABSTRACT]研究により、非定型の解剖学的構造を含む分析画像には依然として主要な制限があることが示されています。これらには、超高血圧の高血圧が含まれます。高血圧を発症させるには、いくつかの研究が必要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-27">
        <br><font color="black">2020-06-27</font>
      </time>
    </span>
</section>
<!-- paper0: Lymphocyte counting -- Error Analysis of Regression versus Bounding Box
  Detection Approaches -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.IV/paper_14.html">
      <font color="black">Lymphocyte counting -- Error Analysis of Regression versus Bounding Box
  Detection Approaches</font>
    </a>
  </h2>
  <font color="black">検出モデルは、トレーニングにおいてより気まぐれで敏感ですが、最適化における過小評価に対してより堅牢です。分類による直接推定と、サンプルサイズが比較的小さいデータセットの境界ボックス予測モデルに対する回帰を比較します。カウントの問題を考慮します。ここではヘマトキシリンおよびエオシン染色によって例示された、細胞型にとらわれない組織病理学的染色からの細胞核。 
[要約]サンプルサイズが比較的小さいデータセットの境界ボックス予測モデルと分析を比較します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Shape-aware Semi-supervised 3D Semantic Segmentation for Medical Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_0.html">
      <font color="black">Shape-aware Semi-supervised 3D Semantic Segmentation for Medical Images</font>
    </a>
  </h2>
  <font color="black">トレーニング中に、ラベル付けされたデータとラベル付けされていないデータの予測されたSDM間の対立損失を導入し、ネットワークが形状認識機能をより効果的にキャプチャできるようにします。コードはhttps://github.com/kleinzcy/SASSnet ..で入手できます。心房セグメンテーションチャレンジデータセットの実験は、その方法を検証する改良された形状推定により、この手法が現在の最先端のアプローチよりも優れていることを示しています。 
[ABSTRACT]ほとんどの既存の半教師付きセグメンテーションアプローチは、オブジェクトセグメントの幾何学的制約を無視する傾向があり、不完全なオブジェクションにつながります。これにより、セマンティックアテンションとオブジェクトサーフェスの符号付き制約マップ（sdm）を一緒に予測するマルチタスクディープネットワークが開発されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Feature Fusion for Online Mutual Knowledge Distillation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_1.html">
      <font color="black">Feature Fusion for Online Mutual Knowledge Distillation</font>
    </a>
  </h2>
  <font color="black">既存の機能融合方法とは異なり、私たちのフレームワークでは、サブネットワーク分類子の集合が知識を融合分類子に転送し、融合分類子がその知識を各サブネットワークに戻し、相互にオンライン知識抽出の方法で互いに教えます..この相互教育システムは、融合分類器のパフォーマンスを向上させるだけでなく、各サブネットワークでパフォーマンスを向上させます。さらに、サブネットワークごとに異なるタイプのネットワークを使用できるため、このモデルはより有益です。 
[ABSTRACT]複数の並列ニューラルネットワークをサブネットワークとしてトレーニングします。それらをフュージョンモジュールと組み合わせて、より意味のあるフィーチャマップを作成します。モデルは、サブネットワークごとに異なるタイプのネットワークを使用できるため、より有益です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-19">
        <br><font color="black">2019-04-19</font>
      </time>
    </span>
</section>
<!-- paper0: SLNSpeech: solving extended speech separation problem by the help of
  sign language -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_2.html">
      <font color="black">SLNSpeech: solving extended speech separation problem by the help of
  sign language</font>
    </a>
  </h2>
  <font color="black">ソースコードはhttp://cheertt.top/homepage/でリリースされます。具体的には、3D残差たたみ込みネットワークを使用して手話の特徴を抽出し、事前学習済みのVGGNetモデルを使用して正確な視覚的特徴を使用します。拡張された音声分離問題に対処するために、手話ニュース音声（SLNSpeech）という名前の大規模データセットを導入します。オーディオ、ビジュアル、手話の3つのモダリティが共存しています。 
[要旨]障害者の実際のシナリオで音声分離技術を適用するために、手話ニュース音声という大規模なデータセットを導入します。3D残差たたみ込みネットワークを使用して手話の特徴を抽出し、事前トレーニング済みのvggnetモデルを使用して視覚的特徴</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: MovieNet: A Holistic Dataset for Movie Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_3.html">
      <font color="black">MovieNet: A Holistic Dataset for Movie Understanding</font>
    </a>
  </h2>
  <font color="black">さらに、MotionNetでは、バウンディングボックスとIDを含む1.1Mのキャラクター、42Kのシーン境界、2.5Kのアラインメントされた説明文、65Kの場所とアクションのタグ、92Kのシネマティックスタイルのタグなど、手動アノテーションのさまざまな側面が提供されています。このような全体的なデータセットは、ストーリーベースの長いビデオの理解などに関する研究を促進します。予告編、写真、プロットの説明など。
[要約]映画は、映画を包括的に理解するための最も豊富な注釈を持つ最大のデータセットです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Uncertainty-Aware Weakly Supervised Action Detection from Untrimmed
  Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_4.html">
      <font color="black">Uncertainty-Aware Weakly Supervised Action Detection from Untrimmed
  Videos</font>
    </a>
  </h2>
  <font color="black">さらに、AVAデータセットの最初の弱く監視された結果とUCF101-24の弱く監視されたメソッドの中で最新の結果を報告します。主な要因は、ビデオにフレームごとに注釈を付けることの法外なコストです。フレーム..この論文では、注釈を付けるのが非常に簡単なビデオレベルのラベルのみでトレーニングされた時空間アクション認識モデルを紹介します。 
[ABSTRACT]動画に注釈を付けるコストが高すぎるため使用できません。この方法の使用方法を知る必要があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-branch and Multi-scale Attention Learning for Fine-Grained Visual
  Categorization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_5.html">
      <font color="black">Multi-branch and Multi-scale Attention Learning for Fine-Grained Visual
  Categorization</font>
    </a>
  </h2>
  <font color="black">取得されたオブジェクト画像には、オブジェクトのほぼ全体の構造だけでなく、詳細も含まれ、パーツ画像には多くの異なるスケールとよりきめの細かい特徴があり、生画像には完全なオブジェクトが含まれています。3種類のトレーニング画像私たちのマルチブランチネットワークによって監視されています。私たちのアプローチはエンドツーエンドでトレーニングでき、推定時間も短くなります。 
[ABSTRACT] ilsvrの年間チャンピオンを直接、きめの細かい視覚的分類（fgvc）タスクに適用しても、パフォーマンスは良好ではありません。このアプローチは、エンドツーエンドでトレーニングできますが、可能性のある時間は短いです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-20">
        <br><font color="black">2020-03-20</font>
      </time>
    </span>
</section>
<!-- paper0: Novel-View Human Action Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_6.html">
      <font color="black">Novel-View Human Action Synthesis</font>
    </a>
  </h2>
  <font color="black">転送されたテクスチャをローカルに、ローカルの測地線近傍内に、およびグローバルに、対称的な意味論的部分にまたがって伝播することにより、セミデンステクスチャメッシュを生成します。これにより、ネットワークは、フォアグラウンドとバックグラウンドの合成タスクの学習に独立して集中できます。次に、コンテキストベースのジェネレーターを導入して、残差の外観情報を修正および完了する方法を学習します。 
[要約]提案されたソリューションは、公開されているntu rgb dデータセットに基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-06">
        <br><font color="black">2020-07-06</font>
      </time>
    </span>
</section>
<!-- paper0: AutoScale: Learning to Scale for Crowd Counting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_7.html">
      <font color="black">AutoScale: Learning to Scale for Crowd Counting</font>
    </a>
  </h2>
  <font color="black">ただし、密集地域の密度マップを正確に予測することは困難です。群集のカウントに関する最近の研究では、主に畳み込みニューラルネットワーク（CNN）を利用して密度マップを回帰することでカウントし、大きな進歩を遂げています。効果的なラーニングトゥースケーリング（L2S）モジュール。密な領域を適切な密度レベルに自動的にスケーリングします。 
[ABSTRACT]密度マップでは、各人はガウスblobで表されます。最終的なカウントはマップ全体の統合から取得されます。この長い尾を持つ分布の問題に対処するため、この問題に対処することを目的としています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-20">
        <br><font color="black">2019-12-20</font>
      </time>
    </span>
</section>
<!-- paper0: Balanced Meta-Softmax for Long-Tailed Visual Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_8.html">
      <font color="black">Balanced Meta-Softmax for Long-Tailed Visual Recognition</font>
    </a>
  </h2>
  <font color="black">理論的には、マルチクラスソフトマックス回帰の一般化限界を導出し、損失が限界を最小化することを示します。さらに、バランスメタソフトマックスを導入し、補完的なメタサンプラーを適用して最適なクラスサンプルレートを推定し、ロングテール学習をさらに改善します。このペーパーでは、トレーニングとテストの間のラベル配布のシフトに対応するための、Softmaxのエレガントで公平な拡張であるBalanced Softmaxを紹介します。 
[要約]このペーパーでは、トレーニングとテストの間のラベル分布のシフトに対応するために、softmaxのエレガントな公平な拡張であるバランスのとれたsoftmaxを示しました。長期学習をさらに改善する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Regularizing Deep Networks with Semantic Data Augmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_9.html">
      <font color="black">Regularizing Deep Networks with Semantic Data Augmentation</font>
    </a>
  </h2>
  <font color="black">ISDAはシンプルですが、CIFAR-10、CIFAR-100、SVHN、ImageNet、Cityscapes、MS COCOなどのさまざまなデータセットで、一般的なディープモデル（ResNetやDenseNetsなど）の一般化パフォーマンスを一貫して向上させます。ディープネットワークは線形化された特徴の学習に効果的であるという興味深い特性にインスパイアされています。つまり、ディープフィーチャー空間の特定の方向は、意味のある意味変換に対応します。たとえば、オブジェクトの背景やビューの角度を変更します。実際、提案された暗黙のセマンティックデータ拡張（ISDA）アルゴリズムは、新規の堅牢なCE損失を最小限に抑え、通常のトレーニングプロシージャに追加の計算コストを最小限に抑えます。 
[ABSTRACT]提案された方法は、ディープネットワークが線形化された特徴の学習に効果的であるという事実に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: AlignNet: Unsupervised Entity Alignment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_10.html">
      <font color="black">AlignNet: Unsupervised Entity Alignment</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、監視されていない整列モジュールであるAlignNetを提示して、整列の問題を解決するための手順を実行します。残念ながら、これらのモデルは単一フレームの優れたセグメンテーションを提供しますが、一度に分割されたオブジェクトがどのように対応するかを追跡しません（または整列）後のタイムステップでそれらに..最近開発された深層学習モデルは、監督なしでシーンをコンポーネントオブジェクトにセグメント化することを学習できます。 
[ABSTRACT]これにより、エージェントはピクセルではなくオブジェクトまたはエンティティを取得できます。これにより、エージェントはピクセルではなくオブジェクトとして取得できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-17">
        <br><font color="black">2020-07-17</font>
      </time>
    </span>
</section>
<!-- paper0: 3D Correspondence Grouping with Compatibility Features -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_11.html">
      <font color="black">3D Correspondence Grouping with Compatibility Features</font>
    </a>
  </h2>
  <font color="black">3D対応のグループ化のためのシンプルで効果的な方法を紹介します。CFは、他の対応への候補の上位ランクの互換性スコアで構成されます。これは、堅牢で回転不変の幾何学的制約に純粋に依存します。対応の空間分布は不規則ですが、インライアは、互いに幾何学的に互換性があることが期待されています。 
[ABSTRACT] 3D対応のシンプルでシンプルでシンプルな定義は、ネットワークのシンプルでシンプルなタイプに基づいています。シンプルな分類方法では、ローカルのcf記述子をインライアおよびアウトライアと照合します。主な目的は、初期の対応を正確に分類することです。機能（cf）</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Understanding Dynamic Scenes using Graph Convolution Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_12.html">
      <font color="black">Understanding Dynamic Scenes using Graph Convolution Networks</font>
    </a>
  </h2>
  <font color="black">以前の方法に比べて、世界中のさまざまな部分のさまざまなデータセットの行動分類精度の形で大幅なパフォーマンスの向上を示し、複数のデータセットを微調整することなくシームレスに転送することを示しています。このような行動予測方法はすぐにわかります行動計画、状態推定などのさまざまなナビゲーションタスクと、ビデオ上のトラフィック違反の検出に関連するアプリケーションとの関連性。グラフの双方向エッジは、エッジの2つのノードを構成するエージェント間の時間的相互作用をエンコードします。 。 
[要約] mrgcnを取得する提案された方法は、当面の問題に特に適していることが示されています。これは、進化した動作のこのような中間表現を使用しない複雑なシステムよりも優れています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-09">
        <br><font color="black">2020-05-09</font>
      </time>
    </span>
</section>
<!-- paper0: Fully Automated Segmentation of the Left Ventricle in Magnetic Resonance
  Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_13.html">
      <font color="black">Fully Automated Segmentation of the Left Ventricle in Magnetic Resonance
  Images</font>
    </a>
  </h2>
  <font color="black">私たちが達成した精度は、公開された文献で報告されている最高の精度よりも高いです。LVセグメンテーションでは、CNNベースの多くの方法が提案されていますが、ロバストで再現性のある結果はまだ達成されていません。当然のことながら、再現結果は彼らの主張された精度。 
[要約] cnnのlvセグメンテーションは、近年畳み込みニューラルネットワーク（cnn）に変更されました。提案された方法は95でした。自動心臓診断チャレンジ（acdc）のテストセットで44％ダイス。2つの比較されたcnnメソッドは90を達成しました。28％と87. 13％のサイコロスコア</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: PODNet: Pooled Outputs Distillation for Small-Tasks Incremental Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_14.html">
      <font color="black">PODNet: Pooled Outputs Distillation for Small-Tasks Incremental Learning</font>
    </a>
  </h2>
  <font color="black">PODNetと3つのデータセット（CIFAR100、ImageNet100、ImageNet1000）の3つの最新モデルを比較して、これらのイノベーションを徹底的に検証します。PODNetは、モデル全体に適用される効率的な空間ベースの蒸留損失で既存の技術を革新し、各クラスの複数のプロキシベクトルで構成される表現。コードはhttps://github.com/arthurdouillard/incremental_learning.pytorchで入手できます。
[要約]ニューヨーク大学のエンジニアはポッドネットを提案しています。学習ツールとして使用するように設計されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-28">
        <br><font color="black">2020-04-28</font>
      </time>
    </span>
</section>
<!-- paper0: General 3D Room Layout from a Single View by Render-and-Compare -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_15.html">
      <font color="black">General 3D Room Layout from a Single View by Render-and-Compare</font>
    </a>
  </h2>
  <font color="black">私たちの方法を定量的に評価するためのデータセットがなかったため、いくつかの適切なメトリックとともに1つを作成しました。これは、人気のあるNYUv2 303ベンチマークの3倍のサンプルと、はるかに多様なレイアウトを提供します。以前の作品では無視されていた問題であるレイアウトの構成要素として、3Dレイアウトの推定値を反復的に改善するための分析による合成方法を紹介します。 
[ABSTRACT] 3Dレイアウトは以前の作品では無視されていた問題です。3Dレイアウトの推定値を改善するために、分析-合成による方法を導入します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-07">
        <br><font color="black">2020-01-07</font>
      </time>
    </span>
</section>
<!-- paper0: Increasing the robustness of DNNs against image corruptions by playing
  the Game of Noise -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_16.html">
      <font color="black">Increasing the robustness of DNNs against image corruptions by playing
  the Game of Noise</font>
    </a>
  </h2>
  <font color="black">人間の視覚システムは、雨や雪などの自然に発生するさまざまな変化や破損に対して非常に堅牢です。ここでは、加法ガウスノイズとスペックルノイズを使用したシンプルだが適切に調整されたトレーニングが目に見えない破損に驚くほどうまく一般化し、簡単に破損のベンチマークImageNet-C（ResNet50を使用）およびMNIST-Cに関する以前の最新技術。これらの強力なベースライン結果に基づいて構築し、相関のない最悪の場合のノイズ分布に対する認識モデルの敵対的なトレーニングがパフォーマンスのさらなる向上につながることを示しています。対照的に、現代の画像認識モデルのパフォーマンスは、以前見られなかった破損。 
[ABSTRACT]最新の画像認識モデルのパフォーマンスは、以前には見られなかった破損を評価すると大幅に低下します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-16">
        <br><font color="black">2020-01-16</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Semi-supervised Knowledge Distillation for Overlapping Cervical
  Cell Instance Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_17.html">
      <font color="black">Deep Semi-supervised Knowledge Distillation for Overlapping Cervical
  Cell Instance Segmentation</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、知識の抽出により精度を向上させて、セグメンテーションのラベル付けされたデータとラベル付けされていないデータの両方を活用することを提案します。各提案の摂動に対する感度と、大規模なケースから有益なサンプルを選択して、高速で効果的な意味論的蒸留を促進します。 
[ABSTRACT]私たちは、訓練中に教師と生徒のネットワークで構成される、摂動に敏感なサンプルマイニング（mmt-psm）を備えた新しいマスク誘導平均教師フレームワークを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Procrustean Regression Networks: Learning 3D Structure of Non-Rigid
  Objects from 2D Annotations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_18.html">
      <font color="black">Procrustean Regression Networks: Learning 3D Structure of Non-Rigid
  Objects from 2D Annotations</font>
    </a>
  </h2>
  <font color="black">NRSfMの最も重要な困難は、回転と変形の両方を同時に推定することです。これまでの研究では、両方を回帰することでこれを処理します。提案された方法は、欠落したエントリを含む入力を処理でき、実験結果は、提案されたフレームワークが優れていることを検証します。基礎となるネットワーク構造は非常に単純ですが、Human 3.6M、300-VW、およびSURREALデータセットの最先端の方法による再構成パフォーマンス。再投影エラーと低コストで構成されるコスト関数でトレーニング整列された形状のランク付け用語であるネットワークは、トレーニング中に人間の骨格や顔などのオブジェクトの3D構造を学習しますが、テストは単一フレームベースで行われます。 
[ABSTRACT]提案された方法は、欠落したエントリのある入力を処理できます。欠落したエントリと実験結果を処理できます。これらのアイデアが初めて開発されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Complementing Representation Deficiency in Few-shot Image
  Classification: A Meta-Learning Approach -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_19.html">
      <font color="black">Complementing Representation Deficiency in Few-shot Image
  Classification: A Meta-Learning Approach</font>
    </a>
  </h2>
  <font color="black">既存の方法は、表現の欠陥を回避するために、表現力の低い特徴を抽出することに主に頼っています。最後に、エンドツーエンドのフレームワークは、3つの標準的な少数ショット学習データセットの画像分類で最先端のパフォーマンスを実現します。実用的なアプリケーションでは豊富なトレーニングサンプルを入手することが困難であるため、少数ショット学習は、最近ますます注目を集めている挑戦的な問題です。 
[ABSTRACT] less than a meta-学習者は新しいタスクに一般化するようにトレーニングできます。これは、メタ学習者が新しいタスクから一般化するために適切にトレーニングできないためです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: FaceHop: A Light-Weight Low-Resolution Face Gender Classification Method -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_20.html">
      <font color="black">FaceHop: A Light-Weight Low-Resolution Face Gender Classification Method</font>
    </a>
  </h2>
  <font color="black">FaceHopは、逐次部分空間学習（SSL）の原理で開発され、PixelHop ++の基盤に基づいて構築されています。LeNet-5のモデルサイズは75.8Kパラメータですが、LeNet-5は分類精度がLeNet-5よりも優れています。モデルサイズが小さく、トレーニングデータ量が小さく、トレーニングの複雑さが低く、入力画像が低解像度です。 
[要約]ディープラーニング（dl）テクノロジーの採用により、顔の性別分類の精度が急速に向上しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-18">
        <br><font color="black">2020-07-18</font>
      </time>
    </span>
</section>
<!-- paper0: Pyramid Multi-view Stereo Net with Self-adaptive View Aggregation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_21.html">
      <font color="black">Pyramid Multi-view Stereo Net with Self-adaptive View Aggregation</font>
    </a>
  </h2>
  <font color="black">実験結果は、私たちのアプローチが\ textsl {\ textbf {DTU}}データセットに新しい最先端技術を確立し、完全性と全体的な品質を大幅に改善し、状態と同等のパフォーマンスを達成することで強力な一般化を示していることを示しています\ textsl {\ textbf {Tanks and Temples}}ベンチマークの最先端の方法.. 3D点群再構成の堅牢性と完全性をさらに高めるために、ピラミッドマルチスケール画像を\として入力して、VA-MVSNetを拡張しますtextbf {PVA-MVSNet}。マルチメトリック制約を利用して、粗いスケールで信頼できる深度推定を集約し、細かいスケールで不一致領域を埋めます。この論文では、効果的で効率的なピラミッドマルチビューを提案します。自己適応型ビュー集約を備えたステレオ（MVS）ネット。正確で完全な高密度の点群を再構成します。 
[ABSTRACT]ニューヨークベースのアプローチにより、完全性と全体的な品質が大幅に改善された新しい状態一致データセットが作成されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-06">
        <br><font color="black">2019-12-06</font>
      </time>
    </span>
</section>
<!-- paper0: Environment-agnostic Multitask Learning for Natural Language Grounded
  Navigation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_22.html">
      <font color="black">Environment-agnostic Multitask Learning for Natural Language Grounded
  Navigation</font>
    </a>
  </h2>
  <font color="black">コードはhttps://github.com/google-research/valan。で入手できます。広範な実験により、環境にとらわれないマルチタスク学習により、見られる環境と見られない環境の間のパフォーマンスギャップが大幅に減少し、トレーニングされたナビゲーションエージェントが目に見えない環境のベースラインを上回りますVLNで16％（成功率の相対測定）およびNDHで120％（目標の進捗）。 
[ABSTRACT]訓練されたナビゲーションエージェントは、目に見えない環境のベースラインよりもvlnで16％（成功率の相対測定値）およびndhで120％（目標の進捗状況）優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-01">
        <br><font color="black">2020-03-01</font>
      </time>
    </span>
</section>
<!-- paper0: Shape and Viewpoint without Keypoints -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_23.html">
      <font color="black">Shape and Viewpoint without Keypoints</font>
    </a>
  </h2>
  <font color="black">私たちはアプローチを教師なしカテゴリー固有メッシュ再構築（U-CMR）と呼び、CUB、Pascal 3D、および新しいWebスクレイピングデータセットで定性的および定量的な結果を提示します。プロジェクトページ：https://shubham-goel.github.io/ ucmr。ポイント推定値を選択する代わりに、トレーニング中に最適化された一連のカメラ仮説を維持して、現在の形状とテクスチャーが与えられた画像を最もよく説明します。 
[ABSTRACT]キーポイントアノテーションや3Dグラウンドトゥルースのない画像コレクションを使用して、オブジェクト全体のさまざまな形状やテクスチャを予測する方法を学習できます。目的は、さまざまな学習カテゴリで画像を生成できる可能性のある形状、テクスチャ、カメラの視点を予測することです特定の事前</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Enhancement of damaged-image prediction through Cahn-Hilliard Image
  Inpainting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_24.html">
      <font color="black">Enhancement of damaged-image prediction through Cahn-Hilliard Image
  Inpainting</font>
    </a>
  </h2>
  <font color="black">次に、損傷した画像のテストにCahn-Hilliardフィルターを適用した場合と適用しない場合のニューラルネットワークの予測精度を比較します。ここで使用されているベンチマークデータセットは、数字の2進画像で構成されるMNISTデータセットです。 Cahn-Hilliardフィルターの適用による損傷画像予測の改善。これは、特定の損傷に対して最大50％増加する可能性があり、一般に低から中程度の損傷に有利です。 
[要約]修正されたカーン-ヒリアード方程式は、画像修復フィルターです。有限体積スキームを使用してコストを削減します。テストセットには、さまざまなタイプと強度の損傷が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Deep-COVID: Predicting COVID-19 From Chest X-Ray Images Using Deep
  Transfer Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_25.html">
      <font color="black">Deep-COVID: Predicting COVID-19 From Chest X-Ray Images Using Deep
  Transfer Learning</font>
    </a>
  </h2>
  <font color="black">提案されたモデルは、監視なしの方法でトレーニングされ、ソーシャルメディアのウェブサイトからのテキストデータの任意のデータセットに適用できます。提案されたモデルは、まずツイートの文レベルの表現を学習し、次に、類似性をいくつかのグループに埋め込むことに基づいてグループ化します。 、その後、他の単語との頻度と平均類似度に基づいて、各クラスタ内の最も重要な単語を検出します。実験結果を通じて、私たちのモデルは、ツイートを文レベルで処理することにより、非常に有益なトピックを検出できることを示します（これにより、ツイートの全体的な意味）。 
[ABSTRACT]モデルは最近数か月間のツイートの主要トピックを検出できます。これはトピックを検出するセンテンストランスフォーマーに基づいています。これにより、ツイートの全体的な意味を保持できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-20">
        <br><font color="black">2020-04-20</font>
      </time>
    </span>
</section>
<!-- paper0: CyCNN: A Rotation Invariant CNN using Polar Mapping and Cylindrical
  Convolution Layers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_26.html">
      <font color="black">CyCNN: A Rotation Invariant CNN using Polar Mapping and Cylindrical
  Convolution Layers</font>
    </a>
  </h2>
  <font color="black">極座標の円筒特性を処理するために、従来のCNNの畳み込み層を円筒畳み込み（CyConv）層に置き換えます。回転したMNIST、CIFAR-10、およびSVHNデータセットでの分類タスクのCyCNNおよび従来のCNNモデルを評価します。トレーニング中にデータの増加がない場合、CyCNNは従来のCNNモデルと比較して分類精度を大幅に向上させることを示します。 
[要約]この論文では、cycnnと呼ばれるディープcnnモデルを提案しています。これは、レイヤーの極座標マッピングを利用して回転を並進に変換します。画像のマッピングは、cyconvレイヤーを畳み込みレイヤーに引き延ばす畳み込みレイヤーです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Controlling Style and Semantics in Weakly-Supervised Image Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_27.html">
      <font color="black">Controlling Style and Semantics in Weakly-Supervised Image Generation</font>
    </a>
  </h2>
  <font color="black">モデルのトレーニングに使用されるラベルマップは、ラージボキャブラリオブジェクト検出器によって生成されます。これにより、ラベルなしデータへのアクセスが可能になり、構造化されたインスタンス情報が提供されます。ユーザーが細かいところにある複雑なシーンの条件付き画像生成には、教師なしのアプローチを提案しますシーンに表示されるオブジェクトを制御します。COCOやVisual Genomeなどの複雑なデータセットのシーンを操作するモデルの機能も紹介します。 
[ABSTRACT]スパースセマンティックマップを利用して、オブジェクトの形状とクラス、およびセマンティック記述を制御します。シーンの制御性をさらに高めるために、画像を分解する2ステップの生成スキームを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-06">
        <br><font color="black">2019-12-06</font>
      </time>
    </span>
</section>
<!-- paper0: Foley Music: Learning to Generate Music from Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_28.html">
      <font color="black">Foley Music: Learning to Generate Music from Videos</font>
    </a>
  </h2>
  <font color="black">実験結果は、私たちのモデルがいくつかの既存のシステムよりも優れており、聴きやすい音楽を生成していることを示しています。さまざまな音楽パフォーマンスを含むビデオでのモデルの有効性を示しています。MIDIイベントは、市販の音楽シンセサイザーツール。 
[ABSTRACT]動画から音楽への生成に成功するための2つの主要な中間表現を特定します。動画と音声の録音からの本体のキーポイントは、動画から音楽へのキーポイントになります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Prototypical Contrastive Learning of Unsupervised Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_29.html">
      <font color="black">Prototypical Contrastive Learning of Unsupervised Representations</font>
    </a>
  </h2>
  <font color="black">PCLは、複数の教師なし表現学習ベンチマークで最先端の結果を達成し、低リソース転送タスクの精度が10％以上向上します。PCLは、インスタンス識別のタスクの低レベル機能だけでなく、より重要なこととして、データの意味構造を学習済みの埋め込み空間に暗黙的にエンコードします。ProtoNCE損失は、コントラスト学習のためのInfoNCE損失の一般化バージョンであり、割り当てられたプロトタイプにより近い表現を促進します。 
[ABSTRACT] pclは教師なし表現の学習ベンチマークを学習できます。また、学習した埋め込みスペースにデータの意味構造をエンコードします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-11">
        <br><font color="black">2020-05-11</font>
      </time>
    </span>
</section>
<!-- paper0: Top-Related Meta-Learning Method for Few-Shot Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_30.html">
      <font color="black">Top-Related Meta-Learning Method for Few-Shot Detection</font>
    </a>
  </h2>
  <font color="black">大量のデータとより多くのパラメーターに依存する多くのメタ学習手法が、少数ショット検出のために提案されています。トレーニング中、カテゴリーに関連するメタ特徴は、検出モデルの予測レイヤーの重みと見なされます。検出パフォーマンスを向上させるために、グループ内のカテゴリ間で共有分布を持つメタ機能を利用します。ただし、カテゴリの不均衡と機能が少ないため、以前の方法には明らかな問題があり、少数ショット検出の強いバイアスと不適切な分類があります。 
[ABSTRACT]カテゴリの場合、True-ラベルexample.itを活用するTCLを提案します。これは、カテゴリベースのグループ化メカニズムで構成され、外観と環境によってカテゴリをグループ化し、類似したカテゴリ間のセマンティック機能を強化します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Dense Hybrid Recurrent Multi-view Stereo Net with Dynamic Consistency
  Checking -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_31.html">
      <font color="black">Dense Hybrid Recurrent Multi-view Stereo Net with Dynamic Consistency
  Checking</font>
    </a>
  </h2>
  <font color="black">そうすることで、すべてのビュー間の幾何学的整合性マッチングエラーを動的に集約します。この論文では、動的整合性チェックを備えた効率的で効果的な高密度ハイブリッド反復マルチビューステレオネット、つまり$ D ^ {2} $ HC-を提案します。 RMVSNet、正確な密な点群の再構成用。この方法では、すべての方法で\ textbf {$ 1 ^ {st} $}を複雑な屋外の\ textsl {Tanks and Temples}ベンチマークでランク付けします。 
[ABSTRACT]当社の新しいハイブリッドハイブリッドリカレントマルチビューステレオネットは、2つのコアモジュールで構成されています。これらのモジュールには、マルチスケールコンテキスト情報を含む元のサイズの高密度フィーチャーマップを抽出するためのライトドネット（高密度受信拡張）モジュールが含まれています。最先端へ-メモリタンクを劇的に削減しながら最先端の方法</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Fine-Grained Image Captioning with Global-Local Discriminative Objective -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_32.html">
      <font color="black">Fine-Grained Image Captioning with Global-Local Discriminative Objective</font>
    </a>
  </h2>
  <font color="black">これは主に、（i）従来のトレーニング目標の保守的な特性が原因で、モデルが類似の画像に対して正確ではあるがほとんど区別できないキャプションを生成すること、および（ii）グラウンドトゥルースキャプションの不均一な単語分布が原因で、非常に頻繁な単語の生成が促進される/ phrasesは、頻度は低くなりますがより具体的なものを抑制します。広く使用されているMS-COCOデータセットに対して提案された方法を評価します。この方法は、ベースラインの方法よりもかなりのマージンで優れており、既存の主要なアプローチに対して競争力のあるパフォーマンスを実現します。近年、視覚と言語の分野で活発に取り上げられている画像キャプションで行われました。 
[ABSTRACT]既存の方法では、過度に一般的なキャプションが生成される傾向があります。これらには、最も頻繁に使用される単語やフレーズが含まれます。これにより、不正確で見分けがつかない説明になります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Video Super-resolution with Temporal Group Attention -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_33.html">
      <font color="black">Video Super-resolution with Temporal Group Attention</font>
    </a>
  </h2>
  <font color="black">広範な結果は、さまざまな動きのあるビデオを処理する提案されたモデルの機能を示しています。さらに、大きな動きのあるビデオを処理するために、高速の空間配置が提案されています。いくつかのベンチマークで最新のメソッドに対して良好なパフォーマンスを実現します。データセット。 
[要約]提案された方法は、超共産主義グループによって提案されました。これらのグループは、注意フレームおよび深いグループ内融合モジュールとさらに統合されている参照フレームの欠落した詳細を回復するための代替情報を提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Soft Expert Reward Learning for Vision-and-Language Navigation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_34.html">
      <font color="black">Soft Expert Reward Learning for Vision-and-Language Navigation</font>
    </a>
  </h2>
  <font color="black">私たちが提案する方法は、2つの補完的なコンポーネントで構成されています。SoftExpert Distillation（SED）モジュールは、エージェントができるだけ専門家のように振る舞うことを奨励します。自己認識（SP）モジュールは、エージェントをできるだけ早く最終的な目的地に向けることを目標としています。教師付き学習クローンエキスパートの動作に基づく主要な方法により、目に見えない環境では制限されたパフォーマンスを示しながら、見られた環境でより優れたパフォーマンスを発揮します。 -Language Navigation（VLN）は、エージェントが自然言語の指示に従って、目に見えない環境で指定されたスポットを見つけることを要求します。 
[ABSTRACT]新しい研究は、教師あり学習に基づく主要な方法が失敗することを示しています。これらは、目に見えないものではパフォーマンスが制限されていることを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: GREEN: a Graph REsidual rE-ranking Network for Grading Diabetic
  Retinopathy -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_35.html">
      <font color="black">GREEN: a Graph REsidual rE-ranking Network for Grading Diabetic
  Retinopathy</font>
    </a>
  </h2>
  <font color="black">この従来の方法では、分類結果を再集計して残差集計方法を実行することにより、画像分類パイプラインを拡張します。このクラス相関により、既存のネットワークが制限され、効果的な分類が実現されます。このホワイトペーパーでは、グラフの残差rEランキングネットワーク（GREEN）を提案して、元の画像分類ネットワークへのクラス依存関係。 
[ABSTRACT]このクラス相関は、既存のネットワークを制限して効果的な分類を実現します。drグレーディングは画像分類の問題です。標準的なベンチマークの実験では、グリーンが最新の手法に対して有利に機能することが示されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-20">
        <br><font color="black">2020-07-20</font>
      </time>
    </span>
</section>
<!-- paper0: Domain Generalization with Optimal Transport and Metric Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_36.html">
      <font color="black">Domain Generalization with Optimal Transport and Metric Learning</font>
    </a>
  </h2>
  <font color="black">経験的な結果は、提案された方法がほとんどのベースラインを上回る可能性があることを示しています。このための1つの可能な解決策は、不変の特徴を抽出するときにラベルの類似性を制約し、クラス固有の凝集と特徴の分離にラベルの類似性を利用することです。したがって、ドメイン間での最適なトランスポートを採用します。これにより、クラスラベルの類似性を抑制し、敵対的なトレーニングを可能にし、メトリック学習目標をさらに展開して、ラベル情報を活用して、区別可能な分類境界を実現します。 
[要旨]ドメインの汎化問題に取り組み、複数のソースドメインから学び、統計が不明なターゲットドメインに汎化します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Movement Assessment from Skeleton Videos: A Review -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_37.html">
      <font color="black">Movement Assessment from Skeleton Videos: A Review</font>
    </a>
  </h2>
  <font color="black">最近の10年間での3Dカメラの利用可能性の向上とコンピュータービジョンアルゴリズムの劇的な改善により、自動運動評価ソリューションの研究が加速しました。このようなソリューションは、手頃な価格の機器と専用ソフトウェアを使用して、自宅で実装できます。 
[ABSTRACT]スケルトンビデオからの自動運動評価の最近のソリューションを確認します。目的、機能、運動ドメイン、アルゴリズムアプローチによってそれらを比較します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Balance Scene Learning Mechanism for Offshore and Inshore Ship Detection
  in SAR Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_38.html">
      <font color="black">Balance Scene Learning Mechanism for Offshore and Inshore Ship Detection
  in SAR Images</font>
    </a>
  </h2>
  <font color="black">このレターは、SAR画像でのオフショアとインショアの両方の船舶検出のための新しいバランスシーン学習メカニズム（BSLM）を提案します。 
[ABSTRACT]この手紙は、sar画像でのオフショアとインショアの両方の船舶検出のための新しいバランスシーン学習メカニズムを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video
  Parsing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_39.html">
      <font color="black">Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video
  Parsing</font>
    </a>
  </h2>
  <font color="black">さらに、モダリティバイアスとノイズの多いラベルの問題を、個別ガイド付きの学習メカニズムとラベルスムージング手法でそれぞれ発見して軽減します。具体的には、単峰性とクロスモーダルの時間的コンテキストを同時に探索するための新しいハイブリッドアテンションネットワークを提案します。論文では、オーディオビジュアルビデオ解析という名前の新しい問題を紹介します。これは、ビデオを時間イベントセグメントに解析し、それらを可聴、可視、またはその両方としてラベル付けすることを目的としています。 
[要旨]これは、ビデオ内に描かれているシーンを完全に理解するために不可欠です。注意深い問題を開発します。これは、マルチモーダルな複数インスタンス学習として自然に構成できます-視覚的なビデオ解析。ビデオだけでも課題を達成できます-レベルの弱いラベル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: PointContrast: Unsupervised Pre-training for 3D Point Cloud
  Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_40.html">
      <font color="black">PointContrast: Unsupervised Pre-training for 3D Point Cloud
  Understanding</font>
    </a>
  </h2>
  <font color="black">以前の作品とは異なり、高レベルのシーン理解タスクに焦点を当てています。私たちの調査結果は非常に励みになります。事前トレーニングにアーキテクチャ、ソースデータセット、および対照的な損失の統一トリプレットを使用して、セグメンテーションの最近の最良の結果を改善します。屋内と屋外の実際のデータセットと合成データセットの6つの異なるベンチマークでの検出-学習された表現がドメイン全体で一般化できることを示しています。この作業では、3D表現の学習に関する研究を促進することを目指します。 
[ABSTRACT]豊富なソースセット（例：imagenet）でネットワークを事前トレーニングすると、通常ははるかに小さいターゲットセットで微調整すると、パフォーマンスが向上することがわかります。これは、データに注釈を付けるために必要な作業を検討する機会です。 3D</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Intra-clip Aggregation for Video Person Re-identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_41.html">
      <font color="black">Intra-clip Aggregation for Video Person Re-identification</font>
    </a>
  </h2>
  <font color="black">ビデオベースの人物の再識別は、ビデオ監視におけるその広範なアプリケーションにより、近年大きな注目を集めています。3つのベンチマークデータセットでの広範な実験により、当社のフレームワークが最新の最先端の方法よりも優れていることが示されています。データセット間の検証を実行して、メソッドの一般性を証明します。 
[ABSTRACT]より深い方法は大きな進歩をもたらしましたが、これらの方法は、代替情報を効果的に使用しないことによって制限されています。これは、トレーニングプロセスで必要なデータの増大が原因です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-05-05">
        <br><font color="black">2019-05-05</font>
      </time>
    </span>
</section>
<!-- paper0: Automated diagnosis of COVID-19 with limited posteroanterior chest X-ray
  images using fine-tuned deep neural networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_42.html">
      <font color="black">Automated diagnosis of COVID-19 with limited posteroanterior chest X-ray
  images using fine-tuned deep neural networks</font>
    </a>
  </h2>
  <font color="black">これらのデータセットでは、COVID-19の陽性事例に関係するサンプルが限られているため、公平な学習の課題が生じます。実験結果を考慮すると、各モデルのパフォーマンスはシナリオに依存します。ただし、NASNetLargeは他のアーキテクチャとは対照的に、より良いスコアを表示し、他の最近提案されたアプローチと比較されています。 
[要約] covid-19の現在の診断手順は、逆転写酵素連鎖反応（rt-pcr）ベースのアプローチに従います。この方法は、診断の初期段階でウイルスを特定する感度が低くなります。これらの患者が初めてよりよい治療と治療を達成するためにディープラーニング技術を使用して識別されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-23">
        <br><font color="black">2020-04-23</font>
      </time>
    </span>
</section>
<!-- paper0: A Deep Ordinal Distortion Estimation Approach for Distortion
  Rectification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_43.html">
      <font color="black">A Deep Ordinal Distortion Estimation Approach for Distortion
  Rectification</font>
    </a>
  </h2>
  <font color="black">私たちの知る限りでは、最初に、不均一な歪みパラメーターを序数歪みを通じて学習に適した中間表現に統合し、画像の特徴と歪み補正の間のギャップを埋めます。暗黙の歪みパラメーターとは対照的に、提案された序数歪みは、画像の特徴。ニューラルネットワークの歪みの知覚を大幅に向上させます。主な理由は、これらのパラメータが画像の特徴に暗黙的に含まれており、歪み情報を完全に学習するためにネットワークに影響を与えるためです。 
[ABSTRACT]歪み補正は、通常の歪みを学習する新しい方法です。これは、ローカル-グローバル関連推定ネットワークの働きです。この問題を解決するために、通常の歪みを学習する新しいアプローチを作成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Making Robots Draw A Vivid Portrait In Two Minutes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_44.html">
      <font color="black">Making Robots Draw A Vivid Portrait In Two Minutes</font>
    </a>
  </h2>
  <font color="black">アルゴリズムによって生成されたポートレートは、連続したブラシストロークの疎なセットを使用して、個々の特性を正常にキャプチャします。また、ローカルスケッチ合成アルゴリズムと、背景と詳細を処理するためのいくつかの前処理および後処理技術を実装します。 、ポートレートは一連の軌道に変換され、3自由度のロボットアームによって再現されます。 
[ABSTRACT] aisketcherはaisketcher.itと呼ばれるユニバーサルポートレート描画ロボットで、ディープラーニングに基づく新しいポートレート合成アルゴリズムに基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-12">
        <br><font color="black">2020-05-12</font>
      </time>
    </span>
</section>
<!-- paper0: Building Information Modeling and Classification by Visual Learning At A
  City Scale -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_45.html">
      <font color="black">Building Information Modeling and Classification by Visual Learning At A
  City Scale</font>
    </a>
  </h2>
  <font color="black">さらに、メタデータの構築における空間パターンを発見するために、新しい機械学習（ML）ベースの統計ツールSURFが提案されています。このホワイトペーパーでは、人工知能が土木工学に力を与える方法を示す2つのケーススタディを提供します。フレームワークは、ディープラーニング技術を使用して、衛星/ストリートビュー画像から建物の視覚情報を抽出します。 
[ABSTRACT]最初のケースでは、ビルディングインフォメーションモデリング用に機械学習の効率的なフレームワークであるブレイルが提案されています。また、大規模な建物の画像データベースと、新しいデータベースエントリに効果的に注釈を付ける半自動の画像ラベリングアプローチも作成しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-14">
        <br><font color="black">2019-10-14</font>
      </time>
    </span>
</section>
<!-- paper0: Recurrent Exposure Generation for Low-Light Face Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_46.html">
      <font color="black">Recurrent Exposure Generation for Low-Light Face Detection</font>
    </a>
  </h2>
  <font color="black">REGは、さまざまな露出設定に対応する漸進的かつ効率的な中間画像を生成し、そのような疑似露出はMEDによって融合されて、さまざまな照明条件にわたって顔を検出します。顔検出に合わせて調整するためのREGコンポーネントのエンドツーエンドの学習。完全なアブレーション研究により、DARK FACE低光顔ベンチマークでREGDetをテストしました。マージン、無視できる余分なパラメーターのみ。 
[要約]自然な解決策は、多重露光からアイデアを借りることであり、複数のショットをキャプチャして、厳しい条件下で十分に露光された画像を取得します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-label Thoracic Disease Image Classification with Cross-Attention
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_47.html">
      <font color="black">Multi-label Thoracic Disease Image Classification with Cross-Attention
  Networks</font>
    </a>
  </h2>
  <font color="black">これらの課題を克服するために、このホワイトペーパーでは、胸部X線画像から自動化された胸部疾患分類のためのクロスアテンションネットワーク（CAN）の新しいスキームを提案します。画像レベルの注釈のみによる注意。放射線画像の自動化された疾患分類は、臨床診断と治療計画をサポートする有望な技術として浮上しています。また、クロスエントロピー損失を超えて、注意横断プロセスを支援する新しい損失関数を設計します。クラスと各クラス内の簡単に支配されるサンプルとの間の不均衡を克服することができます。 
[ABSTRACT]リアル-ac画像分類タスクは非常に困難です。ラベル付けされたデータがマルチラベルである場合にトレーニングデータを収集するのははるかにコストがかかります。そしてもっと真剣に簡単なクラスからのサンプルがしばしば支配する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: REPrune: Filter Pruning via Representative Election -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_48.html">
      <font color="black">REPrune: Filter Pruning via Representative Election</font>
    </a>
  </h2>
  <font color="black">また、REPruneは41.8％以上のFLOPを削減し、ImageNetのResNet-18で1.67％のトップ1検証損失を削減します。経験的に、REPruneは49％以上のFLOPを削減し、CIFAR-10のResNet-110で0.53％の精度向上を実現します。また、この方法では精度がより迅速に回復し、微調整中にフィルターのシフトを小さくする必要があります。 
[ABSTRACT] repruneは49％以上のフロップを削減し、0。53％の精度をresnetで-110はcifar-10.itはまた、再プルーニングプロセスの他のフィルターの数を減らします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Preset: Blending and Retouching Photos with Color Style Transfer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_49.html">
      <font color="black">Deep Preset: Blending and Retouching Photos with Color Style Transfer</font>
    </a>
  </h2>
  <font color="black">実験結果は、私たちのディーププリセットが、定量的および定性的に、カラースタイル転送の以前の作品よりも優れていることを示しています。1）自然な色のコンテンツからレタッチされた参照への色変換を表す機能を一般化し、それを次のコンテキスト機能にブレンドするように設計されています。コンテンツ、2）適用された低レベルの色変換方法のハイパーパラメーター（設定またはプリセット）を予測します。3）コンテンツをスタイル設定して、参照として同様の色スタイルを使用します。ただし、最近の画像スタイル転送の作業は使いすぎています。 
[ABSTRACT]これにより、使いすぎて多くの色が作成されます。ポートレートなどのデリケートなケースではさらに悪化します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: BSD-GAN: Branched Generative Adversarial Network for Scale-Disentangled
  Representation Learning and Image Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_50.html">
      <font color="black">BSD-GAN: Branched Generative Adversarial Network for Scale-Disentangled
  Representation Learning and Image Synthesis</font>
    </a>
  </h2>
  <font color="black">BSD-GANの重要な機能は、トレーニング画像の解像度が高くなり、より細かい機能が明らかになるにつれて、ネットワークの幅と深さの両方を徐々にカバーして、複数のブランチでトレーニングされることです。このような明示的なサブの結果ベクトル指定は、さまざまな機能スケールをモデル化する潜在（サブベクトル）コードを直接操作し、さらには組み合わせることができるということです。広範な実験は、画像表現のスケール解きほぐし学習と新規画像コンテンツの合成におけるトレーニング方法の効果を実証します。余分なラベルがあり、合成された高解像度画像の品質を損なうことはありません。具体的には、BSD-GANのジェネレーターネットワークへの入力としての各ノイズベクトルは、それぞれに対応するいくつかのサブベクトルに故意に分割され、学習するようにトレーニングされています、特定のスケールでの画像表現。 
[ABSTRACT] bsd-ganの主要な機能は、複数のブランチでトレーニングされ、ネットワークの幅と深さの両方を段階的にカバーすることです。トレーニング画像の解像度が高くなるにつれて、より細かいスケールの機能が明らかになります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-03-22">
        <br><font color="black">2018-03-22</font>
      </time>
    </span>
</section>
<!-- paper0: Distractor-Aware Neuron Intrinsic Learning for Generic 2D Medical Image
  Classifications -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_51.html">
      <font color="black">Distractor-Aware Neuron Intrinsic Learning for Generic 2D Medical Image
  Classifications</font>
    </a>
  </h2>
  <font color="black">医療画像ベンチマークデータセットでの広範な実験は、提案された方法が最先端のアプローチに対して有利に機能することを示しています。この論文では、ニューロン固有の学習方法を提案することにより、CNN機能空間からのディストラクタを探索します。新規の損失を元の分類損失と組み合わせて、逆伝播によってネットワークパラメータを更新します。 
[要旨]基本的な分析アプローチは医療画像の分類です。これらの画像は、皮膚病変の診断、糖尿病性網膜症、および癌に役立ちます。これらのディストラクタは、深い分類器にとって重要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-20">
        <br><font color="black">2020-07-20</font>
      </time>
    </span>
</section>
<!-- paper0: Neural Mesh Flow: 3D Manifold Mesh Generationvia Diffeomorphic Flows -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_52.html">
      <font color="black">Neural Mesh Flow: 3D Manifold Mesh Generationvia Diffeomorphic Flows</font>
    </a>
  </h2>
  <font color="black">コードとデータがリリースされます。以前の方法では、幾何学的精度は高いが多様性が低いメッシュが生成されます。NMFのトレーニングは、メッシュベースの明示的な正則化を必要としないため、最新の方法に比べて簡単です。 
[ABSTRACT]メッシュは「不適切」と呼ばれるため、それらが表す実際のオブジェクトのように世界とやり取りできます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Feature-metric Loss for Self-supervised Learning of Depth and Egomotion -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_53.html">
      <font color="black">Feature-metric Loss for Self-supervised Learning of Depth and Egomotion</font>
    </a>
  </h2>
  <font color="black">この作業では、フィーチャーメトリック損失が提案され、フィーチャー表現で定義されます。ここで、フィーチャー表現は自己監視された方法で学習され、1次導関数と2次導関数の両方によって正則化されて、損失ランドスケープを制約して適切な収束を形成します。流域..特に、この手法は、KITTIの最先端の手法を深度推定のために$ \ delta_1 $で測定して0.885から0.925に改善し、視覚オドメトリの以前の手法を大幅に上回ります。包括的な実験と可視化による詳細分析提案された機能メトリック損失の有効性を示します。 
[ABSTRACT]以前の方法では、ロスランドスケープが問題になることがよくあります。これには、特徴の少ないピクセルの経過時間または複数の極小値が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Compose Hypercolumns for Visual Correspondence -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_54.html">
      <font color="black">Learning to Compose Hypercolumns for Visual Correspondence</font>
    </a>
  </h2>
  <font color="black">オブジェクト検出のマルチレイヤー機能構成と分類の適応推論アーキテクチャーの両方に触発された、提案された方法は、ダイナミックハイパーピクセルフローと呼ばれ、深い畳み込みニューラルネットワークから少数の関連レイヤーを選択することで、ハイパーコラム機能をオンザフライで構成することを学びます。 。この作業では、一致する画像に条件付けされた関連レイヤーを活用することで効果的な機能を動的に構成する視覚的対応の新しいアプローチを紹介します。ただし、これらのモデルは、通常、特定のレベルを使用するという意味でモノリシックで静的です。たとえば、最後のレイヤーの出力などの機能の組み合わせで、一致する画像に関係なくそれに準拠します。 
[要旨]提案された方法は、動的ハイパーピクセルフローと呼ばれ、少数の関連レイヤーを選択することで、ハイパーカラム機能をオンザフライで構成することを学習します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Object-Centric Multi-View Aggregation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_55.html">
      <font color="black">Object-Centric Multi-View Aggregation</font>
    </a>
  </h2>
  <font color="black">体積特徴グリッドの形で半陰的3D表現を計算するために、オブジェクトのスパースセットの集合を集約するためのアプローチを提示します。ピクセルから正準座標系への対称性を意識したマッピングの計算を示します。見えない領域に情報をより適切に伝播し、推論中のポーズのあいまいさを確実に克服することができます。このアプローチの鍵は、カメラポーズを明示的に推定せずにビューを持ち上げることができるオブジェクト中心の標準3D座標系です。結合-可変数のビューに対応でき、ビューの順序に依存しない方法で。 
[ABSTRACT]カメラポーズを明示的に推定せずに、ビューの順序を上げてから、組み合わせることができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-20">
        <br><font color="black">2020-07-20</font>
      </time>
    </span>
</section>
<!-- paper0: Optimization of data-driven filterbank for automatic speaker
  verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_56.html">
      <font color="black">Optimization of data-driven filterbank for automatic speaker
  verification</font>
    </a>
  </h2>
  <font color="black">提案されたフィルターバンクは、一般的に使用されるメルフィルターバンクや既存のデータ駆動型フィルターバンクよりもスピーカーの識別能力が高いことを示します。さまざまな分類子バックエンドを使用して、さまざまなコーパスで自動スピーカー検証（ASV）実験を行います。 VoxCeleb1と人気のあるi-vectorバックエンドでは、MFCCと比べて等エラー率（EER）が9.75％改善されています。 
[ABSTRACT]提案されたフィルターバンクは、melフィルターバンクよりもスピーカーの識別力が高くなります。新しい方法では、特定の音声データからフィルターパラメーターを最適化します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Representative-Discriminative Learning for Open-set Land Cover
  Classification of Satellite Imagery -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_57.html">
      <font color="black">Representative-Discriminative Learning for Open-set Land Cover
  Classification of Satellite Imagery</font>
    </a>
  </h2>
  <font color="black">代表的な弁別的なオープンセット認識（RDOSR）フレームワークを提案します。これは、1）生の画像空間から埋め込み特徴空間にデータを投影し、類似のクラスの区別を容易にし、さらに2）変換を通じて代表的な能力と弁別能力の両方を強化します。いわゆるアバンダンススペースです。本質的に分類の問題ですが、未知のクラスと既知のクラスを区別するために、データの代表的な側面と識別的な側面の両方を活用する必要があります。衛星画像の土地被覆分類は、地球の分析に向けた重要なステップです。表面。 
[ABSTRACT]既存のモデルは、トレーニングクラスとテストクラスの両方が同じラベルセットに属しているクローズドセット設定を前提としています。ただし、テスト中に不明なクラスのサンプルを識別できる必要はないと述べていますが、既知のクラスのパフォーマンスを維持します。また、rgb画像を使用したオープンセット分類タスクで有望な結果を達成することにより、提案されたアプローチの一般性を示します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-person 3D Pose Estimation in Crowded Scenes Based on Multi-View
  Geometry -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_58.html">
      <font color="black">Multi-person 3D Pose Estimation in Crowded Scenes Based on Multi-View
  Geometry</font>
    </a>
  </h2>
  <font color="black">この論文では、複数の人の3Dポーズ推定の定式化から出発し、代わりに群集ポーズ推定として再定式化します。まばらな群集シーンでのこの定式化の満足のいくパフォーマンスにもかかわらず、その効果は、主にあいまいさの2つの原因.. 1つ目は、関節とエピポーラ線の間のユークリッド距離によって提供される単純なキューから生じる人間の関節の不一致です。 
[ABSTRACT]最小二乗最小化としての問題の素朴な具体化による堅牢性の欠如。人間の骨の欠如は、「堅牢性」の欠如のせいです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Sparse Nonnegative Tensor Factorization and Completion with Noisy
  Observations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_59.html">
      <font color="black">Sparse Nonnegative Tensor Factorization and Completion with Noisy
  Observations</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、3次テンソルの部分的およびノイズの多い観測から、スパース非負テンソル因数分解および完了問題を研究します。加法ガウスノイズ、加法ラプラスノイズ、およびポアソン観測を含む特定のノイズ分布下の詳細な誤差範囲を導出できます。テンソルのこれらの理論的な結果は、行列で得られた結果よりも優れており、これは、完了とノイズ除去に非負のスパーステンソルモデルを使用する利点を示しています。 
[ABSTRACT]これはスパース性と非負性のために、下にある衣服は1つのまばらな非負と1つの非負の最大量に分解されます。ミニマックス、ミニマックス、ミニマックスは、対数係数までの確立された上限と一致することが示されています。加速度の大きさ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: AinnoSeg: Panoramic Segmentation with High Perfomance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_60.html">
      <font color="black">AinnoSeg: Panoramic Segmentation with High Perfomance</font>
    </a>
  </h2>
  <font color="black">さらに、それらは、オブジェクトのセグメンテーションの閉塞、オブジェクトのセグメンテーションの少なさ、オブジェクトのセグメンテーションにおける境界ピクセルなどの精度を含む問題を解決できません。（b）損失関数を修正して、複数のオブジェクトの境界ピクセルを考慮できるようにします。画像..（d）マルチスケールのトレーニングと推論の使用。 
[ABSTRACT] cnnネットワークの開発により、パノラマタスクの開発に成功しました。ただし、現在のパノラマデータはコンテキストに関係しています。代わりに、画像の詳細が十分に処理されていません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Person Re-identification Models from Videos with Weak
  Supervision -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_61.html">
      <font color="black">Learning Person Re-identification Models from Videos with Weak
  Supervision</font>
    </a>
  </h2>
  <font color="black">より正確なフレームレベルのアノテーションとは対照的に、ビデオに表示される人物のアイデンティティ。具体的には、まずビデオの人物の再識別タスクを複数インスタンスの学習設定にキャストします。この設定では、ビデオの人物の画像がバッグに収集されます。ほとんどの人の再識別方法は、監視された手法であるため、大量のアノテーション要件の負担に悩まされています。 
[ABSTRACT]教師なしの方法は、ラベル付きデータの必要性を克服しますが、教師付きの代替方法ではパフォーマンスが低下します。さらに、このようなビデオレベルのラベルを使用した個人の再識別における複数インスタンス認識学習フレームワークを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Generate Customized Dynamic 3D Facial Expressions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_62.html">
      <font color="black">Learning to Generate Customized Dynamic 3D Facial Expressions</font>
    </a>
  </h2>
  <font color="black">私たちの知る限りでは、これは4Dの表情合成の問題に取り組む最初の研究です。さらに、3Dメッシュの処理は、画像などのグリッド状の構造に存在するデータと比較して、重要な作業です。グラフの畳み込みによるメッシュ処理の最近の進歩を考慮して、ローカル頂点の順序を利用してメッシュ構造に直接作用する最近導入された学習可能な演算子を利用します。 
[要約]このペーパーでは、4dの表情に特に焦点を当てた3d画像からビデオへのアニメーションを研究しました。この目的のために、ディープメッシュエンコーダー-アーキテクチャーのようなデコーダーを使用して、単一のニュートラルフレームと式の識別</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-19">
        <br><font color="black">2020-07-19</font>
      </time>
    </span>
</section>
<!-- paper0: Visual Relation Grounding in Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_63.html">
      <font color="black">Visual Relation Grounding in Videos</font>
    </a>
  </h2>
  <font color="black">このタスクの課題には次のものが含まれますが、これらに限定されません。（1）サブジェクトとオブジェクトの両方を時空間的にローカライズしてクエリの関係を確立する必要がある。 （2）ビデオの視覚的関係の時間的な動的性質をキャプチャするのは困難です。 （3）接地は、空間と時間を直接監視することなく達成されるべきです。関係を接地するために、関係の参加と再構築を通じて、構築された階層的な時空間領域グラフ上で領域の2つのシーケンスを協調的に最適化することにより、課題に取り組みます。ここでは、視覚エンティティ間の空間的注意シフトによるメッセージパッシングメカニズムをさらに提案します。このホワイトペーパーでは、ビデオ（vRGV）の視覚的関係接地という新しいタスクを探索します。 
[要約]タスクは、ビデオの主題-述語-オブジェクトの形式で特定の関係を地域化することを目的としています。関係を確立するために、構築された空間的空間-時間的領域にわたって2つの領域シーケンスを協調的に最適化することにより、課題に取り組みます。グラフ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-17">
        <br><font color="black">2020-07-17</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Monocular Visual Odometry via Self-Supervised Long-Term
  Modeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_64.html">
      <font color="black">Learning Monocular Visual Odometry via Self-Supervised Long-Term
  Modeling</font>
    </a>
  </h2>
  <font color="black">以前の幾何学的システムに触発され、時間的に離れた（たとえば、O（100））フレームを組み込んだ新しい損失を通して、ネットワークがトレーニング中に小さな時間ウィンドウを超えて見えるようにします。GPUメモリの制約を考慮して、ステージを提案します-賢明なトレーニングメカニズム。第1ステージはローカルタイムウィンドウで動作し、第2ステージは第1ステージの機能を考慮して「グローバルな」損失でポーズを調整します。KITTIやTUM RGBなどのいくつかの標準VOデータセットで競争力のある結果を示しますD. 
[要約]この論文では、純粋に自己監視された損失でネットワークをトレーニングするvo.weの自己監視学習方法を提案します。これらには、幾何学的voのループ閉鎖モジュールを模倣するサイクル整合性損失が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: 3D-CVF: Generating Joint Camera and LiDAR Features Using Cross-View
  Spatial Feature Fusion for 3D Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_65.html">
      <font color="black">3D-CVF: Generating Joint Camera and LiDAR Features Using Cross-View
  Spatial Feature Fusion for 3D Object Detection</font>
    </a>
  </h2>
  <font color="black">まず、この方法では自動較正された投影を使用して、2Dカメラ機能を、鳥瞰図（BEV）ドメインのLiDAR機能に最も対応する滑らかな空間機能マップに変換します。融合によって提示される課題の1つカメラとLiDARは、各モダリティから取得された空間的特徴マップが、カメラと世界座標の大幅に異なるビューによって表されることです。したがって、情報を失うことなく2つの異種の特徴マップを組み合わせるのは簡単な作業ではありません。次に、カメラとLiDARの特徴の融合は、後続の提案の調整段階でも行われます。 
[要約] 2つのモダリティを融合すると、3Dオブジェクト検出の精度と堅牢性が向上することが期待されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-27">
        <br><font color="black">2020-04-27</font>
      </time>
    </span>
</section>
<!-- paper0: Graph-PCNN: Two Stage Human Pose Estimation with Graph Pose Refinement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_66.html">
      <font color="black">Graph-PCNN: Two Stage Human Pose Estimation with Graph Pose Refinement</font>
    </a>
  </h2>
  <font color="black">最初の段階では、ヒートマップ回帰ネットワークを適用して大まかなローカリゼーション結果を取得し、ガイドポイントと呼ばれる一連の提案キーポイントをサンプリングします。2番目のステージでは、各ガイドポイントについて、ローカリゼーションによって異なる視覚的特徴を抽出しますサブネット..より正確なローカリゼーション結果を取得するために、ガイドポイント間の関係がグラフポーズ改良モジュールによって探索されます。 
[要旨]キーポイントのローカライズの最終座標は、ヒートマップを直接デコードして取得します。最初の段階では、ヒートマップ圧縮ネットワークを適用して、大まかなローカライズ結果を取得します。「可視化」と呼ばれる提案キーポイントのセットがサンプリングされます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Scene Text Image Super-Resolution in the Wild -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_67.html">
      <font color="black">Scene Text Image Super-Resolution in the Wild</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、野生での低解像度のテキスト認識は解決されていないため、さらなる研究努力が必要であることを示唆しています。直観的な解決策は、超解像度（SR）技術を前処理として導入することです。 TextZoomでのLR画像の認識精度を高める点で、7つの最新のSRメソッドより優れています。 
[ABSTRACT] textzoomはtsrn.itと呼ばれる新しいテキスト超解像度ネットワークで、3つの新しいモジュールを使用して認識精度を向上させます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-07">
        <br><font color="black">2020-05-07</font>
      </time>
    </span>
</section>
<!-- paper0: Relative Pose Estimation for Multi-Camera Systems from Affine
  Correspondences -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_68.html">
      <font color="black">Relative Pose Estimation for Multi-Camera Systems from Affine
  Correspondences</font>
    </a>
  </h2>
  <font color="black">また、たとえばIMUからの既知の重力ベクトルを持つ2つのACを使用した最小解を提案します。ソルバーは、KITTIベンチマークの合成データと実際のシーンの両方でテストされます。制約を使用すると、システムの6DOF相対ポーズ、つまり3D回転と並進を回復するには、最低2つのACで十分です。 
[要約]提案された方法は、1つのACと2つのACを使用したソリューションを含みます。これらは、縮退したケースを解決するために使用されます。推定ポーズの精度が最新の技術よりも優れていることが示されています-最新技術</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Video Representation Learning by Recognizing Temporal Transformations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_69.html">
      <font color="black">Video Representation Learning by Recognizing Temporal Transformations</font>
    </a>
  </h2>
  <font color="black">私たちの表現は、人間の注釈なしでデータから学習することができ、オブジェクトの動きを正確に区別する必要があるアクション認識などのタスクの小さなラベル付きデータセットでニューラルネットワークのトレーニングを大幅に促進します。したがって、以下を紹介します。変換：順方向逆再生、ランダムフレームスキップ、均一フレームスキップ..私たちは、ニューラルネットワークをトレーニングして、時間的に変換されたバージョンからビデオシーケンスを区別することにより、人間の注釈なしでモーションの正確な学習を促進します。 
[要旨]私たちの表現は、人間の注釈なしでフレームから学習できます。これらは、小さなラベル付きデータセットでのニューラルネットワークのトレーニングを大幅に強化します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: NPCFace: A Negative-Positive Cooperation Supervision for Training
  Large-scale Face Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_70.html">
      <font color="black">NPCFace: A Negative-Positive Cooperation Supervision for Training
  Large-scale Face Recognition</font>
    </a>
  </h2>
  <font color="black">ただし、ハードポジティブとハードネガティブの相関関係、およびポジティブロジットのマージンとネガティブロジットのマージンの関係は見落とされます。さらに、ネガティブエンファシスは、安定した収束と柔軟なパラメーターを実現するために改良された定式化で実装されます。設定。大規模な顔認識のさまざまなベンチマークでのアプローチの有効性を検証し、特に低いFAR範囲で以前の方法よりも優れています。特に、低い偽受け入れ率（FAR）の範囲では、さまざまな難しいケースがあります。両方のポジティブ（$ \ textit {ie 
[ABSTRACT]両方のポジティブのハードケースは、低い誤受入率で発生します。既存のトレーニング方法は、ポジティブロジットまたはネガティブロジットのいずれかにマージンを追加することで課題に対処します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-20">
        <br><font color="black">2020-07-20</font>
      </time>
    </span>
</section>
<!-- paper0: Lightweight image super-resolution with enhanced CNN -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_71.html">
      <font color="black">Lightweight image super-resolution with enhanced CNN</font>
    </a>
  </h2>
  <font color="black">これらの問題を解決するために、3つの連続したサブブロック、情報抽出および拡張ブロック（IEEB）、再構成ブロック（RB）および情報改良ブロック（IRB）を備えた軽量の拡張SR CNN（LESRCNN）を提案します。 LESRCNNは、さまざまな縮尺のモデルによって高品質の画像を取得できます。取得された冗長な情報を削除するために、IEEBでは異種アーキテクチャが採用されています。 
[ABSTRACT]提案されたlesrcnnは、さまざまなスケールのモデルによって画像を取得できます。lesrcnnは低解像度の再構成機能を使用します。これらの変更は、メモリのメモリ能力を高めるために使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-08">
        <br><font color="black">2020-07-08</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Continual Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_72.html">
      <font color="black">Adversarial Continual Learning</font>
    </a>
  </h2>
  <font color="black">私たちのコードは\ url {https://github.com/facebookresearch/Adversarial-Continual-Learning}で入手できます。私たちはハイブリッドアプローチが忘却を回避するのに効果的であることを示し、アーキテクチャベースとメモリベースの両方より優れていることを示しています。画像分類における単一のデータセットと一連の複数のデータセットのクラスの漸進的な学習のクラスへのアプローチ..共有機能が忘れる傾向が大幅に少ないことを示し、タスク不変式の素な表現を学習する新しいハイブリッド継続学習フレームワークを提案します一連のタスクを解決するために必要なタスク固有の機能
[ABSTRACT]私たちのモデルは、アーキテクチャの成長を組み合わせて、タスクの忘れを防止します-特定のスキルと共有されたスキルを維持するためのエクスペリエンスリプレイアプローチ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-21">
        <br><font color="black">2020-03-21</font>
      </time>
    </span>
</section>
<!-- paper0: Learning from Noisy Labels with Deep Neural Networks: A Survey -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_73.html">
      <font color="black">Learning from Noisy Labels with Deep Neural Networks: A Survey</font>
    </a>
  </h2>
  <font color="black">次に、46の最先端の堅牢なトレーニング方法の包括的なレビューを提供します。これらすべては、方法論の違いに従って7つのグループに分類され、その後、それらの優位性を評価するために使用される6つのプロパティの体系的な比較が続きます。ノイズの多いラベルは、ディープニューラルネットワークの汎化パフォーマンスを大幅に低下させます。ノイズの多いラベル（堅牢なトレーニング）からの学習は、最新のディープラーニングアプリケーションで重要なタスクになりつつあります。次に、パブリックノイズの多いデータセットや評価指標など、一般的に使用される評価方法をまとめます。 
[ABSTRACT]高品質のリサーチが不足しているため、データラベルの品質が懸念されています。データ品質が低いため、品質の低いデータが心配されています。リサーチの質が低いため、データラベルが心配されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-16">
        <br><font color="black">2020-07-16</font>
      </time>
    </span>
</section>
<!-- paper0: Attention-Guided Generative Adversarial Network to Address Atypical
  Anatomy in Modality Transfer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_74.html">
      <font color="black">Attention-Guided Generative Adversarial Network to Address Atypical
  Anatomy in Modality Transfer</font>
    </a>
  </h2>
  <font color="black">15人の脳癌患者の実験結果は、注意GANが既存のsynCTモデルを上回り、85.22 $ \ pm $ 12.08、232.41 $ \ pm $ 60.86、246.38 $ \ pm $ 42.67の平均MAEを達成したことを示しています。それぞれ、頭、骨、空気の領域。この論文では、非定型の解剖学に対処するための入力としてT1強調MRI画像を使用して正確なsynCTを生成する、新しい空間注意誘導生成敵対的ネットワーク（注意GAN）モデルを提案します。ただし、非定型の解剖学的構造を含む医用画像のクラスソリューションの開発は、依然として大きな制限です。 
[ABSTRACT]研究により、非定型の解剖学的構造を含む分析画像には依然として主要な制限があることが示されています。これらには、超高血圧の高血圧が含まれます。高血圧を発症させるには、いくつかの研究が必要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-27">
        <br><font color="black">2020-06-27</font>
      </time>
    </span>
</section>
<!-- paper0: Globally Optimal Segmentation of Mutually Interacting Surfaces using
  Deep Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_75.html">
      <font color="black">Globally Optimal Segmentation of Mutually Interacting Surfaces using
  Deep Learning</font>
    </a>
  </h2>
  <font color="black">スペクトラルドメイン光干渉断層法（SD-OCT）網膜層セグメンテーションと血管内超音波（IVUS）血管壁セグメンテーションに関する実験は、非常に有望な結果を示しました。機能学習機能..残念ながら、医用画像ではトレーニングデータが不足しているため、DLネットワークでは、表面の相互作用を含むターゲットサーフェスのグローバル構造を暗黙的に学習することは重要です。 
[ABSTRACT]ディープラーニング（dl）は、優れた特徴学習構造により、医用画像セグメンテーションのための強力なツールとして浮上しています。この複雑な作業では、グラフモデルの表面影響関数を章に分け、dlを活用してそのパラメーターを学習することを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-02">
        <br><font color="black">2020-07-02</font>
      </time>
    </span>
</section>
<!-- paper0: Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive
  Review -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_76.html">
      <font color="black">Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive
  Review</font>
    </a>
  </h2>
  <font color="black">したがって、対策を検討し、それらの長所と短所を比較して分析します。したがって、各分類の下での攻撃をくまなく調べます。また、i）ディープラーニングモデルの知的財産を保護するために検討されているバックドア攻撃の裏側についても検討しました。 ii）敵対的な攻撃例をトラップするハニーポットとして機能する。 
[要約]対応策は広く、6つの攻撃に形式化されてリストされています。これらには、コードポイズニング、アウトソーシング、事前トレーニング、データ収集、協調学習、導入後の攻撃が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Visual Distortion in Black-Box Attacks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_77.html">
      <font color="black">Towards Visual Distortion in Black-Box Attacks</font>
    </a>
  </h2>
  <font color="black">敵対的な例と元の画像との間の知覚距離を測定する定量化された視覚的な歪みが損失に導入され、対応する微分不可能な損失関数の勾配は、学習されたノイズ分布からサンプリングノイズによって近似されます。 ImageNetに対する攻撃の有効性。コードはhttps://github.com/Alina-1997/visual-distortion-in-attackから入手できます。 
[要約]ソフトウェアはgithubで入手できます。 com / alina-1997</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse
  Coding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_78.html">
      <font color="black">Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse
  Coding</font>
    </a>
  </h2>
  <font color="black">この発見を活用して、変化する因子の数を想定せずに生成因子を分解するために時間的に隣接する観測でスパースを使用する教師なし表現学習のモデルであるSlowVAEを提示します。基礎となる要素の非線形分解を達成する教師なし学習モデルを構築します。自然主義的なビデオにおける変動の要因..私たちは識別可能性の証拠を提供し、モデルがいくつかの確立されたベンチマークデータセットのもつれのない表現を確実に学習することを示します。 
[ABSTRACT]このモデルは、確立されたいくつかのベンチマークデータセットのもつれのない表現を確実に学習します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Garment Design with Generative Adversarial Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_79.html">
      <font color="black">Garment Design with Generative Adversarial Networks</font>
    </a>
  </h2>
  <font color="black">具体的には、属性GAN（AttGAN）---人間の顔の属性編集に成功したことが証明された生成モデル---は、衣服の視覚的属性の自動編集に利用され、大規模なファッションデータセットでテストされています。デザインコンセプトの属性レベルの編集のためのGANの概要、および今後の作業で対処するいくつかの重要な制限と研究の質問を強調します。このホワイトペーパーでは、デザインコンセプトの属性レベルの自動編集のための生成的敵対ネットワーク（GAN）の機能について説明します。 
[ABSTRACT]デザイナーは、視覚的な編集のための新しいコンセプトを作成しました。彼らは、自動化された属性を開発したと言います-デザインコンセプトのレベル編集</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Self-Supervised Monocular Depth Estimation: Solving the Dynamic Object
  Problem by Semantic Guidance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_80.html">
      <font color="black">Self-Supervised Monocular Depth Estimation: Solving the Dynamic Object
  Problem by Semantic Guidance</font>
    </a>
  </h2>
  <font color="black">いくつかのベンチマーク、特にテスト時間の調整なしですべてのベースラインを超える固有分割で、このメソッドのパフォーマンスを示します。具体的には、（i）（教師あり）セマンティックセグメンテーションの相互に有益なクロスドメイントレーニングを提案し、タスク固有のネットワークヘッドによる自己監視深度推定、（ii）移動するDCオブジェクトが測光損失を汚染するのを防ぐためのガイダンスを提供するセマンティックマスキングスキーム、および（iii）移動しないDCオブジェクトを含むフレームの検出方法DCオブジェクトの深度を学習できます。自己監視単眼深度推定は、単一のカメラ画像から3Dシーン情報を取得する強力な方法を提供します。これは、LiDARセンサーなどの深度ラベルを必要とせずに、任意の画像シーケンスでトレーニングできます。 
[ABSTRACT]移動する車や歩行者などの移動するダイナミッククラス（dc）オブジェクトを処理するための、自己管理の意味論的に誘発された深度推定（sgdepth）メソッド。いくつかのタスクの詳細について、このメソッドのパフォーマンスを示しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Fast, Accurate and Lightweight Super-Resolution with Neural Architecture
  Search -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_81.html">
      <font color="black">Fast, Accurate and Lightweight Super-Resolution with Neural Architecture
  Search</font>
    </a>
  </h2>
  <font color="black">また、進化的計算と強化学習から利益を得るハイブリッドコントローラーに基づいて、マイクロレベルとマクロレベルの両方で弾性検索戦術を提案します。具体的には、超解像度を多目的アプローチで処理します。最近の貢献は手動で苦労していますこのバランスを最大化しながら、私たちの仕事はニューラルアーキテクチャ検索で同じ目標を自動的に達成します。 
[ABSTRACT]一連の調査は、制約されたリソースに適さないはるかに深い層を使用することにより、ピーク信号ノイズ比（psnr）を改善することに焦点を当てています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-01-22">
        <br><font color="black">2019-01-22</font>
      </time>
    </span>
</section>
<!-- paper0: Lymphocyte counting -- Error Analysis of Regression versus Bounding Box
  Detection Approaches -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_82.html">
      <font color="black">Lymphocyte counting -- Error Analysis of Regression versus Bounding Box
  Detection Approaches</font>
    </a>
  </h2>
  <font color="black">検出モデルは、トレーニングにおいてより気まぐれで敏感ですが、最適な過小評価に対してより堅牢です。分類による直接推定と、サンプルサイズが比較的小さいデータセットのバウンディングボックス予測モデルに対する回帰とを比較します。すべてのモデルがかなりの過小評価バイアスに悩まされているMSEエラーの詳細な分析。 
[要約]サンプルサイズが比較的小さいデータセットの境界ボックス予測モデルと分析を比較します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-modal Transformer for Video Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_83.html">
      <font color="black">Multi-modal Transformer for Video Retrieval</font>
    </a>
  </h2>
  <font color="black">詳細については、http：//thoth.inrialpes.fr/research/MMTを参照してください。この新しいフレームワークを使用すると、3つのデータセットでビデオを取得するための最先端の結果を確立できます。トランスフォーマーアーキテクチャは、時間情報をエンコードしてモデル化します。 
[ABSTRACT]マルチモーダルトランスフォーマーを提示して、ビデオのさまざまなモダリティを共同でエンコードし、それぞれが他のモダリティに参加できるようにします。自然言語側では、埋め込み言語とともに共同で言語を最適化するためのベストプラクティスを調査しますマルチコードモデル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: IITK at SemEval-2020 Task 8: Unimodal and Bimodal Sentiment Analysis of
  Internet Memes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_84.html">
      <font color="black">IITK at SemEval-2020 Task 8: Unimodal and Bimodal Sentiment Analysis of
  Internet Memes</font>
    </a>
  </h2>
  <font color="black">私たちの仕事は、さまざまなモダリティの組み合わせに関係するすべてのタスクに関連しています。私たちの結果は、テキストのみのアプローチ、つまり入力としてWord2vec埋め込みを使用した単純なフィードフォワードニューラルネットワーク（FFNN）が他のすべてより優れていることを示しています。 Na \ &quot;ive Bayes分類器からトランスフォーマーベースのアプローチまで、バイモーダル（テキストおよびイメージ）およびユニモーダル（テキストのみ）の手法を検討してください。
[ABSTRACT]ミームは前者のクラスに属する最も人気のある形式です.memesは感情的な内容と感情に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: DeepMSRF: A novel Deep Multimodal Speaker Recognition framework with
  Feature selection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_85.html">
      <font color="black">DeepMSRF: A novel Deep Multimodal Speaker Recognition framework with
  Feature selection</font>
    </a>
  </h2>
  <font color="black">DeepMSRFは2つのストリームVGGNETを使用して両方のモダリティでトレーニングし、話者のIDを正確に認識することができる包括的なモデルに到達します。DeepMSRFは、2つのモダリティ、つまり話者の音声と顔の画像の機能をフィードすることで実行されます。ビデオストリームでは、表情、感情、性別などの高レベルの話者の特徴を抽出することにより、リッチな機械学習モデルを取得するための重要な研究が行われています。 
[要約] 2つのモダリティの機能をフィードすることにより、deepmsrfを実行します。これには、スピーカーの音声と顔の画像の学習が含まれます。結果は、deepmsrfが少なくとも3％の精度でシングルモダリティのスピーカー認識方法より優れていることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Novel View Synthesis on Unpaired Data by Conditional Deformable
  Variational Auto-Encoder -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_86.html">
      <font color="black">Novel View Synthesis on Unpaired Data by Conditional Deformable
  Variational Auto-Encoder</font>
    </a>
  </h2>
  <font color="black">2Dオプティカルフローのように、フィーチャを変形するためにいくつかの変位マップのペアを生成します。新しいビューの合成では、多くの場合、ソースビューとターゲットビューの両方からのペアのデータが必要です。ビューと他の要素の間のもつれをさらに確実にするために、コードの敵対的トレーニング。 
[要約]このペーパーでは、CVAE-ganフレームワークの下でのビュー変換モデルを提案しています。ペアのデータは必要ありません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Joint Visual and Temporal Consistency for Unsupervised Domain Adaptive
  Person Re-Identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CV/paper_87.html">
      <font color="black">Joint Visual and Temporal Consistency for Unsupervised Domain Adaptive
  Person Re-Identification</font>
    </a>
  </h2>
  <font color="black">グローバルマルチクラス分類は、メモリベースの時間ガイドクラスタ（MTC）を使用して、ラベル付けされていないトレーニングセット全体のラベルを予測することで実現されます。MTCは、視覚的類似性と時間的一貫性の両方を考慮してマルチクラスラベルを予測し、ラベル予測.. 2つの分類モデルが統合されたフレームワークで結合され、ラベルのないデータを効果的に利用して識別機能を学習します。 
[ABSTRACT]グローバルマルチクラス分類は、メモリベースの時間ガイドクラスタ（mtc）を使用して、ラベル付けされていないトレーニングセット全体のラベルを予測することで実現されます。2つのドメインは、類似性を効果的に対象とする統合フレームワークに結合されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Human Abnormality Detection Based on Bengali Text -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_0.html">
      <font color="black">Human Abnormality Detection Based on Bengali Text</font>
    </a>
  </h2>
  <font color="black">実験では、構築したデータセットで最大89％の精度と92％のF1スコアを達成しました。自発的な会話によって生成されるベンガル語のデータセット（2000文を含む）を作成しました。自然言語処理では、効果的な意味が潜在的にすべての言葉で伝えます。 
[ABSTRACT]人間の異常の検出は、人間とコンピュータの相互作用の分野では十分に調査されていませんが、ほとんどの作業は画像ベースの情報に依存しています。提案されたモデルは、タイプされたベンガル語を分析することで、人が正常または異常のどちらのシステムにいるかを認識できますテキスト</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Analysis and Optimization of Service Delay for Multi-quality Videos in
  Multi-tier Heterogeneous Network with Random Caching -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_1.html">
      <font color="black">Analysis and Optimization of Service Delay for Multi-quality Videos in
  Multi-tier Heterogeneous Network with Random Caching</font>
    </a>
  </h2>
  <font color="black">サービスの遅延を最小限に抑えることを目的として、デバイス間（D2D）で支援された異種ネットワークにおける新しいランダムキャッシング方式を提案します。D2DヘルパーとSBSの制限されたキャッシュサイズを条件とする遅延最小化問題を定式化します。3つと比較キャッシングポリシーのベンチマークとして、提案されたSVCベースのランダムキャッシングスキームは、サービスの遅延を削減するという点で優れています。 
[ABSTRACT]各ビデオファイルはベースレイヤー（bl）と複数のエンハンスメントレイヤー（els）にエンコードされます。提案されたsvcベースのランダムキャッシングスキームは、サービス遅延の削減の点で優れています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: A Framework for Building Closed-Domain Chat Dialogue Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_2.html">
      <font color="black">A Framework for Building Closed-Domain Chat Dialogue Systems</font>
    </a>
  </h2>
  <font color="black">FoodChatbotは、食品およびレストランドメインのアプリケーションであり、ユーザー調査を通じて開発および評価されています。その結果は、HRIChatを使用して合理的に優れたシステムを開発できることを示唆しています。ベースの対話管理と反応ベースの対話管理。 
[ABSTRACT] hrichatは、食品およびレストランドメインのアプリケーションです。ユーザー調査を通じて開発され、評価されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-30">
        <br><font color="black">2019-10-30</font>
      </time>
    </span>
</section>
<!-- paper0: XD at SemEval-2020 Task 12: Ensemble Approach to Offensive Language
  Identification in Social Media Using Transformer Encoders -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_3.html">
      <font color="black">XD at SemEval-2020 Task 12: Ensemble Approach to Offensive Language
  Identification in Social Media Using Transformer Encoders</font>
    </a>
  </h2>
  <font color="black">アンサンブルモデルの場合、これらの個々のモデルから取得された発話表現は連結され、最終的な決定を行うために線形デコーダーに送られます。テストセットでは、90.9％のマクロF1を達成し、以下の高性能システムの1つになります。この共有タスクのサブタスクAの85人の参加者。私たちのアンサンブルモデルは、個々のモデルよりも優れており、開発セットの個々のモデルに比べて最大8.6％の改善を示しています。 
[要約]分析は、アンサンブルモデルが個々のモデルよりも優れていることを示しています。テストセットでは、改善は明らかではありません。テストテストテストセットは、来週完了する予定です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Using Error Decay Prediction to Overcome Practical Issues of Deep Active
  Learning for Named Entity Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_4.html">
      <font color="black">Using Error Decay Prediction to Overcome Practical Issues of Deep Active
  Learning for Named Entity Recognition</font>
    </a>
  </h2>
  <font color="black">それに応じて、データの複数の機能定義サブセットのエラー減衰曲線を推定することにより、透明なバッチアクティブサンプリングフレームワークを提案します。4つの名前付きエンティティ認識（NER）タスクの実験は、提案された方法がブラックボックスNERタガーは、不確実性ベースの方法と組み合わせると、サンプリングプロセスをラベリングノイズに対してより堅牢にすることができます。ただし、実際には、（a）ブラックボックスモデルで不確実性サンプリングを使用できないなど、いくつかの弱点があります。 （b）ノイズのラベル付けに対する堅牢性の欠如、および（c）透明性の欠如。 
[ABSTRACT]提案された方法はブラックボックスnerタガーに使用できます。ただし、実際にはいくつかの弱点があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-17">
        <br><font color="black">2019-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Scientific Discourse Tagging for Evidence Extraction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_5.html">
      <font color="black">Scientific Discourse Tagging for Evidence Extraction</font>
    </a>
  </h2>
  <font color="black">私たちは、その論文の図に示された証拠を説明する主要な研究論文からテキストフラグメントを自動的に抽出する機能を提示します。これは、論文内で行われた科学的議論の原材料を間違いなく提供します。最初に、最先端の技術を実証します。 2つの科学的談話タグ付けデータセットと新しいデータセットへの転送可能性に関する科学的談話タガー。図の範囲から導出された証拠フラグメントを使用して、証拠フラグメントを独立したドキュメントとしてカタログ化、インデックス付け、再利用することで科学的クレームの品質を向上させる可能性を示します。 
[ABSTRACT]全文論文からの情報抽出を使用して科学的議論のモデルを構築することを目指しています。また、主張などの下流のタスクに科学的談話タグを活用することの利点を示しています-抽出と証拠フラグメントの検出</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-10">
        <br><font color="black">2019-09-10</font>
      </time>
    </span>
</section>
<!-- paper0: Morphological Skip-Gram: Using morphological knowledge to improve word
  representation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_6.html">
      <font color="black">Morphological Skip-Gram: Using morphological knowledge to improve word
  representation</font>
    </a>
  </h2>
  <font color="black">このアプローチは、Word2VecとGloVeがワードの内部構造を認識していないことを意味します。新しいアプローチを評価するために、15の異なるタスクを考慮して本質的な評価を行い、結果はFastTextと比較して競争力のあるパフォーマンスを示しています。 、彼らは単語の形態学的情報を無視し、各単語に対して1つの表現ベクトルのみを考慮するため、限られた情報で表現を学習します。 
[ABSTRACT] wordtext分析とQ＆Aは言語埋め込みの例です。これは、word2vecとgloveがwordの内部構造を認識していないためです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-20">
        <br><font color="black">2020-07-20</font>
      </time>
    </span>
</section>
<!-- paper0: Environment-agnostic Multitask Learning for Natural Language Grounded
  Navigation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_7.html">
      <font color="black">Environment-agnostic Multitask Learning for Natural Language Grounded
  Navigation</font>
    </a>
  </h2>
  <font color="black">コードはhttps://github.com/google-research/valan。で入手できます。ただし、既存の方法は、見られた環境でトレーニングデータを過剰に適合させる傾向があり、以前に見えなかった環境でうまく一般化できません。CVDNリーダーボードへの提出により、ホールドアウトテストセットでのNDHタスクの新しい最先端技術。 
[ABSTRACT]訓練されたナビゲーションエージェントは、目に見えない環境のベースラインよりもvlnで16％（成功率の相対測定値）およびndhで120％（目標の進捗状況）優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-01">
        <br><font color="black">2020-03-01</font>
      </time>
    </span>
</section>
<!-- paper0: Computing Conceptual Distances between Breast Cancer Screening
  Guidelines: An Implementation of a Near-Peer Epistemic Model of Medical
  Disagreement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_8.html">
      <font color="black">Computing Conceptual Distances between Breast Cancer Screening
  Guidelines: An Implementation of a Near-Peer Epistemic Model of Medical
  Disagreement</font>
    </a>
  </h2>
  <font color="black">一連の数十の実験で、3つの広いカテゴリーで、最高のモデルの3-4標準偏差という非常に高い統計的有意性レベルで、エキスパートの注釈付きモデルと私たちのコンセプトベースの高い類似性が自動的に作成されたことを示します、計算モデルは偶然ではありません。この作業の可能な拡張についても説明します。この記事は1つのガイドラインセットに焦点を当てたケーススタディですが、提案された方法論は広く適用できます。 
[ABSTRACT]最高のモデルは約70％の類似性を達成しています。最高のモデルは約70％の類似性を達成しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br><font color="black">2020-07-01</font>
      </time>
    </span>
</section>
<!-- paper0: Compositional Generalization in Semantic Parsing: Pre-training vs.
  Specialized Architectures -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_9.html">
      <font color="black">Compositional Generalization in Semantic Parsing: Pre-training vs.
  Specialized Architectures</font>
    </a>
  </h2>
  <font color="black">マスク言語モデル（MLM）の事前トレーニングはプリミティブホールドアウト分割でSCANに触発されたアーキテクチャに匹敵することを示します。中間表現と一緒にMLM事前トレーニングを使用して、CFQ構成一般化ベンチマークで新しい最先端技術を確立します。より複雑な構成タスクでは、事前トレーニングによってパフォーマンスが大幅に改善されるのに対し、同等の事前トレーニングされていないモデルと比較して、SCANまたはアルゴリズム学習の領域で構成の一般化を促進するために提案されたアーキテクチャは、大幅な改善につながりません。 。 
[ABSTRACT]事前トレーニングにより、同等の事前トレーニングされていないモデルと比較して、パフォーマンスが大幅に向上します。スキャンまたはアルゴリズム学習の領域で構成の一般化を促進するために提案されたアーキテクチャは、改善につながりません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-17">
        <br><font color="black">2020-07-17</font>
      </time>
    </span>
</section>
<!-- paper0: Check_square at CheckThat! 2020: Claim Detection in Social Media via
  Fusion of Transformer and Syntactic Features -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_10.html">
      <font color="black">Check_square at CheckThat! 2020: Claim Detection in Social Media via
  Fusion of Transformer and Syntactic Features</font>
    </a>
  </h2>
  <font color="black">最初の問題であるチェック価値予測については、構文機能とトランスフォーマーからのディープトランスフォーマー双方向エンコーダー表現（BERT）の埋め込みの融合を調べて、ツイートのチェック価値を分類します。結果として、大企業や個人がインターネット上でニュースを検証する能力が非常に限られているため、偽のニュースが日常生活に浸透しています。この論文では、事実の一部である2つの問題の解決に焦点を当てます。ソーシャルメディアのコンテンツストリームが増え続ける中で、クレームの事実確認を自動化するのに役立つエコシステムの確認。 
[ABSTRACT]大企業や個人による申し立てを検証するスキルがないため、偽のニュースが日常生活に浸透しています。詳細な機能分析を行い、英語とアラビア語のツイートに最適なモデルを提示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Enabling Robots to Understand Incomplete Natural Language Instructions
  Using Commonsense Reasoning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_11.html">
      <font color="black">Enabling Robots to Understand Incomplete Natural Language Instructions
  Using Commonsense Reasoning</font>
    </a>
  </h2>
  <font color="black">次に、私たちのアプローチは、近くのオブジェクトを観察し、常識推論を活用することにより、命令の欠落情報を埋めます。このホワイトペーパーでは、言語モデルベースの常識推論（LMCR）を紹介します。人間からの自然言語による指導、その周囲の環境の観察、および環境コンテキストと新しい常識推論アプローチを使用して、指導から欠落している情報を自動的に入力します。常識推論を自動的に学習するために、私たちのアプローチはトレーニングによって大規模な非構造化テキストコーパスから知識を抽出します言語モデル。 
[ABSTRACT]私たちのアプローチは、最初に、制約のない自然言語として提供された命令を、ロボットが動詞フレームに解析することで理解できる形式に変換します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-29">
        <br><font color="black">2019-04-29</font>
      </time>
    </span>
</section>
<!-- paper0: Neural Machine Translation with Error Correction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_12.html">
      <font color="black">Neural Machine Translation with Error Correction</font>
    </a>
  </h2>
  <font color="black">具体的には、XLNetからNMTデコーダーに2ストリームの自己注意を導入します。クエリストリームを使用して次のトークンを予測し、コンテンツストリームを使用して以前の予測トークンからのエラー情報を修正します。トレーニング中に予測エラーをシミュレートするためにサンプリングします。ニューラル機械翻訳（NMT）は、トレーニング中に以前のグラウンドトゥルースターゲットトークンを入力として与えられた次のターゲットトークンを生成し、推論中に以前に生成されたターゲットトークンを生成します。これにより、トレーニングと推論の間に矛盾が生じます。エラー伝播として、そして翻訳精度に影響を与えます。 
[ABSTRACT]ソフトウェアは予測を予測するために使用されており、次のトークンを予測します。以前の研究では、精度を向上させるために情報が必要であることも示されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: newsSweeper at SemEval-2020 Task 11: Context-Aware Rich Feature
  Representations For Propaganda Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_13.html">
      <font color="black">newsSweeper at SemEval-2020 Task 11: Context-Aware Rich Feature
  Representations For Propaganda Classification</font>
    </a>
  </h2>
  <font color="black">2番目のサブタスクでは、プロパガンダ手法を分類するための事前トレーニング済みRoBERTaモデルにコンテキスト機能を組み込みます。このペーパーでは、SemEval 2020タスク11：Spanの2つのサブタスクのそれぞれのニュース記事でのプロパガンダ手法の検出について説明します。識別と手法の分類..宣伝手法の分類サブタスクで5位にランクされました。 
[要約]テキスト内のプロパガンダスパンを識別するシステムを開発しました。プロパガンダテクニック分類サブタスクで5位にランク付けされました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Semi-Supervised Learning Approach to Discover Enterprise User Insights
  from Feedback and Support -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_14.html">
      <font color="black">Semi-Supervised Learning Approach to Discover Enterprise User Insights
  from Feedback and Support</font>
    </a>
  </h2>
  <font color="black">方法論と技術的な観点では、転移学習を採用してBERTベースのマルチ分類システムを微調整し、メイントピックを分類してから、新しいPSHTIモデルを利用して、予測されたメイントピックの下のサブトピックを推測します。ユーザーの洞察を発見し、ビジネス投資の優先順位を高めるために光を当てるのに役立つ、実際の制作における最先端の方法論を活用することにより、優れたショーケースを提供します。このホワイトペーパーでは、ディープラーニングとトピックモデリングを利用して、ユーザーの声をよりよく理解します。このアプローチは、教師あり学習によるBERTベースの多分類アルゴリズムと、教師なし学習による新しい確率的および意味的ハイブリッドトピック推論（PSHTI）モデルを組み合わせ、教師なし学習を通じて、テキストのフィードバックとサポートからメイントピックまたはエリアとサブトピックをより適切に識別するプロセス。 e 3つの主要なブレークスルー：1.ディープラーニングテクノロジーの進歩として、NLP分野には多大な革新がありましたが、NLPアプリケーションの1つとしての従来のトピックモデリングは、ディープラーニングの流れに遅れをとっています。 
[ABSTRACT] nlpフィールドには多大な革新がありましたが、従来のトピックの手作業作業は新しい方法です。このシステムにより、上位の単語をセルフクロールの問題にWebクロールを使用してマッピングできます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-18">
        <br><font color="black">2020-07-18</font>
      </time>
    </span>
</section>
<!-- paper0: IITK-RSA at SemEval-2020 Task 5: Detecting Counterfactuals -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_15.html">
      <font color="black">IITK-RSA at SemEval-2020 Task 5: Detecting Counterfactuals</font>
    </a>
  </h2>
  <font color="black">私たちの最終的に提出されたアプローチは、最初のサブタスクのさまざまな微調整されたトランスベースモデルとCNNベースモデルのアンサンブルと、2番目のサブタスクの依存関係ツリー情報を備えたトランスフォーマモデルです。人間にとって反事実的な推論は自然なことですが、これらの表現を理解することはさまざまな言語の微妙さのために、人工的なエージェントにとっては困難です。リーダーボード全体で4位と9位にランク付けされました。 
[ABSTRACT]タスクには、counterfactualsと呼ばれる文学的な表現のクラスを検出することが含まれていました。構成要素にそれらを分離することが含まれていました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Connecting Embeddings for Knowledge Graph Entity Typing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_16.html">
      <font color="black">Connecting Embeddings for Knowledge Graph Entity Typing</font>
    </a>
  </h2>
  <font color="black">ナレッジグラフ（KG）エンティティのタイピングは、KGで欠落している可能性のあるエンティティタイプのインスタンスを推測することを目的としています。これは、ナレッジグラフの完了の非常に重要ですが、まだ十分に調査されていないサブタスクです。2つの実世界のデータセット（FreebaseとYAGO）の実験結果は、 KGエンティティのタイピングを改善するために提案されたメカニズムとモデルの有効性。このペーパーのソースコードとデータは、https：//github.com/ Adam1679 / ConnectE 
[ABSTRACT]ニューヨーク大学の研究者から提案されています。 kgエンティティタイピングの新しいアプローチ。既存のエンティティタイプアサーションからのローカルタイピングの知識とkgsからのグローバルトリプルナレッジを一緒に習得することでトレーニングされると述べています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: ForecastTB An R Package as a Test-Bench for Time Series Forecasting
  Application of Wind Speed and Solar Radiation Modeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_17.html">
      <font color="black">ForecastTB An R Package as a Test-Bench for Time Series Forecasting
  Application of Wind Speed and Solar Radiation Modeling</font>
    </a>
  </h2>
  <font color="black">さらに、このペーパーでは、自然時系列データセット（つまり、風速と日射量）を使用した実際のアプリケーション例を示し、ForecastTBパッケージの機能を示し、データセットの特性に影響される予測比較分析を評価します。提案されたテストベンチは、デフォルトの予測およびエラーメトリック関数に限定されず、ユーザーは要件に従って目的のメソッドを追加、削除、または選択できます。モデリング結果は、時系列予測用に提案されたRパッケージForecastTBの適用性と堅牢性を示しました。 
[ABSTRACT] forecasttbはプラグアンドプレイの構造化モジュールです。シンプルな指示に含めることができるいくつかの予測方法が含まれています。これらにはデータ分析とメトリックが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-04">
        <br><font color="black">2020-04-04</font>
      </time>
    </span>
</section>
<!-- paper0: BAKSA at SemEval-2020 Task 9: Bolstering CNN with Self-Attention for
  Sentiment Analysis of Code Mixed Text -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_18.html">
      <font color="black">BAKSA at SemEval-2020 Task 9: Bolstering CNN with Self-Attention for
  Sentiment Analysis of Code Mixed Text</font>
    </a>
  </h2>
  <font color="black">HinglishタスクとSpanglishタスクの提出は、それぞれayushkとharsh_6というユーザー名で行われました。CNNコンポーネントはポジティブツイートとネガティブツイートの分類に役立ちますが、自己注意ベースのLSTMは中立的なツイートの分類に役立ちます。複数の感情を持つユニット間で正しい感情を識別する機能。コード混合テキストの感情分析は、ユーザーレビューのタグ付けから、サブポピュレーションの社会的または政治的感情の識別に至るまで、オピニオンマイニングのアプリケーションを多様化しています。 
[ABSTRACT] cnnのerrol barnettは、cnnとlstmがチームを組んだと言います。彼は、ヒンディー語（英語）とスペイン語（スペイン語）で0. 707（ソーシャル）と0. 725（ランク13）のf1スコアを達成したと言っています。 ）データセット</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: CS-NET at SemEval-2020 Task 4: Siamese BERT for ComVE -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_19.html">
      <font color="black">CS-NET at SemEval-2020 Task 4: Siamese BERT for ComVE</font>
    </a>
  </h2>
  <font color="black">作業の目新しさは、矛盾するステートメントの論理的な意味と両方の文からの同時情報抽出を処理するアーキテクチャ設計にあります。3つのサブタスクのうち、このペーパーでは、サブタスクAとサブタスクBのシステム記述を報告します。このペーパーでは、サブタスクに対処するためのトランスフォーマニューラルネットワークアーキテクチャに基づくモデル。 
[要約]この論文は、サブタスクに対処するための変圧器同時アーキテクチャに基づくモデルを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Word Representation for Rhythms -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_20.html">
      <font color="black">Word Representation for Rhythms</font>
    </a>
  </h2>
  <font color="black">BERTモデルは、リズムワードの構文上の可能性を探索するために作成されます。1034個のノッティンガムデータセットを使用して、サイズが450のリズムワードディクショナリ（コントロールトークンなし）が生成されます。より大きなスキームでは、思考モード-言語としての音楽-体系的な検討のために提案されています。 
[ABSTRACT]サイズが450のリズム単語辞書（制御トークンなし）を作成</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: IITK at SemEval-2020 Task 8: Unimodal and Bimodal Sentiment Analysis of
  Internet Memes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_21.html">
      <font color="black">IITK at SemEval-2020 Task 8: Unimodal and Bimodal Sentiment Analysis of
  Internet Memes</font>
    </a>
  </h2>
  <font color="black">研究では、バイモーダル（テキストとイメージ）とユニモーダル（テキストのみ）の手法を、ベイズ分類器からトランスフォーマーベースのアプローチまで検討しています。自然言語処理（NLP）とコンピュータービジョンの手法を活用しています。 （CV）インターネットミームの感情分類に向けて（サブタスクA）。私たちの仕事は、さまざまなモダリティの組み合わせに関係するすべてのタスクに関連しています。
[要約]ミームは、以前のクラスに属する最も人気のある形式です。ミームは、彼らの感情的な内容と感情について</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: IITK at SemEval-2020 Task 10: Transformers for Emphasis Selection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_22.html">
      <font color="black">IITK at SemEval-2020 Task 10: Transformers for Emphasis Selection</font>
    </a>
  </h2>
  <font color="black">私たちは0.810の最高のMatchmスコア（セクション2.2で説明）を達成し、リーダーボードで3位にランク付けされました。この結果は、変圧器ベースのモデルがこのタスクで特に効果的であることを示しています。この論文では、研究問題に対処するために提案されたシステムについて説明しますSemEval-2020のタスク10で提起：ビジュアルメディアで書かれたテキストの強調選択。 
[要旨]テキストを入力として受け取り、各単語に対応する単語が強調される確率を与えるエンドインモデルを提案します。0の最高の一致スコアを達成しました。810で、タスクボードで3番目にランク付けされました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: problemConquero at SemEval-2020 Task 12: Transformer and Soft
  label-based approaches -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_23.html">
      <font color="black">problemConquero at SemEval-2020 Task 12: Transformer and Soft
  label-based approaches</font>
    </a>
  </h2>
  <font color="black">サブタスクC（オフェンスターゲット識別）の2つのモデルを提出しました。1つはソフトラベルを使用し、もう1つはBERTベースの微調整モデルを使用しました。サブタスクBの43のうち28のランクを達成しました。サブタスクAは、ギリシャ語で37のうち19、トルコ語で46のうち22、デンマーク語で39のうち26、アラビア語で39のうち53、および英語で20のうち85でした。
[要約]私たちは3つすべてに参加しましたオフェンスのサブタスク-2020.私たちの最終提出には、トランスベースのアプローチとソフトラベルベースのアプローチが含まれていました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: On Analyzing Antisocial Behaviors Amid COVID-19 Pandemic -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/cs.CL/paper_24.html">
      <font color="black">On Analyzing Antisocial Behaviors Amid COVID-19 Pandemic</font>
    </a>
  </h2>
  <font color="black">特に、反社会的行動のツイートに自動的に注釈を付ける注釈フレームワークを提案します。また、反社会的行動の脆弱なターゲットとオンライン反社会的コンテンツの拡散に影響を与える要因を特定しました。問題の重大さにもかかわらず、非常に少数の研究では、 COVID-19パンデミックの中でオンライン反社会的行動を研究しました。 
[要約]このホワイトペーパーでは、4,000万を超えるcovid-19の関連ツイートの大規模なデータセットを収集して注釈を付けることで、研究のギャップを埋めています。また、注釈付きデータセットの実験的分析を行ったところ、新しい虐待的な辞書がxenomicに導入されていることがわかりました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: SLNSpeech: solving extended speech separation problem by the help of
  sign language -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.AS/paper_0.html">
      <font color="black">SLNSpeech: solving extended speech separation problem by the help of
  sign language</font>
    </a>
  </h2>
  <font color="black">音声分離タスクは、オーディオのみの分離とオーディオとビジュアルの分離に大別できます。次に、特に手話の埋め込みとオーディオまたはオーディオの組み合わせを使用して、3つのモダリティの自主学習用の一般的なディープラーニングネットワークを設計します。音声分離タスクをよりよく解決するための視覚的な情報..音声分離技術を障害者の実際のシナリオに適用するために、ソースコードがhttp://cheertt.top/homepage/ 
[ABSTRACT]でリリースされます。大規模-手話のニューススピーチという名前の大規模なデータセット。3D残差たたみ込みネットワークを使用して手話の特徴を抽出し、事前トレーニングされたvggnetモデルを使用して正確な視覚的特徴</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Read and Follow Music in Complete Score Sheet Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.AS/paper_1.html">
      <font color="black">Learning to Read and Follow Music in Complete Score Sheet Images</font>
    </a>
  </h2>
  <font color="black">また、この方法をOMRベースのアプローチと比較し、それがこのようなシステムの実行可能な代替手段になる可能性があることを経験的に示しています。このペーパーでは、未処理の画像として与えられる楽譜のスコアリングのタスクについて説明します。着信オーディオとスコアの画像が与えられると、Googleのシステムは音声と一致するページ内の最も可能性の高い位置を直接予測し、位置合わせの精度の点で現在の最先端の画像ベースのスコアのフォロワーよりも優れています。 
[要約]スコアの最初のシステムは、全ページに基づいており、完全に未処理のシート画像に基づいています。データに基づいており、このようなシステムの実行可能な代替案になる可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Time-Frequency Scattering Accurately Models Auditory Similarities
  Between Instrumental Playing Techniques -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.AS/paper_2.html">
      <font color="black">Time-Frequency Scattering Accurately Models Auditory Similarities
  Between Instrumental Playing Techniques</font>
    </a>
  </h2>
  <font color="black">それらの応答を分析すると、音色の知覚は、楽器や演奏テクニックだけで提供されるよりも柔軟な分類法の範囲内で機能することがわかります。この記事では、31人の被験者に78の孤立した音符を一連の音色クラスターに編成するよう依頼します。 
[ABSTRACT]音色知覚は、楽器や演奏テクニックだけで提供される分類よりも柔軟な分類内で動作します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Audio Adversarial Examples for Robust Hybrid CTC/Attention Speech
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.AS/paper_3.html">
      <font color="black">Audio Adversarial Examples for Robust Hybrid CTC/Attention Speech
  Recognition</font>
    </a>
  </h2>
  <font color="black">自動音声認識（ASR）の最近の進歩により、エンドツーエンドシステムが最先端のパフォーマンスをどのように実現できるかが実証されました。評価は、2つのリファレンスでハイブリッドCTC /注意エンドツーエンドASRモデルを使用して実行されますケーススタディとしての文章、およびTEDlium v2音声認識タスク。これらの音声攻撃例（AAE）は、以前はConnectionist Temporal Classification（CTC）を使用するASRシステムと、注意ベースのエンコーダー/デコーダーアーキテクチャーで実証されていました。 
[要約]より深いニューラルネットワークに向かう傾向がありますが、それらのasrモデルはより複雑で、特別に細工されたノイズデータに対しても傾向があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Foley Music: Learning to Generate Music from Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.AS/paper_4.html">
      <font color="black">Foley Music: Learning to Generate Music from Videos</font>
    </a>
  </h2>
  <font color="black">実験結果は、私たちのモデルが既存のいくつかのシステムよりも優れており、聴きやすい音楽を生成していることを示しています。この論文では、楽器を演奏する人々についてのサイレントビデオクリップ用のもっともらしい音楽を合成できるシステムであるFoley Musicを紹介します。さまざまな音楽パフォーマンスを含む動画でのモデルの効果を実証します。 
[ABSTRACT]動画から音楽への生成に成功するための2つの主要な中間表現を特定します。動画と音声の録音からの本体のキーポイントは、動画から音楽へのキーポイントになります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video
  Parsing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.AS/paper_5.html">
      <font color="black">Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video
  Parsing</font>
    </a>
  </h2>
  <font color="black">さまざまな時間範囲とモダリティから有用なオーディオおよびビジュアルコンテンツを適応的に探索するための注意深いMMILプーリング手法を開発します。探索を容易にするために、Look、Listen、and Parse（LLP）データセットを収集して、オーディオビジュアルビデオの解析を弱く調査します-教師付きの方法。さらに、モダリティバイアスとノイズの多いラベルの問題を、それぞれ個別の学習メカニズムとラベルの平滑化手法で発見して軽減します。 
[要旨]これは、ビデオ内に描かれているシーンを完全に理解するために不可欠です。注意深い問題を開発します。これは、マルチモーダルな複数インスタンス学習として自然に構成できます-視覚的なビデオ解析。ビデオだけでも課題を達成できます-レベルの弱いラベル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Very Fast Keyword Spotting System with Real Time Factor below 0.01 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.AS/paper_6.html">
      <font color="black">Very Fast Keyword Spotting System with Real Time Factor below 0.01</font>
    </a>
  </h2>
  <font color="black">標準のトライフォンまたはいわゆる疑似モノフォンによる双方向フィードフォワードシーケンシャルメモリネットワーク（リカレントネットの代替）による時間とメモリ効率の高いモデリング、および音声フレームの完全なフォワードデコード（ルックバックの必要性は最小限）を示します。すべての最適化（尤度計算のGPUを含む）を適用すると、完全なシステムがRT係数が0.001に近いシングルパスで実行できることを示しています。提案されたスキームのいくつかのバリアントは、3つの大きなチェコのデータセット（ブロードキャスト、インターネット）で評価されます電話、合計17時間）とそのパフォーマンスは、検出エラートレードオフ（DET）図とリアルタイム（RT）要因によって比較されます。 
[要約]提案されたアーキテクチャは、データセット、ブロードキャスト、インターネット、電話、合計17時間に基づいています。これらには、信号処理と尤度計算、ビタビ復号、スポット候補検出、信頼度計算が含まれます。プロジェクトはプロジェクトの `kwsと呼ばれます」</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Score and Lyrics-Free Singing Voice Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.AS/paper_7.html">
      <font color="black">Score and Lyrics-Free Singing Voice Generation</font>
    </a>
  </h2>
  <font color="black">特に、3つのそのような生成方式の概要を説明し、これらの新しいタスクに取り組むためのパイプラインを提案します。歌声の生成モデルは、主に「歌声合成」のタスク、つまり与えられた歌声波形を生成することに関係しています楽譜とテキスト歌詞..さらに、生成的敵対ネットワークを使用してそのようなモデルを実装し、客観的および主観的にそれらを評価します。 
[ABSTRACT]事前に割り当てられたスコアと歌詞なしで歌声を生成します。この作品では、小説でありながら挑戦的な代替案を模索します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-26">
        <br><font color="black">2019-12-26</font>
      </time>
    </span>
</section>
<!-- paper0: MUSICNTWRK: data tools for music theory, analysis and composition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.AS/paper_8.html">
      <font color="black">MUSICNTWRK: data tools for music theory, analysis and composition</font>
    </a>
  </h2>
  <font color="black">ピッチクラスセットとリズミカルシーケンスの分類と操作のためのpythonライブラリーであるMUSICNTWRKのAPI、一般化された音楽とサウンドスペースでのネットワークの生成、音色認識のためのディープラーニングアルゴリズム、および任意のデータの音波処理を紹介します。ソフトウェアはGPL 3.0で自由に利用でき、www.musicntwrk.comからダウンロードするか、PyPiプロジェクトとしてインストールできます（pip install musicntwrk）。 
[要約]ソフトウェアはgpl 3で無料で入手できます。deep.itはwwwからダウンロードできます。 musicntwrk。またはpypiプロジェクトとしてインストール</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-03">
        <br><font color="black">2019-06-03</font>
      </time>
    </span>
</section>
<!-- paper0: Optimization of data-driven filterbank for automatic speaker
  verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.AS/paper_9.html">
      <font color="black">Optimization of data-driven filterbank for automatic speaker
  verification</font>
    </a>
  </h2>
  <font color="black">さまざまな分類子バックエンドを使用して、さまざまなコーパスで自動話者検証（ASV）実験を行います。最近導入されたディープラーニングベースの方法に対する提案された方法の主な利点は、ラベルのない音声データの量が非常に制限されることです。提案されたフィルターバンクは、一般的に使用されているメルフィルターバンクや既存のデータ駆動型フィルターバンクよりもスピーカーの識別力が高いことを示しています。 
[ABSTRACT]提案されたフィルターバンクは、melフィルターバンクよりもスピーカーの識別力が高くなります。新しい方法では、特定の音声データからフィルターパラメーターを最適化します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Guided multi-branch learning systems for DCASE 2020 Task 4 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.AS/paper_10.html">
      <font color="black">Guided multi-branch learning systems for DCASE 2020 Task 4</font>
    </a>
  </h2>
  <font color="black">したがって、さまざまな目的を追求し、データのさまざまな特性に焦点を当てた複数のブランチは、特徴エンコーダーが特徴空間をより適切にモデル化し、過剰適合を回避するのに役立ちます。このホワイトペーパーでは、DCASE 2020タスク4.のシステムについて詳しく説明します。マルチタスク学習に触発された、強くラベル付けされた合成データをより適切に活用するために、サウンドイベント検出ブランチ（SEDB）も採用しています。 
[ABSTRACT]このシステムは、dcase 2019タスク4の最初の場所のシステムに基づいています。不十分な-監視付きの監視フレームワーク-ベースの埋め込み-レベルの複数インスタンスの学習プールモジュールと、ガイド付き学習と呼ばれる半監視付き学習アプローチを採用しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: The impact of Audio input representations on neural network based music
  transcription -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.AS/paper_11.html">
      <font color="black">The impact of Audio input representations on neural network based music
  transcription</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、さまざまな入力表現がポリフォニックな複数楽器の音楽の転写に及ぼす影響を徹底的に分析しています。適切な入力表現を選択することで、転写精度が$ 8.33 $％向上し、エラーが$ 9.39 $％削減できることがわかりました（ログニュートラルネットワークの設計を変更せずに、STFTウィンドウ長が4,096で2,048の周波数ビンを持つ単一周波数スペクトログラム（単層が完全に接続されている）。私たちの実験では、Melスペクトログラムは、周波数の数を減らすことができるコンパクトな表現であることも示しています。比較的高い音楽の文字起こし精度を維持しながら、ビン数は512までです。 
[要約]スペクトログラム抽出ツールnnaudioを使用して、線形周波数スペクトログラム、メルスペクトログラム、および定数-q変換（cqt）の使用による影響を調査します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-25">
        <br><font color="black">2020-01-25</font>
      </time>
    </span>
</section>
<!-- paper0: Regression-based music emotion prediction using triplet neural networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-22/eess.AS/paper_12.html">
      <font color="black">Regression-based music emotion prediction using triplet neural networks</font>
    </a>
  </h2>
  <font color="black">次に、これらの新しい表現を、サポートベクターマシンや勾配ブースティングマシンなどの回帰アルゴリズムの入力として使用します。これは、オーディオ機能のコンパクトな潜在空間表現を提供することに加えて、提案されたアプローチがベースラインよりも高いパフォーマンスを持っていることを示していますモデル.. DEAMデータセットの結果は、TNNを使用することにより、ベースラインモデル（TNNなし）に対して、価数予測が9％、覚醒予測が4％改善され、90％の特徴次元削減が達成されることを示しています。 
[ABSTRACT]意味のある表現を作成できるメカニズムを提案します。異なる空間システムと比較してパフォーマンスを向上させます。tnnメソッドは、主成分分析などの他のフォームよりも優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-25">
        <br><font color="black">2020-01-25</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
