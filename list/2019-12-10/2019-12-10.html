<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
<title>Akari-2019-12-10の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
  <div class="header-logo">
    <a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
  </div>
</header>
<input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">

<div class="menubar">
<span class="bar"></span>
<span class="bar"></span>
<span class="bar"></span>
</div>

<ul>
<li><a id="home" href="../../index.html">Home</a></li>
<li><a id="about" href="../../teamAkariとは.html">About</a></li>
<li><a id="contact" href="../../contact.html">Contact</a></li>
<li><a id="contact" href="../../list/newest.html">New Papers</a></li>
<li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
</ul>

</label>
<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: MITAS: A Compressed Time-Domain Audio Separation Network with Parameter
  Sharing -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-10/cs.SD/paper_0.html">
      MITAS: A Compressed Time-Domain Audio Separation Network with Parameter
  Sharing
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、ミックスオーディオの2種類の摂動に対するMiTASの堅牢性を最初に確認しました。したがって、新しいMiTAS（Mini TasNet）と呼ばれる新しいものを導き出しました。最近、TasNetとConv-TasNetが提案されました。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-09">
        <br>2019-12-09
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Investigating Deep Neural Transformations for Spectrogram-based Musical
  Source Separation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-10/cs.SD/paper_1.html">
      Investigating Deep Neural Transformations for Spectrogram-based Musical
  Source Separation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの実験は、いくつかの変換方法を比較することにより、将来の作品に豊富な資料を提供します。生の複素数値STFT出力でモデルをトレーニングし、MUSDB歌声分離タスクで1.0の大きなマージンで最先端のSDRパフォーマンスを達成しますdB ..最近、MSSタスク用に多くの機械学習ベースの方法が提案されましたが、さまざまなタイプのネットワークを評価して直接比較する既存の作品はありませんでした。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-02">
        <br>2019-12-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: MelGAN: Generative Adversarial Networks for Conditional Waveform
  Synthesis -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-10/cs.SD/paper_2.html">
      MelGAN: Generative Adversarial Networks for Conditional Waveform
  Synthesis
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      アブレーション研究を通じてモデルのさまざまなコンポーネントを評価し、条件付きシーケンス合成タスク用の汎用識別器とジェネレーターを設計するための一連のガイドラインを提案します。モデルは、非自己回帰、完全畳み込み、競合モデルよりも大幅に少ないパラメーターで一般化されますハードウェア固有の最適化のトリックなしで、GTX 1080Ti GPUではリアルタイムの100倍以上、CPUではリアルタイムの2倍以上の速度で実行されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-08">
        <br>2019-10-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: End-to-End Optimized Speech Coding with Deep Neural Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-10/cs.SD/paper_3.html">
      End-to-End Optimized Speech Coding with Deep Neural Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      生の音声データから直接エンドツーエンドで広帯域音声符号化パイプライン（圧縮、量子化、エントロピー符号化、および解凍）のすべてのステップを最適化するディープニューラルネットワークモデルを提示します。また、3.8GhZ Intel CPUでリアルタイムに実行されます。テストでは、DNNベースのコーダーは、さまざまなビットレート（〜9kbps〜〜24kbps）でAMR-WB標準に匹敵します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2017-10-25">
        <br>2017-10-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Deep Filtering: Signal Extraction and Reconstruction Using Complex
  Time-Frequency Filters -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-10/cs.SD/paper_4.html">
      Deep Filtering: Signal Extraction and Reconstruction Using Complex
  Time-Frequency Filters
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      各混合物のSTFT領域を希望のTFビンにマッピングし、混合物TFビンの破壊的干渉に対処する各混合物TFビンの複雑なTFフィルターをDNNで推定することを提案します。 DNNは、抽出された信号とグラウンドトゥルースの希望信号間の誤差を最小化することにより最適化され、グラウンドトゥルースTFフィルターを指定せずにTFフィルターを学習できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-17">
        <br>2019-04-17
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Modelling Bahdanau Attention using Election methods aided by Q-Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-10/cs.CL/paper_0.html">
      Modelling Bahdanau Attention using Election methods aided by Q-Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      それに加えて、マルコフ決定プロセスであるアテンションメカニズムは強化学習手法で表されています。したがって、Qラーニングを使用して微調整された選挙方法（$ k $ -Borda）を使用することを提案します。アテンションネットワークの置き換え。このネットワークの推論時間は標準のバーダナウ翻訳者よりも短く、翻訳の結果は同等です。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-10">
        <br>2019-11-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: TAPER: Time-Aware Patient EHR Representation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-10/cs.CL/paper_1.html">
      TAPER: Time-Aware Patient EHR Representation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      トランスフォーマーネットワークと最近提案されたBERT言語モデルを使用して、これらのデータストリームを統一されたベクトル表現に埋め込みます。電子健康記録の効果的な表現学習は困難なタスクであり、そのようなデータの可用性が普及するにつれて重要になっています。私たちのモデルは、公開されているMIMIC-III ICUデータセットを使用して、死亡率、再入院、滞在期間に関する優れたパフォーマンスと一般化を実証します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-11">
        <br>2019-08-11
      </time>
    </span>
  </h3>
</article>
<!-- paper0: MITAS: A Compressed Time-Domain Audio Separation Network with Parameter
  Sharing -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-10/cs.CL/paper_2.html">
      MITAS: A Compressed Time-Domain Audio Separation Network with Parameter
  Sharing
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この研究では、TasNetとConv-TasNetのメモリ消費を削減するための2つのパラメータ共有方式を提案しました。最近、TasNetとConv-TasNetは提案された。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-09">
        <br>2019-12-09
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A BERT Baseline for the Natural Questions -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-10/cs.CL/paper_3.html">
      A BERT Baseline for the Natural Questions
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このベースラインは、ai.google.com / research / NaturalQuestionsの公式NQリーダーボードに提出されています。このモデルはBERTに基づいており、元のデータセットペーパーで報告されたモデルF1スコアと人間の上限とのギャップを30それぞれ、長回答タスクと短回答タスクの相対％と50％。このテクニカルノートでは、自然質問の新しいベースラインについて説明します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-01-24">
        <br>2019-01-24
      </time>
    </span>
  </h3>
</article>
<!-- paper0: MelGAN: Generative Adversarial Networks for Conditional Waveform
  Synthesis -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-10/cs.CL/paper_4.html">
      MelGAN: Generative Adversarial Networks for Conditional Waveform
  Synthesis
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      アブレーション研究を通じてモデルのさまざまなコンポーネントを評価し、条件付きシーケンス合成タスクの汎用弁別器とジェネレーターを設計するための一連のガイドラインを提案します。提案手法の一般性を確立するために、音声合成におけるモデルの定性的結果を示します。 、音楽ドメインの翻訳、無条件の音楽合成。このモデルは非自己回帰、完全な畳み込みであり、競合モデルよりもパラメーターが大幅に少なく、メルスペクトログラムの反転のために目に見えないスピーカーに一般化します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-08">
        <br>2019-10-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Effective Attention Modeling for Neural Relation Extraction -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-10/cs.CL/paper_5.html">
      Effective Attention Modeling for Neural Relation Extraction
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このようなシナリオでの関係抽出は、エンティティと文内の他の単語間の長距離相互作用をキャプチャする必要があるため、より困難になります。この問題に対処するために、文の構文情報と多要素アテンションメカニズム。ただし、文は長くなる場合があり、2つのエンティティが文内で互いに離れて配置される場合があります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-09">
        <br>2019-12-09
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Women in ISIS Propaganda: A Natural Language Processing Analysis of
  Topics and Emotions in a Comparison with Mainstream Religious Group -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-10/cs.CL/paper_6.html">
      Women in ISIS Propaganda: A Natural Language Processing Analysis of
  Topics and Emotions in a Comparison with Mainstream Religious Group
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらのテキストと非暴力的な宗教グループによって作成されたテキストの類似点または相違点を識別するために、女性専用のカトリックフォーラムの記事に分析を拡張します。また、感情的要素をよりよく理解するために、これら両方のリソースの感情分析も行いますプロパガンダの..私たちは、Depechemood（語彙ベースの感情分析方法）に依存して、これらの資料の読者に最も喚起される可能性が高い感情を検出します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-09">
        <br>2019-12-09
      </time>
    </span>
  </h3>
</article>
<!-- paper0: AMR Normalization for Fairer Evaluation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-10/cs.CL/paper_7.html">
      AMR Normalization for Fairer Evaluation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      正規化されたAMRと正規化されていない同等のAMRはまったく異なるように見える場合があります。金色のコーパスをそれ自体と関係の具体化だけで比較すると、25の一致ポイントの差が生じ、2つのシステムの出力は正規化なしでは直接比較できない可能性があることを示唆しています。このホワイトペーパーでは、AMRの既存のオープンソースPythonツールキットの上に実装され、同じライセンスの下でリリースされます。このホワイトペーパーでは、概念的に同等のAMRが同等として評価されるようにする4つの正規化方法を提案します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-04">
        <br>2019-09-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: AI2D-RST: A multimodal corpus of 1000 primary school science diagrams -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-10/cs.CL/paper_8.html">
      AI2D-RST: A multimodal corpus of 1000 primary school science diagrams
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      AI2Dのダイアグラムレイアウトのセグメンテーションに基づいて、AI2D-RSTコーパスは、マルチモーダル構造の詳細な説明を提供する新しいマルチレイヤーアノテーションスキーマを提示します。AI2D-RSTの各アノテーションレイヤーは、グラフを使用して表されます。レイヤーは、（1）ダイアグラム要素を知覚単位にグループ化する、（2）矢印や線などのダイアグラム要素によって設定される接続、（3）修辞を使用して記述されるダイアグラム要素間の談話関係を記述する構造理論（RST）。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-09">
        <br>2019-12-09
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: MITAS: A Compressed Time-Domain Audio Separation Network with Parameter
  Sharing -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-10/eess.AS/paper_0.html">
      MITAS: A Compressed Time-Domain Audio Separation Network with Parameter
  Sharing
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最近、TasNetとConv-TasNetが提案されました。標準化されたいくつかのSSタスクで最先端の結果を達成しました。この研究では、TasNetとConv- TasNet。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-09">
        <br>2019-12-09
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Investigating Deep Neural Transformations for Spectrogram-based Musical
  Source Separation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-10/eess.AS/paper_1.html">
      Investigating Deep Neural Transformations for Spectrogram-based Musical
  Source Separation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      生の複素数値STFT出力でモデルをトレーニングし、MUSDB歌唱音声分離タスクで1.0 dBの大きなマージンで最先端のSDRパフォーマンスを達成します。この実験は、いくつかの変換を比較することにより、将来の作品に豊富な資料を提供しますメソッド..最近、MSSタスク用に多くの機械学習ベースのメソッドが提案されましたが、さまざまなタイプのネットワークを評価して直接比較する既存の作品はありませんでした。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-02">
        <br>2019-12-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: MelGAN: Generative Adversarial Networks for Conditional Waveform
  Synthesis -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-10/eess.AS/paper_2.html">
      MelGAN: Generative Adversarial Networks for Conditional Waveform
  Synthesis
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      アブレーション研究を通じてモデルのさまざまなコンポーネントを評価し、条件付きシーケンス合成タスク用の汎用弁別器およびジェネレーターを設計するための一連のガイドラインを提案します。GTX1080Ti GPUでのリアルタイムよりも100倍以上、2倍以上で実行されるpytorch実装ハードウェア固有の最適化トリックなしで、CPUのリアルタイムよりも高速です。このモデルは、非自己回帰で完全に畳み込みであり、競合モデルよりもパラメーターが大幅に少なく、メルスペクトログラム反転のために見えないスピーカーに一般化します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-08">
        <br>2019-10-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: End-to-End Optimized Speech Coding with Deep Neural Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-10/eess.AS/paper_3.html">
      End-to-End Optimized Speech Coding with Deep Neural Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      また、3.8GhZ Intel CPUでリアルタイムに実行されます。テストでは、DNNベースのコーダーは、さまざまなビットレート（〜9kbps〜〜24kbps）でAMR-WB規格と同等の性能を発揮します。広帯域音声符号化パイプラインのすべてのステップ（圧縮、量子化、エントロピー符号化、および解凍）を生の音声データから直接エンドツーエンドで最適化するネットワークモデル-手動の機能エンジニアリングは不要で、数時間でトレーニングします。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2017-10-25">
        <br>2017-10-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Deep Filtering: Signal Extraction and Reconstruction Using Complex
  Time-Frequency Filters -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-10/eess.AS/paper_4.html">
      Deep Filtering: Signal Extraction and Reconstruction Using Complex
  Time-Frequency Filters
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      理想的なマスクの大きさは、TFビン内の望ましくない信号のみに対してゼロであり、完全な破壊的干渉については定義されていません。混合TFビンの破壊的干渉に対処するビン。DNNは、抽出された信号とグラウンドトゥルースの希望信号間の誤差を最小化することにより最適化され、グラウンドトゥルースTFフィルターを指定せずにTFフィルターを学習できる
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-17">
        <br>2019-04-17
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="">
      
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本日更新された論文はありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="">
        <br>
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
