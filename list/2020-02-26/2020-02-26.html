<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-02-26の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Singing Voice Conversion with Disentangled Representations of Singer and
  Vocal Technique Using Variational Autoencoders -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.SD/paper_0.html">
      Singing Voice Conversion with Disentangled Representations of Singer and
  Vocal Technique Using Variational Autoencoders
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      学習された潜在空間で変換は単純なベクトル演算によって実行されます。提案モデルは、非並列コーパスでトレーニングされ、多対多変換に対応し、変分オートエンコーダーの最近の進歩を活用します。シンガーアイデンティティとボーカルテクニックの潜在的表現を別々に、再構成用のジョイントデコーダーを使用して。 
[概要]提案されたモデルは、非並列コーパスでトレーニングされ、多くのもつれた変換に対応します。別のモデルは、非並列コーパスでトレーニングされ、多対セントの変換に対応します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-03">
        <br>2019-12-03
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Counterfactual Data Augmentation for Mitigating Gender Stereotypes in
  Languages with Rich Morphology -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_0.html">
      Counterfactual Data Augmentation for Mitigating Gender Stereotypes in
  Languages with Rich Morphology
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      4つの異なる言語を使用してアプローチを評価することにより、平均して、文法性を犠牲にすることなく、性別のステレオタイプを2.5倍削減することを示します。言語..ジェンダーのステレオタイプは、世界のほとんどの言語で明示されており、その結果、NLPシステムによって伝播または増幅されます。 
[要約]一般的に採用されているアプローチは、英語で性別のステレオタイプを緩和するために、入札で無反射の文を生成します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-11">
        <br>2019-06-11
      </time>
    </span>
  </h3>
</article>
<!-- paper0: In Nomine Function: Naming Functions in Stripped Binaries with Neural
  Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_1.html">
      In Nomine Function: Naming Functions in Stripped Binaries with Neural
  Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      つまり、問題を定義し、トレーニングとテストの設計のために行った選択に対して合理的な正当性を提供します。そのような手法は、NLP分野で一般的でよく知られています。自然言語処理の分野（Seq2SeqネットワークやTransformerなど）。 
[概要]このようなフレームワークでは、自然言語処理の分野からのベースラインの結果をテストします。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-17">
        <br>2019-12-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: The CoNLL--SIGMORPHON 2018 Shared Task: Universal Morphological
  Reinflection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_2.html">
      The CoNLL--SIGMORPHON 2018 Shared Task: Universal Morphological
  Reinflection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      活用タスク（タスク1）では、昨年の活用タスクに存在する52の言語のうち41が、低リソース設定で最高のシステムによる改善を示しました。CoNLL--SIGMORPHON 2018形態生成データの教師付き学習に関する共有タスク103の類型的に多様な言語のセット。両方のタスクは、低、中、高のデータ条件を特色としました。 
[概要]共有タスクには、参加者に文の文脈で単語を活用するように依頼する新しい2番目のタスクもありました。タスク1は27の提出を受け取り、タスク2は6の提出を受け取りました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-10-16">
        <br>2018-10-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Label-guided Learning for Text Classification -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_3.html">
      Label-guided Learning for Text Classification
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      そのラベルガイドレイヤーは、ラベルベースの注意深いエンコードを実行して、（コンテキスト情報学習者によってエンコードされた）ユニバーサルテキスト埋め込みを異なるラベルスペースにマッピングし、ラベル単位の埋め込みをもたらします。テキスト分類は、最も重要で基本的なタスクの1つです。自然言語処理..提案されているフレームワークでは、ラベルガイドレイヤーを簡単かつ直接コンテキストエンコーディング方式で適用して、共同学習を実行できます。 
[ABSTRACT]これらの方法は、ラベル情報などのグローバルな手がかりを活用することを常に怠ります。この方法は、普遍的な表現学習に基づいています。これは、テキストの埋め込みがテキスト分類に対してより堅牢で識別力があるためです。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: MuST-Cinema: a Speech-to-Subtitles corpus -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_4.html">
      MuST-Cinema: a Speech-to-Subtitles corpus
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この作品では、TED字幕から構築された多言語音声翻訳コーパスであるMuST-Cinemaを紹介します。コーパスは（オーディオ、転写、翻訳）トリプレットで構成されています。これは、字幕の効率的な自動アプローチの開発に大きな制限を課します。字幕の長さと形式は、発話の長さに直接依存します。 
[要約]神経機械翻訳は字幕の自動化に貢献できます。字幕はターンアラウンド時間と関連コストを削減できます。既存の字幕コーパスは字幕の字幕に関する情報が必要です。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Aspect and Opinion Term Extraction for Aspect Based Sentiment Analysis
  of Hotel Reviews Using Transfer Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_5.html">
      Aspect and Opinion Term Extraction for Aspect Based Sentiment Analysis
  of Hotel Reviews Using Transfer Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      I-ASPECTおよびB-SENTIMENTの場合、F1スコアを11％増加させることさえできます。デフォルトのBERTモデルが単純なargmaxメソッドを上回ることができなかったことを示します。アスペクトベースのセンチメント分析のタスクの1つは、レビューテキストからのアスペクトと意見の用語。 
[ABSTRACT]私たちの研究は、Bahasa indonesiaのホテルレビューからトークンを抽出するために、bertを使用した移転学習の評価に焦点を当てています。デフォルトのbert tokenizerは、エンティティレベルの評価で、関心のあるフォームのf1スコアを少なくとも5％改善できることを示していますトークナイザーの調整により、87％および89％のf1スコアを達成できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-26">
        <br>2019-09-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Exploring BERT Parameter Efficiency on the Stanford Question Answering
  Dataset v2.0 -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_6.html">
      Exploring BERT Parameter Efficiency on the Stanford Question Answering
  Dataset v2.0
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      arXiv：1907.10597で提案されている浮動小数点演算の効率に基づいてこれらのモデルを評価することはしませんが、トレーニング時間、推論時間、およびモデルパラメーターの総数に関する効率を調べます。さらに、コンテキストを使用して実験します。 SQuAD2.0タスクの最終的な拡張レイヤーとして、$ arXiv：1709.08294v3 $で説明されているように、-aware convolutional（CACNN）フィルター。人工知能モデルの評価基準を広げて、リソース効率のさまざまな尺度を含める。 
[ABSTRACT]さまざまな数の最終トランスフォーマー層を凍結しながら、従業員のバートの効率を評価します。また、コンテキストの追加によるf1スコアの増加を検証します。トレーニングと結論の時間の増加により、畳み込みフィルターは実用的ではありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A more abstractive summarization model -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_7.html">
      A more abstractive summarization model
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ただし、ポインタージェネレーターの基本アーキテクチャに基づくこのようなモデルはすべて、要約に新しい単語を生成できず、ほとんどの場合、ソーステキストから単語をコピーできません。ノベルティ/抽象化の量を大幅に改善します。 
[ABSTRACT]このドメインには、最近の作品があります。研究者の主題数の上に構築されています。これには、正規化されたnグラムのノベルティスコアをメトリックとして使用する、語彙のペナルティの追加が含まれます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Unsupervised Disambiguation of Syncretism in Inflected Lexicons -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_8.html">
      Unsupervised Disambiguation of Syncretism in Inflected Lexicons
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この新しいタスクの評価指標について説明し、5つの言語で結果を報告します。ニューラルネットワークを使用して、フィーチャバンドル（まれなものも含む）の事前分布をスムーズにモデル化するこのようなアプローチを示します。トークンのコンテキスト。このプロパティにより、ユニグラムタイプカウントの単純なリストを操作し、そのユニグラムの異なる分析間で各カウントを分割できます。 
[要約]特定の単語形式は、これらの機能の一部に適合する可能性があります。このモデルを使用して、機能バンドルの事前分布をモデル化できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-06-10">
        <br>2018-06-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Transformer-CNN: Fast and Reliable tool for QSAR -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_9.html">
      Transformer-CNN: Fast and Reliable tool for QSAR
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このような有効性の理由を説明し、メソッド開発の今後の方向性をドラフトします。増強学習と転移学習の両方が埋め込みに基づいているため、メソッドは小さなデータセットに対して良好な結果を提供できます。 QSARモデルのトレーニングについては、https：//github.com/bigchem/transformer-cnnで入手できます。 
[概要]埋め込みにcharnn（2）アーキテクチャを使用すると、多様なベンチマークデータセットに対してより高品質で解釈可能なqsar / qsprモデルが得られます。この方法は、小さなデータセットに対して良好な結果を提供できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-21">
        <br>2019-10-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Novel Approach to Enhance the Performance of Semantic Search in
  Bengali using Neural Net and other Classification Techniques -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_10.html">
      A Novel Approach to Enhance the Performance of Semantic Search in
  Bengali using Neural Net and other Classification Techniques
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この作業では、半教師付きおよび教師なし学習アルゴリズムを使用してセマンティック検索のパフォーマンスを改善するためのエンドツーエンドの手順を示しました。一方、セマンティック検索はこれらの問題を解決しますが、注釈の欠如、WordNetの欠如に苦しみますリソースの少ない言語の場合。検索は長い間、ユーザーが情報を取得するための重要なツールでした。 
[ABSTRACT]検索は、特定のキーワードを含むドキュメントまたはオブジェクトと一致します。ただし、検索とベストアンサーに用語がないか、共通する用語の数が非常に少ない可能性があります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-04">
        <br>2019-11-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving BERT Fine-tuning with Embedding Normalization -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_11.html">
      Improving BERT Fine-tuning with Embedding Normalization
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このような偏った埋め込みは、
[CLS]埋め込みの勾配が爆発し、モデルのパフォーマンスが低下する可能性があるため、微調整中に最適化プロセスに課題をもたらします。チューニング.. BERTのような事前トレーニング済みの大規模な文エンコーダーは、自然言語処理の新しい章を開始します。 
[概要]事前に訓練されたbertをシーケンス分類タスクに適用する一般的な方法。これには、bertと分類器のモデルパラメーターを一緒に微調整することが含まれます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-10">
        <br>2019-11-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Are All Languages Equally Hard to Language-Model? -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_12.html">
      Are All Languages Equally Hard to Language-Model?
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      次に、21の言語で調査を行い、一部の言語では、$ n $ -gramとLSTM言語モデルの両方で情報のテキスト表現を予測するのが難しいことを示しています。パフォーマンスの違いの原因となる複雑な屈折形態を示します。この作業では、翻訳されたテキストを使用して、言語モデルを言語間で公正に比較するための評価フレームワークを開発し、すべてのモデルがほぼ同じ情報を予測するように求めています。 
[要約]言語と言語のパフォーマンスの違いの原因となる複雑な屈折情報を示します。言語モデルとの言語間の比較を公正に行うための評価フレームワークを開発します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-06-10">
        <br>2018-06-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Differentiable Reasoning over a Virtual Knowledge Base -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_13.html">
      Differentiable Reasoning over a Virtual Knowledge Base
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      各ステップで、モジュールは、スパースマトリックスTFIDFインデックスと、言及のコンテキスト表現の特別なインデックスの最大内積検索（MIPS）の組み合わせを使用します。既存のマルチホップシステム..コーパスを仮想知識ベース（KB）として使用して、複雑なマルチホップ質問に答えるタスクを検討します。 
[概要] dr hopは、異なる解釈を使用してトレーニングできる脳モジュールです。また、非常に効率的で、1秒あたり10〜100倍のクエリを処理します。drkitは、3ホップの質問で9ポイント精度を向上させます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: KEML: A Knowledge-Enriched Meta-Learning Framework for Lexical Relation
  Classification -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_14.html">
      KEML: A Knowledge-Enriched Meta-Learning Framework for Lexical Relation
  Classification
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      概念間の語彙関係の正確な予測は、そのような関係の存在を示すパターンの希薄性のために困難です。複数のデータセットでの実験は、KEMLが最新の方法より優れていることを示しています。補助タスクの確率分布はさまざまなタイプの字句関係を認識するモデルの能力を高めるために定義されています。 
[概要]概念間の語彙関係の正確な予測は挑戦的です。keml、lkb-bert（語彙知識ベース-bert）モデルは、大量のテキストコーパスから概念表現を学習するために提示されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Language-Independent Tokenisation Rivals Language-Specific Tokenisation
  for Word Similarity Prediction -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_15.html">
      Language-Independent Tokenisation Rivals Language-Specific Tokenisation
  for Word Similarity Prediction
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      8つの言語をカバーする実験結果は、語彙サイズが大きい場合にLSTが常にLITより優れていることを示していますが、LITは多くの言語で比較的小さい（つまり、平滑化逆周波数（SIF）さらに、固定サイズの語彙を使用して言語をコンパクトに表現し、目に見えない単語やまれな単語を効率的に処理できます。貧弱な単語やまれな単語を処理できる。固定サイズの語彙を使用して言語をコンパクトに表現する。見えない単語やまれな単語を効果的に処理できる。これらのツールは複数の言語でも使用できる
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Detecting Asks in SE attacks: Impact of Linguistic and Structural
  Knowledge -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_16.html">
      Detecting Asks in SE attacks: Impact of Linguistic and Structural
  Knowledge
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      Lexical Conceptual Structureなどの言語リソースを使用して質問の検出に取り組み、リンクや特定の質問への近接などの構造的な手がかりを活用して、結果の信頼性を向上させます。 ..私たちの実験は、アスク検出、フレーミング検出、およびトップアスクの識別のパフォーマンスが、リンクなどの構造的な手がかりと結合した言語的に動機付けされたクラスによって改善されることを示しています。 
[概要]私たちの研究は、アスク情報、フレーミング検出、およびトップアスクの識別のパフォーマンスが、言語的に動機付けられたクラスによって改善されることを示しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: BERT Can See Out of the Box: On the Cross-modal Transferability of Text
  Representations -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_17.html">
      BERT Can See Out of the Box: On the Cross-modal Transferability of Text
  Representations
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      BERTに基づいたテキスト生成のアーキテクチャであるBERT-genを導入します。モノモーダル表現またはマルチモーダル表現のいずれかを活用できます。異なる構成で報告された結果は、私たちの研究質問に対する肯定的な答えを示し、これは、2つの確立されたVisual Question Generationデータセットの最新技術に対する改善です。これは、これらのモデルによって学習された抽象化が、単一言語データでトレーニングされた場合でも言語間で転送できることを示唆しています。 
[概要]単一言語bertモデルは、単一言語bertもゼロ-ショットクロス-リンガル設定で競合する可能性があることを示しています。新しいモデルは、2つの確立された視覚的質問生成データセットの最先端を大幅に改善します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Leveraging Code Generation to Improve Code Retrieval and Summarization
  via Dual Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_18.html">
      Leveraging Code Generation to Improve Code Retrieval and Summarization
  via Dual Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      具体的には、デュアルラーニングを使用してコード要約とコード生成の確率的相関を明示的に活用し、コード要約とコード生成に2つのエンコーダーを使用して、マルチタスク学習によるコード取得タスクをトレーニングします。 SQLとPythonの既存のデータセット、および結果は、モデルが最先端のモデルよりもコード取得タスクの結果を大幅に改善し、コード要約タスクのBLEUスコアに関して競争力のあるパフォーマンスを達成できることを示しています..この論文では、追加のコード生成タスクを導入することにより、2つのタスクの新しいエンドツーエンドモデルを提案します。 
[ABSTRACT]以前の研究では、これら2つのタスクを組み合わせてパフォーマンスを改善しました。自然言語とプログラミング言語との関連をモデル化することができました。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-24">
        <br>2020-02-24
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Structured Variational Autoencoder for Contextual Morphological
  Inflection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_19.html">
      A Structured Variational Autoencoder for Contextual Morphological
  Inflection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      残りの未解決の研究課題の1つは次のとおりです。生のトークンレベルのデータをどのように効果的に活用して、パフォーマンスを向上させることができますか。シミュレートされた低リソース設定でUniversal Dependenciesコーパスを使用して23の言語で実験を行い、場合によっては10％を超える絶対精度の改善を見つけます。潜在変数に対する事後推論を可能にするために、効率的な変分推論手順を導出しますスリープ解除アルゴリズムで。 
[概要]ウェイク-スリープアルゴリズムに基づいて23歳のシステムを作成します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-06-10">
        <br>2018-06-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: UniMorph 2.0: Universal Morphology -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_20.html">
      UniMorph 2.0: Universal Morphology
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      UniMorphは、メリーランド州ボルチモアにあるジョンズ・ホプキンス大学の言語音声処理センター（CLSP）に拠点を置き、DARPA LORELEIプログラムが後援しています。このペーパーでは、プロジェクトリソースの収集、注釈、および普及の詳細を説明します。 LREC 2016で説明されている初期のUniMorphリリース。語彙リソース}}。追加のサポートデータとツールも、利用可能な場合は言語ごとにリリースされます。 
[ABSTRACT] universal tagset、unimorphスキーマはプロジェクトをリリースします。プロジェクトはUniversal tagsetを使用して注釈付きデータをリリースします。追加のサポートデータとツールも言語ごとの大学でリリースされます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-10-25">
        <br>2018-10-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Semantic Relatedness for Keyword Disambiguation: Exploiting Different
  Embeddings -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_21.html">
      Semantic Relatedness for Keyword Disambiguation: Exploiting Different
  Embeddings
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この論文では、トレーニング時に知られていない外部インベントリ（オントロジー）によって提供される単語と感覚の意味的関連性に基づいたキーワードの曖昧性解消へのアプローチを提案します。単語（センスインベントリ）は、埋め込みスペースに含めるために、トレーニング時に事前に知っておく必要があります。残念ながら、そのようなセンスインベントリが事前にわからない場合があります（たとえば、実行時に選択されたオントロジー）または、時間とともに進化し、その状態はトレーニング時のものとは異なります。 
[要約]これは、語義の曖昧性解消（wsd）の研究によって取り組まれています。埋め込みモデルは、語の異なる意味をキャプチャしないため、あいまいさが生じます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Automatically Learning Construction Injury Precursors from Text -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_22.html">
      Automatically Learning Construction Injury Precursors from Text
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらのテキストの中で、有効な傷害の前兆を見つけることができることを示します。より正確には、自然言語処理（NLP）、畳み込みニューラルネットワーク（CNN）および階層の2つの最先端の深層学習アーキテクチャを実験します。アテンションネットワーク（HAN）、および確立された用語頻度-逆文書頻度表現（TF-IDF）+サポートベクターマシン（SVM）アプローチ..提案された方法は、ユーザーがモデルの予測を視覚化して理解するためにも使用できます。 。 
[概要]提案された方法は、モデルの予測を視覚化して理解するためにユーザーが使用することもできます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-07-26">
        <br>2019-07-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and
  Cross-Lingual Transfer for Inflection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_23.html">
      The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and
  Cross-Lingual Transfer for Inflection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      今年は、文脈における見出し語化と形態学的特徴分析に関する新たな2番目の課題も提示します。参加チームはすべて、変曲課題のベースラインよりも精度が向上しました（ただし、レーベンシュタイン距離ではありません）。最新の神経および非神経のベースライン。 
[概要]最初のタスクは過去数年間の語形変化を進化させます。それは、高リソース言語から低リソース言語への定量的な語形変化の知識の移転を調べることを伴います
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-25">
        <br>2019-10-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Towards Learning a Generic Agent for Vision-and-Language Navigation via
  Pre-training -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_24.html">
      Towards Learning a Generic Agent for Vision-and-Language Navigation via
  Pre-training
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最近の2つのタスク、ビジョンとダイアログのナビゲーションと「Help、Anna！」新しいタスクでより効果的に学習し、これまで見えなかった環境でより一般化します。部屋から部屋へのベンチマークでは、このモデルは、パスの長さで重み付けされた成功率を47％から51％に改善します。 
[概要] 3つのvlnタスクでパフォーマンスが検証されます。詳細、学習された表現は他のタスクに改善されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Diversity-Based Generalization for Neural Unsupervised Text
  Classification under Domain Shift -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_25.html">
      Diversity-Based Generalization for Neural Unsupervised Text
  Classification under Domain Shift
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ドメイン適応アプローチは、ソースドメインから学習し、それを目に見えないターゲットドメインに一般化することを目指しています。この概念を、ニューラルネットワークの最も説明可能なコンポーネントであるアテンションレイヤーに適用します。各ヘッドが異なる学習をするように、アテンションヘッド間の多様性制約をモデル化し、注入します。 
[要約]マルチヘッドアテンションモデルを作成し、各ヘッドのアテンションヘッド間に多様性制約を注入します。モデルは予測のために同じ機能に依存することはできません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Small-Footprint Open-Vocabulary Keyword Spotting with Quantized LSTM
  Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_26.html">
      Small-Footprint Open-Vocabulary Keyword Spotting with Quantized LSTM
  Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちのアプローチは、CTCで訓練されたネットワークの予測のいくつかの特性を利用して、信頼スコアを較正し、高速検出アルゴリズムを実装します。モデルは、コネクショニストの時間で訓練された、量子化ロングショートタームメモリ（LSTM）ニューラルネットワークに基づいています分類（CTC）、重量は500 KB未満です。提案されたシステムは、標準のキーワードフィラーモデルアプローチよりも優れています。 
[概要]提案されたシステムは、量子化された長い短期メモリ（lstm）ニューラルネットワークに基づいています。モデル全体を再トレーニングすることなく、ユーザーが独自のキーワードを定義することができます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_27.html">
      Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      2番目のRNNを呼び出してコンテキスト内の新しい単語のスペルを生成することにより、オープン語彙言語モデルを取得します。提案する方法は、任意のクローズド語彙生成モデルを拡張するために使用できます。ニューラル言語モデリング。これら2つのRNNは、それぞれ文構造と単語構造をキャプチャし、言語学のように別々に保たれます。 
[概要]提案する方法は、語彙の範囲を拡張するために使用できます。しかし、このペーパーでは、言語モデリングのデータを特に考慮します。これらの2つのrnnsは、文構造と単語構造をキャプチャし、
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-04-23">
        <br>2018-04-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: End-to-end Emotion-Cause Pair Extraction via Learning to Link -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_28.html">
      End-to-end Emotion-Cause Pair Extraction via Learning to Link
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      結果は、提案されたモデルが有効性と効率の両方の点で一連の最先端のアプローチより優れていることを示しています。感情抽出と原因抽出は、ペア抽出をさらに後押しする補助タスクとしてモデルに組み込まれます。 ECPEに対する既存のアプローチでは、一般に2段階の方法、つまり（1）感情と原因の検出、および（2）検出された感情と原因のペアリングを採用します。 
[ABSTRACT]以前の感情原因抽出（ece）タスクは、ece.theメソッドのように事前に指定された感情節のセットを必要とせず、直感的であると同時に、ステージ間のエラー通信を含む2つの重要な問題に苦しんでいます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: On Feature Normalization and Data Augmentation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_29.html">
      On Feature Normalization and Data Augmentation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちのアプローチは高速で、機能空間で完全に動作し、従来の方法とは異なる信号を混合するため、既存の増強方法と効果的に組み合わせることができます。本論文では、抽出された1番目と2番目のモーメントを活用する新しい増強方法を提案し、機能の正規化によって再注入されます。最新のニューラルネットワークトレーニングは、一般化を改善するためのデータ増加に大きく依存しています。 
[要旨]最近、ラベルに関心が寄せられています-摂動法。トレーニングモーメント全体で機能とラベルを組み合わせて、学習した意思決定面を滑らかにします。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Declarative Memory-based Structure for the Representation of Text Data -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_30.html">
      Declarative Memory-based Structure for the Representation of Text Data
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      テキストは本質的に宣言的であるため、構造的組織はテキスト上で効率的な計算を促進します。インテリジェントコンピューティングの時代、テキスト処理の計算の進歩重要な考慮事項です。 
[要約]さまざまな言語でテキストを処理するために多くのシステムが開発されています。wordnetnetnetnetsystem.itの理解不足があります。これは、単語レベルで動作してテキスト内の個々の単語の理解を促進する意味記憶を模倣するために使用されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multimodal Transformer with Pointer Network for the DSTC8 AVSD Challenge -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_31.html">
      Multimodal Transformer with Pointer Network for the DSTC8 AVSD Challenge
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちのシステムは、自動メトリックスで高いパフォーマンスを達成し、すべての提出物の中で人間の評価で5位と6位を獲得します。入力ビデオのテキスト機能と非テキスト機能を組み合わせます。 
[要約]これは、複数のモダリティのビデオ機能で構成されているため、難しいタスクです。これらには、テキスト、ビジュアル、およびオーディオ機能が含まれます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Abstractive Snippet Generation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_32.html">
      Abstractive Snippet Generation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      抽象スニペットは、検索エンジンの結果ページでWebページを要約するために最初に作成されたテキストです。ClueWeb09およびClueWeb12全体をアンカーコンテキスト用にマイニングし、DMOZ Open Directory Projectを使用して、Webis Abstractive Snippet Corpus 2020をコンパイルします、トレーニング例として$ \ langle $ query、スニペット、document $ \ rangle $の形式の350万以上のトリプルで構成されます。この評価は、提案されたモデルとともに、新しいデータソースにより、テキストの再利用を最小限に抑えながら、使用可能なクエリバイアス抽象スニペットを作成できることを示しています。 
[ABSTRACT]抽象スニペットは著作権の問題を回避します。パーソナライズの扉を開きます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Event Detection with Relation-Aware Graph Convolutional Neural Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_33.html">
      Event Detection with Relation-Aware Graph Convolutional Neural Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ACE2005データセットの実験結果は、モデルがイベント検出の新しい最先端のパフォーマンスを達成することを示しています。さらに、コンテキスト認識リレーション更新モジュールは、単語とこれらの2つのモジュール間のリレーション表現を明示的に更新するように設計されていますしかし、これらの作品は、イベント検出のための豊富で有用な言語知識を伝えるツリー上の構文関係ラベルを無視します。 
[ABSTRACT] related-認識ツリーはリレーションと呼ばれる新しいアーキテクチャです-認識gcn.itは構文リレーションラベルを活用し、特に単語間のリレーションをモデル化します。モジュールは単語間のリレーション表現を明示的に更新するように設計されています。プロモーション方法
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: What Kind of Language Is Hard to Language-Model? -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_34.html">
      What Kind of Language Is Hard to Language-Model?
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      難しい言語に共通する特徴の質問に答えようとして、形態の複雑さに関する以前の（Cotterell et al。、2018）の観察を再現せず、代わりに複雑さを促進するように思われるデータのはるかに単純な統計を明らかにしようとします方法論的に、少なくともペアワイズ並列コーパスから言語の難易度係数を取得するために、新しいペアサンプル乗法混合効果モデルを導入します。この論文では、これらの以前の実験を13言語から69言語をカバーするように拡張します。多言語の聖書コーパスを使用している家族。 
[概要]屈折名詞が主な原因である可能性があることを発見しました。現在のモデルでモデル化するのが簡単な言語を見つけようとしています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-11">
        <br>2019-06-11
      </time>
    </span>
  </h3>
</article>
<!-- paper0: MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression
  of Pre-Trained Transformers -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/cs.CL/paper_35.html">
      MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression
  of Pre-Trained Transformers
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      具体的には、生徒にとって効果的で柔軟な、教師の最後のトランスフォーマー層の自己注意モジュールを蒸留することを提案します。さらに、教師アシスタント（Mirzadeh et al。、2019）の導入も蒸留に役立つことを示します事前トレーニング済みの大きなTransformerモデルの例。事前トレーニング済みの言語モデル（たとえば、BERT（Devlin et al。、2018）およびそのバリアント）は、さまざまなNLPタスクで顕著な成功を収めています。 
[概要]スモールモデル（学生）は、ラージモデル（教師）の自己注意モジュールを深く模倣して訓練されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Singing Voice Conversion with Disentangled Representations of Singer and
  Vocal Technique Using Variational Autoencoders -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/eess.AS/paper_0.html">
      Singing Voice Conversion with Disentangled Representations of Singer and
  Vocal Technique Using Variational Autoencoders
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      定量分析と変換されたスペクトログラムの視覚化の両方により、モデルが歌手のアイデンティティとボーカルテクニックのもつれを解き、これらの属性の変換を正常に実行できることがわかります。個別に、再構成用のジョイントデコーダを使用します。変換は、学習した潜在空間で単純なベクトル演算によって実行されます。 
[概要]提案されたモデルは、非並列コーパスでトレーニングされ、多くのもつれた変換に対応します。別のモデルは、非並列コーパスでトレーニングされ、多対セントの変換に対応します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-03">
        <br>2019-12-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: An LSTM Based Architecture to Relate Speech Stimulus to EEG -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/eess.AS/paper_1.html">
      An LSTM Based Architecture to Relate Speech Stimulus to EEG
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ただし、人間の脳の聴覚処理は複雑で高度に非線形であるため、線形モデルのデコードパフォーマンスは制限されます。モデルは、CEGを使用してEEGとエンベロープの短いセグメントを共通の埋め込みスペースにマッピングします。自然な音声と記録された脳波（EEG）との関係をモデル化することは、脳が音声を処理する方法を理解するのに役立ち、神経科学と脳コンピュータのインターフェースでさまざまな用途があります。 
[概要]提案されたモデルは、分類問題の非線形モデルです。これらのペア（たとえば、音声エンベロープ）が互いに対応するかどうかが含まれます。新しいモデルは、脳の応答遅延に対する脳の応答にも役立ちます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Controllable Sequence-To-Sequence Neural TTS with LPCNET Backend for
  Real-time Speech Synthesis on CPU -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/eess.AS/paper_2.html">
      Controllable Sequence-To-Sequence Neural TTS with LPCNET Backend for
  Real-time Speech Synthesis on CPU
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      PCMと比較したMOSスコアの低下に関しては、システムは品質が6.1〜6.5％、表現力が6.3〜7.0％と低く、Wavenetボコーダーバックエンドを備えた同様のシステムと比較して同等またはそれ以上の品質に達しました。システムは、推論時に文レベルのペースと表現力の制御を可能にします。提案したシステムが汎用CPUでリアルタイムに高品質の22 kHz音声を合成できることを実証します。 
[概要]通常、結合システムはpcでのリアルタイム音声合成には重すぎます。このシステムを使用して、音声音声をpcmよりも多様にすることができます。このシステムは、22の異なるレベルの音声音声にも追加できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Antinociceptive modulation by the adhesion-GPCR CIRL promotes mechanosensory signal discrimination -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/biorxiv.physiology/paper_0.html">
      Antinociceptive modulation by the adhesion-GPCR CIRL promotes mechanosensory signal discrimination
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      しかし、タッチセンシティブなニューロンにおけるその効果とは対照的に、CIRLは機械刺激に対する侵害受容器の応答を減衰させます。光遺伝学的in vivo実験は、CIRLが両方の機械感覚サブモダリティにおけるcAMPレベルをクエンチすることを示します。本研究では、Cirlが生理学的および病態生理学的条件下で侵害防御行動を調整する高閾値機械的侵害受容器。 
[概要] cgpはagpcrファミリーの最も古いメンバーの1つであり、低閾値機械受容器の機械感覚信号伝達を増幅することにより、穏やかな触覚と音に対する幼虫の恐怖の感覚反応を敏感にします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: 14-3-3ζ regulates lipolysis by influencing adipocyte maturity -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-26/biorxiv.physiology/paper_1.html">
      14-3-3ζ regulates lipolysis by influencing adipocyte maturity
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      同様に、成熟3T3-L1脂肪細胞における14-3-3 {zeta}の枯渇は脂肪分解を障害し、初代マウス脂肪細胞と同様に、14-3-3 {zeta}の存在量を減らすと成熟脂肪細胞遺伝子の発現が減少しました。 -3 {zeta}は、性腺白色脂肪組織（gWAT）の脂肪細胞間で不均一に発現することがわかり、14-3-3 {zeta}を削除すると、小さい（&lt;200m）脂肪細胞の数が大幅に減少しました。 14-3-3 {zeta}のPPAR {gamma}およびHSLタンパク質の量も減少し、脂肪細胞の成熟に関連する遺伝子の発現が大幅に減少しました。 
[要旨] pkaによるpkaのhslのリン酸化は、14-3-3タンパク質のリン酸化結合部位を生成します。また、脂肪細胞の成熟に関連する遺伝子の発現を減少させます。オスのネズミ
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br>2020-02-25
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
