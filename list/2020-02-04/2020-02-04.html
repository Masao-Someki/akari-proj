<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
<title>Akari-2020-02-04の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
  <div class="header-logo">
    <a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
  </div>
</header>
<input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">

<div class="menubar">
<span class="bar"></span>
<span class="bar"></span>
<span class="bar"></span>
</div>

<ul>
<li><a id="home" href="../../index.html">Home</a></li>
<li><a id="about" href="../../teamAkariとは.html">About</a></li>
<li><a id="contact" href="../../contact.html">Contact</a></li>
<li><a id="contact" href="../../list/newest.html">New Papers</a></li>
<li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
</ul>

</label>
<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Within-sample variability-invariant loss for robust speaker recognition
  under noisy environments -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.SD/paper_0.html">
      Within-sample variability-invariant loss for robust speaker recognition
  under noisy environments
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、クリーンでノイズの多い発話ペアをオンザフライで生成するためのデータ準備戦略を調査します。ネットワークは、元の話者識別損失と、サンプル内の変動不変損失の補助で訓練されます。 
[要旨]論文では、「きれいな」埋め込みを学習するために話者埋め込みネットワークをトレーニングします。この戦略は、各トレーニングステップで同じクリーンな発声に対して異なるノイズの多いコピーを生成します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Delving into VoxCeleb: environment invariant speaker recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.SD/paper_1.html">
      Delving into VoxCeleb: environment invariant speaker recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      環境の敵対的トレーニングにより、ネットワークは見えない条件により良く一般化できます。タスクに適したより強力なアーキテクチャや損失関数を求めて多くの研究が行われていますが、これらの研究はモデルがどのような情報を学習したかを考慮していません。与えられたラベルを予測できることとは別に、VoxCelebデータセットで以前に使用されていない「ビデオ」情報を利用することでこれを達成します。 
[要約]システムは、voxcelebデータセットを使用して、話者の識別と検証の両方のタスクで評価されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-24">
        <br>2019-10-24
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Time Difference of Arrival Estimation from Frequency-Sliding Generalized
  Cross-Correlations Using Convolutional Neural Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.SD/paper_2.html">
      Time Difference of Arrival Estimation from Frequency-Sliding Generalized
  Cross-Correlations Using Convolutional Neural Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      深層学習ベースの画像ノイズ除去ソリューションから着想を得て、本論文では、不利な音響条件で抽出されたFS-GCCに含まれる時間遅延パターンを学習するために、畳み込みニューラルネットワーク（CNN）の使用を提案します。 GCC（FS-GCC）は、クロスパワースペクトル位相のサブバンド解析に基づいたTDEの新しい手法として提案され、異なる周波数帯域に含まれる時間遅延情報の構造化された2次元表現を提供します。提案されたアプローチが優れたTDEパフォーマンスを提供し、異なる部屋とセンサーの設定に一般化できることを確認します。 
[ABSTRACT]不利な設定での時間遅延推定（tde）は挑戦です。gccに基づくサブ耐性アプローチは何十年も広く使用されてきました。 in fs-悪い音響条件で抽出されたgcc
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Tensor-to-Vector Regression for Multi-channel Speech Enhancement based
  on Tensor-Train Network -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.SD/paper_3.html">
      Tensor-to-Vector Regression for Multi-channel Speech Enhancement based
  on Tensor-Train Network
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      次に、TTNがDNNに匹敵する音声強調品質を達成できるが、パラメータがはるかに少ないことを示します。たとえば、単一チャネルのシナリオでは、2700万から500万のパラメータに減少します。 DNNからTTNベースの回帰。さらに、TTNは、設計により多次元テンソル入力を処理できます。これは、マルチチャネル音声強調の目的の設定に正確に一致します。 
[要旨]重要な考え方は、従来のディープニューラルネットワーク（dnn）ベースの負債をキャストすることです。代わりに、ttnはdnnの表現力を維持しますが、トレーニング可能なモデルの量ははるかに少なくなります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving sequence-to-sequence speech recognition training with
  on-the-fly data augmentation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.SD/paper_4.html">
      Improving sequence-to-sequence speech recognition training with
  on-the-fly data augmentation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      データ増強方法の1つは文献から得られますが、他の2つの方法は私たち自身の開発です-周波数領域での時間摂動とサブシーケンスサンプリングです。スイッチボードとフィッシャーのデータに関する私たちの実験は、音声トレーニングデータのみでトレーニングされ、追加のテキストデータを使用しないS2Sモデルの最新のパフォーマンスを示しています。 
[要約]オーバーフィッティングは、より優れたアーキテクチャから得られるパフォーマンスの改善を上回っています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-29">
        <br>2019-10-29
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Regularized Fast Multichannel Nonnegative Matrix Factorization with
  ILRMA-based Prior Distribution of Joint-Diagonalization Process -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.SD/paper_5.html">
      Regularized Fast Multichannel Nonnegative Matrix Factorization with
  ILRMA-based Prior Distribution of Joint-Diagonalization Process
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      BSS実験から、提案手法は従来のFastMNMFよりもソース分離精度がほぼ同じ計算時間で優れていることを示します。これらの問題を解決するために、まず、対角化プロセスとデミキシングシステムの密接な関係を明らかにします。独立低ランク行列分析（ILRMA）..次に、この事実に動機付けられて、ILRMAでサポートされる新しい正規化FastMNMFを提案し、収束保証されたパラメーター更新規則を導き出します。 
[要約] fastmnmfは、マルチチャネル非負行列因子分解の高速バージョンとして提案されていますが、新しい方法は、ほぼ同じ時間で精度を制限することにより、従来のfastmnmfよりも優れています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Modeling ASR Ambiguity for Dialogue State Tracking Using Word Confusion
  Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.SD/paper_6.html">
      Modeling ASR Ambiguity for Dialogue State Tracking Using Word Confusion
  Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      弊社のconfnetエンコーダーは、DSTの最新の「Global-locally Self-Attentive Dialogue State Tacker」（GLAD）モデルにプラグインされており、トップN ASR仮説の使用と比較して、精度と推論時間の両方で大幅な改善を実現しています。 。本論文では、最新のニューラルダイアログステートトラッカー（DST）を使用して混乱ネットワークを使用する利点を研究します。注意深い混乱を使用して、2次元のconfnetを1次元の埋め込みシーケンスにエンコードします。任意のDSTシステムで使用できるネットワークエンコーダー。 
[概要] asrシステムはどのdst systemでも使用できます。対話システムに使用されますが、top-n asrリストよりも豊富な仮説空間のコンパクトな表現があります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: End-to-End Automatic Speech Recognition Integrated With CTC-Based Voice
  Activity Detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.SD/paper_7.html">
      End-to-End Automatic Speech Recognition Integrated With CTC-Based Voice
  Activity Detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      セグメント化されていないデータの実験結果は、提案された方法が従来のエネルギーベースおよびニューラルネットワークベースのVADメソッドを使用してベースラインメソッドを上回り、0.2未満のRTFを達成したことを示します。非音声領域。提案された方法は公開されています。 
[要約]この予測には、連続した長い空白予測が含まれます。しきい値は、非音声領域の長さに直接関係します。提案された方法は公開されています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: CoTK: An Open-Source Toolkit for Fast Development and Fair Evaluation of
  Text Generation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_0.html">
      CoTK: An Open-Source Toolkit for Fast Development and Fair Evaluation of
  Text Generation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      テキスト生成の評価では、一貫性のない実験設定やメトリックの実装など、多くの実用的な問題はしばしば無視されますが、不公平な評価と受け入れ難い結論につながります。開発ステップを標準化し、一貫性のない実験設定につながる可能性のある人為ミスを減らします。ユニークな機能であるCoTKは、いつどのメトリックを公正に比較できないかを示すことができます。 
[要約]テキスト生成の高速開発と公正な実装をサポートすることを目的とした実験ツールキットcotkを紹介します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Torch-Struct: Deep Structured Prediction Library -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_1.html">
      Torch-Struct: Deep Structured Prediction Library
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      Torch-Structは、https：//github.com/harvardnlp/pytorch-structで入手できます。内部には、クロスアルゴリズム効率を提供するための汎用的な最適化も多数含まれています。実験では、高速ベースラインとケーススタディは、ライブラリの利点を示しています。 
[ABSTRACT]ライブラリは、structuralructを使用して、シンプルなsystem.itを活用し、統合します。多くのアルゴリズムに基づいて、システムの洞察を提供します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Learning Contextualized Document Representations for Healthcare Answer
  Retrieval -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_2.html">
      Learning Contextualized Document Representations for Healthcare Answer
  Retrieval
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      一般化されたモデルは、ヘルスケアパッセージランキングのいくつかの最先端のベースラインを大幅に上回り、追加の微調整を行うことなく異種ドメインに適応できることを示します。コンテキストディスコースベクトル（CDV）、長い医療文書からの効率的な回答の取得。Webから9つの英語の公衆衛生リソースから一貫した回答を検索するためにCDVモデルを適用し、患者と医療専門家の両方に対応します。 
[ABSTRACT]私たちのアプローチは、エンティティの構造化された検索タプルに基づいています。我々は、短い表現でクエリを解決するために連続表現を使用します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Phylogenetic signal in phonotactics -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_3.html">
      Phylogenetic signal in phonotactics
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      オーストラリアの言語は、高度の音韻的均一性を持つと特徴付けられています。3つのデータセットをテストします。（1）辞書内のbiphone（2セグメントシーケンス）の有無を記録するバイナリ変数（2）セグメント間の遷移の頻度、 （3）自然音のクラス間の遷移の頻度。これらの結果は、歴史的および比較言語学で容易に抽出可能なデータの新しいソースを使用することの実行可能性を示しています。 
[ABSTRACT]たとえばバイナリデータを含む3つのデータセットをテストしますが、すべてのデータセットで進化シグナルを検出します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multi Sense Embeddings from Topic Models -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_4.html">
      Multi Sense Embeddings from Topic Models
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      また、各トピック内の単語の確率的表現によって決定される埋め込みを除去する方法も導入します。埋め込みを使用して、コンテキストと単語の類似性を強力にキャプチャし、さまざまな最先端の実装よりも優れていることを示します。 。これらの表現は各単語に単一のベクトルのみを割り当てますが、多数の単語は多義的です（つまり、複数の意味を持ちます）。 
[概要]トピックモデリングベースのスキップグラムアプローチを提案します。複数のプロトタイプの単語の埋め込みを学習するために使用します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-17">
        <br>2019-09-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: How Far are We from Effective Context Modeling ? An Exploratory Study on
  Semantic Parsing in Context -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_5.html">
      How Far are We from Effective Context Modeling ? An Exploratory Study on
  Semantic Parsing in Context
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最近、文脈における意味解析はかなりの注目を集めており、複雑な文脈現象が存在するため困難です。 ..文法ベースのデコードセマンティックパーサーを提示し、その上に典型的なコンテキストモデリング手法を適合させます。 
[概要] 2つの大きな複雑なクロスドメインデータセットで13のコンテキストモデリング手法を評価します。最良のモデルは、両方のデータセットで最先端のパフォーマンスを大幅に改善して達成します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Tensor-to-Vector Regression for Multi-channel Speech Enhancement based
  on Tensor-Train Network -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_6.html">
      Tensor-to-Vector Regression for Multi-channel Speech Enhancement based
  on Tensor-Train Network
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      次に、TTNがDNNに匹敵する音声強調品質を達成できるが、パラメーターがはるかに少ないことを示します。たとえば、単一チャネルのシナリオでは、2700万から500万のパラメーターに減少します。設計による多次元テンソル入力。これは、マルチチャネル音声拡張の目的の設定と完全に一致します。実装は、オンラインでhttps://github.com/uwjunqi/Tensor-Train-Neural-Networkで入手できます。 
[要旨]重要な考え方は、従来のディープニューラルネットワーク（dnn）ベースの負債をキャストすることです。代わりに、ttnはdnnの表現力を維持しますが、トレーニング可能なモデルの量ははるかに少なくなります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention
  and Spatial Memory -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_7.html">
      Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention
  and Spatial Memory
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      アノテーションタスクはAMTプラットフォームでクラウドソーシングされ、$ 10,714 $ルートの新しいTalk2Navデータセットを構築しました。2番目の貢献は、新しい学習方法です。.ナビゲーション指示の精神概念化に関する空間認知研究に触発され、ソフトデュアルを導入しますセグメント化された言語の指示を介して定義されたアテンションメカニズムは、2つの部分的な指示を共同で抽出します.1つは、次の視覚的なランドマークを一致させるためのもので、もう1つは、ローカルの方向を次のランドマークに一致させるためのものです。 
[概要]ナビゲーションシステムはgoogleストリートビューに基づいています。言葉によるナビゲーション指示に基づいて視覚的なガイダンスを提供します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-04">
        <br>2019-10-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving Question Generation with Sentence-level Semantic Matching and
  Answer Position Inferring -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_8.html">
      Improving Question Generation with Sentence-level Semantic Matching and
  Answer Position Inferring
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、我々のモデルがSQuADおよびMARCOデータセットの最先端（SOTA）モデルよりも優れていることを示しています。一般性、私たちの仕事は既存のモデルを大幅に改善します。 
[ABSTRACT]私たちのモデルは、チームとマルコのデータセットに関する最新のモデルよりも優れています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-02">
        <br>2019-12-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: ALBERT: A Lite BERT for Self-supervised Learning of Language
  Representations -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_9.html">
      ALBERT: A Lite BERT for Self-supervised Learning of Language
  Representations
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      その結果、BERTラージに比べてパラメーターが少なく、GLUE、RACE、およびSQuADベンチマークで最高のモデルが新しい最先端の結果を確立します。コードと事前学習済みモデルはhttps：// githubで入手できます。 .com / google-research / ALBERT ..また、文間の一貫性のモデリングに焦点を当てた自己監視型の損失を使用し、それが一貫して複数文の入力を伴うダウンストリームタスクに役立つことを示します。元のBERTと比較してはるかに優れたスケーリングを行うモデルに。 
[ABSTRACT]研究により、提案された方法は、元のbertよりもはるかに優れたスケールのモデルにつながることが示されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-26">
        <br>2019-09-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Bertrand-DR: Improving Text-to-SQL using a Discriminative Re-ranker -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_10.html">
      Bertrand-DR: Improving Text-to-SQL using a Discriminative Re-ranker
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      スキーマにとらわれないBERT微調整分類子としてリランカを構築します。リランカを2つの最新のテキストからSQLモデルに適用し、トップ4スコアを達成することにより、リランカの有効性を実証します。この記事の執筆時点でSpiderリーダーボードに掲載されています。さまざまなクエリの難易度レベルで、テキストからSQLおよび再ランキングモデルの相対的な強さを分析し、最適なパフォーマンスを得るために2つのモデルを組み合わせる方法を提案します
[要約]テキスト-to ------------- in-resistanceモデルは、パフォーマンスを改善するために開発されています。彼らは、ユーザーのパフォーマンスを改善する新しいツールを提案します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Fact-aware Sentence Split and Rephrase with Permutation Invariant
  Training -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_11.html">
      Fact-aware Sentence Split and Rephrase with Permutation Invariant
  Training
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      課題を克服するために、まず、モデルが長い文から事実を学習することを可能にし、それにより文分割の精度を向上させる、事実認識文エンコーディングを提案します。次に、このタスクのseq2seq学習における順序分散の影響を軽減するために、順列不変トレーニングを導入します。（2）生成される単純な文の順序分散は、トレーニング中にseq2seqモデルを混乱させる可能性があります。さらに、oie-benchmarkの外部評価は、OpenIEのパフォーマンスを向上させるために、前処理として最新のモデルで長い文を分割することが役立つという観察により、アプローチの有効性を検証します。 
[ABSTRACT]以前の研究では、seqは並列文のペアから学習します。入力として複雑な文を受け取り、一連の単純な文を生成します。結果は、トレーニング中にseq2seqモデルを混乱させる可能性があります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-16">
        <br>2020-01-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Modeling ASR Ambiguity for Dialogue State Tracking Using Word Confusion
  Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_12.html">
      Modeling ASR Ambiguity for Dialogue State Tracking Using Word Confusion
  Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      弊社のconfnetエンコーダーは、DSTの最新の「Global-locally Self-Attentive Dialogue State Tacker」（GLAD）モデルにプラグインされており、トップN ASR仮説の使用と比較して、精度と推論時間の両方で大幅な改善を実現しています。 。本論文では、最新のニューラルダイアログステートトラッカー（DST）を使用して混乱ネットワークを使用する利点を研究します。注意深い混乱を使用して、2次元のconfnetを1次元の埋め込みシーケンスにエンコードします。任意のDSTシステムで使用できるネットワークエンコーダー。 
[概要] asrシステムはどのdst systemでも使用できます。対話システムに使用されますが、top-n asrリストよりも豊富な仮説空間のコンパクトな表現があります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: End-to-End Automatic Speech Recognition Integrated With CTC-Based Voice
  Activity Detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_13.html">
      End-to-End Automatic Speech Recognition Integrated With CTC-Based Voice
  Activity Detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      セグメント化されていないデータの実験結果は、提案された方法が従来のエネルギーベースおよびニューラルネットワークベースのVAD方法を使用してベースライン方法を上回り、0.2未満のRTFを達成したことを示しています。しきい値処理..この予測には、連続した長い空白ラベルが含まれます。これは、非音声領域と見なすことができます。 
[要約]この予測には、連続した長い空白予測が含まれます。しきい値は、非音声領域の長さに直接関係します。提案された方法は公開されています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Within-sample variability-invariant loss for robust speaker recognition
  under noisy environments -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/eess.AS/paper_0.html">
      Within-sample variability-invariant loss for robust speaker recognition
  under noisy environments
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      VoxCeleb1の実験は、提案されたトレーニングフレームワークが、クリーンな条件とノイズの多い条件の両方で話者検証システムのパフォーマンスを向上させることを示しています。各トレーニングステップで同じクリーンな発声に対して異なるノイズのあるコピーを作成し、ノイズの多い環境でスピーカー埋め込みネットワークをより一般化できるようにします。 
[要旨]論文では、「きれいな」埋め込みを学習するために話者埋め込みネットワークをトレーニングします。この戦略は、各トレーニングステップで同じクリーンな発声に対して異なるノイズの多いコピーを生成します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Delving into VoxCeleb: environment invariant speaker recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/eess.AS/paper_1.html">
      Delving into VoxCeleb: environment invariant speaker recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この方法は、VoxCelebデータセットを使用して話者の識別と検証の両方のタスクで評価されます。VoxCelebデータセットでは、ベースラインよりも大幅にパフォーマンスが向上します。ネットワークを見えない条件により良く一般化する。 
[要約]システムは、voxcelebデータセットを使用して、話者の識別と検証の両方のタスクで評価されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-24">
        <br>2019-10-24
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Time Difference of Arrival Estimation from Frequency-Sliding Generalized
  Cross-Correlations Using Convolutional Neural Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/eess.AS/paper_2.html">
      Time Difference of Arrival Estimation from Frequency-Sliding Generalized
  Cross-Correlations Using Convolutional Neural Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最近、周波数スライディングGCC（FS-GCC）は、クロスパワースペクトル位相のサブバンド解析に基づいたTDEの新しい手法として提案され、異なる領域に含まれる時間遅延情報の構造化された2次元表現を提供します。周波数帯域..我々の実験は、提案されたアプローチが優れたTDE性能を提供すると同時に、異なる部屋とセンサーの設定に一般化できることを確認します。 （CNN）悪音響条件で抽出されたFS-GCCに含まれる時間遅延パターンを学習します。 
[ABSTRACT]不利な設定での時間遅延推定（tde）は挑戦です。gccに基づくサブ耐性アプローチは何十年も広く使用されてきました。 in fs-悪い音響条件で抽出されたgcc
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Tensor-to-Vector Regression for Multi-channel Speech Enhancement based
  on Tensor-Train Network -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/eess.AS/paper_3.html">
      Tensor-to-Vector Regression for Multi-channel Speech Enhancement based
  on Tensor-Train Network
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      次に、TTNがDNNに匹敵する音声強調品質を達成できるが、パラメーターがはるかに少ないことを示します。たとえば、単一チャネルシナリオでは、2700万から500万に減少します。TTNは最近登場したソリューションです。完全に接続された隠れ層を備えた深いモデルのコンパクトな表現のために。最初に、DNNからTTNベースの回帰への理論的拡張を提供します。 
[要旨]重要な考え方は、従来のディープニューラルネットワーク（dnn）ベースの負債をキャストすることです。代わりに、ttnはdnnの表現力を維持しますが、トレーニング可能なモデルの量ははるかに少なくなります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving sequence-to-sequence speech recognition training with
  on-the-fly data augmentation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/eess.AS/paper_4.html">
      Improving sequence-to-sequence speech recognition training with
  on-the-fly data augmentation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      データ増強方法の1つは文献から得られますが、他の2つの方法は私たち自身の開発です-周波数領域での時間摂動とサブシーケンスサンプリングです。スイッチボードとフィッシャーのデータに関する私たちの実験は、音声トレーニングデータのみでトレーニングされ、追加のテキストデータを使用しないS2Sモデルの最新のパフォーマンスを示しています。 
[要約]オーバーフィッティングは、より優れたアーキテクチャから得られるパフォーマンスの改善を上回っています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-29">
        <br>2019-10-29
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Regularized Fast Multichannel Nonnegative Matrix Factorization with
  ILRMA-based Prior Distribution of Joint-Diagonalization Process -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/eess.AS/paper_5.html">
      Regularized Fast Multichannel Nonnegative Matrix Factorization with
  ILRMA-based Prior Distribution of Joint-Diagonalization Process
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      次に、この事実に動機付けて、ILRMAによってサポートされる新しい正規化FastMNMFを提案し、収束保証パラメーター更新ルールを導出します。BSS実験から、提案された方法が、ほぼ同じ計算でソース分離精度において従来のFastMNMFよりも優れていることを示します..しかし、そのソース分離パフォーマンスは改善されず、ジョイント対角化プロセスの物理的意味は不明確でした。 
[要約] fastmnmfは、マルチチャネル非負行列因子分解の高速バージョンとして提案されていますが、新しい方法は、ほぼ同じ時間で精度を制限することにより、従来のfastmnmfよりも優れています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Modeling ASR Ambiguity for Dialogue State Tracking Using Word Confusion
  Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/eess.AS/paper_6.html">
      Modeling ASR Ambiguity for Dialogue State Tracking Using Word Confusion
  Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このペーパーでは、最先端のニューラルダイアログステートトラッカー（DST）を使用して混乱ネットワークを使用する利点を研究します。当社のconfnetエンコーダーは、最先端の「Global-locally Self- DSTのAttentionive Dialogue State Tacker（GLAD）モデルは、上位N ASR仮説を使用する場合と比較して、精度と推論時間の両方で大幅な改善を実現します。任意のDSTシステムで使用できるネットワークエンコーダー。 
[概要] asrシステムはどのdst systemでも使用できます。対話システムに使用されますが、top-n asrリストよりも豊富な仮説空間のコンパクトな表現があります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: End-to-End Automatic Speech Recognition Integrated With CTC-Based Voice
  Activity Detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/eess.AS/paper_7.html">
      End-to-End Automatic Speech Recognition Integrated With CTC-Based Voice
  Activity Detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この論文では、音声アクティビティ検出（VAD）機能とエンドツーエンドの自動音声認識を、オンライン音声インターフェースに向けて統合し、非常に長い音声録音を転写します。実験セグメント化されていないデータの結果は、提案された方法が従来のエネルギーベースおよびニューラルネットワークベースのVAD方法を使用してベースラインメソッドを上回り、0.2未満のRTFを達成したことを示しています。 
[要約]この予測には、連続した長い空白予測が含まれます。しきい値は、非音声領域の長さに直接関係します。提案された方法は公開されています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="">
      
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本日更新された論文はありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="">
        <br>
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
