<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-02-04の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
  <div class="header-logo">
    <a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
  </div>


</header>
<input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">

<div class="menubar">
<span class="bar"></span>
<span class="bar"></span>
<span class="bar"></span>
</div>

<ul>
<li><a id="home" href="../../index.html">Home</a></li>
<li><a id="about" href="../../teamAkariとは.html">About</a></li>
<li><a id="contact" href="../../contact.html">Contact</a></li>
<li><a id="contact" href="../../list/newest.html">New Papers</a></li>
<li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
</ul>

</label>
<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Within-sample variability-invariant loss for robust speaker recognition
  under noisy environments -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.SD/paper_0.html">
      Within-sample variability-invariant loss for robust speaker recognition
  under noisy environments
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      具体的には、ネットワークは元の話者識別損失と補助的なサンプル内変動不変損失を使用して訓練されます。VoxCeleb1の実験は、提案された訓練フレームワークがクリーンな条件とノイズの多い条件の両方で話者検証システムのパフォーマンスを改善することを示しています。戦略は、各クリーンなステップで同じクリーンな発話に対して異なるノイズのあるコピーを生成し、ノイズを含む環境下でネットワークを埋め込むスピーカーの一般化を支援します。 
[要旨]論文では、「きれいな」埋め込みを学習するために話者埋め込みネットワークをトレーニングします。この戦略は、各トレーニングステップで同じクリーンな発声に対して異なるノイズの多いコピーを生成します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Delving into VoxCeleb: environment invariant speaker recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.SD/paper_1.html">
      Delving into VoxCeleb: environment invariant speaker recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これは、VoxCelebデータセットで以前に使用されていない「ビデオ」情報を利用することで達成されます。この方法は、VoxCelebデータセットを使用して話者の識別と検証タスクの両方で評価されます。ネットワークを見えない条件により良く一般化する。 
[要約]システムは、voxcelebデータセットを使用して、話者の識別と検証の両方のタスクで評価されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-24">
        <br>2019-10-24
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Time Difference of Arrival Estimation from Frequency-Sliding Generalized
  Cross-Correlations Using Convolutional Neural Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.SD/paper_2.html">
      Time Difference of Arrival Estimation from Frequency-Sliding Generalized
  Cross-Correlations Using Convolutional Neural Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      深層学習ベースの画像ノイズ除去ソリューションから着想を得て、本論文では、不利な音響条件で抽出されたFS-GCCに含まれる時間遅延パターンを学習するために、畳み込みニューラルネットワーク（CNN）の使用を提案します。 GCC（FS-GCC）は、クロスパワースペクトル位相のサブバンド解析に基づいたTDEの新しい手法として提案され、異なる周波数帯域に含まれる時間遅延情報の構造化された2次元表現を提供します。提案されたアプローチが優れたTDEパフォーマンスを提供し、異なる部屋とセンサーの設定に一般化できることを確認します。 
[ABSTRACT]不利な設定での時間遅延推定（tde）は挑戦です。gccに基づくサブ耐性アプローチは何十年も広く使用されてきました。 in fs-悪い音響条件で抽出されたgcc
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Tensor-to-Vector Regression for Multi-channel Speech Enhancement based
  on Tensor-Train Network -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.SD/paper_3.html">
      Tensor-to-Vector Regression for Multi-channel Speech Enhancement based
  on Tensor-Train Network
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      次に、TTNがDNNに匹敵する音声強調品質を達成できるが、パラメータがはるかに少ないことを示します。たとえば、単一チャネルのシナリオでは、2700万から500万のパラメータに減少します。テンソルトレインネットワーク（TTN）フレームワークの下での従来のディープニューラルネットワーク（DNN）ベースのベクトルからベクトルへの回帰定式化。TTNは、完全に接続された隠れ層を備えたディープモデルのコンパクトな表現のための最近登場したソリューションです。 
[要旨]重要な考え方は、従来のディープニューラルネットワーク（dnn）ベースの負債をキャストすることです。代わりに、ttnはdnnの表現力を維持しますが、トレーニング可能なモデルの量ははるかに少なくなります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving sequence-to-sequence speech recognition training with
  on-the-fly data augmentation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.SD/paper_4.html">
      Improving sequence-to-sequence speech recognition training with
  on-the-fly data augmentation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      過剰適合問題の解決策の1つは、利用可能なトレーニングデータの量とデータ増強の助けを借りてトレーニングデータによって示される多様性を増やすことです。データ増強方法の1つは文献に由来しますが、他の2つの方法は私たち自身の開発です-a周波数領域での時間摂動とサブシーケンスサンプリング。スイッチボードとフィッシャーのデータに関する私たちの実験は、音声トレーニングデータのみでトレーニングされ、追加のテキストデータを使用しないS2Sモデルの最先端のパフォーマンスを示しています。 
[要約]オーバーフィッティングは、より優れたアーキテクチャから得られるパフォーマンスの改善を上回っています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-29">
        <br>2019-10-29
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Regularized Fast Multichannel Nonnegative Matrix Factorization with
  ILRMA-based Prior Distribution of Joint-Diagonalization Process -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.SD/paper_5.html">
      Regularized Fast Multichannel Nonnegative Matrix Factorization with
  ILRMA-based Prior Distribution of Joint-Diagonalization Process
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      次に、この事実に動機付けられて、ILRMAによってサポートされる新しい正規化FastMNMFを提案し、収束保証されたパラメーター更新ルールを導出します。このペーパーでは、コンボリューティブブラインドソース分離（BSS）問題に対処し、FastMNMFの新しい拡張フレームワークを提案しますこれらの問題を解決するために、まず、ジョイント対角化プロセスと独立低ランクマトリックス分析（ILRMA）で使用される分離システムとの密接な関係を明らかにします。 
[要約] fastmnmfは、マルチチャネル非負行列因子分解の高速バージョンとして提案されていますが、新しい方法は、ほぼ同じ時間で精度を制限することにより、従来のfastmnmfよりも優れています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Modeling ASR Ambiguity for Dialogue State Tracking Using Word Confusion
  Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.SD/paper_6.html">
      Modeling ASR Ambiguity for Dialogue State Tracking Using Word Confusion
  Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このペーパーでは、最先端のニューラルダイアログステートトラッカー（DST）を使用して混乱ネットワークを使用する利点を研究します。当社のconfnetエンコーダーは、最先端の「Global-locally Self- DSTのAttentionive Dialogue State Tacker（GLAD）モデルは、上位N ASR仮説を使用する場合と比較して、精度と推論時間の両方で大幅な改善を実現します。任意のDSTシステムで使用できるネットワークエンコーダー。 
[概要] asrシステムはどのdst systemでも使用できます。対話システムに使用されますが、top-n asrリストよりも豊富な仮説空間のコンパクトな表現があります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: End-to-End Automatic Speech Recognition Integrated With CTC-Based Voice
  Activity Detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.SD/paper_7.html">
      End-to-End Automatic Speech Recognition Integrated With CTC-Based Voice
  Activity Detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      セグメント化されていないデータの実験結果は、提案された方法が従来のエネルギーベースおよびニューラルネットワークベースのVAD方法を使用したベースライン方法より優れており、RTFが0.2未満であることを示しています。このペーパーでは、音声アクティビティ検出（VAD）機能とエンドツーエンドの自動音声認識を、オンライン音声インターフェイスに向けて統合し、非常に長い音声録音を転写します。 
[要約]この予測には、連続した長い空白予測が含まれます。しきい値は、非音声領域の長さに直接関係します。提案された方法は公開されています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: CoTK: An Open-Source Toolkit for Fast Development and Fair Evaluation of
  Text Generation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_0.html">
      CoTK: An Open-Source Toolkit for Fast Development and Fair Evaluation of
  Text Generation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      CoTKを使用して、特にさまざまな実験設定でモデルの開発と評価を行うのが便利であることを示します。CoTKは、テキスト生成の迅速な開発と公正な評価をサポートすることを目的としたオープンソースツールキットです。いつどのメトリックを公正に比較できないかを示すことができます。 
[要約]テキスト生成の高速開発と公正な実装をサポートすることを目的とした実験ツールキットcotkを紹介します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Torch-Struct: Deep Structured Prediction Library -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_1.html">
      Torch-Struct: Deep Structured Prediction Library
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      Torch-Structは、https：//github.com/harvardnlp/pytorch-structで入手できます。内部には、アルゴリズム間の効率を高めるための汎用最適化も含まれています。Torch-Structには、確率論の幅広いコレクションが含まれています深層学習モデルに接続するシンプルで柔軟なディストリビューションベースのAPIを介してアクセスされる構造。 
[ABSTRACT]ライブラリは、structuralructを使用して、シンプルなsystem.itを活用し、統合します。多くのアルゴリズムに基づいて、システムの洞察を提供します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Learning Contextualized Document Representations for Healthcare Answer
  Retrieval -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_2.html">
      Learning Contextualized Document Representations for Healthcare Answer
  Retrieval
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      長い医療文書から効率的な回答を取得するための分散文書表現であるコンテキスト談話ベクトル（CDV）を提示します。このモデルは、階層LSTMレイヤーとマルチタスクトレーニングを備えたデュアルエンコーダアーキテクチャを活用して、臨床エンティティの位置と側面をエンコードしますドキュメントの談話..一般化されたモデルは、ヘルスケアパッセージランキングのいくつかの最新のベースラインを大幅に上回り、追加の微調整なしで異種ドメインに適応できることを示しています。 
[ABSTRACT]私たちのアプローチは、エンティティの構造化された検索タプルに基づいています。我々は、短い表現でクエリを解決するために連続表現を使用します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Phylogenetic signal in phonotactics -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_3.html">
      Phylogenetic signal in phonotactics
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      それにもかかわらず、すべてのデータセットで系統発生シグナルを検出します。系統発生シグナルは、バイナリデータよりもきめの細かい頻度データで高く、自然クラスベースのデータで最も高くなります。 。 
[ABSTRACT]たとえばバイナリデータを含む3つのデータセットをテストしますが、すべてのデータセットで進化シグナルを検出します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multi Sense Embeddings from Topic Models -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_4.html">
      Multi Sense Embeddings from Topic Models
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      埋め込みを使用して、コンテキストと単語の類似性を強力にキャプチャし、さまざまな最先端の実装よりも優れていることを示します。この作業では、語彙セマンティクス、つまり多義性のさまざまな感覚を表現するという重要な問題に取り組みますベクトル空間の単語。これらの表現は各単語に単一のベクトルのみを割り当てますが、多数の単語は多義的です（つまり、複数の意味を持ちます）。 
[概要]トピックモデリングベースのスキップグラムアプローチを提案します。複数のプロトタイプの単語の埋め込みを学習するために使用します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-17">
        <br>2019-09-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: How Far are We from Effective Context Modeling ? An Exploratory Study on
  Semantic Parsing in Context -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_5.html">
      How Far are We from Effective Context Modeling ? An Exploratory Study on
  Semantic Parsing in Context
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、潜在的な研究の方向性を明らかにする可能性のある代表的なモデルの詳細な分析で、最も頻繁な文脈現象を要約します。最近、文脈の意味解析はかなりの注目を集めています。 .. 2つの大きな複雑なクロスドメインデータセットで13のコンテキストモデリング手法を評価し、最適なモデルにより、両方のデータセットで最先端のパフォーマンスを大幅に改善して達成します。 
[概要] 2つの大きな複雑なクロスドメインデータセットで13のコンテキストモデリング手法を評価します。最良のモデルは、両方のデータセットで最先端のパフォーマンスを大幅に改善して達成します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Tensor-to-Vector Regression for Multi-channel Speech Enhancement based
  on Tensor-Train Network -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_6.html">
      Tensor-to-Vector Regression for Multi-channel Speech Enhancement based
  on Tensor-Train Network
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      次に、TTNがDNNに匹敵する音声強調品質を達成できるが、パラメータがはるかに少ないことを示します。たとえば、単一チャネルのシナリオでは、2700万から500万のパラメータに減少します。 DNNからTTNベースの回帰。さらに、TTNは、設計により多次元テンソル入力を処理できます。これは、マルチチャネル音声強調の目的の設定に正確に一致します。 
[要旨]重要な考え方は、従来のディープニューラルネットワーク（dnn）ベースの負債をキャストすることです。代わりに、ttnはdnnの表現力を維持しますが、トレーニング可能なモデルの量ははるかに少なくなります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention
  and Spatial Memory -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_7.html">
      Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention
  and Spatial Memory
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      アノテーションタスクは、$ 10,714 $ルートの新しいTalk2Navデータセットを構築するために、AMTプラットフォームでクラウドソース化されました。学習方法。 
[概要]ナビゲーションシステムはgoogleストリートビューに基づいています。言葉によるナビゲーション指示に基づいて視覚的なガイダンスを提供します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-04">
        <br>2019-10-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving Question Generation with Sentence-level Semantic Matching and
  Answer Position Inferring -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_8.html">
      Improving Question Generation with Sentence-level Semantic Matching and
  Answer Position Inferring
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、このモデルがSQuADおよびMARCOデータセットの最先端（SOTA）モデルよりも優れていることを示しています。さらに、応答認識型のゲーテッドフュージョンメカニズムを活用してデコーダーの初期状態を強化します。グローバルな質問のセマンティクスの欠如と、回答の位置認識の悪用が鍵となる根本的な原因です。 
[ABSTRACT]私たちのモデルは、チームとマルコのデータセットに関する最新のモデルよりも優れています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-02">
        <br>2019-12-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: ALBERT: A Lite BERT for Self-supervised Learning of Language
  Representations -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_9.html">
      ALBERT: A Lite BERT for Self-supervised Learning of Language
  Representations
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      また、文間の一貫性のモデリングに焦点を当てた自己監視型の損失を使用し、それが一貫して複数文入力のダウンストリームタスクに役立つことを示します。その結果、私たちの最良のモデルは、 GLUE、RACE、およびSQuADベンチマークは、BERT-largeと比較してパラメーターが少なくなっています。コードと事前学習済みモデルはhttps://github.com/google-research/ALBERTで入手できます。これらの問題に対処するために、2つのパラメーターを提示します-メモリ消費を削減し、BERTのトレーニング速度を向上させる削減技術。 
[ABSTRACT]研究により、提案された方法は、元のbertよりもはるかに優れたスケールのモデルにつながることが示されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-26">
        <br>2019-09-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Bertrand-DR: Improving Text-to-SQL using a Discriminative Re-ranker -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_10.html">
      Bertrand-DR: Improving Text-to-SQL using a Discriminative Re-ranker
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      スキーマにとらわれないBERT微調整分類子としてリランカを構築します。リレーショナルデータベースに格納されたデータにアクセスするには、ユーザーはデータベーススキーマを理解し、SQLなどのクエリ言語を使用してクエリを記述する必要があります。さまざまなクエリの難易度レベルにわたるtext-to-SQLおよびre-rankerモデルを作成し、2つのモデルを組み合わせて最適なパフォーマンスを得る方法を提案します。 
[要約]テキスト-to ------------- in-resistanceモデルは、パフォーマンスを改善するために開発されています。彼らは、ユーザーのパフォーマンスを改善する新しいツールを提案します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Fact-aware Sentence Split and Rephrase with Permutation Invariant
  Training -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_11.html">
      Fact-aware Sentence Split and Rephrase with Permutation Invariant
  Training
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      課題を克服するために、まず、モデルが長い文から事実を学習することを可能にし、それにより文分割の精度を向上させる、事実認識文エンコーディングを提案します。次に、このタスクのseq2seq学習における順序分散の影響を緩和するために、順列不変トレーニングを導入します。SentenceSplit and Rephraseは、複雑な文を意味が保持されたいくつかの単純な文に分解することを目的としています。ベンチマークでは、OpenIEのパフォーマンスを向上させるために、前処理として最新モデルを使用して長い文を分割することが役立つという観察により、アプローチの有効性を検証しています。 
[ABSTRACT]以前の研究では、seqは並列文のペアから学習します。入力として複雑な文を受け取り、一連の単純な文を生成します。結果は、トレーニング中にseq2seqモデルを混乱させる可能性があります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-16">
        <br>2020-01-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Modeling ASR Ambiguity for Dialogue State Tracking Using Word Confusion
  Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_12.html">
      Modeling ASR Ambiguity for Dialogue State Tracking Using Word Confusion
  Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      弊社のconfnetエンコーダーは、DSTの最新の「Global-locally Self-Attentive Dialogue State Tacker」（GLAD）モデルにプラグインされており、トップN ASR仮説の使用と比較して、精度と推論時間の両方で大幅な改善を実現しています。 。本論文では、最新のニューラルダイアログステートトラッカー（DST）を使用して混乱ネットワークを使用する利点を研究します。注意深い混乱を使用して、2次元のconfnetを1次元の埋め込みシーケンスにエンコードします。任意のDSTシステムで使用できるネットワークエンコーダー。 
[概要] asrシステムはどのdst systemでも使用できます。対話システムに使用されますが、top-n asrリストよりも豊富な仮説空間のコンパクトな表現があります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: End-to-End Automatic Speech Recognition Integrated With CTC-Based Voice
  Activity Detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/cs.CL/paper_13.html">
      End-to-End Automatic Speech Recognition Integrated With CTC-Based Voice
  Activity Detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      単純なしきい値処理で音声セグメントを検出するためのキューとしてラベルを使用します。セグメント化されていないデータの実験結果は、提案された方法が従来のエネルギーベースおよびニューラルネットワークベースのVADメソッドを使用してベースラインメソッドを上回り、RTF未満を達成したことを示します0.2 ..アテンションベースのアーキテクチャとは対照的に、CTC（pre）softmax出力を使用した貪欲な検索に基づいて、入力同期ラベル予測を実行できます。 
[要約]この予測には、連続した長い空白予測が含まれます。しきい値は、非音声領域の長さに直接関係します。提案された方法は公開されています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Within-sample variability-invariant loss for robust speaker recognition
  under noisy environments -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/eess.AS/paper_0.html">
      Within-sample variability-invariant loss for robust speaker recognition
  under noisy environments
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      VoxCeleb1の実験は、提案されたトレーニングフレームワークにより、クリーンな条件とノイズの多い条件の両方で話者検証システムのパフォーマンスが向上することを示しています。具体的には、ネットワークは元の話者識別損失で補助的なサンプル内変動不変損失で訓練されます。さらに、クリーンでノイズの多い発話ペアをオンザフライで生成するためのデータ準備戦略を調査します。 
[要旨]論文では、「きれいな」埋め込みを学習するために話者埋め込みネットワークをトレーニングします。この戦略は、各トレーニングステップで同じクリーンな発声に対して異なるノイズの多いコピーを生成します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Delving into VoxCeleb: environment invariant speaker recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/eess.AS/paper_1.html">
      Delving into VoxCeleb: environment invariant speaker recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この方法は、VoxCelebデータセットを使用して話者の識別と検証の両方のタスクで評価されます。VoxCelebデータセットでは、ベースラインよりも大幅にパフォーマンスが向上しています。タスクに適したより強力なアーキテクチャまたは損失関数を求めて多くの作業が行われていますが、作品は、与えられたラベルを予測できることを除いて、モデルがどのような情報を学習するかを考慮しません。この作品では、ネットワークが効果的に話者識別および環境不変埋め込みを学習できる環境敵対トレーニングフレームワークを導入しますトレーニング中の明示的なドメインシフト。 
[要約]システムは、voxcelebデータセットを使用して、話者の識別と検証の両方のタスクで評価されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-24">
        <br>2019-10-24
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Time Difference of Arrival Estimation from Frequency-Sliding Generalized
  Cross-Correlations Using Convolutional Neural Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/eess.AS/paper_2.html">
      Time Difference of Arrival Estimation from Frequency-Sliding Generalized
  Cross-Correlations Using Convolutional Neural Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      深層学習ベースの画像ノイズ除去ソリューションに触発されて、本論文では、畳み込みニューラルネットワーク（CNN）を使用して、逆音響条件で抽出されたFS-GCCに含まれる時間遅延パターンを学習することを提案します。有害なシナリオにおける時間遅延推定（TDE）は、一般的な相互相関（GCC）に基づく古典的なアプローチが何十年もの間広く使用されている挑戦的な問題です。 
[ABSTRACT]不利な設定での時間遅延推定（tde）は挑戦です。gccに基づくサブ耐性アプローチは何十年も広く使用されてきました。 in fs-悪い音響条件で抽出されたgcc
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Tensor-to-Vector Regression for Multi-channel Speech Enhancement based
  on Tensor-Train Network -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/eess.AS/paper_3.html">
      Tensor-to-Vector Regression for Multi-channel Speech Enhancement based
  on Tensor-Train Network
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      次に、TTNがDNNに匹敵する音声強調品質を達成できるが、パラメーターがはるかに少ないことを示します。たとえば、単一チャネルのシナリオでは、2700万から500万のパラメーターに減少します。入力サイズの爆発と隠れ層サイズの拡大の問題に対処するための、マルチチャネル音声強調へのベクトル回帰アプローチ。TTNは、完全に接続された隠れ層を持つ深層モデルのコンパクトな表現のための最近登場したソリューションです。 
[要旨]重要な考え方は、従来のディープニューラルネットワーク（dnn）ベースの負債をキャストすることです。代わりに、ttnはdnnの表現力を維持しますが、トレーニング可能なモデルの量ははるかに少なくなります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving sequence-to-sequence speech recognition training with
  on-the-fly data augmentation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/eess.AS/paper_4.html">
      Improving sequence-to-sequence speech recognition training with
  on-the-fly data augmentation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      スイッチボードとフィッシャーのデータに関する私たちの実験は、音声トレーニングデータのみでトレーニングされ、追加のテキストデータを使用しないS2Sモデルの最新のパフォーマンスを示しています。メソッドは私たち自身の開発です-周波数領域での時間摂動とサブシーケンスサンプリング..このホワイトペーパーでは、2つのS2Sモデルアーキテクチャのパフォーマンスに対する3つのデータ増加方法の影響を調べます。 
[要約]オーバーフィッティングは、より優れたアーキテクチャから得られるパフォーマンスの改善を上回っています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-29">
        <br>2019-10-29
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Regularized Fast Multichannel Nonnegative Matrix Factorization with
  ILRMA-based Prior Distribution of Joint-Diagonalization Process -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/eess.AS/paper_5.html">
      Regularized Fast Multichannel Nonnegative Matrix Factorization with
  ILRMA-based Prior Distribution of Joint-Diagonalization Process
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      次に、この事実に動機付けて、ILRMAでサポートされる新しい正規化FastMNMFを提案し、収束保証パラメーター更新ルールを導き出します。最近、FastMNMFは、次の空間共分散行列複数のソースを共同で対角化することができます。ただし、そのソース分離パフォーマンスは改善されず、ジョイント対角化プロセスの物理的意味は不明でした。 
[要約] fastmnmfは、マルチチャネル非負行列因子分解の高速バージョンとして提案されていますが、新しい方法は、ほぼ同じ時間で精度を制限することにより、従来のfastmnmfよりも優れています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Modeling ASR Ambiguity for Dialogue State Tracking Using Word Confusion
  Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/eess.AS/paper_6.html">
      Modeling ASR Ambiguity for Dialogue State Tracking Using Word Confusion
  Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この論文では、最先端のニューラルダイアログステートトラッカー（DST）を使用して混乱ネットワークを使用する利点を研究します。注意深い混乱ネットワークを使用して、2次元のconfnetを埋め込みの1次元シーケンスにエンコードします。任意のDSTシステムで使用できるエンコーダ。ただし、コンフュージョンネットワーク（confnet）などのASRグラフは、上位N ASRリストよりも豊富な仮説空間のコンパクトな表現を提供します。 
[概要] asrシステムはどのdst systemでも使用できます。対話システムに使用されますが、top-n asrリストよりも豊富な仮説空間のコンパクトな表現があります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: End-to-End Automatic Speech Recognition Integrated With CTC-Based Voice
  Activity Detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-04/eess.AS/paper_7.html">
      End-to-End Automatic Speech Recognition Integrated With CTC-Based Voice
  Activity Detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      注意ベースのアーキテクチャとは対照的に、入力同期ラベル予測は、CTC（pre）softmax出力を使用した貪欲な検索に基づいて実行できます。しきい値は、非音声領域の長さに直接関連しています。セグメント化されていないデータに関する実験結果は、提案された方法が、従来のエネルギーベースおよびニューラルネットワークベースのVAD方法を使用してベースラインメソッドよりも優れており、RTFが0.2未満であることを示しています。 
[要約]この予測には、連続した長い空白予測が含まれます。しきい値は、非音声領域の長さに直接関係します。提案された方法は公開されています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-03">
        <br>2020-02-03
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="">
      
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本日更新された論文はありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="">
        <br>
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
