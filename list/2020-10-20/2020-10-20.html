<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-10-20の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Classification of Manifest Huntington Disease using Vowel Distortion
  Measures -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.SD/paper_0.html">
      <font color="black">Classification of Manifest Huntington Disease using Vowel Distortion
  Measures</font>
    </a>
  </h2>
  <font color="black">そのために、HDの主要な音声症状である母音の歪みに焦点を当てます。音声ベースのパッシブモニタリングは、症状の症状を継続的に追跡することで臨床評価を強化する可能性があります。母音の特徴が顕在化前と顕在化HDを区別できることを示します。 87％の精度で。 
[概要]調査によると、プレマニフェストとマニフェストのhdを区別することは、重要であるが十分に研究されていない問題です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-16">
        <br><font color="black">2020-10-16</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Quantifying Model Uncertainty in Inverse Problems via Bayesian Deep
  Gradient Descent -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.IV/paper_0.html">
      <font color="black">Quantifying Model Uncertainty in Inverse Problems via Bayesian Deep
  Gradient Descent</font>
    </a>
  </h2>
  <font color="black">スケーラビリティは、アーキテクチャ内でハイブリッド化することによって実現されます。各ブロックの最後の層のみがベイジアンであり、他の層は決定論的であり、トレーニングに貪欲です。フレームワークは、1つの代表的な医用画像モダリティで紹介されています。ベイジアンニューラルネットワークを介してモデルの不確実性を定量化するために、スケーラブルでデータ駆動型の知識支援計算フレームワークを開発します。 
[要約]このメソッドは、に基づいて構築され、深い勾配降下法を拡張します。また、確率的フレームワーク内で再キャストします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-20">
        <br><font color="black">2020-07-20</font>
      </time>
    </span>
</section>
<!-- paper0: HVS-Based Perceptual Color Compression of Image Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.IV/paper_1.html">
      <font color="black">HVS-Based Perceptual Color Compression of Image Data</font>
    </a>
  </h2>
  <font color="black">知覚画像コーディングアプリケーションでは、主な目的は、再構成された画像の顕著な歪みを回避しながら、ビット/ピクセル（BPP）を可能な限り減らすことです。画像の知覚再構成品質に関して、PCCはすべてでSSIM = 0.99のスコアを達成します。 1つのテストを除くすべてのテストでMS-SSIM = 0.99のスコアに加えてテスト..私たちのPCCメソッドは、VVCと比較して平均52.6％のBPP削減を含む、4つの参照手法すべてと比較して大幅なBPP削減を達成します（All IntraのVVCはまだ画像コーディングモード）。 
[概要]この論文では、hvが非常に類似した波長帯域の光子を知覚的に区別できないことに基づいて、知覚色圧縮（pcc）と呼ばれる新しい知覚画像コーディング手法を提案します。評価では、提案されたデータを比較します。用途の広いビデオコーディング（vvc）や高効率ビデオ編集（hevc）などのさまざまな新しい方法を使用</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-16">
        <br><font color="black">2020-05-16</font>
      </time>
    </span>
</section>
<!-- paper0: ChestX-Det10: Chest X-ray Dataset on Detection of Thoracic Abnormalities -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.IV/paper_2.html">
      <font color="black">ChestX-Det10: Chest X-ray Dataset on Detection of Thoracic Abnormalities</font>
    </a>
  </h2>
  <font color="black">胸部X線画像の自動診断には、胸部疾患または異常のインスタンスレベルの検出が不可欠です。$ \ sim $ 3,500画像の10のカテゴリの疾患/異常のボックスレベルの注釈を含むChestX-Det10と呼ばれる新しいベンチマークを提供します。 ..注釈はhttps://github.com/Deepwise-AILab/ChestX-Det10-Datasetにあります。 
[概要]胸部X線に関する既存のほとんどの研究は疾患分類に焦点を当てています。新しいベンチマークには、10のカテゴリの疾患のボックスレベルの注釈が含まれています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-17">
        <br><font color="black">2020-06-17</font>
      </time>
    </span>
</section>
<!-- paper0: Visual-Semantic-Pose Graph Mixture Networks for Human-Object Interaction
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.IV/paper_3.html">
      <font color="black">Visual-Semantic-Pose Graph Mixture Networks for Human-Object Interaction
  Detection</font>
    </a>
  </h2>
  <font color="black">また、より小さなV-COCOデータセットで競争力のあるパフォーマンスを実現します。最初の研究では、主要なサブジェクトとオブジェクトの関係および補助的な関係から視覚的、インスタンス空間的、および意味的手がかりを動的に集約する双対グラフ注意ネットワークを提案します。推論を強化するために..Human-ObjectInteraction（HOI）検出は、アクション述語を推論します。<subject, predicate, object>トリプレット。 
[概要]幅広いコンテキストキューを効果的に処理するグラフ混合ネットワーク。その結果、さまざまなパターンを処理するネットワークが実現します。最終モデルは、挑戦的なhico-detデータセットの最先端を大幅に上回ります。ほぼ10％</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-07">
        <br><font color="black">2020-01-07</font>
      </time>
    </span>
</section>
<!-- paper0: Poisson Image Deconvolution by a Plug-and-Play Quantum Denoising Scheme -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.IV/paper_4.html">
      <font color="black">Poisson Image Deconvolution by a Plug-and-Play Quantum Denoising Scheme</font>
    </a>
  </h2>
  <font color="black">さらに、数値結果は、信号対雑音比の低いシナリオと高いシナリオの両方について、最近の最先端技術と比較した提案スキームの優位性を示しています。分散安定化変換を使用する既存のPnPアプローチとは対照的に、はデコンボリューション操作に対して不変ではないので、提案された方法はこの理論上の問題に悩まされません。量子物理学のソリューション。 
[概要] qab-pnpと呼ばれる提案されたアルゴリズムは、ポアソンノイズによく適合します。これは、制限された光子取得などのイメージングアプリケーションで非常に一般的です。また、新しい方法を使用した高度な高度な技術でもよく知られています。 qab、pnpと呼ばれる</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: Solving Bayesian Inverse Problems via Variational Autoencoders -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.IV/paper_5.html">
      <font color="black">Solving Bayesian Inverse Problems via Variational Autoencoders</font>
    </a>
  </h2>
  <font color="black">具体的には、発散ベースの変分推論から、科学的逆問題に通常存在するすべての情報がトレーニング手順で完全に利用されるようにフレームワークが導出されます。これにより、最適化が事後モデルの学習をどのように指示するかを制御する際の柔軟性が高まります。さらに、このフレームワークには、事後モデルとターゲット分布の間の距離の概念を選択できる調整可能なハイパーパラメーターが含まれています。 
[概要]システムを使用すると、モデルモデルモデルをモデル化できます。モデルは、モデルのシステムです。モデルは、すでに多くの同様の問題にリンクされています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br><font color="black">2019-12-05</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Reconstruct and Segment 3D Objects -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.IV/paper_6.html">
      <font color="black">Learning to Reconstruct and Segment 3D Objects</font>
    </a>
  </h2>
  <font color="black">対照的に、大規模な実世界の3Dデータでトレーニングされたディープニューラルネットワークを使用して一般的で堅牢な表現を学習することにより、シーンとその中のオブジェクトを理解することを目指しています。マシンに実世界を知覚する能力を与えるために人間と同じように3次元表現を行うことは、人工知能の基本的かつ長年のトピックです。これらの目的を達成するために、この論文は、単一または複数のビューからのオブジェクトレベルの3D形状推定から、シーンレベルの意味理解まで、3つの主要な貢献をします。 。 
[概要]これらを新しいオブジェクトやシナリオに一般化することは困難です。視覚的な閉塞によって引き起こされる重大な問題を克服するのに苦労しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: Inferring respiratory and circulatory parameters from electrical
  impedance tomography with deep recurrent models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.IV/paper_7.html">
      <font color="black">Inferring respiratory and circulatory parameters from electrical
  impedance tomography with deep recurrent models</font>
    </a>
  </h2>
  <font color="black">直接的な臨床的関連性のある見通しとして、食道内圧の侵襲的測定に代わる可能性のある方法として、EITと絶対気道内圧の組み合わせから絶対経肺圧を再構築する可能性をさらに示します。正確に推測できることを示します。絶対量、絶対流量、正規化された気道内圧、および特定の制限内で、EIT信号のみからの正規化された動脈血圧でさえ、事前のキャリブレーションなしで目に見えない患者に一般化されます。この作業では、証明を提案します。エンドツーエンドの方法でトレーニングされた深部学習モデルを使用して、EIT画像シーケンスから同期的に測定された呼吸または循環パラメータを再構築する方法を示す原理研究。 
[概要]絶対量、絶対流量、深く訓練された気道内圧を特定の制限内で推測できることを実証しました。これらの結果により、実証された作業で提唱されたフレームワークに基づいたさらなる研究を刺激したいと考えています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: Natural Disaster Classification using Aerial Photography Explainable for
  Typhoon Damaged Feature -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.IV/paper_8.html">
      <font color="black">Natural Disaster Classification using Aerial Photography Explainable for
  Typhoon Damaged Feature</font>
    </a>
  </h2>
  <font color="black">ターゲットフィーチャクラスの確率を使用して、災害フィーチャマップを視覚化し、色の範囲を青から黄色にスケーリングできます。このメソッドでは、損傷のない土地カバーと災害のあるエリアを含む8つのクラスを分類でき、航空写真は4,096グリッドに分割されます。は64x 64で、各ユニットイメージは48メートル四方です。回復を高速化するには、即時の応答が重要です。 
[概要]航空写真は、64 x 64の4,096グリッドに分割され、各ユニット画像は48平方メートルです。この方法を使用すると、意思決定支援のために、被災地のグローバルスクリーニングのための航空測量が必要になります。先に回復する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br><font color="black">2020-04-21</font>
      </time>
    </span>
</section>
<!-- paper0: Ensembles of Convolutional Neural Networks for pediatric pneumonia
  diagnosis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.IV/paper_9.html">
      <font color="black">Ensembles of Convolutional Neural Networks for pediatric pneumonia
  diagnosis</font>
    </a>
  </h2>
  <font color="black">私たちの目標は、胸部X線写真（X線）を2つのクラスに分類することです。肺胞肺炎に対応する硬化と非肺胞肺炎に対応する非硬化です。最も頻繁な細菌は次のとおりです。Streptococcuspneumoniae、Haemophilusinfluenzae、Streptococcus pyogenesおよびStaphylococcusaureus、およびMycoplasma pneumoniae .. X線画像分析は、肺炎の診断に最もよく使用される方法の1つです。 
[概要] X線分析は肺炎の診断に最もよく使用される方法の1つですが、これらのシステムを使用すると、これらのモデルは予測の簡単な説明を生成しません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-29">
        <br><font color="black">2020-09-29</font>
      </time>
    </span>
</section>
<!-- paper0: GASNet: Weakly-supervised Framework for COVID-19 Lesion Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.IV/paper_10.html">
      <font color="black">GASNet: Weakly-supervised Framework for COVID-19 Lesion Segmentation</font>
    </a>
  </h2>
  <font color="black">胸部CTボリュームの感染領域のセグメンテーションは、COVID-19患者のさらなる診断と治療にとって非常に重要です。GASNetは、ボクセルレベルの注釈のない多くの健康なCOVID-19被験者の胸部CTボリュームによって監視されます。GASNetは次のように最適化されています。 COVID-19 CTの病変領域をセグメンテーションでセグメント化し、異常な外観をジェネレーターによって生成された正常な外観に置き換えることで、復元されたCTボリュームと識別器による正常なCTボリュームを区別できなくなります。 
[概要]病変セグメンテーションネットワークをトレーニングするには、通常、多数のボクセルレベルのラベル付きサンプルが必要です。ネットワークは、深層学習ベースの医療画像セグメンテーションアルゴリズムを開発するためのボトルネックです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: CS2-Net: Deep Learning Segmentation of Curvilinear Structures in Medical
  Imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.IV/paper_11.html">
      <font color="black">CS2-Net: Deep Learning Segmentation of Curvilinear Structures in Medical
  Imaging</font>
    </a>
  </h2>
  <font color="black">エンコーダーとデコーダーに自己注意メカニズムを組み込んだ新しい曲線構造セグメンテーションネットワーク（CS2-Net）を導入し、曲線構造の豊富な階層表現を学習します。空間的注意とチャネル注意の2種類の注意モジュールを利用します。クラス間識別とクラス内応答性を強化し、ローカル機能をグローバルな依存関係と正規化に適応的に統合します。医療および生物医学画像からの曲線構造、たとえば血管や神経線維の自動検出は重要です。多くの病気の管理に関連する自動画像解釈の初期段階。 
[概要]これらの曲線器官構造は米国で開発されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-15">
        <br><font color="black">2020-10-15</font>
      </time>
    </span>
</section>
<!-- paper0: Nighttime Dehazing with a Synthetic Benchmark -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.IV/paper_12.html">
      <font color="black">Nighttime Dehazing with a Synthetic Benchmark</font>
    </a>
  </h2>
  <font color="black">合成ベンチマークでの実験は、劣化要因が共同で画質を低下させることを示しています。この問題に対処するために、色補正をヘイズ除去から解きほぐす前に最適なスケールの最大反射率を提案し、それらに順次対処します。データセットとソースの両方コードはhttps://github.com/chaimi2013/3Rで入手できます。 
[概要]大規模なベンチマークデータセットがないため、進捗が妨げられます。代わりに、以前の分析分布から実世界の明るい色をサンプリングすることで、現実的な夜間のファジーを作成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-10">
        <br><font color="black">2020-08-10</font>
      </time>
    </span>
</section>
<!-- paper0: FourierNet: Compact mask representation for instance segmentation using
  differentiable shape decoders -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.IV/paper_13.html">
      <font color="black">FourierNet: Compact mask representation for instance segmentation using
  differentiable shape decoders</font>
    </a>
  </h2>
  <font color="black">マスクを表すために8つのパラメーターを使用するだけで23.3mAPに達します（バウンディングボックスの予測にのみ少なくとも4つのパラメーターが必要であることに注意してください）。係数の解釈可能性と高速な実装のために、フーリエ級数を形状エンコーダーとして使用しました。ポリゴン表現方法と比較して有望な結果であり、MS COCO2017ベンチマークで30.6mAPを達成しています。 
[概要]ポリゴンの解釈可能性と高速な実装を使用してマスクを作成できます。これらは、高周波の妥当な比率を制御する認定方法に基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-07">
        <br><font color="black">2020-02-07</font>
      </time>
    </span>
</section>
<!-- paper0: Investigating the Impact of Pre-processing and Prediction Aggregation on
  the DeepFake Detection Task -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.IV/paper_14.html">
      <font color="black">Investigating the Impact of Pre-processing and Prediction Aggregation on
  the DeepFake Detection Task</font>
    </a>
  </h2>
  <font color="black">この論文では、トレーニングデータの品質を改善し、DeepFake検出のパフォーマンスへの影響を調べるための前処理ステップを提案します。実験結果は、提案された前処理アプローチが検出モデルのパフォーマンスの大幅な改善につながることを示しています。提案された予測集約スキームは、ビデオに複数の顔がある場合の検出効率をさらに高めます。また、ビデオレベルの予測集約アプローチの効果を提案および評価します。 
[ABSTRACT]ディープフェイクの検出方法は米国で広く普及しています。しかし、ディープフェイクのケースは数多くありますが、データセットの前処理の影響に焦点を当てているのはごくわずかです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-12">
        <br><font color="black">2020-06-12</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Quantifying Model Uncertainty in Inverse Problems via Bayesian Deep
  Gradient Descent -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_0.html">
      <font color="black">Quantifying Model Uncertainty in Inverse Problems via Bayesian Deep
  Gradient Descent</font>
    </a>
  </h2>
  <font color="black">スケーラビリティは、アーキテクチャでハイブリッド化することによって実現されます。各ブロックの最後のレイヤーのみがベイジアンであり、他のレイヤーは決定論的であり、トレーニングに貪欲です。これらの手法は、いくつかのイメージングタスクで最先端のパフォーマンスを実証しています。 、しかし、それらは得られた再構成に不確実性を提供しないことがよくあります。フレームワークは、1つの代表的な医用画像モダリティ、すなわち、で紹介されています。 
[要約]このメソッドは、に基づいて構築され、深い勾配降下法を拡張します。また、確率的フレームワーク内で再キャストします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-20">
        <br><font color="black">2020-07-20</font>
      </time>
    </span>
</section>
<!-- paper0: HVS-Based Perceptual Color Compression of Image Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_1.html">
      <font color="black">HVS-Based Perceptual Color Compression of Image Data</font>
    </a>
  </h2>
  <font color="black">PCCは、人間の視覚系（HVS）のスペクトル感度とCIELAB丁度可知色差（JNCD）に関連する新しいモデルに基づいています。さらに、実施された主観的評価評価の75％でMOS = 5が達成されます。提案されたPCC手法は次のことができます。さまざまなビット深度と空間解像度のRGB（4：4：4）画像データで使用できます。 
[概要]この論文では、hvが非常に類似した波長帯域の光子を知覚的に区別できないことに基づいて、知覚色圧縮（pcc）と呼ばれる新しい知覚画像コーディング手法を提案します。評価では、提案されたデータを比較します。用途の広いビデオコーディング（vvc）や高効率ビデオ編集（hevc）などのさまざまな新しい方法を使用</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-16">
        <br><font color="black">2020-05-16</font>
      </time>
    </span>
</section>
<!-- paper0: Approach for Document Detection by Contours and Contrasts -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_2.html">
      <font color="black">Approach for Document Detection by Contours and Contrasts</font>
    </a>
  </h2>
  <font color="black">提案された方法は、オープンMIDV-500データセットで比類のない最先端のパフォーマンスを提供し、SmartDocデータセットでの最先端のパフォーマンスに匹敵する結果を示します。実験では、このような変更により、代替順序付けエラーが40％減少し、全体的な検出エラーが10％減少します。オブジェクトと背景のコントラストに依存する領域ベースのアプローチにはアプリケーションの制限はありませんが、既知の実装は非常に優れています。リソースを消費します。 
[概要]提案された方法は、オープンmidv-500データセットで比類のないパフォーマンスを提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: ChestX-Det10: Chest X-ray Dataset on Detection of Thoracic Abnormalities -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_3.html">
      <font color="black">ChestX-Det10: Chest X-ray Dataset on Detection of Thoracic Abnormalities</font>
    </a>
  </h2>
  <font color="black">注釈はhttps://github.com/Deepwise-AILab/ChestX-Det10-Datasetにあります。胸部X線に関する既存の作業のほとんどは、疾患の分類と弱く監視されたローカリゼーションに焦点を当てています。ChestX-と呼ばれる新しいベンチマークを提供します。 Det10、$ \ sim $ 3,500画像の10のカテゴリーの疾患/異常のボックスレベルの注釈を含みます。 
[概要]胸部X線に関する既存のほとんどの研究は疾患分類に焦点を当てています。新しいベンチマークには、10のカテゴリの疾患のボックスレベルの注釈が含まれています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-17">
        <br><font color="black">2020-06-17</font>
      </time>
    </span>
</section>
<!-- paper0: Visual-Semantic-Pose Graph Mixture Networks for Human-Object Interaction
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_4.html">
      <font color="black">Visual-Semantic-Pose Graph Mixture Networks for Human-Object Interaction
  Detection</font>
    </a>
  </h2>
  <font color="black">最初の研究では、主語と対象関係からの視覚的、インスタンス空間的、意味的手がかりと、推論を強化するための補助的手がかりを動的に集約する双対グラフ注意ネットワークを提案します。 、セマンティック、およびポーズ情報？また、より小さなV-COCOデータセットで競争力のあるパフォーマンスを実現します。 
[概要]幅広いコンテキストキューを効果的に処理するグラフ混合ネットワーク。その結果、さまざまなパターンを処理するネットワークが実現します。最終モデルは、挑戦的なhico-detデータセットの最先端を大幅に上回ります。ほぼ10％</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-07">
        <br><font color="black">2020-01-07</font>
      </time>
    </span>
</section>
<!-- paper0: SL-DML: Signal Level Deep Metric Learning for Multimodal One-Shot Action
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_5.html">
      <font color="black">SL-DML: Signal Level Deep Metric Learning for Multimodal One-Shot Action
  Recognition</font>
    </a>
  </h2>
  <font color="black">トレーニングデータのわずか60％で、私たちのアプローチはベースラインアプローチを3.7％上回っています。さらに、慣性データ、骨格データ、融合データのUTD-MHADデータセットと、モーションキャプチャデータ..私たちのアプローチは信号レベルの定式化に基づいており、さまざまなモダリティにわたって柔軟性を維持しています。 
[概要]いくつかのショット方法の大部分は、オブジェクト認識または顔識別に焦点を当てています。信号を画像に融合し、深い残余cnnを使用して特徴を抽出します。結果として得られるエンコーダーは、特徴を埋め込みスペースに変換します。さらに、ワンショット行動認識プロトコルの大規模nturgb d 120データセットは5.6％</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-23">
        <br><font color="black">2020-04-23</font>
      </time>
    </span>
</section>
<!-- paper0: SF-UDA$^{3D}$: Source-Free Unsupervised Domain Adaptation for
  LiDAR-Based 3D Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_6.html">
      <font color="black">SF-UDA$^{3D}$: Source-Free Unsupervised Domain Adaptation for
  LiDAR-Based 3D Object Detection</font>
    </a>
  </h2>
  <font color="black">LiDARポイントクラウドのみに基づく3Dオブジェクト検出器は、最新のストリートビューベンチマークを保持しています。ただし、LiDARベースの検出器は、ドメインシフトのため、ドメイン間での一般化が不十分です。実際、LiDARの場合、ドメインシフトは、RGBカメラからの視覚データの場合のように、環境やオブジェクトの外観の変化だけでなく、点群のジオメトリ（たとえば、点密度の変動）にも関係しています。 
[ABSTRACT] Lidarベースの検出器は、オブジェクトのシフトによりドメイン間でのみ変更されます。この方法は、このケースが変更されるという事実に基づいています。たとえば、この場合、3D検出器は使用できません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-16">
        <br><font color="black">2020-10-16</font>
      </time>
    </span>
</section>
<!-- paper0: Long-Tailed Recognition Using Class-Balanced Experts -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_7.html">
      <font color="black">Long-Tailed Recognition Using Class-Balanced Experts</font>
    </a>
  </h2>
  <font color="black">クラスバランスの取れた専門家のアンサンブルは、最先端に近い結果に到達し、拡張されたアンサンブルは、ロングテール認識のための2つのベンチマークで新しい最先端を確立します。トレーニングおよび評価コードは、次のURLで入手できます。 https://github.com/ssfootball04/class-balanced-experts ..この作業では、トレーニングセットのバランスが非常に悪く、テストセットのバランスが保たれているロングテール認識の問題に対処します。 
[要約]実世界のデータセットは高度にクラスを示します-不均衡な調査。データの不足はミディアムショットまたは少数のクラス用ですが、データの不足よりも難しい問題です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-07">
        <br><font color="black">2020-04-07</font>
      </time>
    </span>
</section>
<!-- paper0: NVAE: A Deep Hierarchical Variational Autoencoder -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_8.html">
      <font color="black">NVAE: A Deep Hierarchical Variational Autoencoder</font>
    </a>
  </h2>
  <font color="black">Nouveau VAE（NVAE）を提案します。これは、深度ごとに分離可能な畳み込みとバッチ正規化を使用して画像を生成するために構築された深い階層型VAEです。NVAEは正規分布の残差パラメーター化を備えており、そのトレーニングはスペクトル正則化によって安定化されます。 CIFAR-10では、NVAEは最先端を2.98ビットから2.91ビット/次元にプッシュし、CelebAHQで高品質の画像を生成します。 
[ABSTRACT] vabaモデルは、これらのニューラルアーキテクチャを制御できます。vabaと呼ばれ、特定のモデルからのデータをフィルタリングする機能があります。celebaモデルで高品質の画像を生成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-08">
        <br><font color="black">2020-07-08</font>
      </time>
    </span>
</section>
<!-- paper0: Temporal Extension Module for Skeleton-Based Action Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_9.html">
      <font color="black">Temporal Extension Module for Skeleton-Based Action Recognition</font>
    </a>
  </h2>
  <font color="black">NTU RGB + DとKinetics-Skeletonの2つの大きなデータセットで広範な実験を行い、モジュールがいくつかの既存のモデルに効果的であり、最終モデルが最先端のパフォーマンスを達成することを実証します。モジュールはシンプルでありながら人間の動きにおける複数の関節の相関する特徴を抽出するための効果的な方法。さらに、私たちのモジュールは、空間グラフのみを最適化する他のGCN方法とともに、さらなるパフォーマンスの向上に役立ちます。 
[ABSTRACT]既存のメソッドは、より適切な空間グラフを表現しようとしますが、代わりに、拡張グラフに基づいて追加の機能を選択します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-19">
        <br><font color="black">2020-03-19</font>
      </time>
    </span>
</section>
<!-- paper0: AUTSL: A Large Scale Multi-modal Turkish Sign Language Dataset and
  Baseline Methods -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_10.html">
      <font color="black">AUTSL: A Large Scale Multi-modal Turkish Sign Language Dataset and
  Baseline Methods</font>
    </a>
  </h2>
  <font color="black">さらに、署名者の空間的な位置と姿勢も記録によって異なります。私たちのデータセットは、43人の異なる署名者によって実行された226の標識と、合計38,336の分離された標識ビデオサンプルで構成されています。96.11％の精度。 
[概要]新しい大規模なマルチモーダルトルコ手話データセット（autsl）にはベンチマークがあり、パフォーマンス評価のベースラインモデルを提供します。これらには、手の形と向き、手の動き、体の姿勢、顔の表情が含まれます。各サンプルはマイクロソフトで記録されています。 kinect v2であり、rgb、depth、およびスケルトンモダリティが含まれています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Evolutionary Algorithms and Efficient Data Analytics for Image
  Processing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_11.html">
      <font color="black">Evolutionary Algorithms and Efficient Data Analytics for Image
  Processing</font>
    </a>
  </h2>
  <font color="black">ユニバーサルステガノグラフィは、ステゴ画像とカバー画像を区別するために多数の特徴を抽出します。したがって、研究者は、理想的にはリアルタイムで、すべての既知および未知のステガノグラフィアルゴリズムを検出する1つのユニバーサルステガノグラフィを開発することを目指しています。このCOD問題はさらにリアルタイムになります。ステガノ分析は難しい。 
[概要]これは、結果の画像の外観に影響を与えることなく、メッセージまたはテキストを画像に埋め込むことによって行われます。ステガナリシスプロセスは非常に困難であるため、困難です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-07-23">
        <br><font color="black">2019-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: PLOP: Probabilistic poLynomial Objects trajectory Planning for
  autonomous driving -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_12.html">
      <font color="black">PLOP: Probabilistic poLynomial Objects trajectory Planning for
  autonomous driving</font>
    </a>
  </h2>
  <font color="black">エゴカーのフロントカメラ画像、バードアイビューグリッドのLidarポイントクラウド、現在および過去のオブジェクトの検出を入力として受け取り、エゴビークルと近隣の可能な軌道だけでなく、補助損失としてのセマンティックセグメンテーションも出力します。公開されているデータセットnuScenesで、最先端のパフォーマンスを示し、アーキテクチャの選択の影響を調査します。ここでは、確率的フレームワークを通じて、自我車両と近隣の両方の複数の実行可能な将来の軌道を予測することに焦点を当てます。 
[概要]ここでは、複数の実行可能な将来の軌道を予測することに焦点を当てます。公開されているデータセットnuscenesでメソッドを評価します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-09">
        <br><font color="black">2020-03-09</font>
      </time>
    </span>
</section>
<!-- paper0: RWF-2000: An Open Large Scale Video Database for Violence Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_13.html">
      <font color="black">RWF-2000: An Open Large Scale Video Database for Violence Detection</font>
    </a>
  </h2>
  <font color="black">提案されたアプローチは、提案されたデータベースのテストセットで87.25％の精度を取得します。このペーパーでは、暴力検出のためのいくつかの既存のビデオデータセットを要約し、実世界のシーンで監視カメラによってキャプチャされた2,000本のビデオを含むRWF-2000データベースを提案します。また、3D-CNNと光フローの両方のメリットを生かした新しい手法、フローゲートネットワークを紹介します。 
[概要]カメラカメラは、犯罪が行われた後に手がかりと証拠を提供します。犯罪活動を時間内に防止または停止するために使用されることはめったにありません。システムは、3D-cnnsとオプティカルフローの両方のメリットを使用します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-14">
        <br><font color="black">2019-11-14</font>
      </time>
    </span>
</section>
<!-- paper0: Real-Time Drone Detection and Tracking With Visible, Thermal and
  Acoustic Sensors -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_14.html">
      <font color="black">Real-Time Drone Detection and Tracking With Visible, Thermal and
  Acoustic Sensors</font>
    </a>
  </h2>
  <font color="black">公開データセットの不足に対抗するために、ドローン、鳥、飛行機、ヘリコプターの650の注釈付き赤外線および可視ビデオを含む新しいビデオデータセットも提示されています（https://github.com/DroneDetectionThesis/Drone-detection-dataset）。さらに、センサーフュージョンを使用して、システムは個々のセンサーよりも堅牢になり、誤検出を減らすのに役立ちます。このペーパーでは、自動マルチセンサードローン検出システムを設計するプロセスについて説明します。 
[概要]システムには、ドローン検出タスクの可能な解決策であることが示されている熱赤外線カメラも含まれています。新しいビデオデータセットには、ドローン、鳥、飛行機、ヘリコプターの650の注釈付き赤外線および可視ビデオが含まれています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: A review of 3D human pose estimation algorithms for markerless motion
  capture -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_15.html">
      <font color="black">A review of 3D human pose estimation algorithms for markerless motion
  capture</font>
    </a>
  </h2>
  <font color="black">さらに、さまざまなポーズ推定システムのメトリック、ベンチマーク、および構造を分析し、将来の研究のためにいくつかの方向性を提案します。精度、速度、および堅牢性のパフォーマンス基準に基づいて、既存の方法を分類法で分類することをお勧めします。人間のポーズ推定（ HPE）in 3Dは、エンターテインメント、健康、スポーツ科学、ロボティクスに多くのアプリケーションがある活発な研究分野です。 
[概要]過去5年間で、マーカーレスモーションキャプチャ技術の平均エラーは、今日では10cm以上から2cm未満に減少しています。過去3年間の20以上の方法を確認します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-13">
        <br><font color="black">2020-10-13</font>
      </time>
    </span>
</section>
<!-- paper0: Natural Disaster Classification using Aerial Photography Explainable for
  Typhoon Damaged Feature -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_16.html">
      <font color="black">Natural Disaster Classification using Aerial Photography Explainable for
  Typhoon Damaged Feature</font>
    </a>
  </h2>
  <font color="black">復旧を早めるためには、早急な対応が重要です。本稿では、台風災害の特徴に焦点を当てた被災地を航空写真で視覚化する実用的な方法を提案します。 
[概要]航空写真は、64 x 64の4,096グリッドに分割され、各ユニット画像は48平方メートルです。この方法を使用すると、意思決定支援のために、被災地のグローバルスクリーニングのための航空測量が必要になります。先に回復する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br><font color="black">2020-04-21</font>
      </time>
    </span>
</section>
<!-- paper0: CONFIG: Controllable Neural Face Image Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_17.html">
      <font color="black">CONFIG: Controllable Neural Face Image Generation</font>
    </a>
  </h2>
  <font color="black">この目的のために、意味的に意味のある方法で出力画像の個々の側面を制御することを可能にし、細かく制御可能な神経レンダリングへの道の重要なステップである神経顔モデルであるConfigNetを提案します。私たちの新しい方法は合成データを使用して潜在空間を従来のレンダリングパイプラインの入力に対応する要素に分割し、頭のポーズ、顔の表情、髪のスタイル、照明など、実際のデータで注釈を付けるのが非常に難しい側面を分離します。ConfigNetは実際の顔でトレーニングされます画像と合成顔のレンダリング。 
[ABSTRACT] confignetは、顔画像と自然な顔のシミュレーションをシミュレートするようにトレーニングされています。confignetは合成顔と合成faces.configneteはネットワークの実際の制御で動作します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-06">
        <br><font color="black">2020-05-06</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Affordance Landscapes for Interaction Exploration in 3D
  Environments -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_18.html">
      <font color="black">Learning Affordance Landscapes for Interaction Exploration in 3D
  Environments</font>
    </a>
  </h2>
  <font color="black">AI2-iTHORを使用してアイデアを示します。自己中心的なRGB-Dカメラと高レベルのアクションスペースが与えられると、エージェントは、画像ベースのアフォーダンスセグメンテーションモデルを同時にトレーニングしながら、成功したインタラクションを最大化することで報われます。新しい家庭環境をインテリジェントに使用する方法と、「ナイフを見つけて引き出しに入れる」などのさまざまなダウンストリームタスクに迅速に対処できるようにする方法を学びます。 
[ABSTRACT]画像領域を各アクションを許可する可能性にマッピングし、探索の報酬を高密度化するシステム。結果は、エージェントが新しい家庭環境をインテリジェントに使用する方法を学習できることを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-21">
        <br><font color="black">2020-08-21</font>
      </time>
    </span>
</section>
<!-- paper0: CS2-Net: Deep Learning Segmentation of Curvilinear Structures in Medical
  Imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_19.html">
      <font color="black">CS2-Net: Deep Learning Segmentation of Curvilinear Structures in Medical
  Imaging</font>
    </a>
  </h2>
  <font color="black">新しい曲線構造セグメンテーションネットワーク（CS2-Net）を紹介します。これには、エンコーダーとデコーダーに自己注意メカニズムが含まれており、曲線構造の豊富な階層表現を学習します。心血管、腎臓、目、肺、神経の状態。 .. 
[要約]これらの曲線器官構造は米国で開発されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-15">
        <br><font color="black">2020-10-15</font>
      </time>
    </span>
</section>
<!-- paper0: Implicit Regularization and Convergence for Weight Normalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_20.html">
      <font color="black">Implicit Regularization and Convergence for Weight Normalization</font>
    </a>
  </h2>
  <font color="black">これは、最急降下法の動作とは異なります。最急降下法は、特徴行列の範囲空間内の点で開始された場合にのみ最小ノルム解に収束するため、初期化の影響を受けやすくなります。WNおよびrPGDは、スケールを使用して重みを再パラメーター化します。 gと単位ベクトルwにより、目的関数は非凸になります。この非凸の定式化は、元の目的の最急降下法と比較して、有益な正規化効果があることを示します。 
[概要]体重正規化（wn）法は、g-mamamamaダイエットに基づいています。これは、再パラメーター化された投影勾配と呼ばれるバリアントでもあります。ただし、このバージョンには、以前のバージョンよりも有益なスニッカーズがあります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-18">
        <br><font color="black">2019-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Face Identity Disentanglement via Latent Space Mapping -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_21.html">
      <font color="black">Face Identity Disentanglement via Latent Space Mapping</font>
    </a>
  </h2>
  <font color="black">広範な実験を通じて、私たちの方法が他の顔の属性からアイデンティティをうまく解きほぐし、既存の方法を超えて、より多くのトレーニングと監督が必要であることを示します。人間の頭の複雑で高次元の領域に対する私たちのアプローチを示します。潜在空間にマッピングすることで、トレーニングの負担なしに、最先端の品質と豊かで表現力豊かな潜在空間の両方を活用します。 
[ABSTRACT]解きほぐされた潜在表現により、モデルは複雑な複合体を制御および構成できます。メソッドは、事前にトレーニングされたネットワークを使用して、プロセスでデータを表現する方法を学習します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-15">
        <br><font color="black">2020-05-15</font>
      </time>
    </span>
</section>
<!-- paper0: Nighttime Dehazing with a Synthetic Benchmark -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_22.html">
      <font color="black">Nighttime Dehazing with a Synthetic Benchmark</font>
    </a>
  </h2>
  <font color="black">大規模なベンチマークデータセットがないため、この分野での進展が妨げられています。データセットとソースコードの両方がhttps://github.com/chaimi2013/3Rで入手できます。合成ベンチマークの実験では、劣化要因が共同で減少することが示されています。画質。 
[概要]大規模なベンチマークデータセットがないため、進捗が妨げられます。代わりに、以前の分析分布から実世界の明るい色をサンプリングすることで、現実的な夜間のファジーを作成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-10">
        <br><font color="black">2020-08-10</font>
      </time>
    </span>
</section>
<!-- paper0: One Ring to Rule Them All: Certifiably Robust Geometric Perception with
  Outliers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_23.html">
      <font color="black">One Ring to Rule Them All: Certifiably Robust Geometric Perception with
  Outliers</font>
    </a>
  </h2>
  <font color="black">単一回転の平均化、形状の位置合わせ、3D点群とメッシュの登録、高整合性の衛星ポーズ推定など、さまざまな知覚問題にわたる数値実験は、緩和の厳しさ、認証の正確さ、提案されたデュアルのスケーラビリティを示しています現在のSDPソルバーの範囲を超えた、大きな問題に対する認証者..私たちの最初の貢献は、-幅広いクラスの幾何学的知覚問題について-TLS推定が、多項式のリングとラッセルの凸モーメントの階層に対する最適化として再定式化できることを示すことです。緩和は、最小緩和次数で経験的にタイトです（つまり、非凸TLS問題のグローバル最小値を確実に取得します）。大量の外れ値が存在する場合に堅牢な幾何学的知覚のための認証可能なアルゴリズムを設計するための最初の一般的かつ実用的なフレームワークを提案します。 。 
[要約]外れ値に対してロバストであることが知られているが、ハード、非凸、および非スモーミングの問題につながるtlsのコスト関数の使用を調査します。彼らはそれが目的の構造的スパース性を活用するための最初のステップであると言います半正定値計画のサイズを大幅に削減するための制約とレバレッジの基礎e</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-11">
        <br><font color="black">2020-06-11</font>
      </time>
    </span>
</section>
<!-- paper0: Self-Supervised Learning of Non-Rigid Residual Flow and Ego-Motion -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_24.html">
      <font color="black">Self-Supervised Learning of Non-Rigid Residual Flow and Ego-Motion</font>
    </a>
  </h2>
  <font color="black">現在のシーンフロー方法のほとんどは、3Dモーションの静的コンポーネントと動的コンポーネントを区別せずに、ポイントごとの変換ベクトルとしてシーンフローをモデル化することを選択します。点群のペアから相対的な剛体変換を学習した後、繰り返し改良することを提案します。さらに、点群シーケンスの時間的一貫性プロパティに基づいて、監視対象フレームワークを自己監視信号で拡張します。 
[概要]この作業では、動的3Dシーンの非剛体残余フローとエゴモーションフローの共同推定によるシーンフロー学習のエンドツーの代替方法を提示します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-22">
        <br><font color="black">2020-09-22</font>
      </time>
    </span>
</section>
<!-- paper0: FourierNet: Compact mask representation for instance segmentation using
  differentiable shape decoders -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_25.html">
      <font color="black">FourierNet: Compact mask representation for instance segmentation using
  differentiable shape decoders</font>
    </a>
  </h2>
  <font color="black">マスクを表すために8つのパラメーターを使用するだけで23.3mAPに達します（バウンディングボックスの予測にのみ少なくとも4つのパラメーターが必要であることに注意してください）。FourierNetは、ポリゴン表現方法と比較して有望な結果を示し、MS COCO2017ベンチマークで30.6mAPを達成します。形状ベクトルを予測する、シングルショット、アンカーフリー、完全に畳み込みのインスタンスセグメンテーション手法であるFourierNetを紹介します。 
[概要]ポリゴンの解釈可能性と高速な実装を使用してマスクを作成できます。これらは、高周波の妥当な比率を制御する認定方法に基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-07">
        <br><font color="black">2020-02-07</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Ensembles for Low-Data Transfer Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_26.html">
      <font color="black">Deep Ensembles for Low-Data Transfer Learning</font>
    </a>
  </h2>
  <font color="black">事前トレーニング自体の性質が多様性のパフォーマンスの源であることを示し、ダウンストリームデータセットの事前トレーニングモデルのサブセットを効率的に識別する実用的なアルゴリズムを提案します。19の異なるダウンストリームタスクで強力なベースラインと一緒に評価した場合（ Visual Task Adaptation Benchmark）、これにより、2,000を超える事前トレーニング済みモデルから選択した場合でも、はるかに低い推論予算で最先端のパフォーマンスが実現します。アプローチは単純です。最近傍の精度を使用して事前にランク付けします。トレーニングされたモデルは、小さなハイパーパラメータスイープで最適なモデルを微調整し、検証のクロスエントロピーを最小限に抑えるためにアンサンブルを貪欲に構築します。 
[ABSTRACT]テクニックテクニックは、モデルをより簡単に作成するために使用されるように設計されています。このメソッドは、モデルを提案するためのシンプルでシンプルなシンプルです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-14">
        <br><font color="black">2020-10-14</font>
      </time>
    </span>
</section>
<!-- paper0: Gastric histopathology image segmentation using a hierarchical
  conditional random field -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_27.html">
      <font color="black">Gastric histopathology image segmentation using a hierarchical
  conditional random field</font>
    </a>
  </h2>
  <font color="black">胃癌のインテリジェント診断に適用される畳み込みニューラルネットワーク（CNN）の場合、既存の方法は主に、統合情報を表現するポリシーなしで、個々の特性またはネットワークフレームワークに焦点を当てます。特に、CNNはピクセルレベルを構築するようにトレーニングされています。ポテンシャルと別の3つのCNNは、十分な空間セグメンテーション情報のパッチレベルのポテンシャルを構築するために微調整されます。HCRFモデルは、高いセグメンテーションパフォーマンスを示し、GHISフィールドでの有効性と将来の可能性を示します。 
[概要]たとえば、条件付き確率場（crf）は、効率的で安定したアルゴリズムで画像の空間的関係を特徴付けることができます。紙ベースの後処理を適用して、ギス場との接続をさらに改善します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-03">
        <br><font color="black">2020-03-03</font>
      </time>
    </span>
</section>
<!-- paper0: Anomaly Detection with Convolutional Autoencoders for Fingerprint
  Presentation Attack Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_28.html">
      <font color="black">Anomaly Detection with Convolutional Autoencoders for Fingerprint
  Presentation Attack Detection</font>
    </a>
  </h2>
  <font color="black">この問題に対処するために、本物のサンプルのみでトレーニングされたオートエンコーダー（AE）に基づく新しいPAD手法を提案します（つまり、45の異なるPAI種を含む19,711の本物の画像と4,339のPA画像のデータベースでの実験的評価では、検出は等しいエラー率（D-EER）2.00％を達成しました。近年、指紋ベースの生体認証システムの人気が大幅に高まっています。
[ABSTRACT]プレゼンテーション攻撃検出（パッド）メソッドを使用して、サンプルが善意からのものかどうかを判断します。善意の主題またはプレゼンテーション攻撃手段から（pai）</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Concept Whitening for Interpretable Image Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_29.html">
      <font color="black">Concept Whitening for Interpretable Image Recognition</font>
    </a>
  </h2>
  <font color="black">概念ホワイトニングモジュールがCNNに追加されると、潜在空間の軸は、関心のある既知の概念と整列します。実験により、CWは、ネットワークが層を超えて概念を徐々に学習する方法をより明確に理解できることを示します。 。隠されたレイヤーの内部を見ようとする試みは、誤解を招くか、使用できないか、潜在的なスペースに依存して、持っていない可能性のあるプロパティを所有する可能性があります。 
[ABSTRACT] cwは、予測パフォーマンスを損なうことなく、ネットワークの任意のレイヤーで使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-05">
        <br><font color="black">2020-02-05</font>
      </time>
    </span>
</section>
<!-- paper0: Locally Linear Region Knowledge Distillation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_30.html">
      <font color="black">Locally Linear Region Knowledge Distillation</font>
    </a>
  </h2>
  <font color="black">最後に、生徒は教師機能のローカルな形状をより適切にキャプチャできるため、パフォーマンスが向上します。これは、ローカルの線形領域で教師機能の出力を模倣するように生徒に強制することで実現されます。この号では、ローカル線形領域の知識を教師から生徒に転送するローカル線形領域知識蒸留（$ \ rm L ^ 2 $ RKD）を提案します。 
[概要]既存の作業は、ロジットまたは表現を調整するためのさまざまな基準の設計に焦点を当てています。生徒が教師の行動をよりよく模倣できるようにするために、テクニックは効果的です。これは、生徒に生徒の機能の出力を模倣するように強制することによって達成されます。ローカルの線形領域</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-09">
        <br><font color="black">2020-10-09</font>
      </time>
    </span>
</section>
<!-- paper0: Investigating the Impact of Pre-processing and Prediction Aggregation on
  the DeepFake Detection Task -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CV/paper_31.html">
      <font color="black">Investigating the Impact of Pre-processing and Prediction Aggregation on
  the DeepFake Detection Task</font>
    </a>
  </h2>
  <font color="black">また、ビデオレベルの予測集計アプローチの効果を提案および評価します。この論文では、トレーニングデータの品質を改善し、DeepFake検出のパフォーマンスへの影響を調べるための前処理ステップを提案します。実験結果は、提案された前処理アプローチは、検出モデルのパフォーマンスの大幅な改善につながり、提案された予測集約スキームは、ビデオに複数の顔がある場合の検出効率をさらに高めます。 
[ABSTRACT]ディープフェイクの検出方法は米国で広く普及しています。しかし、ディープフェイクのケースは数多くありますが、データセットの前処理の影響に焦点を当てているのはごくわずかです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-12">
        <br><font color="black">2020-06-12</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: FiSSA at SemEval-2020 Task 9: Fine-tuned For Feelings -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CL/paper_0.html">
      <font color="black">FiSSA at SemEval-2020 Task 9: Fine-tuned For Feelings</font>
    </a>
  </h2>
  <font color="black">さまざまな微調整戦略を使用して、事前にトレーニングされたさまざまなTransformerモデルのパフォーマンスを調査します。さらに、言語モデリングの目的とタスク固有の目的の2つのステップで微調整するカスタムモデルを提案します。 ..このスコアで、私たちのチームジュピッターは大会全体で10位になりました。 【アブストラクト】スコアに加えて、当チームのジュピッターが総合10位</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-24">
        <br><font color="black">2020-07-24</font>
      </time>
    </span>
</section>
<!-- paper0: When in Doubt, Ask: Generating Answerable and Unanswerable Questions,
  Unsupervised -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CL/paper_1.html">
      <font color="black">When in Doubt, Ask: Generating Answerable and Unanswerable Questions,
  Unsupervised</font>
    </a>
  </h2>
  <font color="black">結果は、混合データセットでトレーニングされた言語モデル（F1スコアとEMスコアで測定）のパフォーマンスが目に見える形で改善されたことを示しています。
[Githubリポジトリへのリンク：https：//github.com/lnikolenko/EQA]。具体的には、回答できない質問回答は、モデルを後押しするのにより効果的であることがわかります。元のデータセットに回答可能な質問回答、回答できない質問回答、および組み合わせた質問回答を追加することによるF1スコアの増加は、それぞれ1.3％、5.0％、および6.7％でした。 
[概要]ディープトランスフォーマーに基づく最先端のモデルを使用して、よく知られている人工のデータセットを補完するために、回答可能な質問と回答できない質問を合成して使用した場合の影響を調査します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-04">
        <br><font color="black">2020-10-04</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Machine Reading Comprehension with Contextualized Commonsense
  Knowledge -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CL/paper_2.html">
      <font color="black">Improving Machine Reading Comprehension with Contextualized Commonsense
  Knowledge</font>
    </a>
  </h2>
  <font color="black">単一タイプのコンテキスト化された知識に基づいて大規模な弱くラベル付けされたデータを使用し、教師と学生のパラダイムを使用して複数のタイプのコンテキスト化された知識を学生のマシンリーダーに注入する2段階の微調整戦略を提案します。この論文では、機械の読みの理解を向上させるために常識的な知識を抽出することを目指しています。事前定義された関係のセットに依存するのではなく、構造化された知識をコンテキストに配置することによって暗黙的に関係を表すことを提案し、それをコンテキスト化された知識と呼びます。 
[概要]事前に述べられた知識を使用する代わりに、メソッドで関係を表すことを提案し、それを「コンテキスト化」と呼びます。単一に基づいて大規模で弱いラベルの付いたデータを使用するための2段階の微調整された戦略を提案します。文脈化された知識の種類</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-12">
        <br><font color="black">2020-09-12</font>
      </time>
    </span>
</section>
<!-- paper0: comp-syn: Perceptually Grounded Word Embeddings with Color -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CL/paper_3.html">
      <font color="black">comp-syn: Perceptually Grounded Word Embeddings with Color</font>
    </a>
  </h2>
  <font color="black">ここでは、Google画像検索結果の知覚的に均一な色分布に基づいて根拠のある単語の埋め込みを提供するPythonパッケージcomp-synを紹介します。comp-synが分布セマンティクスのモデルを大幅に強化することを示します。comp-synはオープンです- PyPiのソースであり、主流のマシン学習Pythonパッケージと互換性があります。 
[概要] pythonパッケージcomp-synは、Google画像検索結果の知覚的に均一な色分布に基づいて、根拠のある単語の埋め込みを提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-08">
        <br><font color="black">2020-10-08</font>
      </time>
    </span>
</section>
<!-- paper0: The variation of the sum of edge lengths in linear arrangements of trees -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CL/paper_4.html">
      <font color="black">The variation of the sum of edge lengths in linear arrangements of trees</font>
    </a>
  </h2>
  <font color="black">一次元の空間ネットワークの最適性スコアに関する研究の基礎を確立します。特に、固定サイズのツリーのエッジ長の合計に関するさまざまな問題を調査します。特定のツリーの合計の最小値と最大値、樹木のクラス（ビスターツリーとキャタピラーツリー）の最小値と最大値、そして最後に任意のツリーの最小値と最大値。ネットワークサイエンスの基本的な問題は、頂点間のトポロジー的または物理的な距離の正規化であり、正規化されていない距離の変動範囲。 
[概要]物理的な距離の限界を理解できるようにする必要があるという証拠はたくさんあります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-24">
        <br><font color="black">2020-06-24</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-teacher Knowledge Distillation for Knowledge Graph Completion -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CL/paper_5.html">
      <font color="black">Multi-teacher Knowledge Distillation for Knowledge Graph Completion</font>
    </a>
  </h2>
  <font color="black">新しい反復蒸留戦略の下で、MulDEモデルは、トレーニングエポックと学生のパフォーマンスに応じて適応的にソフトラベルを生成します。ただし、最近のKGEモデルは、ベクトルディメンションを過度に増やすことでパフォーマンスを向上させる傾向があり、膨大なトレーニングコストが発生し、実際のアプリケーションでのストレージが節約されます。 ..この問題に対処するために、まず、最小エントロピーの原理に基づいて、KG埋め込みの低次元空間の容量を理論的に分析します。 
[概要]新しい調査によると、ムルデ川は知識蒸留の枠組みを効果的に改善できることが示されています。教師として複数の致命的でないkgeモデルを使用しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-14">
        <br><font color="black">2020-10-14</font>
      </time>
    </span>
</section>
<!-- paper0: MISA: Modality-Invariant and -Specific Representations for Multimodal
  Sentiment Analysis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CL/paper_6.html">
      <font color="black">MISA: Modality-Invariant and -Specific Representations for Multimodal
  Sentiment Analysis</font>
    </a>
  </h2>
  <font color="black">ただし、信号の不均一な性質により、重大な課題をもたらす分布モダリティギャップが作成されます。ここでも、モデルは強力なベースラインよりも優れており、MISAを有用なマルチモーダルフレームワークとして確立しています。マルチモーダル感情分析は、マルチモーダルを活用する活発な研究分野です。ユーザーが作成した動画を感情的に理解するためのシグナル。 
[概要]この論文では、効果的な控えめな性質を学ぶことを目指しています。これらの表現は、予測と予測につながる融合に使用されるマルチモーダルデータの全体像を提供します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-07">
        <br><font color="black">2020-05-07</font>
      </time>
    </span>
</section>
<!-- paper0: Mining Dual Emotion for Fake News Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CL/paper_7.html">
      <font color="black">Mining Dual Emotion for Fake News Detection</font>
    </a>
  </h2>
  <font color="black">この論文では、偽のニュース検出のために二重感情とそれらの間の関係をマイニングするための二重感情機能を提案します。3つの実世界データセットの実験結果は、提案された機能の有効性を示しています。さらに、そこにあるかどうかを調査する必要があります。出版社の感情と社会的感情（すなわち、二重の感情）との間に関係が存在し、二重の感情が偽のニュースにどのように現れるか。 
[概要]感情的なシグナルを活用する場合、既存の方法はニュースコンテンツの感情を利用することに焦点を当てています。フェイクニュースは、発行者の感情と社会的感情の関係を特徴としています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-03-05">
        <br><font color="black">2019-03-05</font>
      </time>
    </span>
</section>
<!-- paper0: Single headed attention based sequence-to-sequence model for
  state-of-the-art results on Switchboard -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CL/paper_8.html">
      <font color="black">Single headed attention based sequence-to-sequence model for
  state-of-the-art results on Switchboard</font>
    </a>
  </h2>
  <font color="black">クロス発話言語モデルを使用すると、シングルパススピーカーに依存しないシステムは、発音レキシコンなしで、Hub5&#39;00のSwitchboardおよびCallHomeサブセットで6.4％および12.5％の単語誤り率（WER）に達します。全体として、さまざまな組み合わせ正規化とシンプルだがかなり大きなモデルは、外部データリソースなしでSWB-2000を使用して、スイッチボードとCallHomeセットで4.7％と7.8％のWERという新しい最先端の結果をもたらします。注意深い正規化とデータ拡張はこのレベルのパフォーマンスを達成するために、Switchboard-2000での実験では、より多くのデータほど有用なものはないことが示されています。 
[概要]新しい調査によると、すべてのモデルの7％が、注意を認識できる単一のシステムを使用できます。新しいテストでは、より多くのデータほど有用なものはないことが示されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-20">
        <br><font color="black">2020-01-20</font>
      </time>
    </span>
</section>
<!-- paper0: Joint Spatio-Textual Reasoning for Answering Tourism Questions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CL/paper_9.html">
      <font color="black">Joint Spatio-Textual Reasoning for Answering Tourism Questions</font>
    </a>
  </h2>
  <font color="black">次に、共同モデルで空間推論とテキスト推論を組み合わせて、実際のPOI推奨タスクの実験を示します。共同空間テキスト推論を使用しない既存のモデルに比べて大幅な改善を報告します。このような質問は、さまざまな種類の質問を表します。空間的および非空間的制約。テキストと空間的推論の組み合わせが必要です。 
[要約]簡単な質問は、さまざまな種類の空間的および非空間的制約を表します。これは、数学的および空間的推論の知識の組み合わせを組み合わせたものです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-28">
        <br><font color="black">2020-09-28</font>
      </time>
    </span>
</section>
<!-- paper0: COMET: A Neural Framework for MT Evaluation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CL/paper_10.html">
      <font color="black">COMET: A Neural Framework for MT Evaluation</font>
    </a>
  </h2>
  <font color="black">私たちのフレームワークを紹介するために、人間による判断の種類が異なる3つのモデルをトレーニングします。直接評価、人間が仲介する翻訳編集率、多次元品質メトリクスです。私たちのモデルは、WMT2019メトリクス共有タスクで新しい最先端のパフォーマンスを実現します。私たちのフレームワークは、クロスリンガルの事前トレーニング済み言語モデリングにおける最近のブレークスルーを活用して、ソース入力とターゲット言語の参照翻訳の両方からの情報を活用して、より多くのことを実現する、高度に多言語で適応性のあるMT評価モデルを実現します。 MTの品質を正確に予測します。 
[概要]私たちのフレームワークは、言語を超えた事前トレーニングされた言語モデリングにおける最近のブレークスルーを活用しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-18">
        <br><font color="black">2020-09-18</font>
      </time>
    </span>
</section>
<!-- paper0: Utterance-level Dialogue Understanding: An Empirical Study -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/cs.CL/paper_11.html">
      <font color="black">Utterance-level Dialogue Understanding: An Empirical Study</font>
    </a>
  </h2>
  <font color="black">完全な発話レベルの理解には、多くの場合、近くの発話によって定義されるコンテキストの理解が必要です。具体的には、さまざまな摂動を使用して、特定の発話のコンテキストを歪め、さまざまなタスクやベースラインへの影響を調査します。これにより、基本的な洞察が得られます。対話のさまざまな側面の文脈制御要因。 
[要約]完全な発話-レベルの理解には、多くの場合、コンテキストの理解が必要です。これらのアプローチの多くは、効果的な理解のためのコンテキストを説明します。これらの洞察は、より効果的な対話理解モデルを刺激することができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-29">
        <br><font color="black">2020-09-29</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: Ensemble Chinese End-to-End Spoken Language Understanding for Abnormal
  Event Detection from audio stream -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.AS/paper_0.html">
      <font color="black">Ensemble Chinese End-to-End Spoken Language Understanding for Abnormal
  Event Detection from audio stream</font>
    </a>
  </h2>
  <font color="black">このデータセットに基づいて、この論文では、中国語環境で使用されるアンサンブルエンドツーエンドSLUモデルを提案しました。従来の音声言語理解（SLU）は、2つの段階で構成され、最初の段階は自動音声認識（ASR）によって音声をテキストにマッピングします。第2段階では、自然言語理解（NLU）によってテキストを意図にマッピングします。この提案されたアプローチにより、以前のエンドツーエンドSLUモデルと比較して精度が9.7％向上します。 
[概要]ライブオーディオストリームで自動音声を検出するために、中国語の大規模なsluデータセットが収集されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: Fast accuracy estimation of deep learning based multi-class musical
  source separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.AS/paper_1.html">
      <font color="black">Fast accuracy estimation of deep learning based multi-class musical
  source separation</font>
    </a>
  </h2>
  <font color="black">この作業では、任意のデータセットまたは曲の楽器の分離可能性を評価するための高速な方法を提案し、ディープニューラルネットワークをトレーニングおよび調整する必要のない任意の楽器について..理想的な比率のオラクル原理に基づくアプローチマスクは、TasNetやOpen-Unmixなどの時間周波数マスキングに基づく最先端の深層学習アプローチの分離パフォーマンスを推定するための優れたプロキシです。この分離可能性の尺度は、ニューラルの効率的なトレーニングに適切なサンプルを選択するのに役立ちますネットワーク。 
[ABSTRACT] musdbは4つの機器クラスに制限されています。データセットが大きくなると、データの収集と深いネットワークのトレーニングの点でコストと時間がかかります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: A bio-inspired geometric model for sound reconstruction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.AS/paper_2.html">
      <font color="black">A bio-inspired geometric model for sound reconstruction</font>
    </a>
  </h2>
  <font color="black">音の再構成中に人間の聴覚システムによって構築された再構成メカニズムはまだ議論の余地があります。モデルは、過去10年間で大きな発展を遂げた視覚の幾何学的モデリングに触発されています。そのようなイメージはその後持ち上げられます。ハイゼンベルググループで、ウィルソン-コーワン微分積分方程式を介して再構築されます。 
[概要]この研究の目的は、聴覚皮質の機能的アーキテクチャに基づいた音の再構成の数学的モデルを提案することです（a1）</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-06">
        <br><font color="black">2020-04-06</font>
      </time>
    </span>
</section>
<!-- paper0: CLAR: Contrastive Learning of Auditory Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.AS/paper_3.html">
      <font color="black">CLAR: Contrastive Learning of Auditory Representations</font>
    </a>
  </h2>
  <font color="black">この論文では、より良い聴覚表現を学ぶために以前の研究（SimCLR）を拡張します。さらに、自己教師ありアプローチと比較して、フレームワークは大幅に優れた表現でより速く収束します。これらすべての方法を組み合わせることにより、大幅に少ない方法でそれを説明します。ラベル付けされたデータである当社のフレームワーク（CLAR）は、教師ありアプローチと比較して予測パフォーマンスを大幅に向上させます。 
[概要]優れた聴覚表現の学習は、同様のアプローチを使用して優れた聴覚表現を学習できるかどうかという大きな問題です。時間周波数オーディオ機能を使用したトレーニングにより、生の信号と比較して、学習した表現の品質が大幅に向上します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: Classification of Manifest Huntington Disease using Vowel Distortion
  Measures -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.AS/paper_4.html">
      <font color="black">Classification of Manifest Huntington Disease using Vowel Distortion
  Measures</font>
    </a>
  </h2>
  <font color="black">そのために、HDの主要な発話症状である母音の歪みに焦点を当てます。HDを引き起こす遺伝子変異を持つ個人は、顕在化前から顕在化するHDに進むにつれて、さまざまな発話症状を示す可能性があります。症状の症状を継続的に追跡することにより、臨床評価を強化する可能性。 
[概要]調査によると、プレマニフェストとマニフェストのhdを区別することは、重要であるが十分に研究されていない問題です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-16">
        <br><font color="black">2020-10-16</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-channel target speech extraction with channel decorrelation and
  target speaker adaptation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.AS/paper_5.html">
      <font color="black">Multi-channel target speech extraction with channel decorrelation and
  target speaker adaptation</font>
    </a>
  </h2>
  <font color="black">提案された方法を2つの強力な最先端のベースラインと比較します。2つ目は、チャネル間差分情報を抽出してマルチチャネルエンコーダ表現を強化するチャネル非相関メカニズムを設計することです。1つ目は使用しています。並列エンコーダアーキテクチャのターゲット音声適応層。 
[ABSTRACT]終了から％までの音声抽出の研究はまだ比較的限られています。最初の研究は、並列チャネルでターゲット音声適応層を使用することです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: DiDiSpeech: A Large Scale Mandarin Speech Corpus -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.AS/paper_6.html">
      <font color="black">DiDiSpeech: A Large Scale Mandarin Speech Corpus</font>
    </a>
  </h2>
  <font color="black">コーパスはhttps://outreach.didichuxing.com/research/opendata/en/で入手できます。このペーパーでは、DiDiSpeechと呼ばれる新しいオープンソースの北京語音声コーパスを紹介します。コーパス内のすべての音声データは静かな環境で記録されました。また、音声変換、マルチスピーカーのテキストから音声への変換、自動音声認識など、さまざまな音声処理タスクに適しています。 
[概要] 6000人の話者からの約800時間の音声データと対応するテキストで構成されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: MicAugment: One-shot Microphone Style Transfer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.AS/paper_7.html">
      <font color="black">MicAugment: One-shot Microphone Style Transfer</font>
    </a>
  </h2>
  <font color="black">ターゲットデバイスによって録音されたオーディオが数秒しかない場合、MicAugmentは入力取得パイプラインに関連付けられた変換を識別し、学習した変換を使用して、ターゲットオーディオと同じ条件下で録音されたかのようにオーディオを合成します。 「インザワイルド」のオーディオベースモデルの展開の成功は、異種の取得条件によって導入された変換に対する堅牢性です。私たちの方法が実際のオーディオにスタイル転送を正常に適用できること、およびモデルの堅牢性が大幅に向上することを示しますダウンストリームタスクでデータ拡張として使用されます。 
[概要]この作業では、ワンショットマイクスタイルの転送を実行する方法を検討します。この方法は、実際のオーディオに正常に適用できます。データ拡張として使用すると、モデルの堅牢性が大幅に向上します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: Single headed attention based sequence-to-sequence model for
  state-of-the-art results on Switchboard -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.AS/paper_8.html">
      <font color="black">Single headed attention based sequence-to-sequence model for
  state-of-the-art results on Switchboard</font>
    </a>
  </h2>
  <font color="black">クロス発話言語モデルを使用すると、シングルパススピーカーに依存しないシステムは、発音レキシコンなしで、Hub5&#39;00のSwitchboardおよびCallHomeサブセットで6.4％および12.5％の単語誤り率（WER）に達します。全体として、さまざまな組み合わせ正規化とシンプルだがかなり大きなモデルは、外部データリソースなしでSWB-2000を使用して、スイッチボードとCallHomeセットで4.7％と7.8％のWERという新しい最先端の結果をもたらします。注意深い正規化とデータ拡張はこのレベルのパフォーマンスを達成するために、Switchboard-2000での実験では、より多くのデータほど有用なものはないことが示されています。 
[概要]新しい調査によると、すべてのモデルの7％が、注意を認識できる単一のシステムを使用できます。新しいテストでは、より多くのデータほど有用なものはないことが示されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-20">
        <br><font color="black">2020-01-20</font>
      </time>
    </span>
</section>
<!-- paper0: Learnable Spectro-temporal Receptive Fields for Robust Voice Type
  Discrimination -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.AS/paper_9.html">
      <font color="black">Learnable Spectro-temporal Receptive Fields for Robust Voice Type
  Discrimination</font>
    </a>
  </h2>
  <font color="black">特に、静的STRFまたは制約のないカーネルと比較した学習可能なSTRFの使用の効果を研究します。また、私たちのシステムが、存在下でのなりすまし検出において、広範囲の信号対雑音比にわたって競合ベースラインシステムを一貫して改善することを示します。 VTDディストラクタノイズ..この分野の研究をサポートするために収集された新しい標準化されたVTDデータベースで私たちのアプローチを評価します。 
[概要]この作業では、ディープラーニングベースのvtdシステムを提案します。これは、学習可能なスペクトルの初期層である時間受容野を特徴としています。また、システムが競争力のあるベースラインシステムを一貫して改善することも示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-End Text-to-Speech using Latent Duration based on VQ-VAE -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.AS/paper_10.html">
      <font color="black">End-to-End Text-to-Speech using Latent Duration based on VQ-VAE</font>
    </a>
  </h2>
  <font color="black">明示的な持続時間モデリングは、テキスト読み上げ合成（TTS）で堅牢かつ効率的なアライメントを実現するための鍵です。条件付きVQ-VAEに基づいてメソッドを定式化し、変分オートエンコーダーで離散持続時間を処理し、理論的な説明を提供して、方法..リスニングテストで提案した方法を評価し、ソフトアテンションまたは明示的な持続時間モデリングに基づいて他のTTS方法と比較しました。 
[概要]明示的な期間モデリングを使用した新しいttsフレームワークを提案します。これにより、モジュール全体を最初から共同で拡張できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: Reduce and Reconstruct: Improving Low-resource End-to-end ASR Via
  Reconstruction Using Reduced Vocabularies -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-20/eess.AS/paper_11.html">
      <font color="black">Reduce and Reconstruct: Improving Low-resource End-to-end ASR Via
  Reconstruction Using Reduced Vocabularies</font>
    </a>
  </h2>
  <font color="black">私たちの目的は2つあります。出力語彙スペースを削減することで低リソースのエンドツーエンドASRシステムの負担を軽減することと、削減された語彙のシーケンスから元の語彙のシーケンスを復元する強力な再構成モジュールを設計することです。 。2つのインド言語、GujaratiとTeluguのASRシステムを使用して、提案された手法の有効性を示します。2つの再構成モジュール、エンコーダデコーダベースのアーキテクチャと有限状態トランスデューサベースのモデルを紹介します。 
[概要]これらのシステムは、データを大量に消費し、低リソース設定ではパフォーマンスが低下することが知られています。最後の出力語彙を圧縮しますが、元の語彙を再構築します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
