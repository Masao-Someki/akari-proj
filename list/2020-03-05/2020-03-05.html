<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-03-05の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: LP-WaveNet: Linear Prediction-based WaveNet Speech Synthesis -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.SD/paper_0.html">
      LP-WaveNet: Linear Prediction-based WaveNet Speech Synthesis
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この問題に対処するために、LP-WaveNetボコーダーを提案します。ここでは、声源と声道コンポーネント間の複雑な相互作用が、混合密度ネットワークベースのWaveNetモデル内で一緒にトレーニングされます。パラメトリックテキスト読み上げ（TTS）システム。ただし、音声生成プロセス全体を考慮せずに音声ソースコンポーネントが独立して処理されるため、合成ノイズが生成される傾向があります。発声源と声道フィルターの間に不一致が生じることは避けられません。 
[概要]ウェーブネットベースのニューラルボコーダーは、パラメトリックテキスト音響システムの品質を大幅に向上させました。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-11-29">
        <br>2018-11-29
      </time>
    </span>
  </h3>
</article>
<!-- paper0: GraphTTS: graph-to-sequence modelling in neural text-to-speech -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.SD/paper_1.html">
      GraphTTS: graph-to-sequence modelling in neural text-to-speech
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これにより、リファレンスオーディオプロセスの手動選択が不要になり、韻律モデリングがエンドツーエンドの手順になります。このホワイトペーパーでは、グラフの埋め込みをマッピングするニューラルテキスト読み上げ（GraphTTS）のグラフからシーケンスへの方法を活用します。スペクトログラムへの入力シーケンス。GAEは、合成オーディオの一時停止、換気、およびトーンを自動的に調整できます。 
[概要] graphttsのエンコーダーをグラフ補助エンコーダー（gae）として使用すると、テキストの意味構造から韻律情報を分析できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: ASMD: an automatic framework for compiling multimodal datasets -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.SD/paper_2.html">
      ASMD: an automatic framework for compiling multimodal datasets
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      すべてのコードと根拠は、適切なオープンライセンスの下でリリースされます。フレームワークは、マルチモーダル音楽処理のために一般的に使用されるいくつかのデータセットの自動ダウンロードとインストールを可能にします。独自のコレクションを構築、変換、および拡張できるだけでなく、APIを活用するために準拠した形式でコレクションを配布できるようにするための注釈。 
[ABSTRACT]フレームワークは、新しいデータセットの組み込みを容易にするように設計されています。ツールは、人々が独自のコレクションを構築、変換、および拡張できるように設計されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Robust End-to-End Speaker Verification Using EEG -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.SD/paper_3.html">
      Robust End-to-End Speaker Verification Using EEG
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      EEG信号は話者検証システムの堅牢性を向上させることができます。話者検証を実行するための最先端の深層学習モデルを使用し、ノイズのある音声の結果を示します。スピーカー検証システムは、脳波記録（EEG）信号機能と音声信号を連結することで改善できます。 
[概要]話者検証を実行するために、最新のエンドツーエンドのディープラーニングモデルを使用します。ノイズのある音声の結果を示しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-17">
        <br>2019-06-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multi-Microphone Complex Spectral Mapping for Speech Dereverberation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.SD/paper_4.html">
      Multi-Microphone Complex Spectral Mapping for Speech Dereverberation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      マルチチャネル音声残響除去に関する実験結果は、提案されたアプローチの有効性を実証します。また、ビームフォーミングおよびポストフィルタリングとマルチマイク複合スペクトルマッピングの統合を調査します。この研究は、音声用のマルチマイク複合スペクトルマッピングアプローチを提案します固定配列ジオメトリでの残響除去。 
[概要]ディープニューラルネットワーク（dnn）は、複数のマイクの積み重なった残響（およびノイズ）ri部分からの直接音の実部および虚部（ri）成分を予測するように訓練されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: AlignTTS: Efficient Feed-Forward Text-to-Speech System without Explicit
  Alignment -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.SD/paper_5.html">
      AlignTTS: Efficient Feed-Forward Text-to-Speech System without Explicit
  Alignment
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      LJSpeechデータセットの実験では、モデルが、Transformer TTSを平均オプションスコア（MOS）で0.03上回る最先端のパフォーマンスを達成するだけでなく、リアルタイムの50倍以上の高効率を達成することが示されています。 ..高い効率とパフォーマンスの両方をターゲットとして、AlignTTSを提案し、並行してメルスペクトルを予測します。AlignTTSは、フィードフォワードトランスフォーマーに基づいており、文字のシーケンスからメルスペクトルを生成し、各文字の持続時間はTransformer TTSのアテンションメカニズムを採用してテキストをメルスペクトルに揃える代わりに、ダイナミックプログラミングを使用したトレーニングで考えられるすべてのアライメントを考慮するために、アライメント損失が表示されます。 
[ABSTRACT] alignttsは、1期間から50,000メル以上のスペクトルを生成するトランスフォーマーに基づいています。各文字の継続時間は、継続時間予測子によって決定されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Learning Fast Adaptation on Cross-Accented Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.SD/paper_6.html">
      Learning Fast Adaptation on Cross-Accented Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちのアプローチは、単語誤り率の観点から、混合領域および領域間設定でのゼロショット、少数ショット、および全ショットの両方での共同トレーニングを大幅に上回ります。このペーパーでは、クロスアクセント英語スピーチを紹介します。既存のCommonVoiceコーパスを使用して、見えないアクセントに適応するモデルの能力を測定するためのベンチマークとしての認識タスク。アクセントの大きな変動性と複雑な特性は、堅牢でアクセントに依存しない自動音声認識（ASR）システム。 
[ABSTRACT]アクセント認識システムを使用して、新しいシステムをトレーニングできます。また、アクセント-ag theoメタ学習（maml）アルゴリズムを提案します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving Uyghur ASR systems with decoders using morpheme-based language
  models -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.SD/paper_7.html">
      Improving Uyghur ASR systems with decoders using morpheme-based language
  models
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      アルゴリズムは、動的に生成されたグラフに権限を与え、4グラムの形態素ベースの言語モデル（LM）が使用される場合、静的で完全に構成されたグラフと同様に、格子内の形態素シーケンスを効果的に制約します。自動音声認識（ASR）研究のリソースは常に不十分です。オープンソースのMLDG-Decoderに基づいて、読者はこの論文の実験結果を簡単に再現できます。 
[概要] asrシステムの最適化を選択することにより、ギャップを埋めようとします。新しいデコーダーは、アルゴリズムを使用して、オンオフ状態と遷移がオンの中継局の役割を果たします。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-03">
        <br>2020-03-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Robust Speaker Clustering Method Based on Discrete Tied Variational
  Autoencoder -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.SD/paper_8.html">
      A Robust Speaker Clustering Method Based on Discrete Tied Variational
  Autoencoder
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      一般に、モデルは確率の入力としてiベクトルなどの機能を使用し、線形判別分析モデル（PLDA）は長い音声アプリケーションシナリオで距離行列を形成することを目的とし、クラスタリングモデルを通じてクラスタリング結果が取得されます。 AHCに基づくクラスタリング手法には、長時間の実行の欠点があり、環境ノイズに敏感なままです。経過時間を大幅に短縮するDiscrete Tied Variational Autoencoder（DTVAE）という名前の提案手法。 
[概要] pldaと呼ばれるモデルは、長音声アプリケーションシナリオで距離行列を形成することを目的としており、クラスタリングモデルを通じてクラスタリング結果が取得されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: XGPT: Cross-modal Generative Pre-Training for Image Captioning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_0.html">
      XGPT: Cross-modal Generative Pre-Training for Image Captioning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験により、XGPTは、COCOキャプションやFlickr30kキャプションなどのベンチマークデータセットで最新の結果を取得することが示されています。その結果、事前にトレーニングされたXGPTは、タスク固有のアーキテクチャを変更することなく微調整できます画像キャプションの最新モデル。XGPTを使用して、画像検索タスクのデータ増強として新しい画像キャプションを生成し、すべてのリコールメトリックを大幅に改善します。 
[ABSTRACT] xgptは、3つの新しい生成タスクを介してテキスト-リーダージェネレーターを事前にトレーニングするように設計された、イメージキャプションのクロスモーダル属の事前トレーニングの新しい方法です。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-03">
        <br>2020-03-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: jiant: A Software Toolkit for Research on General-Purpose Text
  Understanding Models -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_1.html">
      jiant: A Software Toolkit for Research on General-Purpose Text
  Understanding Models
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      jiantは、すべてのGLUEおよびSuperGLUEベンチマークタスクを含む50を超えるNLUタスクを実装します。jiantはhttps://jiant.info。で入手できます。jiantは、最先端のモデルを使用したモジュール式および構成駆動型の実験を可能にし、広範なプロービング、転移学習、およびマルチタスクトレーニング実験のための一連のタスク。 
[要約] jiantは、bertやrobertaを含むさまざまなタスクやモデルで公開されたパフォーマンスを再現することを実証しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Revisiting Simple Domain Adaptation Methods in Unsupervised Neural
  Machine Translation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_2.html">
      Revisiting Simple Domain Adaptation Methods in Unsupervised Neural
  Machine Translation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらのシナリオに基づいて、UNMTでのバッチ重み付けや微調整方法など、既存のドメイン適応方法の効果を再検討します。ドメイン適応は、教師付きニューラル機械翻訳（SNMT）でよく研究されています。 SNMTのテストデータでは、UNMTの2つの単一言語トレーニングデータの間に一貫性のないドメインが存在することがあります。 
[ABSTRACT] unmtは最近、いくつかの分野で特定の言語ペアで顕著な結果を達成しました。以前の研究では、教師なし神経機械翻訳のさまざまなシナリオが示されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-26">
        <br>2019-08-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Posterior-GAN: Towards Informative and Coherent Response Generation with
  Posterior Generative Adversarial Network -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_3.html">
      Posterior-GAN: Towards Informative and Coherent Response Generation with
  Posterior Generative Adversarial Network
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      直観的に、高品質の応答は、指定されたクエリに応答するだけでなく、将来の会話にもリンクします。本書では、クエリと応答と将来のターントリプルを利用して、指定されたコンテキストと将来の会話..実験結果は、本手法が自動評価と人間評価の両方で生成された応答の情報性と一貫性を効果的に高めることを示し、2つの評価の観点を考慮する利点を検証します。これらのトリプルのモデリングを促進するために、生成された応答が2つの補完的な評価の観点から有益で一貫性のあるものとなるように協調的に促進するための前方および後方の生成的識別器で構成される、後発生成的敵対ネットワーク（Posterior-Generative Adversarial Network）（Posterior Generative Adversarial Network） 
[要約]これらのモデルは、通常、最大尤度推定目的の誘導-応答ペアに対して最適化されます。ただし、一般的な鈍い応答問題は、モデルが無意味な応答トレーニングインスタンスに直面するとさらに悪化します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: GraphTTS: graph-to-sequence modelling in neural text-to-speech -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_4.html">
      GraphTTS: graph-to-sequence modelling in neural text-to-speech
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これにより、リファレンスオーディオプロセスの手動選択が不要になり、韻律モデリングがエンドツーエンドの手順になります。GAEは、合成オーディオの一時停止、換気、およびトーンを自動的に調整できます。テキスト読み上げ（GraphTTS）。入力シーケンスのグラフ埋め込みをスペクトログラムにマッピングします。 
[概要] graphttsのエンコーダーをグラフ補助エンコーダー（gae）として使用すると、テキストの意味構造から韻律情報を分析できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: SeMemNN: A Semantic Matrix-Based Memory Neural Network for Text
  Classification -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_5.html">
      SeMemNN: A Semantic Matrix-Based Memory Neural Network for Text
  Classification
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      結果は、提案された方法が小規模データセットのVDCNNと比較して、より良い結果を達成できることを示しています。提案された方法の最高のパフォーマンスは、テキスト分類タスクのベースラインVDCNNモデルよりも優れており、セマンティクスの学習を高速化します。このペーパーは、2020年にカリフォルニア州サンディエゴで開催された、セマンティックコンピューティングに関する2014年のIEEE 14th国際会議（ICSC 2020）の議事録に掲載されます。
[概要]このペーパーは、セマンティックコンピューティングに関する2020年の会議（icsc 2020 ）
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: On Emergent Communication in Competitive Multi-Agent Teams -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_6.html">
      On Emergent Communication in Competitive Multi-Agent Teams
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      テストベッドとしての2つの協力エージェント間の以前に提案された参照ゲームであるTask＆Talkから開始し、前述の2つの協力エージェントで構成される2つの競争チームを含むゲームTask、Talk＆Competeに拡張します。この新しい設定を使用して、マルチエージェントチームに対する競争力の影響を実証する実証的研究を提供します。 
[要旨]人間の集団は、コミュニケーション行動に関わる複雑な課題を解決することを学びます。外部の競争的影響により、精度と一般化が向上し、より有益で構成的なコミュニケーション言語がより速く形成されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Unsupervised Adversarial Domain Adaptation for Implicit Discourse
  Relation Classification -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_7.html">
      Unsupervised Adversarial Domain Adaptation for Implicit Discourse
  Relation Classification
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      暗黙の談話関係は、明示的な対応よりも分類が難しいだけでなく、注釈付けも困難です。暗黙的な関係のトレーニングデータが不足している状況に取り組み、明示的な関係からドメイン適応を活用します（Ji et al。、2015）。 。さらに、ラベル付きデータを利用できるようにシステムを拡張します。 
[ABSTRACT]私たちのシステムは、以前の作品や、教師なしドメイン適応のための他の敵対的ベンチマークよりも優れています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Efficient Sentence Embedding via Semantic Subspace Analysis -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_8.html">
      Efficient Sentence Embedding via Semantic Subspace Analysis
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、最新技術と同等以上のパフォーマンスを提供することを示しています。次に、複数のセマンティックグループ間の相互作用をグループ間記述子で特徴付けます。具体的には、2つの側面から文モデルを構築します。 
[概要]提案されたs3eメソッドは、提案された作業に応じて構築されます。空間タスクと監視された相互作用の両方に構築されます。これは、文表現スキームを作成するために使用できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-22">
        <br>2020-02-22
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Restoration of Fragmentary Babylonian Texts Using Recurrent Neural
  Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_9.html">
      Restoration of Fragmentary Babylonian Texts Using Recurrent Neural
  Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      現在、これらの欠落部分は専門家によって手動で完成されています。この研究では、学者を支援し、リカレントニューラルネットワークを使用して言語をモデル化することにより、アケメネス朝時代のバビロニアの古代アッカド語テキストの中断を自動的に完了する可能性を調査します..貴重なリソースであるにもかかわらず、多くのタブレットが断片化されており、情報が欠落しています。 
[ABSTRACT]タブレットはしばしば断片化され、情報が欠落しています。現在、リカレントニューラルネットワークを使用してプロジェクトに取り組んでいます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: AlignTTS: Efficient Feed-Forward Text-to-Speech System without Explicit
  Alignment -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_10.html">
      AlignTTS: Efficient Feed-Forward Text-to-Speech System without Explicit
  Alignment
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      LJSpeechデータセットの実験では、モデルが、Transformer TTSを平均オプションスコア（MOS）で0.03上回る最先端のパフォーマンスを達成するだけでなく、リアルタイムの50倍以上の高効率を達成することが示されています。 ..高い効率とパフォーマンスの両方をターゲットとして、AlignTTSを提案し、並行してメルスペクトルを予測します。AlignTTSは、フィードフォワードトランスフォーマーに基づいており、文字のシーケンスからメルスペクトルを生成し、各文字の持続時間はTransformer TTSのアテンションメカニズムを採用してテキストをメルスペクトルに揃える代わりに、ダイナミックプログラミングを使用したトレーニングで考えられるすべてのアライメントを考慮するために、アライメント損失が表示されます。 
[ABSTRACT] alignttsは、1期間から50,000メル以上のスペクトルを生成するトランスフォーマーに基づいています。各文字の継続時間は、継続時間予測子によって決定されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Trends of digitalization and adoption of big data & analytics among UK
  SMEs: Analysis and lessons drawn from a case study of 53 SMEs -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_11.html">
      Trends of digitalization and adoption of big data & analytics among UK
  SMEs: Analysis and lessons drawn from a case study of 53 SMEs
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらすべての形式のデータは、適切なデータバリューチェーンに入れると金銭的価値に変換できます。これには、ビジネスの長期的な利益のためにスキルとIT投資の両方が必要です。主にイングランドのウェストミッドランズ地域のSMEは、ビッグデータ管理、分析、および関連するIT問題の分野で、3年間のERDFプロジェクトであるビッグデータコリドーの一部としてサポートしました。 
[ABSTRACT]データデータデータは、適切なデータバリューチェーンに入れると金銭的価値に変換できます。これらの支出は、限られたリソースと財政へのアクセス制限のため、ほとんどのスチームの能力を超えています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-14">
        <br>2020-02-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Modernizing Historical Documents: a User Study -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_12.html">
      Modernizing Historical Documents: a User Study
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      結果は、改善の余地はあるものの、モダナイゼーションは目標に到達していることを示しています。このアプローチを自動評価と人間評価の両方でテストし、ユーザー調査を実施しました。 
[要約]これは、人間の言語の言語障壁によるものです。これにより、より多くの聴衆が歴史文書にアクセスできるようになります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-07-01">
        <br>2019-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading
  and Annotation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_13.html">
      ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading
  and Annotation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この新しいデータセットは、自然な読みと注釈の間の認知処理の違いを分析するために設計された実験を提供することにより、ZuCo 1.0を補完します。データはhttps://osf.io/2urht/。自然な読書中および注釈中の同時アイトラッキングと脳波記録の新しいデータセット。 
[要約]データには、739文の注視および脳活動データ、通常のリーディングツールでは349、タスク固有のコマンドでは390が含まれます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-02">
        <br>2019-12-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Evaluating Low-Resource Machine Translation between Chinese and
  Vietnamese with Back-Translation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_14.html">
      Evaluating Low-Resource Machine Translation between Chinese and
  Vietnamese with Back-Translation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      以前の作品からのいくつかの結論は部分的に確認されており、BTをさらに理解するのに有益な他の興味深い発見と結論も引き出しています。本論文では、BTがアジアの言語翻訳に及ぼす影響を極度に低い中国語とベトナム語のペア..中国語からベトナム語およびベトナム語から中国語のNMTおよび統計的機械翻訳（SMT）モデルの両方で、文字ベースおよび単語ベースの設定で、異なるサイズの合成データの効果を評価および比較します。 
[概要] btに関連するいくつかの作品は、主にヨーロッパの言語に焦点を当てています。それらのほとんどは、世界中の他の地域の言語を研究しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Computing rank-revealing factorizations of matrices stored out-of-core -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_15.html">
      Computing rank-revealing factorizations of matrices stored out-of-core
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      正確には、$ n \ times n $行列を完全に因数分解するための計算時間は$ cn ^ {3} $としてスケーリングします。スケーリング定数$ c $は、行列がコア外に格納されている場合にわずかに大きくなります。 2番目の方法では、行列$ A $を$ A = UTV ^ * $として表現する、いわゆるUTV分解が行われます。ここで、$ U $と$ V $はユニタリで、$ T $は三角形です。行列の大きな連続ブロックを一括処理できるように再定式化。 
[ABSTRACT]列ピボットqr分解など、因子を明らかにするランクを計算するための従来のアルゴリズムは、非常に通信集約的です。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-17">
        <br>2020-02-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving Uyghur ASR systems with decoders using morpheme-based language
  models -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_16.html">
      Improving Uyghur ASR systems with decoders using morpheme-based language
  models
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      アルゴリズムは、4グラムの形態素ベースの言語モデル（LM）を使用した場合に、静的で完全に構成されたグラフと同じように、格子内の形態素シーケンスを効果的に制約する動的に生成されたグラフを強化します。 ASRシステムを最終的に最適化し、形態素ベースのデコーダーであるMLDG-Decoder（Mypheme Lattice Dynamically Generate 14.54％でありながら、メモリ消費を合理的に維持します。 
[概要] asrシステムの最適化を選択することにより、ギャップを埋めようとします。新しいデコーダーは、アルゴリズムを使用して、オンオフ状態と遷移がオンの中継局の役割を果たします。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-03">
        <br>2020-03-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: LibriVoxDeEn: A Corpus for German-to-English Speech Translation and
  German Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_17.html">
      LibriVoxDeEn: A Corpus for German-to-English Speech Translation and
  German Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      スピーチ翻訳データは、5万以上の並列センテンスに合わせて調整された110時間のオーディオ素材で構成されています。スピーチを読むため、流dis性が低い。 
[概要]音声翻訳データは、5万以上の並列文に合わせて調整された110時間の音声素材で構成されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-17">
        <br>2019-10-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Data Augmentation using Pre-trained Transformer Models -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_18.html">
      Data Augmentation using Pre-trained Transformer Models
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、さまざまな事前トレーニングモデルベースのデータ増強がデータ多様性の観点でどのように異なり、そのような方法がクラスラベル情報をどの程度保存するかを調べます。BERTなどの言語モデルベースの事前トレーニングモデルは、さまざまな分野で大きな利益をもたらしましたNLPタスク..クラスラベルをテキストシーケンスの前に追加することで、事前にトレーニングされたモデルをデータ増強のために調整するためのシンプルで効果的な方法が提供されることを示します。 
[概要]これらのタイプの事前トレーニング済みトランスベースモデルはテスト済みです。これらには、自動回帰モデル（gpt-2）および自動エンコーダモデル（bart）が含まれます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Attention over Parameters for Dialogue Systems -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_19.html">
      Attention over Parameters for Dialogue Systems
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      対話システムは、人間を支援し、情報を提供し、楽しませるために、非常に多くの異なる補完的な専門知識を必要とします。この論文では、異なる対話スキルを独立してパラメーター化し、注意を通してそれぞれを選択し、組み合わせることを学ぶ対話システムを学ぶことを提案しますover parameters（AoP）..実験結果は、このアプローチがMultiWOZ、In-Car Assistant、およびPersona-Chatの組み合わせデータセットで競争力のあるパフォーマンスを達成することを示しています。 
[要約]インタラクティブな対話システムはさまざまなスキルと見なすことができ、チットチャットシステムの通常のチャット機能も同様です。このアプローチにより、マルチウォズ、車内アシスタント、ペルソナチャットを組み合わせたデータセットで競争力のあるパフォーマンスを実現します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-07">
        <br>2020-01-07
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving Relation Extraction with Knowledge-attention -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/cs.CL/paper_20.html">
      Improving Relation Extraction with Knowledge-attention
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、提案された知識注意メカニズムが自己注意と補完的な強みを持ち、統合モデルが既存のCNN、RNN、および自己注意ベースのモデルよりも優れていることを示しています。 -知識とデータの両方を最大限に活用するための注意。注意メカニズムは多くのNLPタスクで効果的であることが証明されていますが、それらの大半はデータ駆動型です。 
[ABSTRACT]アテンションエンコーダーは、外部のレキシパーリソースからの事前知識をリレーション抽出タスク用のディープニューラルネットワークに結合します。提案されたリレーション抽出システムは、効果的で完全なアテンションベースです。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-07">
        <br>2019-10-07
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: LP-WaveNet: Linear Prediction-based WaveNet Speech Synthesis -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/eess.AS/paper_0.html">
      LP-WaveNet: Linear Prediction-based WaveNet Speech Synthesis
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、提案されたシステムが従来のWaveNetボコーダーよりも客観的および主観的に優れていることを検証します。 WaveNetボコーディングフレームワークによる線形予測（LP）ベースの波形生成方法を提案します。 
[概要]ウェーブネットベースのニューラルボコーダーは、パラメトリックテキスト音響システムの品質を大幅に向上させました。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-11-29">
        <br>2018-11-29
      </time>
    </span>
  </h3>
</article>
<!-- paper0: GraphTTS: graph-to-sequence modelling in neural text-to-speech -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/eess.AS/paper_1.html">
      GraphTTS: graph-to-sequence modelling in neural text-to-speech
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これにより、リファレンスオーディオプロセスの手動選択が不要になり、韻律モデリングがエンドツーエンドの手順になります。GAEは、合成オーディオの一時停止、換気、およびトーンを自動的に調整できます。テキスト読み上げ（GraphTTS）。入力シーケンスのグラフ埋め込みをスペクトログラムにマッピングします。 
[概要] graphttsのエンコーダーをグラフ補助エンコーダー（gae）として使用すると、テキストの意味構造から韻律情報を分析できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: ASMD: an automatic framework for compiling multimodal datasets -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/eess.AS/paper_2.html">
      ASMD: an automatic framework for compiling multimodal datasets
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このフレームワークにより、マルチモーダル音楽処理のために一般的に使用されるいくつかのデータセットの自動ダウンロードとインストールが可能になります。具体的には、作曲家、楽器、楽器の共通部分や結合などの特定の属性に基づいてブール集合演算を介してデータセットにアクセスするPython APIを提供しますなど..すべてのコードと根拠は、適切なオープンライセンスの下でリリースされます。 
[ABSTRACT]フレームワークは、新しいデータセットの組み込みを容易にするように設計されています。ツールは、人々が独自のコレクションを構築、変換、および拡張できるように設計されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Robust End-to-End Speaker Verification Using EEG -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/eess.AS/paper_3.html">
      Robust End-to-End Speaker Verification Using EEG
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      EEG信号は、スピーカー検証システムの堅牢性を向上させることができます。この論文では、脳波（EEG）信号の特徴を音声信号と連結することにより、スピーカー検証システムの性能を向上できることを示します。話者検証を実行するためのディープラーニングモデルを終了し、ノイズの多い音声の結果を示します。 
[概要]話者検証を実行するために、最新のエンドツーエンドのディープラーニングモデルを使用します。ノイズのある音声の結果を示しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-17">
        <br>2019-06-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multi-Microphone Complex Spectral Mapping for Speech Dereverberation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/eess.AS/paper_4.html">
      Multi-Microphone Complex Spectral Mapping for Speech Dereverberation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      マルチチャネル音声残響除去の実験結果は、提案されたアプローチの有効性を実証します。また、ビームフォーミングおよびポストフィルタリングとマルチマイクロフォン複合スペクトルマッピングの統合を調査します。提案されたアプローチでは、ディープニューラルネットワーク（DNN）複数のマイクロフォンの積み重ねられた反響（およびノイズのある）RIコンポーネントからの直接音の実数および虚数（RI）コンポーネントを予測するようにトレーニングされています。 
[概要]ディープニューラルネットワーク（dnn）は、複数のマイクの積み重なった残響（およびノイズ）ri部分からの直接音の実部および虚部（ri）成分を予測するように訓練されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: AlignTTS: Efficient Feed-Forward Text-to-Speech System without Explicit
  Alignment -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/eess.AS/paper_5.html">
      AlignTTS: Efficient Feed-Forward Text-to-Speech System without Explicit
  Alignment
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      LJSpeechデータセットの実験では、モデルが、Transformer TTSを平均オプションスコア（MOS）で0.03上回る最先端のパフォーマンスを達成するだけでなく、リアルタイムの50倍以上の高効率を達成することが示されています。 ..高い効率とパフォーマンスの両方をターゲットとして、AlignTTSを提案し、並行してメルスペクトルを予測します。AlignTTSは、フィードフォワードトランスフォーマーに基づいており、文字のシーケンスからメルスペクトルを生成し、各文字の持続時間はTransformer TTSのアテンションメカニズムを採用してテキストをメルスペクトルに揃える代わりに、ダイナミックプログラミングを使用したトレーニングで考えられるすべてのアライメントを考慮するために、アライメント損失が表示されます。 
[ABSTRACT] alignttsは、1期間から50,000メル以上のスペクトルを生成するトランスフォーマーに基づいています。各文字の継続時間は、継続時間予測子によって決定されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Learning Fast Adaptation on Cross-Accented Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/eess.AS/paper_6.html">
      Learning Fast Adaptation on Cross-Accented Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちのアプローチは、単語誤り率の観点から、混合領域および領域間設定のゼロショット、少数ショット、および全ショットの両方での共同トレーニングよりも大幅に優れています。ローカル方言は、同じ言語の単語を異なる発音をする人々に影響を与えますまた、モデルに依存しないメタ学習（MAML）アルゴリズムを拡張して、目に見えないアクセントにすばやく適応させる、アクセントに依存しないアプローチを提案します。 
[ABSTRACT]アクセント認識システムを使用して、新しいシステムをトレーニングできます。また、アクセント-ag theoメタ学習（maml）アルゴリズムを提案します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving Uyghur ASR systems with decoders using morpheme-based language
  models -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/eess.AS/paper_7.html">
      Improving Uyghur ASR systems with decoders using morpheme-based language
  models
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このアルゴリズムは、4-Gram形態素ベースの言語モデル（LM）が使用される場合に、静的で完全に構成されたグラフと同様に、格子内の形態素シーケンスを効果的に制約する動的に生成されたグラフを強化します。デコーダー、読者はこの論文の実験結果を簡単に再現できます。実験結果は、静的で完全に構成されたグラフに基づくデコードにより、クリーンでノイズのないスピーチテストの最新のワード誤り率（WER）が減少することを示していますTHUYG-20でのタスクは14.24％です。 
[概要] asrシステムの最適化を選択することにより、ギャップを埋めようとします。新しいデコーダーは、アルゴリズムを使用して、オンオフ状態と遷移がオンの中継局の役割を果たします。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-03">
        <br>2020-03-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Robust Speaker Clustering Method Based on Discrete Tied Variational
  Autoencoder -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-05/eess.AS/paper_8.html">
      A Robust Speaker Clustering Method Based on Discrete Tied Variational
  Autoencoder
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ただし、AHCに基づく従来のスピーカークラスタリング方法には、長時間の実行の欠点があり、環境ノイズに敏感なままです。一般に、モデルは、確率の入力としてiベクトルなどの機能を取り、線形判別分析モデル（PLDA）の形成を目的としています長い音声アプリケーションシナリオでの距離行列、そしてクラスタリングモデルを介してクラスタリング結果が得られます。経過時間を大幅に短縮するDiscrete Tied Variational Autoencoder（DTVAE）という名前の提案手法。 
[概要] pldaと呼ばれるモデルは、長音声アプリケーションシナリオで距離行列を形成することを目的としており、クラスタリングモデルを通じてクラスタリング結果が取得されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br>2020-03-04
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="">
      
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本日更新された論文はありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="">
        <br>
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
