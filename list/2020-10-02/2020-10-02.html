<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-10-02の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: A convolutional neural-network model of human cochlear mechanics and
  filter tuning for real-time applications -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.SD/paper_0.html">
      <font color="black">A convolutional neural-network model of human cochlear mechanics and
  filter tuning for real-time applications</font>
    </a>
  </h2>
  <font color="black">CoNNearモデルは、人間の蝸牛周波数選択性とその音の強さへの依存性を正確にシミュレートします。これは、負の音声対バックグラウンドノイズ比でのロバストな音声明瞭度に不可欠な品質です。これらの独自のCoNNear機能により、次世代の人間のような機械が可能になります。ヒアリングアプリケーション..CoNNearアーキテクチャは、並列で微分可能な計算に基づいており、リアルタイムの人間のパフォーマンスを実現する能力を備えています。 
[ABSTRACT] connearモデルは、音響音声素材でトレーニングされました。そのパフォーマンスと適用性は、音刺激を使用して評価されました。connearアーキテクチャは、並列で微分可能な計算に基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-30">
        <br><font color="black">2020-04-30</font>
      </time>
    </span>
</section>
<!-- paper0: SEANet: A Multi-modal Speech Enhancement Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.SD/paper_1.html">
      <font color="black">SEANet: A Multi-modal Speech Enhancement Network</font>
    </a>
  </h2>
  <font color="black">加速度計データを活用して、非常にノイズの多い状況で音声強調を実行する可能性を探ります。この観察に基づいて、マルチモーダル入力を、波から波への完全畳み込みモデルであるSEANet（Sound EnhAncement Network）に供給します。機能損失と敵対的損失の組み合わせにより、ユーザーの音声の拡張バージョンを再構築します。私たちの実験結果は、同じレベルの音量で音声を妨害する場合でも、非常に高品質の結果を達成できることを示しています。 
[概要]イヤフォンに取り付けられたセンサーによって収集され、オーディオ信号にさまざまな種類のノイズ源を追加することによって合成的に破損したデータを使用してモデルをトレーニングしました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-04">
        <br><font color="black">2020-09-04</font>
      </time>
    </span>
</section>
<!-- paper0: Phase retrieval with Bregman divergences and application to audio signal
  recovery -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.SD/paper_2.html">
      <font color="black">Phase retrieval with Bregman divergences and application to audio signal
  recovery</font>
    </a>
  </h2>
  <font color="black">確かに、二次損失はオーディオのいくつかの知覚特性を適切に説明しておらず、ベータ発散などの代替の不一致測定が多くの設定で好まれています。したがって、ブレグマン発散を含む新しい最小化問題としてPRを定式化します。紙、私たちは別の見方を採用しています。 
[ABSTRACT] prは通常、2次損失関数を含む最小化問題を考慮することで対処されます。2次損失はオーディオのいくつかの知覚特性を考慮していません。ベータなどの代替の不一致測定-発散は多くの設定で好まれています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: SESQA: semi-supervised learning for speech quality assessment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.SD/paper_3.html">
      <font color="black">SESQA: semi-supervised learning for speech quality assessment</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、このような半教師ありアプローチにより、既存の方法のエラーを36％以上削減できると同時に、再利用可能な機能や補助出力の点で追加のメリットを提供できることを示しています。有望な一般化機能..自動音声品質評価は、人間の注釈の不足、目に見えない録音条件への不十分な一般化、および既存のアプローチの柔軟性の欠如によって進行が妨げられる重要な横断的タスクです。 
[概要]サンプル外のテストは、有望な一般化機能を示しています。サンプル外のテストによって、改善がさらに裏付けられます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Joint Articulatory-Acoustic Representations with Normalizing
  Flows -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.SD/paper_4.html">
      <font color="black">Learning Joint Articulatory-Acoustic Representations with Normalizing
  Flows</font>
    </a>
  </h2>
  <font color="black">私たちのモデルは、畳み込みオートエンコーダアーキテクチャと正規化フローベースモデルを利用して、1D音響波モデルを備えた2自由度の調音シンセサイザーの正中矢状声道ジオメトリ間で、半監視方式で順方向と逆方向の両方のマッピングを可能にします私たちのアプローチは、調音から音響へのマッピングと音響から調音へのマッピングの両方を達成する上で満足のいくパフォーマンスを達成し、それによって両方のドメインの共同エンコーディングを達成することに成功したことを示しています。この論文は、それぞれのドメイン固有の機能を同時に維持しながら、反転可能なニューラルネットワークモデルを介して、声道音の調音ドメインと音響ドメインの間の共同潜在表現を見つけることを目的としています。 
[ABSTRACT]調音-から-音響および音響との関係。新しい研究は、母音の調音と音響領域の間の共同潜在表現を見つけることを目的としています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-16">
        <br><font color="black">2020-05-16</font>
      </time>
    </span>
</section>
<!-- paper0: FSD50K: an Open Dataset of Human-Labeled Sound Events -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.SD/paper_5.html">
      <font color="black">FSD50K: an Open Dataset of Human-Labeled Sound Events</font>
    </a>
  </h2>
  <font color="black">元のオーディオトラックのダウンロードも、構成要素であるYouTubeビデオが徐々に消え、使用権の問題があるため問題があり、システムのベンチマークに対するこのリソースの適合性に疑問が投げかけられます。サウンドイベント認識（SER）の既存のデータセットのほとんどは、比較的小さく、および/またはドメイン固有。AudioSetを除いて、YouTubeビデオからの大量のオーディオトラックに基づいており、500クラスを超える日常のサウンドを網羅しています。ただし、AudioSetはオープンデータセットではありません---そのリリースは事前に計算されたもので構成されています一部のSERメソッドの採用を制限するオーディオ機能（波形の代わりに）。 
[ABSTRACT] fsd50kは、フリーサウンドオーディオクリップのオープンデータセットです。これには、合計100時間以上のオーディオの51,000を超えるオーディオクリップが含まれます。これらには、一部のserメソッドの使用を制限するフリーサウンドオーディオ機能が含まれます。これは、代替のベンチマークデータセットと育成によるものです。 serリサーチ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: A computationally efficient reconstruction algorithm for circular
  cone-beam computed tomography using shallow neural networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.IV/paper_0.html">
      <font color="black">A computationally efficient reconstruction algorithm for circular
  cone-beam computed tomography using shallow neural networks</font>
    </a>
  </h2>
  <font color="black">円形コーンビーム（CCB）コンピューター断層撮影（CT）は、産業品質管理、材料科学、医用画像の不可欠な部分になりました。さらに、NN-FDKアルゴリズムは、トレーニングデータの要件が低く、高速であるように設計されています。 train ..これにより、提案されたアルゴリズムを使用して、高スループットCTスキャン設定で画質を向上させることができます。FDKは現在、すぐに利用できる計算リソースを使用して取得速度に対応するために使用されています。 
[概要]このアルゴリズムは、fdkアルゴリズムに機械学習コンポーネントを追加します。これにより、アルゴリズムを使用して画質を向上させることができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Realistic Image Normalization for Multi-Domain Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.IV/paper_1.html">
      <font color="black">Realistic Image Normalization for Multi-Domain Segmentation</font>
    </a>
  </h2>
  <font color="black">そのために、完全に自動化された敵対的でタスク主導の正規化アプローチが採用されています。これは、最先端のパフォーマンスを維持しながら、現実的で解釈可能な画像のトレーニングを容易にするためです。ただし、この戦略は、複数のデータセットにわたって利用可能な複雑な共同情報を完全に活用することによる現在の正規化アルゴリズム。私たちの方法は、複数の画像ドメインから学習するときに利用可能なサンプルの数を増やすことによって、データの可用性を高めることもできます。 
[ABSTRACT]複数のデータセットを正規化すると、一貫性のある正規化された画像と改善された画像セグメンテーションが得られることが示されています。したがって、このような結合情報を無視すると、セグメンテーションアルゴリズムのパフォーマンスに直接影響します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-29">
        <br><font color="black">2020-09-29</font>
      </time>
    </span>
</section>
<!-- paper0: Improving spatial domain based image formation through compressed
  sensing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.IV/paper_2.html">
      <font color="black">Improving spatial domain based image formation through compressed
  sensing</font>
    </a>
  </h2>
  <font color="black">さらに、マルチレベルサンプリングは、平均してシングルレベルの均一ランダムサンプリングよりも優れていると結論付けます。画質の比較では、「枯れ葉」データセット、ベイジアン推定、およびピーク信号対雑音比（PSNR）測定を使用します。さらに、単一ピクセルスキャンシステムでのマルチレベルサンプリングは、検出器の視野を動的に変更することによってシミュレートされます。 
[概要] psnr分布は、補間された凝視アレイよりも画質を向上させるレベルの小さなセットがあることを示しています。レベルサンプリングは、ピーク信号対雑音比の分布を向上させます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: C-Arm Non-Circular Orbits: Geometric Calibration, Image Quality, and
  Avoidance of Metal Artifacts -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.IV/paper_3.html">
      <font color="black">C-Arm Non-Circular Orbits: Geometric Calibration, Image Quality, and
  Avoidance of Metal Artifacts</font>
    </a>
  </h2>
  <font color="black">補間ベースの幾何学的キャリブレーションによる非円軌道は、適度に正確な3D画像再構成をもたらしました。以前に提案された金属アーチファクト回避（MAA）法を拡張して、金属誘起を回避する非円軌道を前向きに定義することにより、金属アーチファクトの影響を低減します。投影領域のバイアス..椎弓根スクリュー器具を備えた脊椎ファントムでは、MAA法によって識別された非円軌道により、CBCT再構成における金属の「ブロミング」アーチファクト（スクリューシャフトの見かけの幅）の大きさが約70％減少しました。 
[概要]モバイルc-armシステムの最近の進歩により、非円軌道での3Dイメージング機能が可能になりました。この方法では、エンドツーエンドのトレーニング済みニューラルネットワークを使用して2つのスカウトビューのみを介して金属オブジェクトを3Dローカライズします。投影データの金属誘導バイアスを低減する非円形スキャン</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: DeepSEE: Deep Disentangled Semantic Explorative Extreme Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.IV/paper_4.html">
      <font color="black">DeepSEE: Deep Disentangled Semantic Explorative Extreme Super-Resolution</font>
    </a>
  </h2>
  <font color="black">現在の文献のほとんどは、高い再構成忠実度またはフォトリアリスティックな知覚品質のいずれかの単一の決定論的ソリューションを目指しています。私たちの知る限り、DeepSEEは探索的超解像のためにセマンティックマップを活用する最初の方法です。 （SR）は定義上不適切です。 
[ABSTRACT]深い解きほぐされたセマンティック探索的超解像。それはセマンティック領域、それらの解きほぐされた外観の知識を提供します。これにより、幅広い画像操作が可能になります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-09">
        <br><font color="black">2020-04-09</font>
      </time>
    </span>
</section>
<!-- paper0: High Quality Remote Sensing Image Super-Resolution Using Deep Memory
  Connected Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.IV/paper_5.html">
      <font color="black">High Quality Remote Sensing Image Super-Resolution Using Deep Memory
  Connected Network</font>
    </a>
  </h2>
  <font color="black">パラメータをさらに削減し、時間を節約するために、ダウンサンプリングユニットを提案し、フィーチャマップの空間サイズを縮小します。空間解像度が異なる3つのリモートセンシングデータセットでDMCNをテストします。ローカルおよびグローバルメモリ接続を構築して、画像の詳細を環境情報。 
[ABSTRACT]ニューラルネットワークに基づく既存の方法は、通常、小さな受容野を持ち、画像の詳細を無視します。この方法は、メモリ接続を使用して、画像の詳細と環境情報を組み合わせます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning based Dimple Segmentation for Quantitative Fractography -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.IV/paper_6.html">
      <font color="black">Deep Learning based Dimple Segmentation for Quantitative Fractography</font>
    </a>
  </h2>
  <font color="black">破壊の原因を特定することは、材料特性、機械的特性の予測、および新しい耐破壊性材料の開発に役立ちます。提案された新しいモデルは、他の以前のアプローチと比較して最高のパフォーマンスを実現します。金属の破壊の原因を特定するために、フラクトグラフのディンプルのセグメンテーションの問題に対処します。つまり、
[ABSTRACT]金属の破壊の原因を特定するために、フラクトグラフのディンプルのセグメンテーションの問題に対処します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-05">
        <br><font color="black">2020-07-05</font>
      </time>
    </span>
</section>
<!-- paper0: On Box-Cox Transformation for Image Normality and Pattern Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.IV/paper_7.html">
      <font color="black">On Box-Cox Transformation for Image Normality and Pattern Classification</font>
    </a>
  </h2>
  <font color="black">ここでは、2次元データ、すなわちデジタル画像を変換し、その効果を研究するための前処理ステップなどのツールの有用性を中心に展開します。さらに、時間の複雑さを軽減するには、パラメータラムダを推定するだけで十分です。確率密度関数を基礎となるデータ分布の統計的推論と見なすだけで、大きな2次元マトリックスをリアルタイムで処理できます。電力変換ファミリーのユニークなメンバーは、Box-Cox変換として知られています。 
[概要]ボックス-cox変換がヒストグラム変換を利用してデジタル画像に拡張されるのはこれが初めてです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-15">
        <br><font color="black">2020-04-15</font>
      </time>
    </span>
</section>
<!-- paper0: FocusLiteNN: High Efficiency Focus Quality Assessment for Digital
  Pathology -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.IV/paper_8.html">
      <font color="black">FocusLiteNN: High Efficiency Focus Quality Assessment for Digital
  Pathology</font>
    </a>
  </h2>
  <font color="black">9つの異なる染色色にわたる多様な組織スライドを含むFocusPathを使用してトレーニングデータセットを作成します。染色の多様性は、モデルが多様な色スペクトルと組織構造を学習するのに大いに役立ちます。この種の最大の新しい包括的な評価データセットを紹介します。モデルの評価と比較のためにTCGAリポジトリから注釈を付けてコンパイルします。提案された方法は、既存の知識駆動型およびデータ駆動型のFQAアプローチと比較して、優れた精度と速度のトレードオフを示します。CNNの複雑さを軽減するために、 CNNを最小限に抑えても、非常に競争力のあるパフォーマンスを実現していることに驚かされます。 
[概要]新しいfqaモデルには、知識主導のアプローチとデータ主導のアプローチが含まれます。これらには、gpusなどの過度のハードウェア要件なしに、知識主導の方法と同様の高速計算を維持する新しいモデルが含まれます。ただし、モデルを最小限のレベルでも、非常に競争力のあるパフォーマンスを実現します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-11">
        <br><font color="black">2020-07-11</font>
      </time>
    </span>
</section>
<!-- paper0: Attention based Writer Independent Handwriting Verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.IV/paper_9.html">
      <font color="black">Attention based Writer Independent Handwriting Verification</font>
    </a>
  </h2>
  <font color="black">注意マップは、ネットワークの出力尤度スコアの説明の前提として機能します。注意メカニズムにより、ネットワークは入力の関連領域により焦点を合わせることができるため、分類パフォーマンスが向上します。提案されたアプローチにより、86 \％の精度が達成されます。 CEDAR筆記体「AND」データセットでライター内のケースを検出するため。 
[概要]クロスアテンションネットワークとソフトアテンションネットワークを実装および統合して、2D入力の特徴空間内の相関性の高い顕著なポイントをキャプチャします。アテンションメカニズムにより、ネットワークは入力の関連領域により集中できるようになり、分類性能</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-07">
        <br><font color="black">2020-09-07</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Group-wise Variational Diffeomorphic Image Registration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.IV/paper_10.html">
      <font color="black">Deep Group-wise Variational Diffeomorphic Image Registration</font>
    </a>
  </h2>
  <font color="black">これを実現するために、ペアワイズ変分および微分同相VoxelMorphアプローチに基づいて、複数の画像を測地線平均に登録することと、利用可能な画像のいずれかを固定画像として使用できる登録の両方を可能にする一般的な数学的フレームワークを提示します。 .ElastixおよびVoxelMorphとの比較は、画像の類似性と参照ランドマーク距離の点で、大幅に高速な登録で提案された方法の競争力のある定量的パフォーマンスを示しています。複数にわたって取得された乳房MRIおよび胸部4DCT試験の患者内登録を使用してアプローチをトレーニングおよび評価しました。時点。 
[概要]複数の画像の同時登録を可能にするために現在の学習を拡張することを提案します。これらには、正規化された相互情報量に基づく尤度と、粘性流体エネルギーの明示的な制御を可能にする事前情報が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Utilizing Transfer Learning and a Customized Loss Function for Optic
  Disc Segmentation from Retinal Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.IV/paper_11.html">
      <font color="black">Utilizing Transfer Learning and a Customized Loss Function for Optic
  Disc Segmentation from Retinal Images</font>
    </a>
  </h2>
  <font color="black">包括的な実験から得られた結果は、さまざまなソースから取得した網膜画像のディスクセグメンテーションに対するアプローチの堅牢性を示しています。私たちのアプローチは、2人の検眼医によって注釈が付けられた民間クリニックからのデータセットによって補強された7つの公開データセットでテストされました。この目的のために構築されたWebポータル。私たちのアプローチは、ImageNetデータセットでトレーニングされたVGG16エンコーダーを備えたUNETベースのモデルを利用します。 
[概要]新しい研究では、高精度の網膜眼底画像を指定して視神経乳頭をセグメント化するディープラーニングベースのアプローチを提案しています。これは、ディスクをセグメント化するためのシンプルでシンプルなアプローチを使用しています。この方法を使用して、新しいモデルを作成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Attention Based Real Image Restoration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.IV/paper_12.html">
      <font color="black">Attention Based Real Image Restoration</font>
    </a>
  </h2>
  <font color="black">コード、トレーニング済みモデル、および結果は、https：//github.com/saeed-anwar/R2Netで入手できます。また、合成ノイズ除去に関するメソッドの機能を示すために、ノイズ除去のために合成で生成された3つの劣化データセットの比較を示します。 。30を超える最先端のアルゴリズムに対する11の実際に劣化したデータセットでのノイズ除去、超解像、雨滴除去、およびJPEG圧縮は、R $ ^ 2 $ Netの優位性を示しています。 
[ABSTRACT]復元アルゴリズムの実用性を向上させるために、この論文は、新しい単段ブラインド実画像復元ネットワークを提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-26">
        <br><font color="black">2020-04-26</font>
      </time>
    </span>
</section>
<!-- paper0: Cloud detection in Landsat-8 imagery in Google Earth Engine based on a
  deep neural network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.IV/paper_13.html">
      <font color="black">Cloud detection in Landsat-8 imagery in Google Earth Engine based on a
  deep neural network</font>
    </a>
  </h2>
  <font color="black">このようなデータセットでは、雲の検出が必要な前提条件のステップであることがよくあります。提案されたDeepGEE-CDアプローチは、Landsat-8画像をダウンロードせずに正確に検出できるため、Landsat-8画像の日常的な雲検出の有望な方法になります。 GEEで..ディープニューラルネットワーク（DNN）は最初にローカルでトレーニングされ、次にトレーニングされたDNNがGEEのJavaScriptクライアントにデプロイされました。 
[概要]この手紙は、雲の検出を直接実行する方法を提案しています。deepgee-cdは、広く使用されているマスク（fmask）アルゴリズムの機能を上回りました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-18">
        <br><font color="black">2020-06-18</font>
      </time>
    </span>
</section>
<!-- paper0: Mini-DDSM: Mammography-based Automatic Age Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.IV/paper_14.html">
      <font color="black">Mini-DDSM: Mammography-based Automatic Age Estimation</font>
    </a>
  </h2>
  <font color="black">続いて、収集したデータセットから深層学習の特徴を抽出し、ランダムフォレストリグレッサを使用してモデルを構築して年齢を自動的に推定しました。サンプルのランダム選択に関する10回のテストの平均エラー値は約8年でした。年齢属性を持つ公開マンモグラフィーデータセットが不足しているため、Webクローラーを使用して公開データセットからサムネイルマンモグラフィー画像とその年齢フィールドをダウンロードします。マンモグラフィ検診用のデジタルデータベース。 
[概要]この研究の目的は、マンモグラム画像から年齢を再分類するためのaiベースのモデルを考案することです。マンモグラム画像は、壊れたソフトウェアによってのみ取得できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: High Accuracy VLP based on Image Sensor using Error Calibration Method -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.IV/paper_15.html">
      <font color="black">High Accuracy VLP based on Image Sensor using Error Calibration Method</font>
    </a>
  </h2>
  <font color="black">分散円については、位置決め計算時の異なる座標間の会話によって生じるシフト誤差を相殺することができます。実験結果によると、提案手法の平均位置決め誤差は0.82cmに低減でき、 -VLP分野の最先端技術。回転アルゴリズムは、画像の中心を回転の中心として扱うのではなく、画像の回転の中心を推定し、位置決めエラーを低減します。 【概要】回転アルゴリズムは、画像中心を回転中心として扱うのではなく、画像内の回転中心を推定し、位置決め誤差を低減する。実験結果によれば、提案手法の平均位置決め誤差を0に低減することができる。 82cm</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Bag of Tricks for Adversarial Training -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_0.html">
      <font color="black">Bag of Tricks for Adversarial Training</font>
    </a>
  </h2>
  <font color="black">この作業では、敵対的にトレーニングされたモデルの基本的なトレーニングトリックとハイパーパラメータ設定の効果に関する包括的な評価を提供します。たとえば、重み減衰の値がわずかに異なると、モデルのロバスト精度が7％以上低下する可能性があります。提案された方法によって誘発される潜在的な促進を無効にします。敵対的訓練（AT）は、モデルの堅牢性を促進するための最も効果的な戦略の1つです。 
[概要]新しいベンチマークでは、atで提案された改善のほとんどは、トレーニング手順を早期に停止するよりも効果が低いことが示されています。驚くべきことに、基本的なトレーニング設定がモデルに影響を与える可能性があることがわかりました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-label Classification of Common Bengali Handwritten Graphemes:
  Dataset and Challenge -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_1.html">
      <font color="black">Multi-label Classification of Common Bengali Handwritten Graphemes:
  Dataset and Challenge</font>
    </a>
  </h2>
  <font color="black">ラテン語は、歴史的に最先端の手書き光学式文字認識（OCR）研究をリードしてきました。データセットは、Bengali.AIのKaggleでの手書き書記素分類チャレンジの一部としてオープンソース化されており、マルチラベル書記素の分類..文字に対応するグラフィック構成要素のセグメンテーションは、筆跡システムとアルファ音節ファミリーの言語でのダイアクリティックスの頻繁な使用により、非常に困難になります。 
[ABSTRACT]既存のシステムをアルファ音節言語に適応させることは、それらの正書法のコントラストがはっきりしているため困難です。アルファ音節文字内でセグメンテーションを行う書記素に基づくラベル付けスキームを提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Speech2Video Synthesis with 3D Skeleton Regularization and Expressive
  Body Poses -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_2.html">
      <font color="black">Speech2Video Synthesis with 3D Skeleton Regularization and Expressive
  Body Poses</font>
    </a>
  </h2>
  <font color="black">スケルトンの動きをリアルで表現力豊かにするために、学習パイプラインとテストパイプラインの両方で、関節のある3D人間の骨格の知識と、個人の音声の象徴的なジェスチャーの学習辞書を生成プロセスに組み込みます。写真のようにリアルで高解像度のビデオを作成するモーションの詳細を使用して、条件付きGANにパーツアテンションメカニズムを挿入することを提案します。この論文では、与えられたスピーチオーディオを特定の人物の写実的なスピーキングビデオに変換する新しいアプローチを提案します。出力ビデオは、同期され、リアルで、表現力豊かな豊かなボディダイナミクスを備えています。 
[ABSTRACT]スケルトンの動きは、リカレントニューラルネットワーク（rnn）を使用して生成されます。次に、条件付きの関節式3Dネットワーク（gan）を介して出力ビデオを合成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-17">
        <br><font color="black">2020-07-17</font>
      </time>
    </span>
</section>
<!-- paper0: Can You Trust Your Pose? Confidence Estimation in Visual Localization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_3.html">
      <font color="black">Can You Trust Your Pose? Confidence Estimation in Visual Localization</font>
    </a>
  </h2>
  <font color="black">このタスクを実行するための新しい信頼度測定を開発し、屋内または屋外のさまざまなデータセット、およびさまざまな視覚的ローカリゼーションパイプラインに柔軟に適用できることを示します。また、提案された手法を使用して、2番目の目標である改善を達成できることも示します。既存のポーズ推定パイプラインの精度..大規模環境でのカメラポーズ推定は未解決の問題であり、最近の有望な結果にもかかわらず、状況によっては失敗する可能性があります。最後に、提案されたアプローチは計算が軽量であり、ポーズ推定の計算量の増加はごくわずかです。 
[概要]提案されたアプローチは、位置の位置を測定することを目的としています。提案されたモデルは、モデルを作成するために使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: ISAAQ -- Mastering Textbook Questions with Pre-trained Transformers and
  Bottom-Up and Top-Down Attention -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_4.html">
      <font color="black">ISAAQ -- Mastering Textbook Questions with Pre-trained Transformers and
  Bottom-Up and Top-Down Attention</font>
    </a>
  </h2>
  <font color="black">私たちのシステムISAAQは、すべてのTQA質問タイプで前例のない成功を報告し、真/偽、テキストのみ、および図の多肢選択問題で81.36％、71.11％、55.12％の精度を示します。言語ビジュアルトランスフォーマーを最初からトレーニングするのではなく、事前に訓練された変圧器、微調整、およびアンサンブルについて..教科書の質問回答は、テキストと図からのマルチモーダル情報による推論を必要とする、機械理解と視覚的な質問回答の交差点での複雑なタスクです。 
[概要]初めて、トランスフォーマー言語モデルの可能性に焦点を当てます。関心領域を特定するために、ボトムアップとトップダウンの注意を追加します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Anomaly Detection With Partitioning Overfitting Autoencoder Ensembles -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_5.html">
      <font color="black">Anomaly Detection With Partitioning Overfitting Autoencoder Ensembles</font>
    </a>
  </h2>
  <font color="black">正則化が強すぎると、データが不十分になり、多くの誤検出が発生します。ただし、主な問題の1つは、適切な量の正則化を見つけることです。この論文では、POTATOES（Partitioning OverfiTting AuTOencoder EnSemble）を提案します。監視されていない異常検出のための新しいタイプのオートエンコーダアンサンブル。 
[ABSTRACT]オートエンコーダーは、特にデータが周囲空間よりも小さい次元の部分多様体の近くにある場合、オートエンコーダーの一般的な方法です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-06">
        <br><font color="black">2020-09-06</font>
      </time>
    </span>
</section>
<!-- paper0: Improving spatial domain based image formation through compressed
  sensing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_6.html">
      <font color="black">Improving spatial domain based image formation through compressed
  sensing</font>
    </a>
  </h2>
  <font color="black">さらに、マルチレベルサンプリングは、平均してシングルレベルの均一ランダムサンプリングよりも優れていると結論付けます。画質の比較では、「枯れ葉」データセット、ベイジアン推定、およびピーク信号対雑音比（PSNR）測定を使用します。この論文では、検出器の最適な視野を選択することにより、単一ピクセルスキャンシステムでの画像再構成を改善します。 
[概要] psnr分布は、補間された凝視アレイよりも画質を向上させるレベルの小さなセットがあることを示しています。レベルサンプリングは、ピーク信号対雑音比の分布を向上させます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: DeepSEE: Deep Disentangled Semantic Explorative Extreme Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_7.html">
      <font color="black">DeepSEE: Deep Disentangled Semantic Explorative Extreme Super-Resolution</font>
    </a>
  </h2>
  <font color="black">超解像（SR）は、定義上、不適切な設定です。コードとモデルは、https：//mcbuehler.github.io/DeepSEE/で入手できます。この作品では、深く解きほぐされたセマンティック探索的超解像超解像のための探索的顔の超解像フレームワーク、DeepSEEを提案します。 
[ABSTRACT]深い解きほぐされたセマンティック探索的超解像。それはセマンティック領域、それらの解きほぐされた外観の知識を提供します。これにより、幅広い画像操作が可能になります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-09">
        <br><font color="black">2020-04-09</font>
      </time>
    </span>
</section>
<!-- paper0: RefVOS: A Closer Look at Referring Expressions for Video Object
  Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_8.html">
      <font color="black">RefVOS: A Closer Look at Referring Expressions for Video Object
  Segmentation</font>
    </a>
  </h2>
  <font color="black">私たちの研究は、タスクの主な課題が動きと静的アクションの理解に関連していることを示しています。参照式（言語ガイドVOS）を使用したビデオオブジェクトセグメンテーションのタスクは、言語フレーズとビデオが与えられた場合、次のバイナリマスクを生成することです。フレーズが参照するオブジェクト..このデータを利用して、言語ガイド付き画像セグメンテーションのタスクで競争力のある結果を取得する新しいニューラルネットワークであるRefVOSの結果と、言語ガイド付きVOSの最新の結果を分析します。 
[概要]非営利プロジェクトは、refvosと呼ばれる非営利プロジェクトの作業に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Patch-based Brain Age Estimation from MR Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_9.html">
      <font color="black">Patch-based Brain Age Estimation from MR Images</font>
    </a>
  </h2>
  <font color="black">このようにして、脳年齢を推定するために最も重要な役割を果たす領域の視覚化を得ることができ、より解剖学的に駆動され、解釈可能な結果につながり、したがって、脳室と海馬が最も有益な..アルツハイマー病の一部として..脳全体のボリュームを使用するほとんどの研究とは対照的に、この研究では、脳の3Dパッチと畳み込み神経ネットワーク（CNN）を使用する新しい深層学習アプローチを開発します。ローカライズされた脳年齢推定器を開発する。 
[概要]脳年齢の上昇として現れる神経変性の早期発見は、影響を受けた個人のより良い医療と計画を促進する可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-29">
        <br><font color="black">2020-08-29</font>
      </time>
    </span>
</section>
<!-- paper0: EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational
  Reasoning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_10.html">
      <font color="black">EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational
  Reasoning</font>
    </a>
  </h2>
  <font color="black">また、トレーニング効率を向上させ、収束を加速するだけでなく、モデルのパフォーマンスを向上させる2段階のトレーニングパイプラインを紹介します。提案されたフレームワークは、合成物理シミュレーションとさまざまな領域の複数の実世界ベンチマークデータセットの両方で評価されます。論文では、複数の異種の対話型エージェント間の潜在的な相互作用グラフを介した明示的な関係構造の認識と予測を備えた一般的な軌道予測フレームワーク（EvolveGraphという名前）を提案します。 
[概要]状況の効果的な理解とインタラクティブエージェントの正確な軌道予測は、ポジショニングと計画において重要な役割を果たします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-31">
        <br><font color="black">2020-03-31</font>
      </time>
    </span>
</section>
<!-- paper0: Understanding the Role of Adversarial Regularization in Supervised
  Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_11.html">
      <font color="black">Understanding the Role of Adversarial Regularization in Supervised
  Learning</font>
    </a>
  </h2>
  <font color="black">唯一の監督を上回る敵対的正則化の経験的証拠を提供しようとする多くの試みにもかかわらず、そのような現象の理論的理解はとらえどころのないままです。さらに、最近導入されたユニットごとの容量ベースの汎化限界に動機付けられて、敵対的フレームワークの汎化誤差を分析します。 。この洞察を実現するために、私たちは、消失する勾配の問題、漸近的な反復の複雑さ、勾配の流れ、および唯一の監督と敵対的な正則化のコンテキストでの証明可能な収束を研究します。 
[要約]調査によると、敵対的な正則化は、基本的なレベルでの単独の監督よりも優れたパフォーマンスを示しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Answer-Driven Visual State Estimator for Goal-Oriented Visual Dialogue -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_12.html">
      <font color="black">Answer-Driven Visual State Estimator for Goal-Oriented Visual Dialogue</font>
    </a>
  </h2>
  <font color="black">ただし、既存の方法では、はるかに長い質問の後に常に無差別に回答がエンコードされるため、回答の利用率が低くなります。特に、回答が異なれば、視覚的信念や将来の質問も異なります。提案されたADVSEを、質問ジェネレータと推測タスクの両方に対して評価します。大規模なGuessWhat ?! 
[ABSTRACT]オラクルの回答-駆動型視覚状態推定器（advse）は、視覚状態にさまざまな回答の影響を与えるように設計されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Neural encoding with visual attention -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_13.html">
      <font color="black">Neural encoding with visual attention</font>
    </a>
  </h2>
  <font color="black">新しいアプローチを使用して、視線追跡に依存せずに、fMRI応答データのみでエンドツーエンドの学習を行うことで、視覚的注意ポリシーを学習できることを示します。これらの調査結果は、注意モジュールが役立つ可能性があることを示しています。視覚刺激の神経エンコーディングモデルで..興味深いことに、独立したデータのモデルによって推定された注意位置は、そうするための明示的な監督がないにもかかわらず、対応する視線固定パターンとよく一致することがわかります。 
[概要]これはリソースが限られているためです。注意に応じて神経モデルにバイアスがかかることはよく知られていますが、その代わりに、明示的な監視がないにもかかわらず、注意の場所が対応する固視パターンとよく一致していることがわかります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: MulayCap: Multi-layer Human Performance Capture Using A Monocular Video
  Camera -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_14.html">
      <font color="black">MulayCap: Multi-layer Human Performance Capture Using A Monocular Video
  Camera</font>
    </a>
  </h2>
  <font color="black">実験結果は、MulayCapが、以前の単眼ビデオカメラシステムでは達成されなかった動的に変化する詳細のリアルなレンダリングを生成することを示しています。この方法では、ジオメトリの再構築とテクスチャのレンダリングにそれぞれ「マルチレイヤー」表現を使用します。服を着た人間を複数のジオメトリレイヤー、つまりボディメッシュレイヤーとガーメントピースレイヤーに分解します。 
[概要]背後にある重要なテクニックは、衣服の形状を最適化し、入力ビデオシーケンスに合うように動的な布を再構築するための衣服からのビデオ（gfv）メソッドです。mulaycapは、布の編集など、さまざまな重要な編集アプリケーションに適用できます。 re-ターゲティング、リライト、およびarアプリケーション</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-13">
        <br><font color="black">2020-04-13</font>
      </time>
    </span>
</section>
<!-- paper0: MLRSNet: A Multi-label High Spatial Resolution Remote Sensing Dataset
  for Semantic Scene Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_15.html">
      <font color="black">MLRSNet: A Multi-label High Spatial Resolution Remote Sensing Dataset
  for Semantic Scene Understanding</font>
    </a>
  </h2>
  <font color="black">リモートセンシングの分野でシーン画像をよりよく理解するには、シーン画像のマルチラベル注釈が必要です。この問題に対処するために、この論文では、セマンティックシーン理解のためのMLRSNetという名前のマルチラベル高空間解像度リモートセンシングデータセットを構築します。オーバーヘッドの観点からディープラーニングを行います。高解像度の光学衛星または航空画像で構成されています。 
[概要]これは、マルチラベルの高空間解像度リモートセンシングデータセットが不足しているためです。これらは、シーン分類や画像検索などのマルチラベルベースのタスクのモデルをトレーニングするように設計されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning based Dimple Segmentation for Quantitative Fractography -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_16.html">
      <font color="black">Deep Learning based Dimple Segmentation for Quantitative Fractography</font>
    </a>
  </h2>
  <font color="black">破壊の原因を特定することは、材料特性、機械的特性の予測、および新しい耐破壊性材料の開発に役立ちます。この作業では、特に機械学習手法を使用して、チタン合金のディンプル検出とセグメンテーションの困難な問題に対処しようとします。ニューラルネットワーク..提案された新しいモデルは、他の以前のアプローチと比較して最高のパフォーマンスを実現します。 
[ABSTRACT]金属の破壊の原因を特定するために、フラクトグラフのディンプルのセグメンテーションの問題に対処します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-05">
        <br><font color="black">2020-07-05</font>
      </time>
    </span>
</section>
<!-- paper0: On Box-Cox Transformation for Image Normality and Pattern Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_17.html">
      <font color="black">On Box-Cox Transformation for Image Normality and Pattern Classification</font>
    </a>
  </h2>
  <font color="black">ここでは、2次元データ、すなわちデジタル画像を変換し、その効果を研究するための前処理ステップなどのツールの有用性を中心に展開します。さらに、時間の複雑さを軽減するには、パラメータラムダを推定するだけで十分です。確率密度関数を基礎となるデータ分布の統計的推論と見なすだけで、大きな2次元マトリックスをリアルタイムで..この軽量のBox-Cox変換の効果を確立された状態と比較します。 -アートローライト画像強調技術。 
[概要]ボックス-cox変換がヒストグラム変換を利用してデジタル画像に拡張されるのはこれが初めてです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-15">
        <br><font color="black">2020-04-15</font>
      </time>
    </span>
</section>
<!-- paper0: First Order Motion Model for Image Animation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_18.html">
      <font color="black">First Order Motion Model for Image Animation</font>
    </a>
  </h2>
  <font color="black">複雑なモーションをサポートするために、学習したキーポイントのセットとそのローカルアフィン変換で構成される表現を使用します。ジェネレータネットワークは、ターゲットモーション中に発生するオクルージョンをモデル化し、ソース画像から抽出された外観とドライビングビデオから派生したモーションを組み合わせます。 ..これを実現するために、自己監視式を使用して外観とモーション情報を分離します。 
[ABSTRACT]私たちのフレームワークは、アニメーション化する特定のオブジェクトに関する注釈や事前情報を使用せずに問題に対処します。私たちのフレームワークは、さまざまなベンチマークとさまざまなオブジェクトカテゴリで最高のスコアを獲得します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-29">
        <br><font color="black">2020-02-29</font>
      </time>
    </span>
</section>
<!-- paper0: From Handcrafted to Deep Features for Pedestrian Detection: A Survey -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_19.html">
      <font color="black">From Handcrafted to Deep Features for Pedestrian Detection: A Survey</font>
    </a>
  </h2>
  <font color="black">特徴強化、部分認識、後処理の方法が主な注目を集めているこれらの方法の統計分析と傾向を示します。深い特徴ベースのアプローチの場合、純粋なCNNベースの方法と採用する方法に分割します。手作りとCNNベースの両方の機能..歩行者の検出は、コンピュータビジョン、特に人間中心のタスクにおいて重要ですが、挑戦的な問題です。 
[概要]近年、複数の改善が行われています。これらには、手作りの機能ベースの方法と深い機能が含まれます。これらには、手作りの機能とcnnベースの機能が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: FocusLiteNN: High Efficiency Focus Quality Assessment for Digital
  Pathology -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_20.html">
      <font color="black">FocusLiteNN: High Efficiency Focus Quality Assessment for Digital
  Pathology</font>
    </a>
  </h2>
  <font color="black">9つの異なる染色色にわたる多様な組織スライドを含むFocusPathを使用してトレーニングデータセットを作成します。染色の多様性は、モデルが多様な色スペクトルと組織構造を学習するのに大いに役立ちます。この種の最大の新しい包括的な評価データセットを紹介します。モデルの評価と比較のためにTCGAリポジトリから注釈を付けてコンパイルします。提案された方法は、既存の知識駆動型およびデータ駆動型のFQAアプローチと比較して、優れた精度と速度のトレードオフを示します。ConvolutionalNeuralNetworkなどのデータ駆動型アプローチ（CNN）ベースの方法は大きな期待を示しており、計算が非常に複雑で転送性が低いため、実際に使用するのは困難です。 
[概要]新しいfqaモデルには、知識主導のアプローチとデータ主導のアプローチが含まれます。これらには、gpusなどの過度のハードウェア要件なしに、知識主導の方法と同様の高速計算を維持する新しいモデルが含まれます。ただし、モデルを最小限のレベルでも、非常に競争力のあるパフォーマンスを実現します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-11">
        <br><font color="black">2020-07-11</font>
      </time>
    </span>
</section>
<!-- paper0: Attention based Writer Independent Handwriting Verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_21.html">
      <font color="black">Attention based Writer Independent Handwriting Verification</font>
    </a>
  </h2>
  <font color="black">私たちが提案するアプローチは、CEDAR筆記体「AND」データセットでライター内のケースを検出するための86 \％の精度を達成します。注意メカニズムにより、ネットワークは入力の関連領域により集中できるため、分類パフォーマンスが向上します。ライター検証のタスクは、クエリされた既知の手書き画像サンプルが同じライターに属しているかどうかの可能性スコアを提供することです。 
[概要]クロスアテンションネットワークとソフトアテンションネットワークを実装および統合して、2D入力の特徴空間内の相関性の高い顕著なポイントをキャプチャします。アテンションメカニズムにより、ネットワークは入力の関連領域により集中できるようになり、分類性能</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-07">
        <br><font color="black">2020-09-07</font>
      </time>
    </span>
</section>
<!-- paper0: Referring Image Segmentation via Cross-Modal Progressive Comprehension -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_22.html">
      <font color="black">Referring Image Segmentation via Cross-Modal Progressive Comprehension</font>
    </a>
  </h2>
  <font color="black">このようにして、マルチレベルの機能は相互に通信し、テキストのコンテキストに基づいて洗練されます。具体的には、CMPCモジュールは最初にエンティティと属性の単語を使用して、式によって考慮される可能性のあるすべての関連エンティティを認識します。 CMPCモジュールに加えて、シンプルでありながら効果的なTGFEモジュールをさらに活用して、さまざまなレベルの推論されたマルチモーダル機能をテキスト情報のガイダンスと統合します。 
[概要] cmpcモジュールは、最初にエンティティと関連する単語を使用して、式によって考慮される可能性のあるすべての関連エンティティを認識します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Meta-Consolidation for Continual Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_23.html">
      <font color="black">Meta-Consolidation for Continual Learning</font>
    </a>
  </h2>
  <font color="black">すでに習得した知識の把握を失うことなく、継続的に学習し、新しいタスクに適応する能力は、現在の深層学習システムでは不十分な生物学的学習システムの特徴です。MNIST、CIFAR-10、の継続的な学習ベンチマークを使用した実験CIFAR-100およびMini-ImageNetデータセットは、MERLINの約束を裏付ける最近の最先端技術を含む、5つのベースラインにわたって一貫した改善を示しています。私たちは、データポイントが見られる挑戦的なオンライン継続学習環境で運営しています。モデルは一度だけ。 
[概要] merlin：メタ-継続的な学習のための統合は新しい方法です。学習に2,000ドル（2,000ドル）を提供します。システムを改善に使用したのはこれが初めてです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Few-Shot Classification By Few-Iteration Meta-Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_24.html">
      <font color="black">Few-Shot Classification By Few-Iteration Meta-Learning</font>
    </a>
  </h2>
  <font color="black">さらに、私たちの戦略は、メタトレーニング中に基本学習者の目的の重要な側面を学習できるようにします。効率的な初期化モジュールと最急降下法に基づく最適化アルゴリズムを採用することにより、基本学習者はわずか数回の反復で強力な分類器を予測します。いくつかのラベル付けされた例からの低データ体制では、重要ですが、挑戦的な問題です。 
[概要]システムは基本学習者によって設計されました。埋め込みネットワークで構成され、画像の一般的な表現を提供します。これにより、基本学習者の目的の重要な側面をメタトレーニング中に学習できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: An Ultra Lightweight CNN for Low Resource Circuit Component Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_25.html">
      <font color="black">An Ultra Lightweight CNN for Low Resource Circuit Component Recognition</font>
    </a>
  </h2>
  <font color="black">その結果に基づいて、元の画像を小さな断片にトリミングします。これは工学的に重要であり、低リソース設定での回路コンポーネントの認識に適しています。次に、断片は畳み込みニューラルネットワーク（CNN）に送られ、次のように分類されます。各回路コンポーネントを識別します。 
[ABSTRACT]選択的検索を適用して、各回路コンポーネントの位置を見つけました。次に、ピースは分類のために畳み込みニューラルネットワークに送られます。システムの精度は93.4％に達します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Linguistic Structure Guided Context Modeling for Referring Image
  Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_26.html">
      <font color="black">Linguistic Structure Guided Context Modeling for Referring Image
  Segmentation</font>
    </a>
  </h2>
  <font color="black">文のマルチモーダルコンテキストは、指示対象を背景から区別するために重要です。4つのベンチマークでの広範な実験は、私たちの方法が以前のすべての最先端技術よりも優れていることを示しています。参照画像セグメンテーションは、オブジェクトの前景マスクを予測することを目的としています。自然言語の文で参照されます。 
[ABSTRACT] `マルチモーダルコンテキスト &#39;は、参照フォームを背景から区別するために重要です。モデルを使用して、依存関係解析ツリーの抑制された単語グラフを作成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Group-wise Variational Diffeomorphic Image Registration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_27.html">
      <font color="black">Deep Group-wise Variational Diffeomorphic Image Registration</font>
    </a>
  </h2>
  <font color="black">複数の時点で取得した乳房MRIおよび胸部4DCT検査の患者内登録を使用してアプローチをトレーニングおよび評価しました。これを実現するために、ペアワイズ変分および異形VoxelMorphアプローチに基づいて、両方を可能にする一般的な数学的フレームワークを提示します。複数の画像をそれらの地理的平均に登録し、利用可能な画像のいずれかを固定画像として使用できる登録.. ElastixおよびVoxelMorphとの比較は、画像の類似性と参照ランドマーク距離の点で、提案された方法の競争力のある定量的パフォーマンスを示しています。より速い登録。 
[概要]複数の画像の同時登録を可能にするために現在の学習を拡張することを提案します。これらには、正規化された相互情報量に基づく尤度と、粘性流体エネルギーの明示的な制御を可能にする事前情報が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Quantum Annealing Approaches to the Phase-Unwrapping Problem in
  Synthetic-Aperture Radar Imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_28.html">
      <font color="black">Quantum Annealing Approaches to the Phase-Unwrapping Problem in
  Synthetic-Aperture Radar Imaging</font>
    </a>
  </h2>
  <font color="black">さまざまなソフトウェアベースのQUBOソルバーと、合成および実数の両方のさまざまな画像でアプローチをテストします。ソフトウェアベースのソルバーは、最先端の位相アンラッピングソルバーに匹敵する高品質のソリューションを取得します。 。これらの個々のソリューションは、サブイメージごとに1つの定数を使用して、整数定数まで最適に近くなります。 
[ABSTRACT]量子アニーラーは、量子ビットの数に制限があります。問題を個別に解決できる一連のサブ問題に分解します。これらのサブ問題は、個別の作業には制限がありますが、個別に取り組むことができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Open-Set Hypothesis Transfer with Semantic Consistency -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_29.html">
      <font color="black">Open-Set Hypothesis Transfer with Semantic Consistency</font>
    </a>
  </h2>
  <font color="black">その結果、ラベルなしデータは、ソースクラスまたは未知のクラスのいずれかと一致する識別クラスに分類できます。教師なし開集合ドメイン適応（UODA）は、ラベルなしターゲットデータに未知のクラスが含まれる現実的な問題です。具体的には、モデルが最初に発見します。自信を持って予測し、疑似ラベルを使用して分類を実行します。 
[概要]以前の方法では、ソースデータとターゲットデータの両方の共存に依存してドメインの調整を実行します。これにより、プライバシーの懸念からソースドメインデータが制限されている場合、アプリケーションが制限されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Action Units Recognition with Pairwise Deep Architecture -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_30.html">
      <font color="black">Action Units Recognition with Pairwise Deep Architecture</font>
    </a>
  </h2>
  <font color="black">ベースラインスコアは0.31ですが、私たちの方法は競争の検証データセットで0.65を達成しました。この論文では、競争で使用される新しい自動アクションユニット（AU）認識方法であるAffective Behavior Analysis in-the-wild（ABAW）を提案します。 ..私たちの方法は、ペアワイズディープアーキテクチャを使用して、さまざまなビデオでのAUのラベル基準の変更の問題に取り組みます。 
[概要]私たちの方法は、ペアワイズディープアーキテクチャを使用して、さまざまな動画でのausラベル基準の変更の問題に取り組んでいます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: DASGIL: Domain Adaptation for Semantic and Geometric-aware Image-based
  Localization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_31.html">
      <font color="black">DASGIL: Domain Adaptation for Semantic and Geometric-aware Image-based
  Localization</font>
    </a>
  </h2>
  <font color="black">マルチスケールモデルは、仮想KITTI 2データセットでトレーニングされているにもかかわらず、実世界のKITTIデータセットで強力な一般化能力を示します。この論文では、幾何学的情報と意味情報をマルチスケールに融合する新しいマルチタスクアーキテクチャを提案します。視覚的な場所認識のための潜在的な埋め込み表現..人間の努力なしに高品質のグラウンドトゥルースを使用するために、深度およびセグメンテーションジェネレータモデルは仮想合成データセットでトレーニングされ、ドメイン適応は合成データセットから実世界データセットに採用されます。 
[概要]提案されたアプローチは、拡張されたcmu-seasonsデータセットで検証されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: EndoSLAM Dataset and An Unsupervised Monocular Visual Odometry and Depth
  Estimation Approach for Endoscopic Videos: Endo-SfMLearner -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_32.html">
      <font color="black">EndoSLAM Dataset and An Unsupervised Monocular Visual Odometry and Depth
  Estimation Approach for Endoscopic Videos: Endo-SfMLearner</font>
    </a>
  </h2>
  <font color="black">ただし、現在利用可能なデータセットは、効果的な定量的ベンチマークをサポートしていません。この論文では、6つのブタ器官の3D点群データ、カプセルおよび標準内視鏡記録、および合成的に生成されたデータで構成される包括的な内視鏡SLAMデータセットを紹介します。データセットへのリンクはhttps://github.com/CapsuleEndoscope/EndoSLAMで公開されています。 
[ABSTRACT]データセットは効果的な定量的ベンチマークをサポートしていません。これらのデータセットには効果的な定量的ベンチマークが含まれていません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-30">
        <br><font color="black">2020-06-30</font>
      </time>
    </span>
</section>
<!-- paper0: Domain aware medical image classifier interpretation by counterfactual
  impact analysis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_33.html">
      <font color="black">Domain aware medical image classifier interpretation by counterfactual
  impact analysis</font>
    </a>
  </h2>
  <font color="black">コンピュータービジョンタスクの機械学習手法の成功により、医学と生物学のコンピューター支援予測が急増しました。公開マンモグラフィーデータを評価し、既存の最先端の手法と比較します。ソリューションは、両方を定量的に示しています。また、定性的には、時間効率を犠牲にすることなく、ローカリゼーションのあいまいさを大幅に削減し、結果をより明確に伝えることができます。 
[概要]これらの予測子は、前例のない精度を提供しますが、それでもシンプルです。画像と病理学的分類の間のデータ駆動型の関係に基づくシステムが含まれています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: A Multi-modal Machine Learning Approach and Toolkit to Automate
  Recognition of Early Stages of Dementia among British Sign Language Users -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_34.html">
      <font color="black">A Multi-modal Machine Learning Approach and Toolkit to Automate
  Recognition of Early Stages of Dementia among British Sign Language Users</font>
    </a>
  </h2>
  <font color="black">認知症の治療法はありませんが、タイムリーな診断は必要なサポートと適切な投薬を得るのに役立ちます。一方、画像とビデオの分析と理解のための深層学習ベースのアプローチは、特に畳み込みニューラルネットワーク（CNN）の採用が有望です。 、大量のトレーニングデータが必要です。研究者は、医師が認知障害の早期発見に着手するのに役立つ効果的な技術ツールの開発に緊急に取り組んでいます。 
[概要]英国手話（bsl）の高齢の聴覚障害者の認知症のスクリーニングは、診断プロセスが通訳者の質や利用可能性、適切な質問票や認知テストなどの条件に縛られているため、追加の課題をもたらします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: DeepFakesON-Phys: DeepFakes Detection based on Heart Rate Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_35.html">
      <font color="black">DeepFakesON-Phys: DeepFakes Detection based on Heart Rate Estimation</font>
    </a>
  </h2>
  <font color="black">DeepFakesON-Physという名前の提案された偽の検出器は、畳み込み注意ネットワーク（CAN）を使用します。これは、ビデオフレームから空間的および時間的情報を抽出し、両方のソースを分析および組み合わせて、偽のビデオをより適切に検出します。この作業では、rPPGがどの程度役立つかを調査します。 DeepFakeビデオの検出用..rPPGメソッドは、ビデオシーケンスを分析して、人間の皮膚の微妙な色の変化を探し、組織の下に人間の血液が存在することを明らかにします。 
[ABSTRACT]ディープフェイク動画は心拍数の変化を検出するために使用されます。最新の公開データベースと呼ばれ、これらの動画は検出できます。この方法は実験的に評価されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Cost-Sensitive Regularization for Diabetic Retinopathy Grading from Eye
  Fundus Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_36.html">
      <font color="black">Cost-Sensitive Regularization for Diabetic Retinopathy Grading from Eye
  Fundus Images</font>
    </a>
  </h2>
  <font color="black">結果を再現するコードはhttps://github.com/agaldran/cost_sensitive_loss_classificationでリリースされています。さらに、DRグレーディングに関連する各サブ問題のラベルノイズのモデリングにメソッドを適応させる方法を示します。アトミックサブタスクモデリングと呼びます。このような構造は、さまざまな疾患グレード間の単調な関係を反映しています。 
[ABSTRACT]構造は、異なる疾患グレード間の単一線形関係を反映します。正則化として機能する追加の項を使用して、標準分類損失を拡張します。モデルモデルは、drグレードの注釈に存在するノイズを考慮に入れることができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Why Adversarial Interaction Creates Non-Homogeneous Patterns: A
  Pseudo-Reaction-Diffusion Model for Turing Instability -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_37.html">
      <font color="black">Why Adversarial Interaction Creates Non-Homogeneous Patterns: A
  Pseudo-Reaction-Diffusion Model for Turing Instability</font>
    </a>
  </h2>
  <font color="black">理論的および経験的研究により、これらの現象の根底にあるメカニズムを説明するための疑似反応拡散モデルを提示します。チューリングの独創的な反応拡散（RD）モデルのずっと後、彼の基本方程式の優雅さは周囲の懐疑論の多くを軽減しましたパターン形成..監視された学習は均一な平衡を達成しますが、この論文は、敵対者の導入がこの均一性を破り、平衡状態で不均一なパターンを作成するのに役立つことを示唆しています。 
[ABSTRACT]チューリングモデルは単純化と理想化ですが、パターンを説明するための最もよく知られている理論モデルの1つです。この研究では、このようなパターンを作成するためのチューリング不安定性の関与を確立します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Ray-based classification framework for high-dimensional data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_38.html">
      <font color="black">Ray-based classification framework for high-dimensional data</font>
    </a>
  </h2>
  <font color="black">二重および三重量子ドットデバイスの合成データセットを使用してこのフレームワークを経験的に研究し、デバイスの状態を識別する分類問題に適用します。光線ベースの分類器のパフォーマンスは、従来の2D画像とすでに同等であることを示します。低次元システムでありながら、データ収集コストを大幅に削減します。高密度の多次元データを使用するのではなく、\ emph {と呼ばれる1次元表現の最小限のコレクションを利用するディープニューラルネットワーク（DNN）分類フレームワークを提案します。光線}、大幅に削減された情報に基づいて構造の「指紋」を構築します。 
[ABSTRACT]ディープニューラルネットワーク（dnn）分類フレームワークは、1つの知覚表現の最小限のコレクションを使用して、構造（s）の「フィンガープリント」を作成します。これは、データ収集コストを削減しながら、大幅に削減された情報に基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Utilizing Transfer Learning and a Customized Loss Function for Optic
  Disc Segmentation from Retinal Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_39.html">
      <font color="black">Utilizing Transfer Learning and a Customized Loss Function for Optic
  Disc Segmentation from Retinal Images</font>
    </a>
  </h2>
  <font color="black">包括的な実験から得られた結果は、さまざまなソースから取得した網膜画像のディスクセグメンテーションに対するアプローチの堅牢性を示しています。私たちのアプローチは、2人の検眼医によって注釈が付けられた民間クリニックからのデータセットによって補強された7つの公開データセットでテストされました。この目的のために構築されたWebポータル..網膜画像からのディスクセグメンテーションで、0.03秒で99.78 \％の精度と94.73 \％のダイス係数を達成しました。 
[概要]新しい研究では、高精度の網膜眼底画像を指定して視神経乳頭をセグメント化するディープラーニングベースのアプローチを提案しています。これは、ディスクをセグメント化するためのシンプルでシンプルなアプローチを使用しています。この方法を使用して、新しいモデルを作成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_40.html">
      <font color="black">X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation</font>
    </a>
  </h2>
  <font color="black">X-Field表現は、数分以内に1つのシーンに対してトレーニングされ、トレーニング可能なパラメーターのコンパクトなセットにつながるため、ビュー、時間、および照明のリアルタイムナビゲーションが実現します。NNは、そのレンダリングへの入力を暗黙のマップとして表します。任意のビュー、時間、または光の座標、および任意のピクセルについて、ビュー、時間、または光の座標が変化した場合にどのように移動するかを定量化できます（ビュー、時間、照明などに関するピクセル位置のジャコビアン）。この実行可能なものは、ハードコードされた識別可能な形式でグラフィックスの「基本的なトリック」（照明、3D投影、オクルージョン）をすでに知っているNNです。 
[ABSTRACT]新しい座標でのこのnnは、ジョイントビュー、時間、または光の補間になります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Physical Exercise Recommendation and Success Prediction Using
  Interconnected Recurrent Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_41.html">
      <font color="black">Physical Exercise Recommendation and Success Prediction Using
  Interconnected Recurrent Neural Networks</font>
    </a>
  </h2>
  <font color="black">次に、システムは、個人による予測アクティビティの正常な完了の確率を予測します。この論文では、個人の成功率も予測する運動推奨システムを提案します。この相互接続されたRNNモデルの予測精度は、以前に公開されたもので評価されます。 4週間のモバイルヘルス実験からのデータであり、計算認知モデルからの以前の予測を改善することが示されています。 
[ABSTRACT]システムは、相互接続された2つのリカレントニューラルネットワーク（rnns）で構成されています。ワークアウトの履歴を使用して、各個人の次のワークアウトアクティビティを推奨します。予測精度は、モバイルヘルス実験から以前に公開されたデータで評価されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: How Neural Networks Extrapolate: From Feedforward to Graph Neural
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_42.html">
      <font color="black">How Neural Networks Extrapolate: From Feedforward to Graph Neural
  Networks</font>
    </a>
  </h2>
  <font color="black">第二に、GNNの成功と制限の分析に関連して、これらの結果は、理論的および経験的証拠を提供する仮説を示唆しています。アルゴリズムタスクを新しいデータ（たとえば、より大きなグラフやエッジの重み）に外挿する際のGNNの成功は、エンコードタスクに依存します。 -アーキテクチャまたは機能の特定の非線形性..理論的な説明に向けて、MLPおよびGNNが適切に外挿する条件を特定します。最初に、ReLUMLPが原点から任意の方向に沿って線形関数にすばやく収束するという観察結果を定量化します。 、これは、ReLUMLPがほとんどの非線形関数を外挿しないことを意味します。 mlpモジュールを備えた構造化ネットワークである
[ABSTRACT] gnnsは、より複雑なタスクである程度の成功を収めています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-24">
        <br><font color="black">2020-09-24</font>
      </time>
    </span>
</section>
<!-- paper0: Deformable Kernel Convolutional Network for Video Extreme
  Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_43.html">
      <font color="black">Deformable Kernel Convolutional Network for Video Extreme
  Super-Resolution</font>
    </a>
  </h2>
  <font color="black">新しく設計されたDeformableKernel Convolution Alignment（DKC_Align）およびDeformable Kernel Spatial Attention（DKSA）モジュールのおかげで、DKSANは空間的および時間的冗長性の両方をより有効に活用して、さまざまなレイヤー間での情報伝播を促進できます。AIM2020VideoExtremeSuperでDKSANをテストしました。スケールファクターが16までのビデオを超解像するための解像度の課題。ほとんどの既存のアプローチは、変形可能な畳み込みを使用して隣接するフレームを時間的に整列させ、従来の空間的注意メカニズム（畳み込みベース）を適用して再構築された特徴を強化することを選択します。 
[概要]ほとんどの既存のアプローチは、変形可能なビデオソフトウェアを使用することを選択します。変形可能なカーネル空間注意ネットワーク（dksan）は新しい方法です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Attention Based Real Image Restoration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_44.html">
      <font color="black">Attention Based Real Image Restoration</font>
    </a>
  </h2>
  <font color="black">残差構造の残差を使用して、低頻度の情報の流れを容易にし、機能の注意を適用してチャネルの依存関係を活用します。コード、トレーニング済みモデル、および結果は、https：//github.com/saeed-anwarで入手できます。 / R2Net .. 30を超える最先端のアルゴリズムに対する11の実際に劣化したデータセットでのノイズ除去、超解像度、雨滴除去、およびJPEG圧縮は、R $ ^ 2 $ Netの優位性を示しています。 
[ABSTRACT]復元アルゴリズムの実用性を向上させるために、この論文は、新しい単段ブラインド実画像復元ネットワークを提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-26">
        <br><font color="black">2020-04-26</font>
      </time>
    </span>
</section>
<!-- paper0: Dynamic Facial Asset and Rig Generation from a Single Scan -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_45.html">
      <font color="black">Dynamic Facial Asset and Rig Generation from a Single Scan</font>
    </a>
  </h2>
  <font color="black">また、アイデンティティと表現の間の同時分布をモデル化し、単一のニュートラル入力スキャンから動的な外観を持つパーソナライズされたブレンドシェイプのフルセットの推論を可能にします。4,000ドル以上のスキャンで細孔レベルの詳細で構成される顔データベースに基づいて構築されています。さまざまな表現とアイデンティティを使用して、自己監視型ニューラルネットワークを採用し、一連のテンプレート表現からパーソナライズされたブレンドシェイプを学習します。生成されたパーソナライズされたフェイスリグアセットは、顔のアニメーションとレンダリングのための最先端の業界パイプラインとシームレスに互換性があります。 
[概要]詳細な顔の顔のアセットは、顔のアニメーションとレンダリングのための最先端の業界パイプラインとシームレスに互換性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: A novel Region of Interest Extraction Layer for Instance Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_46.html">
      <font color="black">A novel Region of Interest Extraction Layer for Instance Segmentation</font>
    </a>
  </h2>
  <font color="black">したがって、さまざまな最先端のアーキテクチャでGRoIEを使用することによってもたらされる改善も評価されます。私たちの直感では、FPNのすべてのレイヤーが有用な情報を保持しています。提案されたレイヤーは1.1％の増加につながります。バウンディングボックス検出でのAPの改善、およびインスタンスセグメンテーションでの1.7％のAPの改善。 
[概要]提案されたレイヤーは、汎用ROIエクストラクターと呼ばれます-groie.itは、オブジェクト検出およびインスタンスコンポーネントコンポーネントタスクのために、2段階のアーキテクチャごとにシームレスに統合できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-28">
        <br><font color="black">2020-04-28</font>
      </time>
    </span>
</section>
<!-- paper0: Mini-DDSM: Mammography-based Automatic Age Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_47.html">
      <font color="black">Mini-DDSM: Mammography-based Automatic Age Estimation</font>
    </a>
  </h2>
  <font color="black">年齢属性を持つ公開マンモグラフィデータセットがないため、Webクローラーを使用して、公開データセットからサムネイルマンモグラフィ画像とその年齢フィールドをダウンロードします。マンモグラフィ検診用のデジタルデータベース..提案された作業の利点をさらに検証するために、別の独立したデータセットに対してロジスティックおよび線形回帰モデルを実行しました。その後、収集したデータセットから深層学習の特徴を抽出し、それによってモデルを構築しました。ランダムフォレストリグレッサを使用して、年齢を自動的に推定します。 
[概要]この研究の目的は、マンモグラム画像から年齢を再分類するためのaiベースのモデルを考案することです。マンモグラム画像は、壊れたソフトウェアによってのみ取得できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: CariMe: Unpaired Caricature Generation with Multiple Exaggerations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CV/paper_48.html">
      <font color="black">CariMe: Unpaired Caricature Generation with Multiple Exaggerations</font>
    </a>
  </h2>
  <font color="black">顔の誇張をより適切に表現し、きめの細かいワープを生成するために、変形フィールドベースのワープ方法も提案されています。これは、他のポイントベースのワープ方法よりも詳細な誇張をキャプチャするのに役立ちます。この論文では、似顔絵を一般化します。インスタンスレベルのワーピング予測から分布レベルの変形モデリングまでの生成問題。この仮定に基づいて、複数の誇張を伴う対になっていないCARIcature生成（CariMe）の最初の調査を示します。 
[ABSTRACT]似顔絵は、さまざまな空間変形が存在するため、より困難な作業です。これにより、多様な誇張生成に対する能力が制限されます。複数の誇張を伴う、対になっていない最初の探索を紹介します（carime）。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Multi-label Classification of Common Bengali Handwritten Graphemes:
  Dataset and Challenge -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_0.html">
      <font color="black">Multi-label Classification of Common Bengali Handwritten Graphemes:
  Dataset and Challenge</font>
    </a>
  </h2>
  <font color="black">データセットは、マルチラベル書記素分類のビジョンアルゴリズムをベンチマークするためのKaggleでのBengali.AI手書き書記素分類チャレンジの一部としてオープンソースです。文字に対応するグラフィック構成要素のセグメンテーションは、書記体系とアルファシラバリー言語ファミリーでのダイアクリティックスの頻繁な使用..アルファシラバリー単語内のセグメンテーションを線形にする書記素（単語形成の言語セグメント）に基づくラベル付けスキームを提案し、一般的にあるベンガル語手書きグラフの最初のデータセットを提示します日常のコンテキストで使用されます。 
[ABSTRACT]既存のシステムをアルファ音節言語に適応させることは、それらの正書法のコントラストがはっきりしているため困難です。アルファ音節文字内でセグメンテーションを行う書記素に基づくラベル付けスキームを提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: ISAAQ -- Mastering Textbook Questions with Pre-trained Transformers and
  Bottom-Up and Top-Down Attention -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_1.html">
      <font color="black">ISAAQ -- Mastering Textbook Questions with Pre-trained Transformers and
  Bottom-Up and Top-Down Attention</font>
    </a>
  </h2>
  <font color="black">私たちのシステムISAAQは、すべてのTQA質問タイプで前例のない成功を報告し、真/偽、テキストのみ、および図の多肢選択問題で81.36％、71.11％、55.12％の精度を示します。このペーパーでは、初めて、トランスフォーマー言語モデルと、言語と視覚的理解の課題に取り組むためのボトムアップとトップダウンの注意は、このタスクに伴います。言語ビジュアルトランスフォーマーを最初からトレーニングするのではなく、事前にトレーニングされたトランスフォーマー、微調整、およびアンサンブルに依存します。 
[概要]初めて、トランスフォーマー言語モデルの可能性に焦点を当てます。関心領域を特定するために、ボトムアップとトップダウンの注意を追加します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Understanding tables with intermediate pre-training -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_2.html">
      <font color="black">Understanding tables with intermediate pre-training</font>
    </a>
  </h2>
  <font color="black">データ拡張の利点に動機付けられて、微調整の前の中間ステップで学習される、自動的に作成された何百万ものトレーニング例のバランスの取れたデータセットを作成します。さまざまな方法により、TabFactに新しい最先端が設定されます。 （Chen et al。、2020）およびSQAデータセット..含意を認識するために、テーブルベースのBERTモデルであるTAPAS（Herzig et al。、2020）を適応させます。 adam scottによると、
[ABSTRACT]テーブルの含意はあまりよく研究されていません。彼は、数百万の例のバランスの取れたデータセットを作成すると述べています。トレーニングと予測の効率を向上させるには、テーブルの剪定技術が必要です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: "Did you really mean what you said?" : Sarcasm Detection in
  Hindi-English Code-Mixed Data using Bilingual Word Embeddings -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_3.html">
      <font color="black">"Did you really mean what you said?" : Sarcasm Detection in
  Hindi-English Code-Mixed Data using Bilingual Word Embeddings</font>
    </a>
  </h2>
  <font color="black">FastTextおよびWord2Vecアプローチから派生したバイリンガル単語埋め込みを使用して、ヒンディー語と英語のコード混合ツイートにおける皮肉検出の問題に対処するための深層学習ベースのアプローチを提案します。カスタム単語埋め込みをトレーニングするためのツイートのコーパスと、皮肉の検出..そのようなものの1つは、ソーシャルメディアテキストでの皮肉の検出です。 
[概要]ヒンディー語での皮肉検出の問題に対処するためのディープラーニングベースのアプローチを提案します-バイリンガルの単語埋め込みを使用した英語のコード混合ツイート</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Joint Persian Word Segmentation Correction and Zero-Width Non-Joiner
  Recognition Using BERT -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_4.html">
      <font color="black">Joint Persian Word Segmentation Correction and Zero-Width Non-Joiner
  Recognition Using BERT</font>
    </a>
  </h2>
  <font color="black">慎重に収集された500文のコーパスで、高レベルの難易度でマクロ平均F1スコア92.40％を達成します。単語はペルシア語の書記体系で適切にセグメント化されます。ただし、実際には、これらの書き込みルールは無視されることが多く、1つの単語がばらばらに書き込まれ、複数の単語が空白なしで書き込まれます。このペーパーでは、単語のセグメンテーションとゼロ幅非接合子（ZWNJ）認識の問題について説明します。ペルシャ語で、シーケンスラベリング問題として共同でアプローチします。 
[概要]この論文は、ペルシア語での単語のセグメンテーションとゼロ幅非接合子（zwnj）認識の問題に取り組んでいます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Adapting BERT for Word Sense Disambiguation with Gloss Selection
  Objective and Example Sentences -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_5.html">
      <font color="black">Adapting BERT for Word Sense Disambiguation with Gloss Selection
  Objective and Example Sentences</font>
    </a>
  </h2>
  <font color="black">また、WordNetの既存のサンプル文を使用したWSDのデータ拡張手法を紹介します。提案されたトレーニング目標とデータ拡張手法を使用して、モデルは英語の全単語ベンチマークデータセットで最先端の結果を達成できます。 .BERTなどの事前トレーニング済み言語モデルを使用したドメイン適応または転移学習は、多くの自然言語処理タスクに効果的なアプローチであることが証明されています。 
[概要]提案されたトレーニング目標とデータ拡張手法を使用して、私たちのモデルは英語のすべての単語のベンチマークデータセットで結果を達成することができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-24">
        <br><font color="black">2020-09-24</font>
      </time>
    </span>
</section>
<!-- paper0: A Survey on Explainability in Machine Reading Comprehension -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_6.html">
      <font color="black">A Survey on Explainability in Machine Reading Comprehension</font>
    </a>
  </h2>
  <font color="black">表現と推論の課題がどのように進化したか、およびこれらの課題に取り組むために取られた手順を示します。このペーパーでは、機械読解（MRC）における説明可能性のベンチマークとアプローチの系統的レビューを示します。評価するための評価方法も示します。説明可能なシステムのパフォーマンス。 
[概要]表現と可能性の課題がどのように進化したかを示します。永続的な未解決のリサーチクエスチョンを特定し、将来の作業の重要な方向性を強調します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Utterance-level Dialogue Understanding: An Empirical Study -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_7.html">
      <font color="black">Utterance-level Dialogue Understanding: An Empirical Study</font>
    </a>
  </h2>
  <font color="black">完全な発話レベルの理解には、多くの場合、近くの発話によって定義されるコンテキストの理解が必要です。このペーパーでは、対話のさまざまな側面、つまり感情、意図、および対話行為の識別について、状態を使用してコンテキストの役割を調査および定量化します。ベースラインとしての最先端のダイアログ理解方法。これにより、ダイアログのさまざまな側面の基本的なコンテキスト制御要因についての洞察が得られます。 
[要約]完全な発話-レベルの理解には、多くの場合、コンテキストの理解が必要です。これらのアプローチの多くは、効果的な理解のためのコンテキストを説明します。これらの洞察は、より効果的な対話理解モデルを刺激することができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-29">
        <br><font color="black">2020-09-29</font>
      </time>
    </span>
</section>
<!-- paper0: CoLAKE: Contextualized Language and Knowledge Embedding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_8.html">
      <font color="black">CoLAKE: Contextualized Language and Knowledge Embedding</font>
    </a>
  </h2>
  <font color="black">知識コンテキストと言語コンテキストの異質性を処理するために、それらを統合されたデータ構造である単語知識グラフ（WKグラフ）に統合します。CoLAKEは、変更されたTransformerエンコーダーを使用して大規模なWKグラフで事前トレーニングされています。事実知識をBERTなどの事前トレーニング済み言語モデルに組み込む新しい分野である既存のモデルのほとんどは、浅く、静的で、個別に事前トレーニング済みのエンティティの埋め込みを考慮しているため、これらのモデルのパフォーマンスの向上が制限されます。 
[ABSTRACT] colakeは、変更されたデータモデルを使用して大規模なwkネットワークで事前トレーニングされています。用語に関しては、エンティティの知識コンテキストを抽出します。これは、同じものでは機能しないためです。言語</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: A survey on natural language processing (nlp) and applications in
  insurance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_9.html">
      <font color="black">A survey on natural language processing (nlp) and applications in
  insurance</font>
    </a>
  </h2>
  <font color="black">SCORのデータ分析チームは、テキスト分析に関する最新の調査を使用できるようにする革新的なツールまたは製品の実装に取り組んでいます。最後に、読者が確実に理解できるように、自然言語処理の調査を構成するすべてのステップについて詳しく説明しました。実装に関する深い理解..実際に今日使用されているさまざまな方法の詳細は、それらのストーリーをさかのぼります。 
[概要]調査により、テキストデータの価値を解き放つアルゴリズムの機能が崩壊しました。これにより、特に保険セクターに多くの新しい機会がもたらされます。保険におけるテキストマイニング技術を理解することで、引受リスクの監視と、最終的に保険契約者に利益をもたらす多くのプロセスが強化されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Referring Image Segmentation via Cross-Modal Progressive Comprehension -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_10.html">
      <font color="black">Referring Image Segmentation via Cross-Modal Progressive Comprehension</font>
    </a>
  </h2>
  <font color="black">このようにして、マルチレベルの機能は相互に通信し、テキストのコンテキストに基づいて洗練される可能性があります。以前のアプローチでは、暗黙的な機能の相互作用と視覚的モダリティと言語的モダリティの融合を使用してこの問題に取り組んでいますが、通常、参照されるエンティティを正確に識別するために、2つのモダリティからの機能を適切に調整する式。このペーパーでは、困難なタスクに効果的に対処するために、クロスモーダルプログレッシブ理解（CMPC）モジュールとテキストガイド機能交換（TGFE）モジュールを提案します。 。 
[概要] cmpcモジュールは、最初にエンティティと関連する単語を使用して、式によって考慮される可能性のあるすべての関連エンティティを認識します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Are All Languages Created Equal in Multilingual BERT? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_11.html">
      <font color="black">Are All Languages Created Equal in Multilingual BERT?</font>
    </a>
  </h2>
  <font color="black">mBERTは、高リソース言語のベースラインよりも優れているか、同等ですが、低リソース言語でははるかに劣ります。同様の言語と組み合わせると、単一言語BERTとmBERTの間のパフォーマンスギャップを狭めることができます。3つのタスクを検討します。名前付きエンティティの認識（99言語）、音声部分のタグ付け、および依存関係の解析（各54言語）。 
[概要]これらのテストは、高リソース言語での言語間転送に焦点を当てています。これらの言語の単一言語バートモデルはさらに悪化します。低リソース言語のより良いモデルには、より効率的な事前トレーニング手法またはより多くのデータが必要であることがわかります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br><font color="black">2020-05-18</font>
      </time>
    </span>
</section>
<!-- paper0: WeChat Neural Machine Translation Systems for WMT20 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_12.html">
      <font color="black">WeChat Neural Machine Translation Systems for WMT20</font>
    </a>
  </h2>
  <font color="black">私たちの制約付き中国語から英語へのシステムは、すべての提出物の中で最高である36.9の大文字と小文字を区別するBLEUスコアを達成します。私たちの実験では、データ選択、いくつかの合成データ生成アプローチ（つまり、逆翻訳、知識蒸留、および反復-ドメイン知識の伝達）、高度な微調整アプローチ、およびセルフブリューベースのモデルアンサンブル..私たちのシステムは、効果的なバリアントとDTMT（Meng and Zhang、2019）アーキテクチャを備えたTransformer（Vaswani et al。、2017a）に基づいています。 
[概要]私たちのシステムは変圧器に基づいており、dtmt.ourの制約付き中国語から英語へのシステムは36.9、細かいブルースコアを達成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Linguistic Structure Guided Context Modeling for Referring Image
  Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_13.html">
      <font color="black">Linguistic Structure Guided Context Modeling for Referring Image
  Segmentation</font>
    </a>
  </h2>
  <font color="black">文のマルチモーダルコンテキストは、指示対象を背景から区別するために重要です。4つのベンチマークでの広範な実験は、私たちの方法が以前のすべての最先端技術よりも優れていることを示しています。既存の方法は、マルチモーダルコンテキストを不十分または冗長にモデル化します。この問題に取り組むために、クロスモーダル相互作用によってマルチモーダルコンテキストをモデル化し、このスキームを新しい言語構造ガイド付きコンテキストモデリング（LSCM）モジュールとして実装するための「収集-伝播-配布」スキームを提案します。 
[ABSTRACT] `マルチモーダルコンテキスト &#39;は、参照フォームを背景から区別するために重要です。モデルを使用して、依存関係解析ツリーの抑制された単語グラフを作成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Citation Sentiment Changes Analysis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_14.html">
      <font color="black">Citation Sentiment Changes Analysis</font>
    </a>
  </h2>
  <font color="black">予備的な証拠は、EDRベースの方法が時系列形式で出版物の影響を分析する可能性を保持していることを示しました。引用感情の変化はグローバル引用感情シーケンス（GCSS）から観察できます。引用感情の変化を測定するためのメトリックが導入されました。 
[ABSTRACT]グローバルな引用感情シーケンス（gcsss）から変化を観察できます。gcsssデータの分析はデータの分析に基づいていました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Dual Attention Model for Citation Recommendation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_15.html">
      <font color="black">Dual Attention Model for Citation Recommendation</font>
    </a>
  </h2>
  <font color="black">提案されたモデルのコアは、自己注意と付加的注意で構成され、前者は入力情報間の関連性をキャプチャすることを目的とし、後者は入力の重要性を学習することを目的としています。実際のデータセットでの実験は有効性を示しています提案されたアプローチの..従来の引用推奨方法は深刻な情報損失に苦しんでいます。 
[概要]従来の引用推奨者は深刻な情報損失に苦しんでいます。これらの欠点により、原稿を作成するときに十分な引用を推奨するには、このような方法では不十分です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: The importance of fillers for text representations of speech transcripts -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_16.html">
      <font color="black">The importance of fillers for text representations of speech transcripts</font>
    </a>
  </h2>
  <font color="black">話し言葉の重要な要素である一方で、フィラー（たとえば、深い文脈化された埋め込みでそれらを表現する可能性を探り、話し言葉のモデリングと2つのダウンストリームタスクの改善を示します-話者のスタンスを予測し、自信を表現します。ええと」）音声言語理解（SLU）タスクでは見過ごされがちです。 
[概要]「ええと」または「ええと」は、話し言葉の理解タスクでは見過ごされがちです。話し言葉のスキルは見過ごされがちです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Phonemer at WNUT-2020 Task 2: Sequence Classification Using COVID
  Twitter BERT and Bagging Ensemble Technique based on Plurality Voting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_17.html">
      <font color="black">Phonemer at WNUT-2020 Task 2: Sequence Classification Using COVID
  Twitter BERT and Bagging Ensemble Technique based on Plurality Voting</font>
    </a>
  </h2>
  <font color="black">最終的なアプローチでは、F1スコア0.9037を達成し、評価基準としてF1スコアを使用して全体で6位にランク付けされました。最後の段階では、提供されたデータセットのさまざまなサブセットでトレーニングされた最良のモデルのアンサンブルを提案します。最初の段階では、関連情報のみをフィルタリングしてデータセットを前処理します。 
[概要]タスクは、新しいコロナウイルスに関連する英語のツイートが有益であるかどうかを自動的に識別するシステムを開発することです。最初の段階では、関連情報のみをフィルタリングしてデータセットを前処理します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: A Feature Analysis for Multimodal News Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_18.html">
      <font color="black">A Feature Analysis for Multimodal News Retrieval</font>
    </a>
  </h2>
  <font color="black">ほとんどの情報検索方法は、テキストまたは画像のいずれかに基づいています。さらに、テキストの特徴の間では、エンティティのオーバーラップが単語の埋め込みよりも優れているのに対し、ジオロケーションの埋め込みは、検索タスクの視覚的特徴の間でより優れたパフォーマンスを実現します。政治、健康、環境、スポーツ、金融など、さまざまな分野でのクロスリンガルニュース検索のためのマルチモーダル機能の有用性。 
[概要]ほとんどの情報検索方法は、テキストまたは画像に基づいています。さらに、画像とテキストには5つの機能タイプがありますが、さまざまな組み合わせを使用してシステムのパフォーマンスを比較します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: AMUSED: An Annotation Framework of Multi-modal Social Media Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_19.html">
      <font color="black">AMUSED: An Annotation Framework of Multi-modal Social Media Data</font>
    </a>
  </h2>
  <font color="black">このフレームワークは、Twitter、YouTube、Redditなどの複数のプラットフォームから注釈付きデータをフェッチできます。AMUSEDは複数のアプリケーションドメインに適用できます。ユースケースとして、さまざまなソーシャルメディアからCOVID-19の誤った情報データを収集するためのフレームワークを実装しました。プラットフォーム..このペーパーでは、複数のソーシャルメディアプラットフォームからマルチモーダル注釈付きデータを収集するためのAMUSEDと呼ばれる半自動フレームワークを紹介します。 
[概要]フレームワークは、ソーシャルメディアデータの収集と注釈付けの問題を軽減するように設計されています。Twitter、youtube、redditなどの複数のプラットフォームから注釈付きデータを取得できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: LiveQA: A Question Answering Dataset over Sports Live -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_20.html">
      <font color="black">LiveQA: A Question Answering Dataset over Sports Live</font>
    </a>
  </h2>
  <font color="black">スポーツゲームの特性から派生したLiveQAは、タイムラインベースのライブブロードキャスト全体で推論能力をテストできる可能性があります。これは、既存のデータセットと比較して困難です。1,670を超えるNBAゲームについて、人間のコメンテーターによって書かれた117kの複数選択の質問が含まれています。中国のHupu（https://nba.hupu.com/games）Webサイトから収集されます。LiveQAでは、質問にはタイムラインの理解、イベントの追跡、または数学的な計算が必要です。 
[概要] liveqaでは、質問にはタイムラインの理解、イベントの追跡、または数学的な計算が必要です。このペーパーには、中国のhupuWebサイトからの117kの多肢選択問題が含まれています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Aspect and Opinion Term Extraction for Hotel Reviews using Transfer
  Learning and Auxiliary Labels -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_21.html">
      <font color="black">Aspect and Opinion Term Extraction for Hotel Reviews using Transfer
  Learning and Auxiliary Labels</font>
    </a>
  </h2>
  <font color="black">微調整されたモデルは、エンティティレベルでBi-LSTMモデルを明らかに上回っています。多言語モデルからの転移学習を利用することにより、最先端技術と比較して、トークンレベルのF1スコアで最大2％の違いを達成しました。トレーニングエポックが少ないBi-LSTMモデル（3エポックと200エポック）。CRFの追加により、トークンレベルとエンティティレベルの両方のF1スコアがさらに向上します。 
[概要]この研究は、事前に訓練されたバートを使用して、インドネシアのバハサのホテルレビューからトークンを分類する転移学習の評価に焦点を当てています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-26">
        <br><font color="black">2019-09-26</font>
      </time>
    </span>
</section>
<!-- paper0: RRF102: Meeting the TREC-COVID Challenge with a 100+ Runs Ensemble -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_22.html">
      <font color="black">RRF102: Meeting the TREC-COVID Challenge with a 100+ Runs Ensemble</font>
    </a>
  </h2>
  <font color="black">私たちのアブレーション研究は、アンサンブル全体に対するこれらの各システムの貢献を示しています。提出されたアンサンブルの実行は、TREC-COVIDチャレンジのラウンド4および5で最先端のパフォーマンスを達成しました。急速に進化する生物医学コレクションの検索エンジンである、シンプルでありながら効果的な加重階層ランク融合アプローチを提案します。これは、（a）語彙および意味検索システム、（b）事前トレーニングおよび微調整されたBERTランカーからの102の実行をまとめたものです。 c）関連性フィードバックの実行。 
[概要]急速に進化するコレクションの検索エンジンを構築するという課題に対応するために、シンプルで効果的なシステムで提案します。提出されたアンサンブルランは、トレックのラウンド4と5で最先端のパフォーマンスを達成しました-covidチャレンジ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Detecting White Supremacist Hate Speech using Domain Specific Word
  Embedding with Deep Learning and BERT -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_23.html">
      <font color="black">Detecting White Supremacist Hate Speech using Domain Specific Word
  Embedding with Deep Learning and BERT</font>
    </a>
  </h2>
  <font color="black">このデータセットは、Twitterから作成されたデータセットと、その白人至上主義者フォーラムからコンパイルされたStormfrontデータセットから結合されました。2番目のアプローチは、最新の言語モデルの1つであるBERTを使用することです。BERTモデルは、ほとんどのNLPの最先端を提供します。タスク..0.79605F1スコアに達しました。 
[概要]これは、白人至上主義者のヘイトスピーチの影響によるものです。これらのグループはソーシャルメディアに大きな影響を及ぼします。また、人種的憎悪や暴力を助長します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented
  Dialogue -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_24.html">
      <font color="black">TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented
  Dialogue</font>
    </a>
  </h2>
  <font color="black">事前にトレーニングされたタスク指向の対話BERT（TOD-BERT）は、意図認識、対話状態の追跡、対話行為の予測、応答の選択など、4つのダウンストリームのタスク指向の対話アプリケーションでBERTのような強力なベースラインを上回ります。事前トレーニング中に、ユーザートークンとシステムトークンをマスクされた言語モデリングに組み込みます。また、TOD-BERTには、タスク指向の対話のデータ不足の問題を軽減できる、より強力な数ショット能力があることも示しています。 
[概要]言語モデリングのために9つの人間（人間とマルチシステム）のデータセットを統合します。応答タスクをシミュレートするための対照的な関数を提案します。tod-一般には、データ不足の問題を軽減できるショット能力があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-15">
        <br><font color="black">2020-04-15</font>
      </time>
    </span>
</section>
<!-- paper0: Fact or Fiction: Verifying Scientific Claims -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_25.html">
      <font color="black">Fact or Fiction: Verifying Scientific Claims</font>
    </a>
  </h2>
  <font color="black">私たちの実験は、SciFactが、特殊なドメイン知識を含むコーパスを取得して推論するように設計された新しいシステムの開発に挑戦的なテストベッドを提供することを示しています。この新しいタスクのデータとコードは、https：//github.com/allenai/で公開されています。 scifact ..このタスクを研究するために、SciFactを構築します。これは、ラベルと理論的根拠で注釈が付けられた証拠を含む要約と組み合わせた、1.4Kの専門家が書いた科学的主張のデータセットです。 
[概要]1。4kエキスパートのデータセットであるscifactを構築します-書かれた科学的主張。ラベルと理論的根拠で注釈が付けられたメトリックが含まれています。このタスクのデータとファクトは公開されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-30">
        <br><font color="black">2020-04-30</font>
      </time>
    </span>
</section>
<!-- paper0: DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented
  Dialogue -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_26.html">
      <font color="black">DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented
  Dialogue</font>
    </a>
  </h2>
  <font color="black">いくつかの強力なベースラインモデルをリリースし、大規模なオープンドメインダイアログコーパスとタスク適応型自己監視で事前トレーニングすることにより、バニラBERTアーキテクチャに対するパフォーマンスの向上と7つのタスクのうち5つでの最先端の結果を示します。トレーニング..DialoGLUEベンチマーク、ベースラインメソッド、および評価スクリプトを通じて、より一般的なタスク指向の対話モデルを開発するという目標に向けた進展を促進したいと考えています。タスク指向の対話研究の長年の目標は能力です。対話モデルを新しいドメインに柔軟に適応させるため。 
[要約]公開ベンチマークは、7つのタスク（4つの異なる自然言語理解タスクをカバーする客観的な対話データセット）で構成されています。目的は、表現ベースの転送、ドメイン適応、およびサンプルの開発を促進することです。効率的なタスク学習</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-28">
        <br><font color="black">2020-09-28</font>
      </time>
    </span>
</section>
<!-- paper0: Evaluating Multilingual BERT for Estonian -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_27.html">
      <font color="black">Evaluating Multilingual BERT for Estonian</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、多言語BERTモデルは、POSおよび形態学的タグ付けとテキスト分類のすべてのベースラインモデルを上回り、NERの最良のベースラインと同等のレベルに達し、XLM-RoBERTaが他の多言語モデル..この論文では、4つの多言語モデル---多言語BERT、多言語蒸留BERT、XLMおよびXLM-RoBERTa ---をPOSおよび形態学的タグ付け、NERおよびテキスト分類を含むいくつかのNLPタスクで評価します。これらの多言語BERTモデルとこれらのタスクの既存のベースラインニューラルモデルとの比較を確立することです。 
[概要]目的は、これらの多言語のbertモデル間の比較を確立することです。ただし、複数の言語を同時に処理できる多言語のモデルがいくつかあります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Question-Answering as an Automatic Metric for Evaluating the
  Content Quality of a Summary -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_28.html">
      <font color="black">Towards Question-Answering as an Automatic Metric for Evaluating the
  Content Quality of a Summary</font>
    </a>
  </h2>
  <font color="black">QAEvalは、スコアリング集計システムですでに最先端のパフォーマンスを達成しており、ゴールドスタンダードのピラミッドメソッドを含む他のすべての指標を上回っていますが、個々の要約でのパフォーマンスは、他の自動指標との競争力があります。提案されたメトリック、QAEval。これは前の作業よりも広く適用できます。前の作業はこの方向で最初の有望な結果を示していますが、実験は限られており、要約コンテンツの評価におけるQAの有用性についての理解が不十分です。 
[概要] qaのテストが不足しているため、qaの全体的なパフォーマンスが十分に理解されていません。包括的な結果が不足していると、qaの品質が向上します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Vietnamese Named Entity Recognition from Speech Using Word
  Capitalization and Punctuation Recovery Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_29.html">
      <font color="black">Improving Vietnamese Named Entity Recognition from Speech Using Word
  Capitalization and Punctuation Recovery Models</font>
    </a>
  </h2>
  <font color="black">この論文では、（1）NERタスク用の最初のベトナム語音声データセット、および（2）ベトナム語の新しい最先端を実現したベトナム語用の最初の事前トレーニング済みの大規模単一言語モデルを提示しました。最新の研究と比較して1.3％の絶対F1スコアによるNERタスク..固有表現抽出（NER）タスクに関する研究は、適切な句読点や大文字化など、正しいテキスト形式の入力テキストで人間の同等性に達する優れた結果を示しています。モデルはASRシステムから入力テキストを受け取り、2つのタスクを同時に実行して、NERのパフォーマンスを向上させるのに役立つ適切なテキストフォーマットを生成します。 
[ABSTRACT]テキストの大文字化と句読点の回復モデル（capu）がパイプラインに導入されました。新しいモデルは、音声からのnerタスクの新しいモデルを導入することで機能します。音声認識システムで機能し、システムはテキストの書式設定を考慮しません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Interpreting Graph Neural Networks for NLP With Differentiable Edge
  Masking -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_30.html">
      <font color="black">Interpreting Graph Neural Networks for NLP With Differentiable Edge
  Masking</font>
    </a>
  </h2>
  <font color="black">トレーニングされたGNNモデルが与えられると、すべてのレイヤーのすべてのエッジについて、そのエッジをドロップできるかどうかを予測する単純な分類器を学習します。このような分類器は、確率的ゲートを使用し、スパース性を促進して、完全に微分可能な方法でトレーニングできることを示します。予想される$ L_0 $ノルムを介して..グラフニューラルネットワーク（GNN）は、構造的誘導バイアスをNLPモデルに統合するための一般的なアプローチになりました。 
[概要] gnnの解釈、特にそれらのラベル付けに関する作業はほとんどありません。不要なエッジを識別するgnnの予測を解釈するための事後的な方法を説明します。このような分類器は、完全に微分可能な方法でトレーニングできることを示しました。 、確率的ゲートを採用</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Contextual Knowledge Selection and Embedding towards Enhanced
  Pre-Trained Language Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_31.html">
      <font color="black">Contextual Knowledge Selection and Embedding towards Enhanced
  Pre-Trained Language Models</font>
    </a>
  </h2>
  <font color="black">パフォーマンスの向上に加えて、DKPLMで動的に選択された知識は、従来のPLMよりも解釈しやすい形式でテキスト関連の知識のセマンティクスを記述できます。DKPLMの詳細を提供するために、ソースコードとデータセットが利用可能になります。これらの知識強化PLMのうち、PLMに必要な知識が特定のテキスト（「テキストコンテキスト」）に従って動的に変化する可能性があるかどうかに関係なく、KGの静的サブグラフ（「知識コンテキスト」）が埋め込まれます。 
[要約] plmsはkgの静的サブグラフを埋め込みます（「知識コンテキスト」）.dkplmsは、典型的な知識主導のnlpタスクのさまざまなベースラインを上回ります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-29">
        <br><font color="black">2020-09-29</font>
      </time>
    </span>
</section>
<!-- paper0: Examining the rhetorical capacities of neural language models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_32.html">
      <font color="black">Examining the rhetorical capacities of neural language models</font>
    </a>
  </h2>
  <font color="black">私たちの実験は、BERTベースのLMが他のTransformer LMよりも優れていることを示しており、中間層の表現でより豊富な談話知識を明らかにしています。さらに、GPT-2とXLNetは明らかに修辞的知識が少ないため、言語哲学から説明を引き出すことをお勧めします。本論文では、神経LMの修辞能力を定量的に評価する方法を提案する。 
[要約]談話のレトリックを理解する神経lmsの能力を調べます。言語哲学からの説明を提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: A Compare Aggregate Transformer for Understanding Document-grounded
  Dialogue -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_33.html">
      <font color="black">A Compare Aggregate Transformer for Understanding Document-grounded
  Dialogue</font>
    </a>
  </h2>
  <font color="black">CMUDoGデータセットの実験結果は、提案されたCATモデルが最先端のアプローチと強力なベースラインよりも優れていることを示しています。ノイズを低減するために2つの異なる比較メカニズムを設計しました（デコード前とデコード中）。ただし、対話履歴は現在のダイアログとは関係なく、KS処理にノイズが発生する可能性があります。 
[概要]ダイアログコンテキストを組み合わせ、応答生成のためにドキュメント情報を集約するための比較集約トランスフォーマー（cat）を提案します。さらに、単語の重複に基づいてドキュメントの使用効率を評価するための2つのメトリックを提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: How LSTM Encodes Syntax: Exploring Context Vectors and Semi-Quantization
  on Natural Text -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_34.html">
      <font color="black">How LSTM Encodes Syntax: Exploring Context Vectors and Semi-Quantization
  on Natural Text</font>
    </a>
  </h2>
  <font color="black">内部ゲートの出力は、Suzgun et al。のように、言語モデルがネストの深さを正確にカウントするのに役立つように、バイナリ値またはターナリ値に近似的に量子化されます。さらに、$ L_1 $の正規化により、次のことを正確に予測できることもわかりました。単語が句構造内にあるか、コンテキストベクトルの少数のコンポーネントからではないか。構文構造が暗黙的に与えられる言語モデルを学習することによってそれらを分析します。 
[概要]コンテキスト更新システムがよく知られていることを分析的に示します。これらは自然なテキストでよく知られていますが、そのような情報がシステムにどのように反映されるかはまだ調査されていません。単語が句構造内にあるかどうかを正確に予測できます。またはコンテキストの少数のコンポーネントからではない</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Masked Language Modeling for Proteins via Linearly Scalable Long-Context
  Transformers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/cs.CL/paper_35.html">
      <font color="black">Masked Language Modeling for Proteins via Linearly Scalable Long-Context
  Transformers</font>
    </a>
  </h2>
  <font color="black">タンパク質配列モデリングの困難なタスクでの有効性を示し、詳細な理論的分析を提供します。この課題に対処するために、直交ランダム機能（FAVOR）による高速注意に基づく新しいTransformerアーキテクチャであるPerformerを紹介します。学習した注意マトリックスの構造と希薄性を利用するものが開花しました。 
[概要]注意メカニズムのトレーニングコストに対する懸念は高まり続けていますが、長いシーケンスを含む実際のアプリケーションでは、これらの仮定を満たすことができない場合があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-05">
        <br><font color="black">2020-06-05</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: Speech2Video Synthesis with 3D Skeleton Regularization and Expressive
  Body Poses -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.AS/paper_0.html">
      <font color="black">Speech2Video Synthesis with 3D Skeleton Regularization and Expressive
  Body Poses</font>
    </a>
  </h2>
  <font color="black">スケルトンの動きをリアルで表現力豊かにするために、関節のある3D人間の骨格の知識と、学習した個人的な音声の象徴的なジェスチャーの辞書を、学習パイプラインとテストパイプラインの両方の生成プロセスに組み込みます。アプローチを検証するために、さまざまなトピックの下でさまざまなドキュメントを読んでいる男性1人と女性1人のモデルからの20の高品質ビデオ..動きの詳細を含むフォトリアリスティックで高解像度のビデオを作成するために、条件付きGANにパーツアテンションメカニズムを挿入することを提案します。たとえば、
[ABSTRACT]スケルトンの動きは、反復ニューラルネットワーク（rnn）を使用して生成されます。次に、条件付きの関節式3Dネットワーク（gan）を介して出力ビデオを合成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-17">
        <br><font color="black">2020-07-17</font>
      </time>
    </span>
</section>
<!-- paper0: A convolutional neural-network model of human cochlear mechanics and
  filter tuning for real-time applications -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.AS/paper_1.html">
      <font color="black">A convolutional neural-network model of human cochlear mechanics and
  filter tuning for real-time applications</font>
    </a>
  </h2>
  <font color="black">CoNNearモデルは、音響音声素材でトレーニングされ、そのパフォーマンスと適用性は、蝸牛力学研究で一般的に使用される（目に見えない）音刺激を使用して評価されました。聴覚モデルは、自動音声認識システムの特徴抽出器として、またはのフロントエンドとして一般的に使用されます。ロボット工学、機械聴覚および聴覚補助アプリケーション..畳み込み神経ネットワークを計算神経科学と組み合わせて、レベル依存フィルターチューニングを含む人間の蝸牛力学のリアルタイムエンドツーエンドモデルを生成するハイブリッドアプローチを提示します（ CoNNear）。 
[ABSTRACT] connearモデルは、音響音声素材でトレーニングされました。そのパフォーマンスと適用性は、音刺激を使用して評価されました。connearアーキテクチャは、並列で微分可能な計算に基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-30">
        <br><font color="black">2020-04-30</font>
      </time>
    </span>
</section>
<!-- paper0: SEANet: A Multi-modal Speech Enhancement Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.AS/paper_2.html">
      <font color="black">SEANet: A Multi-modal Speech Enhancement Network</font>
    </a>
  </h2>
  <font color="black">この観察に基づいて、マルチモーダル入力をSEANet（Sound EnhAncement Network）にフィードします。これは、機能損失と敵対的損失の組み合わせを採用してユーザーの音声の拡張バージョンを再構築する、波から波への完全畳み込みモデルです。加速度計データを利用して、非常にノイズの多い状況で音声強調を実行する可能性を探ります。イヤフォンに取り付けられたセンサーによって収集され、オーディオ信号にさまざまな種類のノイズ源を追加することによって合成的に破損したデータを使用してモデルをトレーニングしました。 
[概要]イヤフォンに取り付けられたセンサーによって収集され、オーディオ信号にさまざまな種類のノイズ源を追加することによって合成的に破損したデータを使用してモデルをトレーニングしました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-04">
        <br><font color="black">2020-09-04</font>
      </time>
    </span>
</section>
<!-- paper0: SESQA: semi-supervised learning for speech quality assessment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.AS/paper_3.html">
      <font color="black">SESQA: semi-supervised learning for speech quality assessment</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、そのような半教師ありアプローチが既存の方法のエラーを36％以上削減できる一方で、再利用可能な機能や補助出力の点で追加の利点を提供できることを示しています。自動音声品質評価は重要な横断的タスクであり、その進歩は人間の注釈の不足、目に見えない記録条件への不十分な一般化、および既存のアプローチの柔軟性の欠如によって妨げられています。改善は、有望な一般化機能を示すサンプル外テストでさらに裏付けられます。 
[概要]サンプル外のテストは、有望な一般化機能を示しています。サンプル外のテストによって、改善がさらに裏付けられます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Joint Articulatory-Acoustic Representations with Normalizing
  Flows -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.AS/paper_4.html">
      <font color="black">Learning Joint Articulatory-Acoustic Representations with Normalizing
  Flows</font>
    </a>
  </h2>
  <font color="black">私たちのモデルは、畳み込みオートエンコーダアーキテクチャと正規化フローベースモデルを利用して、1D音響波モデルを備えた2自由度の調音シンセサイザーの正中矢状声道ジオメトリ間で、半監視方式で順方向と逆方向の両方のマッピングを可能にします私たちのアプローチは、調音から音響へのマッピングと音響から調音へのマッピングの両方を達成する上で満足のいくパフォーマンスを達成し、それによって両方のドメインの共同エンコーディングを達成することに成功したことを示しています。この論文は、それぞれのドメイン固有の機能を同時に維持しながら、反転可能なニューラルネットワークモデルを介して、声道音の調音ドメインと音響ドメインの間の共同潜在表現を見つけることを目的としています。 
[ABSTRACT]調音-から-音響および音響との関係。新しい研究は、母音の調音と音響領域の間の共同潜在表現を見つけることを目的としています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-16">
        <br><font color="black">2020-05-16</font>
      </time>
    </span>
</section>
<!-- paper0: FSD50K: an Open Dataset of Human-Labeled Sound Events -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-02/eess.AS/paper_5.html">
      <font color="black">FSD50K: an Open Dataset of Human-Labeled Sound Events</font>
    </a>
  </h2>
  <font color="black">元のオーディオトラックのダウンロードも問題があります。これは、構成要素であるYouTubeビデオが徐々に消え、使用権の問題が発生し、システムのベンチマークに対するこのリソースの適合性に疑問が投げかけられます。オーディオクリップはクリエイティブコモンズライセンスの下でライセンスされており、データセットを自由に配布できます（波形を含む）。オーディオ情報に基づいた使用を可能にするための制限と主要な要因の説明とともに、包括的なデータセットの特性評価が含まれています。 
[ABSTRACT] fsd50kは、フリーサウンドオーディオクリップのオープンデータセットです。これには、合計100時間以上のオーディオの51,000を超えるオーディオクリップが含まれます。これらには、一部のserメソッドの使用を制限するフリーサウンドオーディオ機能が含まれます。これは、代替のベンチマークデータセットと育成によるものです。 serリサーチ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
