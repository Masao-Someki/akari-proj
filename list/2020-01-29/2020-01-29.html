<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
<title>Akari-2020-01-29の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
  <div class="header-logo">
    <a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
  </div>
</header>
<input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">

<div class="menubar">
<span class="bar"></span>
<span class="bar"></span>
<span class="bar"></span>
</div>

<ul>
<li><a id="home" href="../../index.html">Home</a></li>
<li><a id="about" href="../../teamAkariとは.html">About</a></li>
<li><a id="contact" href="../../contact.html">Contact</a></li>
<li><a id="contact" href="../../list/newest.html">New Papers</a></li>
<li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
</ul>

</label>
<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: CLCNet: Deep learning-based Noise Reduction for Hearing Aids using
  Complex Linear Coding -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.SD/paper_0.html">
      CLCNet: Deep learning-based Noise Reduction for Hearing Aids using
  Complex Linear Coding
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ノイズの多い環境でのモノラル音声強化を改善するために、複雑な値の線形コーディングに基づくフレームワークであるCLCNetを提案します。最初に、複雑な周波数領域で適用される線形予測コーディング（LPC）を動機とする複雑な線形コーディング（CLC）を定義します。 。CLCNetは、EUROMデータベースと、補聴器で記録された実際のノイズデータセットの混合物で評価され、従来の実際のWiener-Filterゲインと比較されました。 
[要約]-深層学習-最先端のアルゴリズムが見つかりません。これらには、リアルタイムおよび周波数分解能、または低品質が含まれます。これは、非常にノイズの多い条件下で低品質につながる可能性があります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-28">
        <br>2020-01-28
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Deep speech inpainting of time-frequency masks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.SD/paper_1.html">
      Deep speech inpainting of time-frequency masks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      評価結果は、提案されたフレームワークが音声の欠落または歪んだ部分の大部分を回復するのに有効であることを示しています。具体的には、LibriSpeechデータセットを使用して評価されるSTOIおよびPESQ客観的メトリックの顕著な改善をもたらします音声の時間周波数表現の欠落部分またはひどく歪んだ部分を短期コンテキストから検索するためのエンドツーエンドのフレームワークを導入します。したがって、音声の修復を行います。 
[概要]このシステムは、音声分類タスクで事前学習された深層音声特徴抽出器であるspeechvggを介して取得された、深層特徴損失で学習された畳み込みu-netに基づいています。データセット
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-20">
        <br>2019-10-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Deep Xi as a Front-End for Robust Automatic Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.SD/paper_2.html">
      Deep Xi as a Front-End for Robust Automatic Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験的調査により、フロントエンドとしてのDeepXiは、最近のマスキングおよびマッピングベースのディープラーニングフロントエンドよりも低いワードエラー率を生成できることが示されています。DeepXiは、実世界の非定常および有色ノイズソースを使用して評価されますこの作業で示された結果は、Deep Xiが実行可能なフロントエンドであり、ASRシステムの堅牢性を大幅に向上できることを示しています。 
[要旨]ディープxiは実行可能なフロントエンドであり、asrシステムの堅牢性を大幅に向上させることができます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-18">
        <br>2019-06-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Time-Domain Audio Source Separation Based on Wave-U-Net Combined with
  Discrete Wavelet Transform -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.SD/paper_3.html">
      Time-Domain Audio Source Separation Based on Wave-U-Net Combined with
  Discrete Wavelet Transform
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      離散ウェーブレット変換（DWT）に基づいて、ダウンサンプリング（DS）およびアップサンプリング（US）レイヤーを使用する時間領域オーディオソース分離方法を提案します。この信念では、DWTがアンチエイリアシングフィルターと完全な再構成プロパティを使用して、提案されたレイヤーを設計します。提案された方法は、最新のディープニューラルネットワークの1つであるWave-U-Netに基づいています。機能マップのサンプル。 
[概要]提案された方法は、最新のディープニューラルネットワークの1つであるwave-u-netに基づいており、data.itを分析して、これらの問題の影響を再現できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-28">
        <br>2020-01-28
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Speech-VGG: A deep feature extractor for speech processing -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.SD/paper_4.html">
      Speech-VGG: A deep feature extractor for speech processing
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      抽出器のパフォーマンスに対するさまざまなハイパーパラメータの影響を推定するために、speechVGGのいくつかの構成を適用して、インフォームドスピーチインペインティングのシステムをトレーニングしました。この研究では、音声処理フレームワークをトレーニングするための深部特徴抽出ツールであるspeechVGGに関する以前の研究を拡張します。 
[要約]抽出プログラムは、古典的なvgg-対数マグニチュードstft特徴から単語を識別するために再訓練された16個のコンパリューショナルニューラルネットワークに基づいています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-22">
        <br>2019-10-22
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Europarl-ST: A Multilingual Corpus For Speech Translation Of
  Parliamentary Debates -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.SD/paper_5.html">
      Europarl-ST: A Multilingual Corpus For Speech Translation Of
  Parliamentary Debates
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      コーパスは、クリエイティブコモンズライセンスの下でリリースされ、自由にアクセスおよびダウンロードできます。このコーパスは、2008年から2012年の間に欧州議会で開催された議論を使用して編集されています。現在利用可能なSLTデータセットは限られた言語ペアのセットに制限されているため、音声からテキストへの翻訳は、このタスクの特定のデータリソースの不足によって妨げられることがよくあります。 
[要旨]論文では、このリソースの可能性を強調する一連の言語を紹介しています。これは、ヨーロッパおよびヨーロッパの研究者の研究です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-08">
        <br>2019-11-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Singing Voice Conversion with Disentangled Representations of Singer and
  Vocal Technique Using Variational Autoencoders -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.SD/paper_6.html">
      Singing Voice Conversion with Disentangled Representations of Singer and
  Vocal Technique Using Variational Autoencoders
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      個別のエンコーダーを使用して、シンガーアイデンティティとボーカルテクニックのもつれを解く潜在的表現を個別に学習し、再構築のためのジョイントデコーダーを使用します。私たちの知る限り、これはシンガーアイデンティティとボーカルテクニックの変換に共同で取り組む最初の作品です深層学習アプローチ。変換は、学習した潜在空間で単純なベクトル演算によって実行されます。 
[概要]提案されたモデルは、非並列コーパスでトレーニングされ、多くのもつれた変換に対応します。別のモデルは、非並列コーパスでトレーニングされ、多対セントの変換に対応します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-03">
        <br>2019-12-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: HI-MIA : A Far-field Text-Dependent Speaker Verification Database and
  the Baselines -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.SD/paper_7.html">
      HI-MIA : A Far-field Text-Dependent Speaker Verification Database and
  the Baselines
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、トレーニングにシングルチャネルとマルチチャネルの両方のデータを採用する一連のエンドツーエンドニューラルネットワークベースラインシステムを提案します。結果は、フュージョンシステムが遠方で3.29 \％EERを達成できることを示しています。私たちのデータベースには、遠距離シナリオ用に設計された部屋の340人の記録が含まれています。 
[概要]ファーフィールドマイクアレイベースのスピーカー検証のデータ要件を満たすことを目指しています。公開されているデータベースのほとんどは、シングルチャンネルクローズ-会話とテキスト-独立しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-03">
        <br>2019-12-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Short-Range Audio Channels Security: Survey of Mechanisms, Applications,
  and Research Challenges -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.SD/paper_8.html">
      Short-Range Audio Channels Security: Survey of Mechanisms, Applications,
  and Research Challenges
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      しかし、最も有望なソリューションは価値のある商用製品に変わりつつありますが、音響チャネルはシステムやデバイスに対する攻撃を開始するためにも使用されるようになり、その採用を妨げる可能性のあるセキュリティ上の懸念につながります。さらに、派生する長所と短所も指摘します。短距離オーディオチャンネルの使用から..厳密で科学的な、セキュリティ指向の分野のレビューを提供するために、このペーパーでは、以下のために短距離オーディオチャンネルに根ざした方法、アプリケーション、およびユースケースを調査および分類します。セキュリティサービスのプロビジョニング---二要素認証技術、ペアリングソリューション、デバイス認証戦略、防御方法論、攻撃スキームなど。 
[概要]セキュリティサービス用の短距離オーディオチャネルが提案されています。これは、セキュリティコンテキストへのシームレスな適応性のためです。ただし、多くの技術が最近提案されています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-09">
        <br>2020-01-09
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Incorporating Joint Embeddings into Goal-Oriented Dialogues with
  Multi-Task Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.CL/paper_0.html">
      Incorporating Joint Embeddings into Goal-Oriented Dialogues with
  Multi-Task Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、モデルは推論中にナレッジグラフエンティティルックアップを組み込んで、生成された出力が提供されたローカルナレッジグラフに基づいてステートフルであることを保証します。 。最終的にBLEUスコアを使用してモデルを評価し、経験的評価は、提案されたアーキテクチャがタスク指向の対話システムのパフォーマンスの向上に役立つことを示しています。 
[要約]このモデルは、マルチタスク学習ディレクティブで訓練されたテキスト生成とともにユーザーの意図をさらに統合し、出力として間違ったエンティティを生成することを罰するための追加の正則化手法を提供します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-28">
        <br>2020-01-28
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Towards robust word embeddings for noisy texts -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.CL/paper_1.html">
      Towards robust word embeddings for noisy texts
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの新しい埋め込みは、標準的なテキストでの良好なパフォーマンスを維持しながら、幅広い評価タスクでのノイズの多いテキストに関する最新のパフォーマンスよりも優れています。ソーシャルメディアからのツイートや他の種類の非標準的な文章の形式のノイズの多いテキストによってもたらされる困難を無視します。私たちの知る限り、これは単語埋め込みでこのタイプのノイズの多いテキストを扱う最初の明示的なアプローチです。語彙外の単語のサポートを超えるレベル。 
[概要]ブリッジグラムの概念を導入するスキップグラムモデルの単純な拡張を提案します。単語は、標準の単語とそのノイズのあるバリアント間の類似性を強化するためにモデルに追加される人工的な単語です。これは、このタイプのノイズの多いテキストを処理する最初の評価アプローチです
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-25">
        <br>2019-11-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Deep Neural Framework for Contextual Affect Detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.CL/paper_2.html">
      A Deep Neural Framework for Contextual Affect Detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      提案されているCADフレームワークは、Gated Recurrent Unit（GRU）に基づいています。GRUは、コンテキストワードの埋め込みやその他のさまざまな手作りの機能セットによってさらに支援されます。評価と分析により、モデルが最先端の方法よりも優れていることが示唆されますFriendsとEmotionPushデータセットでは、それぞれ5.49％と9.14％増加します。感情を持たない短くシンプルなテキストは、文脈とともに読むときに強い感情を表すことができます。つまり、同じ文は、そのコンテキスト。 
[要約]文脈的影響検出（cad）フレームワークを提案します。それは文中の単語の相互依存性を学習し、同時に対話中の文の相互依存性を学習します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-28">
        <br>2020-01-28
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Extraction of Templates from Phrases Using Sequence Binary Decision
  Diagrams -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.CL/paper_3.html">
      Extraction of Templates from Phrases Using Sequence Binary Decision
  Diagrams
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      Relaxed SeqBDD構築中のテキスト内の共有構造の圧縮プロセスは、当然、抽出したいテンプレートを誘導します。この論文の主な貢献は、SeqBDD構築アルゴリズムのリラックスした形式であり、データ量..実験は、このメソッドがコーパスからの動詞+前置詞テンプレートとソーシャルメディアからのショートメッセージからの句節テンプレートに基づくタスクで高品質の抽出が可能であることを示しています。 
[概要]この論文の主な貢献は、seqbdd構築アルゴリズムの緩和された形式です。これは、視覚化ダイアグラム（seqbdd）の緩和されたバリアントを使用します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-28">
        <br>2020-01-28
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Deep speech inpainting of time-frequency masks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.CL/paper_4.html">
      Deep speech inpainting of time-frequency masks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      評価結果は、提案されたフレームワークが音声の欠落または歪んだ部分の大部分を回復するのに効果的であることを示しています。これらの問題に対処するために、ここでは時間の欠落またはひどく歪んだ部分を検索するためのエンドツーエンドのフレームワークを紹介します-短期コンテキストからの音声の周波数表現、したがって音声修復。フレームワークは、単語分類タスクで事前に訓練された深層音声特徴抽出器であるspeechVGGを介して得られた、深層特徴損失を介して訓練された畳み込みU-Netに基づいています。 
[概要]このシステムは、音声分類タスクで事前学習された深層音声特徴抽出器であるspeechvggを介して取得された、深層特徴損失で学習された畳み込みu-netに基づいています。データセット
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-20">
        <br>2019-10-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: RTFM: Generalising to Novel Environment Dynamics via Reading -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.CL/paper_5.html">
      RTFM: Generalising to Novel Environment Dynamics via Reading
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、txt2 $ \ pi $を提案します。これは、目標、ドキュメント、および観察の間の3方向の相互作用をキャプチャするモデルです。RTFMでは、txt2 $ \ pi $は、読み取りによるトレーニング中に見られないダイナミクスを持つ新しい環境を一般化します。さらに、このモデルは、FiLMやRTFMの言語条件付きCNNなどのベースラインよりも優れています。 
[ABSTRACT]リーディングポリシー学習者による言語理解は、新しい環境での一般化の有望なモデルです。言語ダイナミクスなど、環境ダイナミクスとダイナミクスの対応する言語記述を手続き的に生成します。新しい環境へ
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-18">
        <br>2019-10-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Generating Representative Headlines for News Stories -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.CL/paper_6.html">
      Generating Representative Headlines for News Stories
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最初に、さまざまなレベルでさまざまな品質と量のバランスを持つ大量のラベルなしコーパスを組み込むマルチレベルの事前トレーニングフレームワークを提案します。人間のラベルを組み込むことでモデルをさらに強化でき、遠隔監視アプローチを大幅に示します。ラベル付きデータの需要を減らします。このアプローチは、2つの技術コンポーネントに集中しています。 
[要旨]文書セットの量化は数十年にわたって研究されてきました。いくつかの研究は、一連の記事の代表的な見出しを生成することに焦点を当てています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-26">
        <br>2020-01-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Speech-VGG: A deep feature extractor for speech processing -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.CL/paper_7.html">
      Speech-VGG: A deep feature extractor for speech processing
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      抽出器のパフォーマンスに対するさまざまなハイパーパラメータの影響を推定するために、speechVGGのいくつかの構成を適用して、インフォームドスピーチインペインティング、時間周波数マスクスピーチセグメントからの欠落部分のコンテキストベースの回復のためのシステムをトレーニングしました。サイズの変更を示します本研究では、音声処理フレームワークをトレーニングするための深い特徴抽出機能であるspeechVGGに関する以前の研究を拡張します。 
[要約]抽出プログラムは、古典的なvgg-対数マグニチュードstft特徴から単語を識別するために再訓練された16個のコンパリューショナルニューラルネットワークに基づいています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-22">
        <br>2019-10-22
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Bringing Stories Alive: Generating Interactive Fiction Worlds -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.CL/paper_8.html">
      Bringing Stories Alive: Generating Interactive Fiction Worlds
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この知識グラフは、テーマ知識を利用して自動的に完成し、他の世界をひっくり返す神経言語生成モデルを導くために使用されます。人間の参加者ベースの評価を行い、知識を抽出および入力する神経モデルの能力をテストします。既存のストーリープロットをインスピレーションとして使用して、場所やオブジェクトなどの世界構造に関する基本情報をエンコードする部分知識グラフを最初に抽出する方法を提示します。 
[ABSTRACT]この作品では、プレイヤーが自然言語を使用して「見る」「話す」インタラクティブなフィクションの世界を作成します。既存のストーリープロットをインスピレーションとして使用し、最初に部分的な知識グラフを抽出する方法を提示します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-28">
        <br>2020-01-28
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multi-modal Sentiment Analysis using Super Characters Method on
  Low-power CNN Accelerator Device -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.CL/paper_9.html">
      Multi-modal Sentiment Analysis using Super Characters Method on
  Low-power CNN Accelerator Device
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      テキストとCL-Aff共有タスクの表形式データ。現在の最先端のアルゴリズムのほとんどはGPUに実装されていますが、電力効率が悪く、導入コストも非常に高くなります。 CNN-DSAにスーパーキャラクターメソッドを実装します。 
[概要] cnnドメイン固有のアクセラレータ（cnn-dsa）は、低電力と低コストのコンピューティングパワーを提供する大量生産されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-28">
        <br>2020-01-28
      </time>
    </span>
  </h3>
</article>
<!-- paper0: The POLAR Framework: Polar Opposites Enable Interpretability of
  Pre-Trained Word Embeddings -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.CL/paper_10.html">
      The POLAR Framework: Polar Opposites Enable Interpretability of
  Pre-Trained Word Embeddings
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      POLAR-セマンティック差分の採用により事前に訓練された単語埋め込みに解釈可能性を追加するフレームワークを導入します。また、私たちのフレームワークは、オラクル、すなわち外部ソースによって提供される極次元のセットから最も識別的な次元を選択することを可能にします。 
[概要]フレームワークによって選択された解釈可能な次元は、人間の判断と一致します。これらのタスクは、2つの正反対のスケール間の言語の位置に基づいています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-27">
        <br>2020-01-27
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Europarl-ST: A Multilingual Corpus For Speech Translation Of
  Parliamentary Debates -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/cs.CL/paper_11.html">
      Europarl-ST: A Multilingual Corpus For Speech Translation Of
  Parliamentary Debates
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このコーパスは、Creative Commonsライセンスの下でリリースされており、自由にアクセスおよびダウンロードできます。この新しいリソースの可能性を際立たせる自動音声認識、機械翻訳、音声言語翻訳の実験を行っています。 
[要旨]論文では、このリソースの可能性を強調する一連の言語を紹介しています。これは、ヨーロッパおよびヨーロッパの研究者の研究です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-08">
        <br>2019-11-08
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: CLCNet: Deep learning-based Noise Reduction for Hearing Aids using
  Complex Linear Coding -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/eess.AS/paper_0.html">
      CLCNet: Deep learning-based Noise Reduction for Hearing Aids using
  Complex Linear Coding
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      第二に、複雑なスペクトログラム入力と係数出力を組み込むフレームワークを提案します。ノイズリダクションは、現代の補聴器の重要な部分であり、ほとんどの市販のデバイスに含まれています。低遅延およびオンライン処理。 
[要約]-深層学習-最先端のアルゴリズムが見つかりません。これらには、リアルタイムおよび周波数分解能、または低品質が含まれます。これは、非常にノイズの多い条件下で低品質につながる可能性があります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-28">
        <br>2020-01-28
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Deep speech inpainting of time-frequency masks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/eess.AS/paper_1.html">
      Deep speech inpainting of time-frequency masks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      評価結果は、提案されたフレームワークが音声の欠落または歪んだ部分の大部分を回復するのに有効であることを示しています。具体的には、LibriSpeechデータセットを使用して評価されるSTOIおよびPESQ客観的メトリックの顕著な改善をもたらします音声の時間周波数表現の欠落部分またはひどく歪んだ部分を短期コンテキストから検索するためのエンドツーエンドのフレームワークを導入します。したがって、音声の修復を行います。 
[概要]このシステムは、音声分類タスクで事前学習された深層音声特徴抽出器であるspeechvggを介して取得された、深層特徴損失で学習された畳み込みu-netに基づいています。データセット
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-20">
        <br>2019-10-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Deep Xi as a Front-End for Robust Automatic Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/eess.AS/paper_2.html">
      Deep Xi as a Front-End for Robust Automatic Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験的調査により、DeepXiはフロントエンドとして、最近のマスキングおよびマッピングベースのディープラーニングフロントエンドよりも低いワードエラー率を生成できることが示されています。複数のSNRレベルで。堅牢な自動音声認識（ASR）の現在のフロントエンドには、マスキングおよびマッピングベースの音声強化への深層学習アプローチが含まれます。 
[要旨]ディープxiは実行可能なフロントエンドであり、asrシステムの堅牢性を大幅に向上させることができます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-18">
        <br>2019-06-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Time-Domain Audio Source Separation Based on Wave-U-Net Combined with
  Discrete Wavelet Transform -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/eess.AS/paper_3.html">
      Time-Domain Audio Source Separation Based on Wave-U-Net Combined with
  Discrete Wavelet Transform
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらの問題の影響はトレーニングによって減少する可能性がありますが、より信頼性の高いソース分離方法を実現するために、問題を克服できるDSレイヤーを設計する必要があります。音楽ソース分離の実験は、提案された方法の有効性と同時の重要性を示しますアンチエイリアシングフィルターと完全な再構成プロパティを考慮します。この信念では、DWTにアンチエイリアシングフィルターと完全な再構成プロパティがあるという事実に焦点を当てて、提案されたレイヤーを設計します。 
[概要]提案された方法は、最新のディープニューラルネットワークの1つであるwave-u-netに基づいており、data.itを分析して、これらの問題の影響を再現できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-28">
        <br>2020-01-28
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Speech-VGG: A deep feature extractor for speech processing -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/eess.AS/paper_4.html">
      Speech-VGG: A deep feature extractor for speech processing
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      抽出器のパフォーマンスに対するさまざまなハイパーパラメータの影響を推定するために、speechVGGのいくつかの構成を適用して、インフォームドスピーチインペインティングのシステムをトレーニングしました。このフレームワークのアプリケーションは通常、有益な結果をもたらしますが、転送可能な音声の特徴を抽出するための最適な設定は何かという質問があります。計算損失は未調査のままです。 
[要約]抽出プログラムは、古典的なvgg-対数マグニチュードstft特徴から単語を識別するために再訓練された16個のコンパリューショナルニューラルネットワークに基づいています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-22">
        <br>2019-10-22
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Europarl-ST: A Multilingual Corpus For Speech Translation Of
  Parliamentary Debates -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/eess.AS/paper_5.html">
      Europarl-ST: A Multilingual Corpus For Speech Translation Of
  Parliamentary Debates
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このコーパスは、Creative Commonsライセンスの下でリリースされており、自由にアクセスおよびダウンロードできます。このペーパーでは、ヨーロッパ言語6か国語のSLTのペアのオーディオテキストサンプルを含む新しい多言語SLTコーパスEuroparl-STを紹介します。このコーパスは、2008年から2012年の間に欧州議会で開催された討論を使用して編集されました。
[概要]論文では、このリソースの可能性を強調する一連の言語を紹介しています。ヨーロッパおよびヨーロッパの研究者の仕事
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-08">
        <br>2019-11-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Singing Voice Conversion with Disentangled Representations of Singer and
  Vocal Technique Using Variational Autoencoders -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/eess.AS/paper_6.html">
      Singing Voice Conversion with Disentangled Representations of Singer and
  Vocal Technique Using Variational Autoencoders
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      定量分析と変換されたスペクトログラムの視覚化の両方は、モデルが歌手のアイデンティティとボーカルテクニックを解き、これらの属性の変換を正常に実行できることを示しています。提案されたモデルは、非並列コーパスでトレーニングされ、 -多くの変換、および変分オートエンコーダーの最近の進歩を活用します。私たちの知る限り、これは深層学習アプローチに基づいて歌手アイデンティティとボーカル技術の変換に共同で取り組む最初の作品です。 
[概要]提案されたモデルは、非並列コーパスでトレーニングされ、多くのもつれた変換に対応します。別のモデルは、非並列コーパスでトレーニングされ、多対セントの変換に対応します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-03">
        <br>2019-12-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: HI-MIA : A Far-field Text-Dependent Speaker Verification Database and
  the Baselines -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/eess.AS/paper_7.html">
      HI-MIA : A Far-field Text-Dependent Speaker Verification Database and
  the Baselines
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、トレーニング用にシングルチャネルデータとマルチチャネルデータの両方を採用する、エンドツーエンドニューラルネットワークベースのベースラインシステムのセットを提案します。データベースには、遠距離シナリオ用に設計された部屋の340人の記録が含まれます。 ..録音は、異なる方向とスピーカーまでの距離にある複数のマイクアレイと、忠実度の高い近接マイクによってキャプチャされます。 
[概要]ファーフィールドマイクアレイベースのスピーカー検証のデータ要件を満たすことを目指しています。公開されているデータベースのほとんどは、シングルチャンネルクローズ-会話とテキスト-独立しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-03">
        <br>2019-12-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Short-Range Audio Channels Security: Survey of Mechanisms, Applications,
  and Research Challenges -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/eess.AS/paper_8.html">
      Short-Range Audio Channels Security: Survey of Mechanisms, Applications,
  and Research Challenges
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最後に、短距離オーディオチャンネルセキュリティのコンテキストで開かれた研究課題を提供し、学界と産業界の両方からの貢献を求めます。さらに、短距離オーディオチャンネルの使用に由来する長所と短所も指摘します。短距離のオーディオチャンネルには、いくつかの顕著な特徴があります。使いやすさ、展開コストの低さ、周波数の調整が簡単であるなどです。 
[概要]セキュリティサービス用の短距離オーディオチャネルが提案されています。これは、セキュリティコンテキストへのシームレスな適応性のためです。ただし、多くの技術が最近提案されています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-09">
        <br>2020-01-09
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: The Prorenin Receptor and its Soluble Form Contribute to Lipid Homeostasis -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-29/biorxiv.physiology/paper_0.html">
      The Prorenin Receptor and its Soluble Form Contribute to Lipid Homeostasis
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      肝臓のPRR欠失は体重を変化させなかったが、肝臓の重量を増加させた。肝臓PRR KOマウスは、対照マウスよりも高い血漿コレステロールレベルおよび低い肝臓LDLRタンパク質を示した。3T3-L1細胞では、sPRR治療はsPARRが刺激することを示すPPARを上方制御した脂肪細胞分化のマスターレギュレーター。 
[要旨]私たちは以前、プロレニン受容体（prr）を肝臓脂肪症の潜在的な要因として特定しました。以前、その神経外科医の神経外科医を肝臓脂肪症の原因として特定しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-28">
        <br>2020-01-28
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
