<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-08-04の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Exploring Deep Hybrid Tensor-to-Vector Network Architectures for
  Regression Based Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.SD/paper_0.html">
      <font color="black">Exploring Deep Hybrid Tensor-to-Vector Network Architectures for
  Regression Based Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">次に、エジンバラの騒々しい音声コーパスに関する実験的証拠を提供して、単一チャネルの音声強調では、CNNがモデルサイズの小さな増分を犠牲にしてDNNよりも優れていることを示します。CNN-TTは、下部のいくつかの畳み込み層で構成されています。特徴抽出で音声品質を改善し、上部にテンソルトレイン（TT）出力レイヤーを追加してモデルパラメーターを削減します。まず、畳み込みニューラルネットワーク（CNN）に基づくベクトルへの一般化力の新しい上限を導出します。ベクトル回帰モデル。 
[ABSTRACT] cnn-ttのハイブリッドアーキテクチャは、モデルファクターサイズを小さくしても、高品質のパフォーマンスを維持できます。cnn-さらに、cnn-は、モデルパラメータ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-25">
        <br><font color="black">2020-07-25</font>
      </time>
    </span>
</section>
<!-- paper0: Lite Audio-Visual Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.SD/paper_1.html">
      <font color="black">Lite Audio-Visual Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">このシステムには2つの視覚的データ圧縮技術が含まれており、視覚的特徴抽出ネットワークをトレーニングモデルから削除して、オンライン計算の効率を高めています。この研究では、これらの問題に対処するためにLite AVSE（LAVSE）システムを提案します。実験結果は、視覚データ圧縮のための2つの手法の有効性を確認します。 
[要約]システムには2つの視覚的データ圧縮技術が含まれています。結果は2つの技術の有効性を確認します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-24">
        <br><font color="black">2020-05-24</font>
      </time>
    </span>
</section>
<!-- paper0: Exploiting Deep Sentential Context for Expressive End-to-End Speech
  Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.SD/paper_2.html">
      <font color="black">Exploiting Deep Sentential Context for Expressive End-to-End Speech
  Synthesis</font>
    </a>
  </h2>
  <font color="black">私たちのコンテキストエクストラクターは、最初に異なるSANレイヤーから韻律関連のセンテンスコンテキスト情報を収集し、それらを集約して包括的な文章表現を学習し、最終的に生成される音声の表現力を高めます。具体的には、コンテキスト集約の2つの方法を調査します。1）直接集約これは、さまざまなSANレイヤーの出力を直接連結します。2）マルチヘッドアテンションを使用して、さまざまなSANレイヤーの寄与を自動的に学習する加重集計です。 、および加重集計は、表現力のモデリングにおいてより優れています。 
[要約] seq2seqフレームワークは、テキストエンコーダーのみから韻律情報を抽出します。これは、表現力豊かなコンテンツの平均的な表現に簡単に折りたたむことができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Audiovisual Speech Synthesis using Tacotron2 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.SD/paper_3.html">
      <font color="black">Audiovisual Speech Synthesis using Tacotron2</font>
    </a>
  </h2>
  <font color="black">出力音響機能は、WaveRNNを調整して音声波形を再構築するために使用され、出力フェイシャルコントローラーは、対応する話し顔のビデオを生成するために使用されます。2つのシステムのパフォーマンスを分析し、グラウンドトゥルースビデオと比較します。主観的な評価テストを使用します。2番目の視聴覚音声合成システムはモジュール化されており、従来のTacotron2を使用してテキストから音響音声が合成されます。 
[要約] 2つの生体視覚音声合成システムが顔認識と比較されています。これらは、感情的な声を作成するために使用される2つの顔の顔のフェイシャルと比較されます。2つのシステムを分析および分析して、人間が顔認識をより簡単に使用できるようにすることができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: A fully recurrent feature extraction for single channel speech
  enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.SD/paper_4.html">
      <font color="black">A fully recurrent feature extraction for single channel speech
  enhancement</font>
    </a>
  </h2>
  <font color="black">この目的のために、CNNレイヤーを抽出する機能に反復係数を追加して、シングルチャネル音声拡張のための堅牢なコンテキスト認識機能抽出戦略を導入します。たたみ込みニューラルネットワーク（CNN）モジュールは、ハイエンドの音声拡張を構築するために広く使用されていますニューラルモデル..抽出された特徴でのノイズ属性のローカル統計のキャプチャにロバストであるため、提案されたモデルは、非常に騒々しい状況でも、音声キューの区別に非常に効果的です。 
[ABSTRACT] cnnモジュールの特徴抽出能力は、ネットワークのノイズコンテキストを適切にモデル化できませんでした。新しいモデルは、非常に騒々しい状況でも、音声キューの区別に非常に効果的です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-09">
        <br><font color="black">2020-06-09</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Deep Learning on Image Denoising: An overview -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.IV/paper_0.html">
      <font color="black">Deep Learning on Image Denoising: An overview</font>
    </a>
  </h2>
  <font color="black">次に、量的および質的分析の観点から、公共のノイズ除去データセットの最先端の方法を比較します。最初に、加法性白色ノイズ画像の深い畳み込みニューラルネットワーク（CNN）を分類します。実際のノイズの多い画像の深いCNN。ブラインドノイズ除去のディープCNNとハイブリッドノイズ画像のディープCNN。これは、ノイズの多いぼやけた画像と低解像度画像の組み合わせを表します。ディープラーニングに基づく最適化モデルは、実際のノイズの推定に効果的です。 
[要旨]さまざまなタイプのディープラーニングメソッドには大きな違いがあります。これらには、ワイヤレス分析とこれらのタイプの分析が含まれます。これらのタイプは、ディープラーニングテクニックを使用してマッピングされます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-31">
        <br><font color="black">2019-12-31</font>
      </time>
    </span>
</section>
<!-- paper0: VTGNet: A Vision-based Trajectory Generation Network for Autonomous
  Vehicles in Urban Environments -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.IV/paper_1.html">
      <font color="black">VTGNet: A Vision-based Trajectory Generation Network for Autonomous
  Vehicles in Urban Environments</font>
    </a>
  </h2>
  <font color="black">シーンを理解するために正面カメラの画像から時空間特徴を抽出し、数秒後に衝突のない軌道を生成できます。最近、パフォーマンスがよく、新しい環境に一般化するエンドツーエンドの駆動方法が登場しました。エクスポートで提供されたデータから直接学習することによって。ただし、このトピックに関するこれまでの多くの方法では、運転行動の信頼性と運転ミスから回復する能力のチェックを怠っています。 
[ABSTRACT]エンドツーエンドの駆動方法が登場しました。これは、パフォーマンスがよく、新しい環境に一般化されます。この方法は、エクスポートで提供されたデータから学習することで開発できます。次のようなシステムの効果をテストするために使用できます。衝突回避のために減速する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-27">
        <br><font color="black">2020-04-27</font>
      </time>
    </span>
</section>
<!-- paper0: Multiscale assay of unlabeled neurite dynamics using phase imaging with
  computational specificity (PICS) -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.IV/paper_2.html">
      <font color="black">Multiscale assay of unlabeled neurite dynamics using phase imaging with
  computational specificity (PICS)</font>
    </a>
  </h2>
  <font color="black">次に、この計算によって推測された蛍光画像を使用して、セマンティックセグメンテーションマップを生成し、ラベル付けされていない生きている神経培養の細胞内コンパートメントに注釈を付けます。定量的位相イメージングと深い畳み込みニューラルネットワークを介して関連する蛍光信号を推定することにより、抗体染色特異性を用いたラベルフリーの生細胞イメージングの方法。 
[要約]ニューロンの独特の行動特性により、ニューロンの研究が困難になります。ただし、細胞数などの特徴をアッセイするのは容易ではありません。これらには、抗体染色特異性によるラベルフリーの細胞イメージングの方法が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Photo Cropper and Enhancer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.IV/paper_3.html">
      <font color="black">Deep Photo Cropper and Enhancer</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、新しいタイプの画像強調問題を紹介します。写真クロッパーネットワークでは、空間トランスフォーマーを使用して埋め込み画像を抽出します。提案するアプローチを、ディープ写真クロッパーとディープ画像エンハンサーの2つのディープネットワークに分割します。 
[要約]提案されたタスクは、写真に埋め込まれた画像をトリミングすることです。空間変換を使用して、埋め込まれた画像を抽出します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Speaker dependent articulatory-to-acoustic mapping using real-time MRI
  of the vocal tract -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.IV/paper_4.html">
      <font color="black">Speaker dependent articulatory-to-acoustic mapping using real-time MRI
  of the vocal tract</font>
    </a>
  </h2>
  <font color="black">現在の論文では、話し手固有の方法でrtMRIを入力として使用して、調音-音声変換用のさまざまなDNN（完全に接続された畳み込みおよびリカレントニューラルネットワーク）をトレーニングします。目的（正規化されたMSEおよびMCD）と主観的な測定（知覚テスト）を行い、複数の画像を入力として取り込み、2.8〜4.5 dBのMCDスコアを達成するCNN-LSTMネットワークが好ましいことを示しています。MRIの利点は、「相対」空間が高いことです。解像度：舌、唇、顎の動きだけでなく、他の手法では不可能な、ベロムや咽頭領域もキャプチャできます。 
[ABSTRACT] mriは、他の技法では通常は不可能である、ベロムと咽頭領域をキャプチャできます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Detection of COVID-19 from Chest X-rays using Deep Learning: Comparing
  COGNEX VisionPro Deep Learning 1.0 Software with Open Source Convolutional
  Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.IV/paper_5.html">
      <font color="black">Detection of COVID-19 from Chest X-rays using Deep Learning: Comparing
  COGNEX VisionPro Deep Learning 1.0 Software with Open Source Convolutional
  Neural Networks</font>
    </a>
  </h2>
  <font color="black">この研究では、COGNEXのディープラーニングソフトウェアであるVisionProのディープラーニングを使用して、COVIDxデータセットからこれらの胸部X線を分類します。結果は、COVID-Netの結果および他のさまざまな最先端のディープラーニングモデルと比較されます。オープンソースコミュニティ。VisionProディープラーニングの結果-ROIとして画像全体で94.0％の全体的なFスコアを達成し、セグメント化された肺では95.3％のFスコアを取得します。 COVID-Netおよびその他の最先端のオープンソースの深層学習モデルよりも優れています。 
[要約]ウォータールー大学は、ダーウィンaiとともに、ディープラーニングモデルcovid-netを設計し、covidxと呼ばれるデータセットを作成しました。結果は、オープンソースコミュニティのディープラーニングモデルの結果と比較されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Segmenting overlapped objects in images. A study to support the
  diagnosis of sickle cell disease -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.IV/paper_6.html">
      <font color="black">Segmenting overlapped objects in images. A study to support the
  diagnosis of sickle cell disease</font>
    </a>
  </h2>
  <font color="black">最後に、このアルゴリズムの結果と最新の2つの実験の結果を比較します。1つはこの作業のために特別に開発された新しいデータセット、もう1つは鎌状赤血球症患者の赤血球塗抹標本です。複数のタイプのアルゴリズムを使用して、単純で単純な方法からより複雑な方法まで、この問題に対処します。この作業では、重複オブジェクトのセグメンテーションのための新しい方法を提案します。 
[要約]複数のタイプのアルゴリズムがこの問題に対処するために使用されます。複数のタイプが問題に対処するために見つかります。2つの実験で、1つは新しいデータセットと鎌状赤血球症患者からの赤血球塗抹標本です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Self-evolving ghost imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.IV/paper_7.html">
      <font color="black">Self-evolving ghost imaging</font>
    </a>
  </h2>
  <font color="black">照明アルゴリズムをリアルタイムで最適化して、測定された全光強度に応じてオブジェクトの形状を一致させる遺伝的アルゴリズムを導入します。従来、ゴーストイメージングは、測定された光強度と適用された照明パターンを相関させることにより、オフラインでオブジェクトの画像を取得します。 。静的および動的イメージングのこの概念を理論的および実験的に示します。 
[ABSTRACT]波長帯の作成は困難で費用がかかります。ここでは、フィードバックに基づいて、後処理をバイパスできる結果をオンラインで更新するためのアプローチを紹介します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Scale Deep Compressive Imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.IV/paper_8.html">
      <font color="black">Multi-Scale Deep Compressive Imaging</font>
    </a>
  </h2>
  <font color="black">分解方法（ピラミッド、ウェーブレット、スケール空間を含む）、サンプリング行列、および測定値を分析し、従来のアプローチとディープラーニングベースのアプローチの両方を一貫して上回るMS-DCIの経験的利点を示しました。この作業では、マルチスケールで画像を分解、サンプリング、および再構築することを共同で学習するマルチスケールの深圧縮イメージング（MS-DCI）フレームワーク。3段階のエンドツーエンドのトレーニングスキームが導入され、初期および2つの強化再構成マルチスケールサンプリングの効率を実証し、再構成パフォーマンスをさらに向上させるフェーズ。 
[ABSTRACT] DCIでの調査は単一スケールのサンプリングに限定されています。ネットワークがマルチスケールサンプリングアーキテクチャでマルチスケール機能を学習する方が簡単です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Bias-based Universal Adversarial Patch Attack for Automatic Check-out -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.IV/paper_9.html">
      <font color="black">Bias-based Universal Adversarial Patch Attack for Automatic Check-out</font>
    </a>
  </h2>
  <font color="black">敵対的な例は、ディープニューラルネットワーク（DNN）を簡単に誤解させる知覚できない摂動を伴う入力です。実験結果は、提案されたフレームワークが最新の敵対的なパッチ攻撃方法よりも優れていることを示しています。問題に対処するために、この論文ではバイアスを提案します。モデルの知覚的バイアスと意味的バイアスの両方を利用する強力な汎化能力を持つクラスにとらわれない普遍的な敵対的なパッチを生成するためのフレームワーク。 
[ABSTRACT]小さなローカルパッチに限定されたノイズを持つ敵対的パッチが、現実世界のシナリオでの容易な実現可能性のために登場しました。ユニバーサル攻撃のトレーニングにおける大量のデータへの重い依存をさらに緩和するために、セマンティックバイアスをさらに活用します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br><font color="black">2020-05-19</font>
      </time>
    </span>
</section>
<!-- paper0: Intensity-only Mode Decomposition on Multimode Fibers using a Densely
  Connected Convolutional Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.IV/paper_10.html">
      <font color="black">Intensity-only Mode Decomposition on Multimode Fibers using a Densely
  Connected Convolutional Network</font>
    </a>
  </h2>
  <font color="black">この作業では、121層のDenseNetを使用することにより、6つのモードのハードルを突破できることが初めて示されました。これらの欠点を克服するために、強度でモード分解を実行するニューラルネットワークが提案されています。マルチモードファイバーファセットのカメラのみの記録。伝送行列を測定するには、マルチモードファイバーの個々のモードを入力で順次励起し、モード分解を出力で実行します。 
[ABSTRACT]モード分解は通常、デジタルホログラフィーを使用して実行されます。この方法は、実験的に10モードのモード分解によって示されます。デジタルホログラフィーを使用する従来のアプローチと定量的に比較されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Tensorizing GAN with High-Order Pooling for Alzheimer's Disease
  Assessment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.IV/paper_11.html">
      <font color="black">Tensorizing GAN with High-Order Pooling for Alzheimer's Disease
  Assessment</font>
    </a>
  </h2>
  <font color="black">生成されたサンプルの視覚化は、提案されたモデルが半教師付き学習目的のためのもっともらしいサンプルを生成できることも示しています。高次プーリング方式を分類子に組み込むことにより、提案されたモデルは、全体的な磁気共鳴画像（MRI）画像。3プレイヤーの協力型ゲームベースのフレームワークをテンソル化することにより、提案されたモデルは、脳の構造情報から利益を得ることができます。 
[要約]高次プーリングを備えた新しいタイプのガンが軽度認知障害（mci）を評価するために提案されており、提案されたモデルはホリスティック磁気共鳴イメージング（mri）画像の2次統計を十分に活用できる</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Color Texture Image Retrieval Based on Copula Multivariate Modeling in
  the Shearlet Domain -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.IV/paper_12.html">
      <font color="black">Color Texture Image Retrieval Based on Copula Multivariate Modeling in
  the Shearlet Domain</font>
    </a>
  </h2>
  <font color="black">KLDの対称バージョンであるJeffery divergence（JD）基準は、提案されたフレームワークの類似性を調査するために使用されます。4つのテクスチャ画像検索ベンチマークデータセットで実験を実装し、その結果は提案された提案の優位性を示しています。既存の最先端の方法に対するフレームワーク。さらに、提案されたフレームワークの検索時間は、特徴抽出と類似性マッチングの2つのステップでも分析されます。これは、提案されたフレームワークが適切な検索時間を享受していることも示しています。 。 
[ABSTRACT]提案されたフレームワークでは、ガウスコピュラを使用して、提案されたサブsus-ususususususususususususususususususususususususususususususususususuのサブバンド間の依存関係をモデル化します。これらのタイプは、参加の限界モデリングに使用されます。kldは、調査対象の対称モデルの対称モデルですフレームワーク</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.IV/paper_13.html">
      <font color="black">The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study</font>
    </a>
  </h2>
  <font color="black">これにより、レート、歪み、分類精度の間の相互作用を考慮した統合フレームワークを提供できます。さらに複雑な考慮事項は、特定の分類子による認識パフォーマンスへの圧縮の影響（精度）です。ケーススタディとして、 JPEG圧縮規格の量子化テーブルの設計。 
[ABSTRACT]これにより、割り当てられたビットバジェット（レート）と元のイメージへの忠実性（歪み）の間に避けられない緊張が生じます。この作業は、このレート-歪み-精度の設計を調査することを目的としています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Branch Tensor Network Structure for Tensor-Train Discriminant
  Analysis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.IV/paper_14.html">
      <font color="black">Multi-Branch Tensor Network Structure for Tensor-Train Discriminant
  Analysis</font>
    </a>
  </h2>
  <font color="black">ただし、このモデルには、高メモリや計算コストを含む大きなテンソルに対するいくつかの制限があります。このアプローチは、計算時間、ストレージコスト、分類精度に関して画像とビデオの分類タスクで評価され、ベクトルとテンソルに基づく判別の両方と比較されます。分析方法..テンソル分解と低ランクテンソル近似の分野では多くの作業が行われていますが、教師あり学習、特徴抽出、分類への拡張はまだ制限されています。 
[要約]提案されたアプローチは、ネットワークの柔軟性を利用して、ttdaのさまざまな効率バージョンを実装します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-15">
        <br><font color="black">2019-04-15</font>
      </time>
    </span>
</section>
<!-- paper0: Automated Segmentation of Brain Gray Matter Nuclei on Quantitative
  Susceptibility Mapping Using Deep Convolutional Neural Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.IV/paper_15.html">
      <font color="black">Automated Segmentation of Brain Gray Matter Nuclei on Quantitative
  Susceptibility Mapping Using Deep Convolutional Neural Network</font>
    </a>
  </h2>
  <font color="black">実験結果により、QSMとT $ _ \ text {1} $加重イメージング（T $ _ \ text {1} $ WI）を入力として併用することにより、提案された方法は、単一ブランチよりも優れたセグメンテーション精度を達成できることが判明しましたカウンターパート、ならびに従来のアトラスベースの方法と古典的な3D-UNet構造。この論文では、3D畳み込みニューラルネットワーク（CNN）に基づく二重分岐残差構造U-Net（DB-ResUNet）を提案しました。このような脳の灰白質の核を自動的にセグメント化します。セグメンテーションの精度とメモリ効率のトレードオフを改善するために、提案されたDB-ResUNetは、高解像度の画像パッチと低解像度で視野が大きいパッチをローカルブランチとグローバルブランチにフィードしました。それぞれ。 
[ABSTRACT]提案されたdb-resunetは、脳サイズの脳サイズの脳麻痺に乱れた脳を与えました。提案された方法は、磁化率を測定することです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Rethinking Image Deraining via Rain Streaks and Vapors -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.IV/paper_16.html">
      <font color="black">Rethinking Image Deraining via Rain Streaks and Vapors</font>
    </a>
  </h2>
  <font color="black">雨ストリークの透過マップを学習するために、SNetという名前のエンコーダーデコーダーCNNを提案します。雨ストリークはさまざまな形と方向で表示されるため、SNet内のShuffleNetユニットを使用して異方性表現をキャプチャします。蒸気は雨ストリークによってもたらされるため、我々は雨の縞のそれに基づいてマルチスケールでの蒸気の透過マップを予測するために空間ピラミッドプーリング（SSP）を含むVNetを提案します。 
[ABSTRACT]画像は画像復元用の高度なモデルによって作成されました。ただし、雨ストリークは伝送媒体ではなく背景と同じプロパティと見なされます。雲が現れるので、snet内のシャッフルネットユニットを使用して異方性表現をキャプチャします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: The End-of-End-to-End: A Video Understanding Pentathlon Challenge (2020) -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_0.html">
      <font color="black">The End-of-End-to-End: A Video Understanding Pentathlon Challenge (2020)</font>
    </a>
  </h2>
  <font color="black">コンピュータービジョンとパターン認識（CVPR）2020に関するIEEE会議と併せて開催されたオープンコンペティションである、ペンタスロンチャレンジという新しいビデオを紹介します。このレポートでは、初版のチャレンジの結果と参加者の調査結果をまとめています。 ..課題の目的は、テキストからビデオへの取得、つまり自然言語クエリを使用してビデオのコーパス内のコンテンツを検索するタスクの新しい方法を探索して評価することでした。 
[要約]課題の目標は、テキストからコンピュータへの取得のための新しい方法を探索および評価することでした</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Isotropy Maximization Loss and Entropic Score: Accurate, Fast,
  Efficient, Scalable, and Turnkey Neural Networks Out-of-Distribution
  Detection Based on The Principle of Maximum Entropy -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_1.html">
      <font color="black">Isotropy Maximization Loss and Entropic Score: Accurate, Fast,
  Efficient, Scalable, and Turnkey Neural Networks Out-of-Distribution
  Detection Based on The Principle of Maximum Entropy</font>
    </a>
  </h2>
  <font color="black">IsoMax損失は、シームレスなSoftMax損失のドロップイン交換として機能し、ソリューション全体を正確、高速、効率的、スケーラブル、およびターンキーに保ちます。そのため、等方性（距離ベース）で高エントロピーを生成するIsoMax損失を提案します（低信頼性）クロスエントロピー最小化に依拠しているにもかかわらず、事後確率分布。私たちの実験は、ニューラルネットワークのOOD検出パフォーマンスが、敵対的なトレーニングや検証、データ増強、アンサンブル法、生成アプローチ、モデルのアーキテクチャの変更、メトリックの学習、または追加の分類子または回帰。 
[要約]ニューラルネットワークのフード検出パフォーマンスが低いのは、クロスシンプソン損失の異方性と極端な傾向が原因です。フード検出の迅速なエントロピースコアを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-15">
        <br><font color="black">2019-08-15</font>
      </time>
    </span>
</section>
<!-- paper0: Who Make Drivers Stop? Towards Driver-centric Risk Assessment: Risk
  Object Identification via Causal Inference -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_2.html">
      <font color="black">Who Make Drivers Stop? Towards Driver-centric Risk Assessment: Risk
  Object Identification via Causal Inference</font>
    </a>
  </h2>
  <font color="black">リスクオブジェクト識別と呼ばれる新しいタスクが導入されました。このタスクを原因効果問題として定式化し、提案されたオブジェクトレベルの操作可能な運転モデルとの因果関係推論に基づく新しい2段階リスクオブジェクト識別フレームワークを提示します。強力なベースラインと比較して平均パフォーマンスが7.5％大幅に向上しました。 
[ABSTRACT]衝突は潜在的なリスクの発生源にすぎず、より一般的な定義が必要です。リスク効果効果効果識別と呼ばれる新しいタスク-導入</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-05">
        <br><font color="black">2020-03-05</font>
      </time>
    </span>
</section>
<!-- paper0: AE TextSpotter: Learning Visual and Linguistic Representation for
  Ambiguous Text Spotting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_3.html">
      <font color="black">AE TextSpotter: Learning Visual and Linguistic Representation for
  Ambiguous Text Spotting</font>
    </a>
  </h2>
  <font color="black">1（c））..テキストの検出に視覚的な機能のみを使用した以前の作品とは異なり、この作品は、テキストのあいまいさを大幅に減らすために視覚的機能と言語的機能の両方を学習する、あいまいさ除去テキストスポッター（AE TextSpotter）という新しいテキストスポッターを提案します。検出..「ベルリン」は、図の「BERL」および「IN」として誤って検出されます。
[要約]文字間の間隔が大きい場合、通信が頻繁に発生します。これにより、文字の視覚的にもっともらしいグループが多数作成されます。提案されたaeテキストスポッターには3つの重要な利点</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning on Image Denoising: An overview -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_4.html">
      <font color="black">Deep Learning on Image Denoising: An overview</font>
    </a>
  </h2>
  <font color="black">最初に、加法性の白いノイズの多い画像の深い畳み込みニューラルネットワーク（CNN）を分類します。実際のノイズの多い画像の深いCNN。ブラインドノイズ除去用のディープCNNとハイブリッドノイズ画像用のディープCNN。これは、ノイズの多いぼやけた画像と低解像度画像の組み合わせを表します。ディープラーニング技術は、画像ノイズ除去の分野で多くの注目を集めています。学習は、実際のノイズを推定するのに効果的です。 
[要旨]さまざまなタイプのディープラーニングメソッドには大きな違いがあります。これらには、ワイヤレス分析とこれらのタイプの分析が含まれます。これらのタイプは、ディープラーニングテクニックを使用してマッピングされます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-31">
        <br><font color="black">2019-12-31</font>
      </time>
    </span>
</section>
<!-- paper0: VTGNet: A Vision-based Trajectory Generation Network for Autonomous
  Vehicles in Urban Environments -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_5.html">
      <font color="black">VTGNet: A Vision-based Trajectory Generation Network for Autonomous
  Vehicles in Urban Environments</font>
    </a>
  </h2>
  <font color="black">シーンを理解するために正面カメラの画像から時空間特徴を抽出し、数秒後に衝突のない軌道を生成できます。実験結果から、さまざまな気象条件や照明条件下で、私たちのネットワークはさまざまな都市の軌道を確実に生成できることがわかります交差点で曲がったり、衝突を回避するために速度が低下したりするような環境。最近では、エンドツーエンドの運転方法が登場しました。この方法は、エクスポートで提供されるデータから直接学習することにより、適切に機能し、新しい環境に一般化します。 
[ABSTRACT]エンドツーエンドの駆動方法が登場しました。これは、パフォーマンスがよく、新しい環境に一般化されます。この方法は、エクスポートで提供されたデータから学習することで開発できます。次のようなシステムの効果をテストするために使用できます。衝突回避のために減速する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-27">
        <br><font color="black">2020-04-27</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-view Contrastive Learning for Online Knowledge Distillation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_6.html">
      <font color="black">Multi-view Contrastive Learning for Online Knowledge Distillation</font>
    </a>
  </h2>
  <font color="black">画像分類と少数ショット学習に関する実験結果は、追加の推論コストを犠牲にすることなく、MCL-OKDが他の最先端の方法であるOKDとKDの両方を大幅に上回っていることを示しています。既存のオンライン知識蒸留（OKD）は、確率的出力の観点から複数のピアネットワーク間で協調的および相互学習を実行しますが、表現の知識は無視します。MCLの利点は、以前のOKDメソッドよりも分類のためのより特徴的な表現を学習することです。 
[要旨] okdのマルチビューコントラスト学習（mcl）は学習に役立つツールです。複数のピアネットワークによってエンコードされた表現の相関をキャプチャします。mclを使用すると、分類のためのより特徴的な表現を学習できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-07">
        <br><font color="black">2020-06-07</font>
      </time>
    </span>
</section>
<!-- paper0: Geometry-Aware Gradient Algorithms for Neural Architecture Search -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_7.html">
      <font color="black">Geometry-Aware Gradient Algorithms for Neural Architecture Search</font>
    </a>
  </h2>
  <font color="black">私たちのジオメトリ対応フレームワークは、（1）既存の勾配ベースの方法よりも高速な収束保証を享受し、（2）コンピュータビジョンの最新のNASベンチマークで最先端の精度を達成する、シンプルでありながら斬新なアルゴリズムにつながります。特に、 DARTSサーチスペースとNAS-Bench-201の両方で、CIFARとImageNetの両方で最も優れた公開結果を超えています。後者のベンチマークでは、CIFAR-10とCIFAR-100でoracle-optimalに近いパフォーマンスを達成しています。一緒に、理論と実験は、オプティマイザと離散NAS検索スペースの連続パラメータ化を共同設計する原理的な方法を示しています。 
[ABSTRACT]新しいnasベンチマークは、コンピュータービジョンの最新のnasベンチマークで公開されています。このシステムは、ユーザーが重力のシステムを利用できるシステムに基づいています。ただし、新しい方法を使用して、システム</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-16">
        <br><font color="black">2020-04-16</font>
      </time>
    </span>
</section>
<!-- paper0: Predictive online optimisation with applications to optical flow -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_8.html">
      <font color="black">Predictive online optimisation with applications to optical flow</font>
    </a>
  </h2>
  <font color="black">これは、メソッドが漸近的に最小化するモデルに影響を与えます。対応する予測オンライン主双対近位分割メソッドを導入します。オンライン最適化は、問題がまだ解決されている間に導入される新しいデータを中心に展開します。より多くのトレーニングサンプルが利用可能になったら、ディープラーニングを考えてください。 
[ABSTRACT]オプティカルフローを使用したビデオ処理などの瞬時の逆問題に適応します。ビデオフレームはアルゴリズムの反復に完全に似ています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-08">
        <br><font color="black">2020-02-08</font>
      </time>
    </span>
</section>
<!-- paper0: Measuring Human and Economic Activity from Satellite Imagery to Support
  City-Scale Decision-Making during COVID-19 Pandemic -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_9.html">
      <font color="black">Measuring Human and Economic Activity from Satellite Imagery to Support
  City-Scale Decision-Making during COVID-19 Pandemic</font>
    </a>
  </h2>
  <font color="black">このCNNアンサンブルフレームワークは、米国国防総省のxViewチャレンジ、衛星画像でのオブジェクト検出の最も高度なベンチマークで3位にランクインしました。COVID-19の発生前後のさまざまなサイトの実際の例の結果も示し、可能性を示しています。回復プロセスも大雑把になると予想されます。 
[ABSTRACT]経済活動は社会行動に影響を及ぼし、分類可能な衛星画像に署名を残します。システムは、それに基づいて自動的に分類された経済指標を計算するために使用できます。これらの衛星画像が初めて識別されたのはこのときです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-16">
        <br><font color="black">2020-04-16</font>
      </time>
    </span>
</section>
<!-- paper0: Anti-Bandit Neural Architecture Search for Model Defense -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_10.html">
      <font color="black">Anti-Bandit Neural Architecture Search for Model Defense</font>
    </a>
  </h2>
  <font color="black">大規模な実験により、ABanditNASは他のNAS手法よりも高速であり、PGD- $ 7 $でCIFAR-10の先行技術よりも$ 8.73 \％$向上します。評価のみにUCBを使用する従来のバンディットアルゴリズムとは異なり、UCBを使用して探索効率の武器と武器間の公正な競争のためのLCB。結果として得られる盗難防止NAS（ABanditNAS）には、信頼限界の下限と上限（LCBとUCB）に基づく新しい運用評価指標と探索プロセスが組み込まれています。 
[ABSTRACT]ニューラルアーキテクチャ検索（nas）は、ノイズ除去ブロック、無重量演算、ガボールフィルター、およびたたみ込みの包括的な検索に基づいています。システムは、刺激的なブロックと無重み演算の高度な検索を使用します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Photo Cropper and Enhancer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_11.html">
      <font color="black">Deep Photo Cropper and Enhancer</font>
    </a>
  </h2>
  <font color="black">写真エンハンサーでは、超解像を使用して埋め込み画像のピクセル数を増やし、ピクセルのストレッチと歪みの影響を減らします。写真クロッパーネットワークでは、空間変換を使用して埋め込み画像を抽出します。クロッパーには画像の特徴とグラウンドトゥルース間のコサイン距離損失を使用し、エンハンサーには平均二乗損失を使用します。 
[要約]提案されたタスクは、写真に埋め込まれた画像をトリミングすることです。空間変換を使用して、埋め込まれた画像を抽出します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Complementary Joint Model for Complex Scene Registration and
  Few-shot Segmentation on Medical Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_12.html">
      <font color="black">Deep Complementary Joint Model for Complex Scene Registration and
  Few-shot Segmentation on Medical Images</font>
    </a>
  </h2>
  <font color="black">ピクセル単位の弁別器を使用して、弱監視データの整列領域を強調表示する整列信頼マップを抽出します。これにより、整列不良領域の乱れが重み付けによって抑制されます。セグメンテーションモデルからの出力は、ディープベースの領域制約を実装するために利用されるため、ラベル要件と細かい登録をもたらします。ディープラーニングベースの医用画像登録とセグメンテーションジョイントモデルは、相補性（拡張からのデータまたは登録からの弱く監視されたデータ、セグメンテーションからの領域制約）を利用して、複雑なシーンとショット数の少ない状況で相互の改善をもたらします。 
[ABSTRACT]関節モデルの新しいモデルが導入され、怪我の活動が増加します。その結果、結果を元に戻すことができますが、モデルのさらなる採用は妨げられます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: BCNet: Learning Body and Cloth Shape from A Single Image -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_13.html">
      <font color="black">BCNet: Learning Body and Cloth Shape from A Single Image</font>
    </a>
  </h2>
  <font color="black">コードと一部のデータはhttps://github.com/jby1993/BCNet。で入手できます。単一のメッシュまたはノンパラメトリック表現と比較して、このメソッドは、個別のメッシュでより柔軟な制御を実現し、ポーズ変更、衣服の転送などのアプリケーションを作成できます。 、および衣服のテクスチャマッピングが可能です。モデルをトレーニングするために、グラウンドトゥルースのボディと衣服のジオメトリ、およびペアのカラー画像を使用して、2つの大規模なデータセットを構築します。 
[ABSTRACT]シンプルなファブリック表現を使用して、ボディメッシュから独立した衣服のスキニングウェイトを作成できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-01">
        <br><font color="black">2020-04-01</font>
      </time>
    </span>
</section>
<!-- paper0: Self-supervised Object Tracking with Cycle-consistent Siamese Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_14.html">
      <font color="black">Self-supervised Object Tracking with Cycle-consistent Siamese Networks</font>
    </a>
  </h2>
  <font color="black">ビジュアルオブジェクトトラッキング用のVOTデータセットとビデオオブジェクトセグメンテーション伝播用のDAVISデータセットでの実験は、我々の方法が両方のタスクで以前のアプローチよりも優れていることを示しています。追跡..ビジュアルオブジェクトトラッキングの自己監視学習には、面倒な人間による注釈やオンライントレーニングが不要など、監視学習と比較して貴重な利点があります。 
[ABSTRACT]オブジェクトトラッキングシステムを追跡できるのは今回が初めてです。制御方法を知らせるエンドオブショーを利用しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Multimodal Interaction-aware Motion Prediction for Autonomous Street
  Crossing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_15.html">
      <font color="black">Multimodal Interaction-aware Motion Prediction for Autonomous Street
  Crossing</font>
    </a>
  </h2>
  <font color="black">これらのアプローチはアーバンナビゲーションを実現する重要な要素ですが、そのようなアプローチを採用するロボットの機能は、信号交差点を含む道路のみをナビゲートすることに限定されています。シーンで観測されたすべての交通参加者の将来の状態を予測する相互作用認識軌道推定ストリームIA-TCNN、および信号機認識ストリームAtteNet ..歩道をナビゲートするモバイルロボットの場合、安全に横断できることが不可欠です。通りの交差点。 
[ABSTRACT] our ia-tcnnは、拡張された因果畳み込みを使用して、交差点の道路交差点の安全性を予測します。これらには、信号機認識ストリームからの学習表現が、モーション予測ストリームの推定軌跡と融合されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-08-21">
        <br><font color="black">2018-08-21</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Semantic Data Augmentation for Human Pose Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_16.html">
      <font color="black">Adversarial Semantic Data Augmentation for Human Pose Estimation</font>
    </a>
  </h2>
  <font color="black">パイプライン全体が敵対的な方法で最適化されます。さらに、生成ネットワークを利用して、調整された貼り付け構成を動的に予測する敵対的セマンティックデータ拡張（ASDA）を提案します。は、最も紛らわしい変換を探して、弁別器の損失を増やしますが、弁別器は、生成されたサンプルを入力として受け取り、そこから学習します。 
[ABSTRACT]状態-engeメソッドは、困難なケースの不十分な例に悩まされています。代わりに、セマンティックデータ拡張（sda）を代わりに提案します。代わりに、セマンティックデータ拡張を提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Universal Self-Attention Network for Graph Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_17.html">
      <font color="black">Universal Self-Attention Network for Graph Classification</font>
    </a>
  </h2>
  <font color="black">特に、U2GNNは、自己注意メカニズムとそれに続く反復遷移を使用して強力な集約関数を誘導し、隣接ノードから各ノードのベクトル表現を更新します。その結果、U2GNNはノード間の潜在的な依存関係を効果的に推測し、グラフ構造のより優れたモデリング..グラフ分類にグラフニューラルネットワーク（GNN）を使用する際の制限を検討します。ノード間の依存関係を集約する効率の欠如が原因である、ノード間の依存関係を悪用するメカニズムの欠如です。 
[要旨] u2gnnは、トランスフォーマーネットワークを活用する新しい埋め込みモデルです。これは、もっともらしいノードとグラフのrecreatings.githubを学習するように設計されています。新しいモデルであるu2gnnに、より多くの過食を学習させる</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-26">
        <br><font color="black">2019-09-26</font>
      </time>
    </span>
</section>
<!-- paper0: User independent Emotion Recognition with Residual Signal-Image Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_18.html">
      <font color="black">User independent Emotion Recognition with Residual Signal-Image Network</font>
    </a>
  </h2>
  <font color="black">音楽とEDA信号を含む現在最大の感情データセットであるPMEmoデータセットに対するアプローチを評価します。Res-SINは、個々の感情機能と外部感情ベンチマークを組み合わせて収束を加速します。実験結果は、モデルとバイナリ分類の信頼性を示しています覚醒および価数の次元での精度73.65％および73.43％は、ベースラインとして使用できます。 
[ABSTRACT]このシステムは、個々の感情の特徴と外部の感情のベンチマークを組み合わせたものです。これは、457人の被験者からの7962個のeda信号で大規模な被験者、樹脂を分類する最初の試みです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-10">
        <br><font color="black">2019-08-10</font>
      </time>
    </span>
</section>
<!-- paper0: Pseudo-Rehearsal: Achieving Deep Reinforcement Learning without
  Catastrophic Forgetting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_19.html">
      <font color="black">Pseudo-Rehearsal: Achieving Deep Reinforcement Learning without
  Catastrophic Forgetting</font>
    </a>
  </h2>
  <font color="black">私たちのモデルは、人間のレベルを超えたパフォーマンスを継続しながらAtari 2600ゲームを順次学習し、各ゲームで個別にトレーニングされた独立したモデルと同じように動作します。ドメインと強化学習ドメイン..比較すると、以前の最先端のソリューションは、これらの複雑な深層強化学習タスクを忘れることに対してかなり脆弱です。 
[要約]システムは人間の行動を分析することによって開発されました。これらには、記憶喪失、記憶喪失、記憶喪失が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-12-06">
        <br><font color="black">2018-12-06</font>
      </time>
    </span>
</section>
<!-- paper0: 2018 Robotic Scene Segmentation Challenge -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_20.html">
      <font color="black">2018 Robotic Scene Segmentation Challenge</font>
    </a>
  </h2>
  <font color="black">2018年に、解剖学的オブジェクトと医療機器のセットをセグメント化されたクラスに導入することにより、複雑さを追加しました。課題が過度に複雑になるのを避けるために、脂肪組織が不足しているため、人間の組織よりもはるかに単純なブタのデータを使い続けました多くの臓器を閉塞します。2017年に、ケベックの同じワークショップで、10のチームがロボットの楽器のセグメンテーションデータセットを導入し、ダヴィンチの楽器のバイナリ、関節式部品、タイプのセグメンテーションを実行する課題に参加しました。 
[ABSTRACT]この課題には、現実的な器具の動きと背景としてのより複雑なブタ組織が含まれていました。u-潜水艦やその他の一般的なCNNアーキテクチャの変更により、幅広く対処されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-30">
        <br><font color="black">2020-01-30</font>
      </time>
    </span>
</section>
<!-- paper0: Faster than FAST: GPU-Accelerated Frontend for High-Speed VIO -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_21.html">
      <font color="black">Faster than FAST: GPU-Accelerated Frontend for High-Speed VIO</font>
    </a>
  </h2>
  <font color="black">最近の強力な組み込みグラフィックスプロセッシングユニット（GPU）の導入により、リアルタイムコンピュータービジョンアプリケーションの予期しない改善が可能になりました。さらに、VIOパイプラインに統合された作業を示し、メトリック状態の推定を200fpsまで達成しています。は、特にGPUでの特徴検出の非最大値抑制の問題を再検討し、局所応答の最大値を選択し、空間特徴分布を強制し、特徴を同時に抽出するソリューションを提案します。 
[ABSTRACT]システムにより、アルゴリズムがオンボードで実行できるようになりました。標準のビデオレートを大幅に上回っています。視覚的な機能に重点を置いていますが、検出と追跡は画像データに依存しており、どちらのステップも並列コンピューターに適しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-30">
        <br><font color="black">2020-03-30</font>
      </time>
    </span>
</section>
<!-- paper0: High Throughput Matrix-Matrix Multiplication between Asymmetric
  Bit-Width Operands -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_22.html">
      <font color="black">High Throughput Matrix-Matrix Multiplication between Asymmetric
  Bit-Width Operands</font>
    </a>
  </h2>
  <font color="black">対称オペランドサイズの命令用に設計されたシストリックアレイアーキテクチャを変更して、非対称オペランドサイズの命令をサポートする方法を示します。対称ビット幅オペランド用の既存のSIMD行列乗算命令は、ゼロによる混合精度のオペランドをサポートできますが、または、他のオペランドのサイズと一致するように狭いオペランドを符号拡張すると、オペランドの1つの狭いビット幅の利点を活用できません。提案されている非対称オペランドサイズのSIMD命令は、行列乗算のスループットを2倍向上させます。既存の対称オペランドサイズの命令を使用して取得したスループットと比較して、代表的な機械学習ワークロードの16ビットアキュムレータからの無視できる（0.05％）オーバーフロー。 
[ABSTRACT]提案された非対称-オペランド-改善に使用できます。ビットを含めることができます-オペランドの1つの幅</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Bias-based Universal Adversarial Patch Attack for Automatic Check-out -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_23.html">
      <font color="black">Bias-based Universal Adversarial Patch Attack for Automatic Check-out</font>
    </a>
  </h2>
  <font color="black">以前のパッチは決定の境界により近く、攻撃を促進します。この問題に対処するために、このペーパーでは、バイアスに基づくフレームワークを提案し、クラスにとらわれない普遍的な敵対的なパッチを生成します。モデル..ユニバーサル攻撃のトレーニングで大量のデータへの重い依存をさらに軽減するために、セマンティックバイアスをさらに活用します。 
[ABSTRACT]小さなローカルパッチに限定されたノイズを持つ敵対的パッチが、現実世界のシナリオでの容易な実現可能性のために登場しました。ユニバーサル攻撃のトレーニングにおける大量のデータへの重い依存をさらに緩和するために、セマンティックバイアスをさらに活用します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br><font color="black">2020-05-19</font>
      </time>
    </span>
</section>
<!-- paper0: PP-YOLO: An Effective and Efficient Implementation of Object Detector -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_24.html">
      <font color="black">PP-YOLO: An Effective and Efficient Implementation of Object Detector</font>
    </a>
  </h2>
  <font color="black">主に、モデルパラメーターとFLOPの数をほとんど増加させないさまざまな既存のトリックを組み合わせて、速度がほとんど変わらないようにしながら、検出器の精度を可能な限り向上させるという目標を達成しようとします。YOLOv3は広く実際に使用して、YOLOv3に基づく新しいオブジェクト検出器を開発します。複数のトリックを組み合わせることにより、PP-YOLOは、既存の状態を超えて、有効性（45.2％mAP）と効率（72.9 FPS）のより良いバランスを実現できます。 -EfficientDetやYOLOv4などのアート検出器。ソースコードはhttps://github.com/PaddlePaddle/PaddleDetectionにあります。 
[要約]このペーパーの目標は、新しいアプリケーションシナリオに直接適用できる、比較的バランスの取れた有効性と効率を備えたオブジェクト検出器を実装することです。pp-yoloは、有効性（45. 2％マップ）と効率（ 72.9 fps）、efficientdetやyolov4などの既存の最先端検出器を超える</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: AQD: Towards Accurate Quantized Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_25.html">
      <font color="black">AQD: Towards Accurate Quantized Object Detection</font>
    </a>
  </h2>
  <font color="black">具体的には、マルチレベルバッチ正規化（マルチレベルBN）を使用して、各検出ヘッドのバッチ統計を個別に推定することを提案します。ネットワーク量子化は、重みとアクティブ化のビット幅を下げ、モデルサイズを減らして推論を加速することを目的としていますこれを解決するために、正確な量子化オブジェクト検出（AQD）法を提案します。 
[ABSTRACT]これを解決するために、正確な量子化オブジェクト検出-aqd）方法を提案します。オブジェクト検出でのビット幅量子化の量を減らすには、まだ課題です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_26.html">
      <font color="black">The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study</font>
    </a>
  </h2>
  <font color="black">これにより、レート、歪み、分類精度の間の相互作用を考慮した統一されたフレームワークを提供できます。この作業は、このレート-歪み-精度のトレードオフを調査することを目的としています。追加の複雑な考慮事項は、与えられた認識パフォーマンスに対する圧縮の影響です。分類子（精度）。 
[ABSTRACT]これにより、割り当てられたビットバジェット（レート）と元のイメージへの忠実性（歪み）の間に避けられない緊張が生じます。この作業は、このレート-歪み-精度の設計を調査することを目的としています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: GmFace: A Mathematical Model for Face Image Representation Using
  Multi-Gaussian -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_27.html">
      <font color="black">GmFace: A Mathematical Model for Face Image Representation Using
  Multi-Gaussian</font>
    </a>
  </h2>
  <font color="black">さらに、GmFaceを使用すると、単純なパラメーター計算によって数学的にいくつかの顔画像変換操作を実現できます。次に、GmFaceの問題を変換するために、GmFaceの各パラメーターに対応するパラメーターを使用して、ニューロンとしてガウス関数を使用してGmNetを設計します。 GmNetのネットワーク最適化問題へのパラメーター解決。顔のモデリングプロセスは、次の手順で説明できます。（1）GmNetの初期化。 （2）GmNetに顔画像をフィードする。 （3）収束するまでGmNetをトレーニングする。 （4）GmNetのパラメータを引き出す（GmFaceと同じ）。 （5）顔モデルGmFaceの記録。 
[ABSTRACT]人間の顔の数学的表現は特に困難な作業です。モデルは2歳のガウス関数の利点を使用しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Are Labels Necessary for Neural Architecture Search? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_28.html">
      <font color="black">Are Labels Necessary for Neural Architecture Search?</font>
    </a>
  </h2>
  <font color="black">サンプルベースの実験では、教師ありまたは教師なしの目的で多数（500）の多様なアーキテクチャをトレーニングし、ラベルありとなしで生成されたアーキテクチャランキングが高度に相関していることを確認します。次に、2セットの実験を行います。検索ベースの実験では、監視なしのさまざまな目的を使用して定評のあるNASアルゴリズム（DARTS）を実行し、ラベルなしで検索されたアーキテクチャは、ラベル付きで検索されたものと競合する可能性があることを報告します。 
[ABSTRACT]教師なし画像なしで検索されたアーキテクチャは、ラベルで検索された対応物と競合する可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-26">
        <br><font color="black">2020-03-26</font>
      </time>
    </span>
</section>
<!-- paper0: Machine-learned Regularization and Polygonization of Building
  Segmentation Masks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_29.html">
      <font color="black">Machine-learned Regularization and Polygonization of Building
  Segmentation Masks</font>
    </a>
  </h2>
  <font color="black">最後に、正規化された建物のセグメンテーション結果から建物の角に対応するまばらな結果を予測するように適合されたバックボーンたたみ込みニューラルネットワーク（CNN）をトレーニングします。だけでなく、ポリゴンとしてパラメータ化された視覚的に心地よい建物の輪郭を生成することもできます。入力として画像を取り、まず、一般的な完全畳み込みネットワーク（FCN）を利用して建物のセグメンテーションマップを予測します。 
[ABSTRACT]提案された方法は、視覚的に楽しい建物の輪郭を生成することができます。ポリゴンとして視覚的に楽しい建物の輪郭を生成することもできます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-24">
        <br><font color="black">2020-07-24</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Palmprint Verification On Smartphones -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_30.html">
      <font color="black">Towards Palmprint Verification On Smartphones</font>
    </a>
  </h2>
  <font color="black">結果を完全に再現できるようにするために、ラベル付きのデータセットと関連するソースコードがhttps://cslinzhang.github.io/MobilePalmPrint/で公開されています。しかし、現在、スマートフォンの掌紋検証に特化した研究はまだ散発的です特に、顔や指紋を重視するものと比較した場合。この論文では、前述の研究ギャップを埋めることを目的として、スマートフォンでの掌紋認証の徹底的な調査を行いました。 
[ABSTRACT]スマートフォン用の手のひらのアプリケーションの可能性は深刻に過小評価されています。スマートフォンの手のひらの検証に焦点を当てた研究はまだ散発的です。生体認証は効果的であることが確認されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-30">
        <br><font color="black">2020-03-30</font>
      </time>
    </span>
</section>
<!-- paper0: PIC-Net: Point Cloud and Image Collaboration Network for Large-Scale
  Place Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_31.html">
      <font color="black">PIC-Net: Point Cloud and Image Collaboration Network for Large-Scale
  Place Recognition</font>
    </a>
  </h2>
  <font color="black">比較結果は、画像と点群のコラボレーションが、画像ベースの方法と点群ベースの方法の両方よりも優れていることを示しています。注意戦略と昼夜変換により、パフォーマンスがさらに向上する可能性があります。場所の認識は、オートメーションのホットな研究分野の1つです。技術であり、未解決の問題です。カメラとLidarはこのタスクで使用される2つの主流センサーです。カメラベースの方法は、照明と季節の変化によって簡単に影響を受けます。LIDARは、画像のように豊富なデータを取得できません。この論文では、注意メカニズムを使用して画像と点群の機能を融合し、2つの間の補足情報をマイニングするPIC-Net（点群および画像コラボレーションネットワーク）。さらに、夜間の認識パフォーマンスを向上させるために、夜のイメージを昼間のスタイルに変換します。 
[ABSTRACT]画像を改善するために、画像を昼間のスタイルに変換します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: The pursuit of beauty: Converting image labels to meaningful vectors -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_32.html">
      <font color="black">The pursuit of beauty: Converting image labels to meaningful vectors</font>
    </a>
  </h2>
  <font color="black">情報が豊富であることに加えて、これらの表現は、各画像ラベルが別々のベクトルにエンコードされている絡み合っていない低次元の潜在空間を構成します。一連の実験でこれらの表現の品質を評価し、その結果、提案されたモデルがデータの概念を取り込むことができることが示唆されていますコンピュータビジョンコミュニティの課題は、既存の高レベルの機能に基づいて画像を再構成したり、（半）ラベル付けされたデータセットをより適切に分析したりするために、画像のセマンティクスを理解することです。 
[ABSTRACT]新しい方法はオクルージョンベースの潜在表現（olr）と呼ばれます。これは、画像ラベルを、大量のデータをキャプチャする意味のある表現に変換することを目的としています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Point-Set Anchors for Object Detection, Instance Segmentation and Pose
  Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_33.html">
      <font color="black">Point-Set Anchors for Object Detection, Instance Segmentation and Pose
  Estimation</font>
    </a>
  </h2>
  <font color="black">ポイントセットのユーティリティは、そのスケール、アスペクト比、回転がターゲットにどの程度一致するかに依存するため、これらの変換をサンプリングするアンカーボックス手法を採用して、追加のポイントセット候補を生成します。アンカーをオブジェクト検出、インスタンスセグメンテーション、人間の姿勢推定に設定します。このポイントセットは、ポーズ推定のトレーニングデータのモードなど、所定のタスクの適切な初期化を反映するように配置されています。中心点であり、回帰のためのより有益な機能を提供します。 
[要約]ポイントベースの人間の姿勢推定はシンプルで効率的です。これは、ポイントと呼ばれる新しい方法に基づいています-人間の姿勢を置き換えるセット。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-06">
        <br><font color="black">2020-07-06</font>
      </time>
    </span>
</section>
<!-- paper0: Disentangling Human Error from the Ground Truth in Segmentation of
  Medical Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_34.html">
      <font color="black">Disentangling Human Error from the Ground Truth in Segmentation of
  Medical Images</font>
    </a>
  </h2>
  <font color="black">次に、3つのパブリックメディカルイメージングセグメンテーションデータセットに対して、シミュレーション（必要な場合）および実際の多様なアノテーションを使用して、メソッドの有用性を示します。1）MSLSC（多発性硬化症病変）。 2）BraTS（脳腫瘍）; 3）LIDC-IDRI（肺の異常）。2つの分離は、ノイズの多いトレーニングデータで高い忠実度を達成しながら、推定アノテーターを最大限に信頼できないようにすることで達成されます。実験は、複雑な空間特性をキャプチャする強力な能力も示していますアノテーターの間違いの。 
[ABSTRACT]アルゴリズムはアノテーターからのデータに基づいており、コストは2,000ドルです。これらのアルゴリズムはさまざまな人間の専門家に基づいています。これらのアルゴリズムの結果はラベルの品質に依存すると述べています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-31">
        <br><font color="black">2020-07-31</font>
      </time>
    </span>
</section>
<!-- paper0: Mix Dimension in Poincaré Geometry for 3D Skeleton-based Action
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_35.html">
      <font color="black">Mix Dimension in Poincaré Geometry for 3D Skeleton-based Action
  Recognition</font>
    </a>
  </h2>
  <font color="black">最終的に得られたアーキテクチャを使用して、2つの現在の最大規模の3Dデータセット、つまりNTU RGB + DとNTU RGB + D 120でメソッドを評価します。人間の行動認識では、現在の作業で動的グラフ生成メカニズムが導入され、基礎となるキャプチャが向上していますセマンティックスケルトン接続とパフォーマンスの向上。具体的には、Poincar \ &#39;eジオメトリを介して定義される新しい時空間GCN（ST-GCN）アーキテクチャを提示し、構造の潜在的な解剖学的構造をより適切にモデル化できるようにします。データ。 
[要約]リーマン多様体上に、より効率的なgcnを構築します。これは、グラフデータをモデル化するのにより適切な空間であると考えます。異なる次元を混合して、人間の行動認識の最適な投影を探索します。この方法は、基本的な意味スケルトン接続</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-30">
        <br><font color="black">2020-07-30</font>
      </time>
    </span>
</section>
<!-- paper0: Robust Collaborative Learning of Patch-level and Image-level Annotations
  for Diabetic Retinopathy Grading from Fundus Image -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_36.html">
      <font color="black">Robust Collaborative Learning of Patch-level and Image-level Annotations
  for Diabetic Retinopathy Grading from Fundus Image</font>
    </a>
  </h2>
  <font color="black">フレームワーク全体をエンドツーエンドで最適化することにより、きめの細かい病変と画像レベルのグレード情報を双方向で交換して、DRグレーディングのより特徴的な機能を活用できます。最近の最先端のアルゴリズムと比較すると、3つ提案されたアルゴリズムは、9年以上の臨床経験のある眼科医を超えて良好なパフォーマンスを示します。広範なアブレーション研究により、提案されたフレームワークが分析され、各モチベーションの有効性と必要性が示されます。 
[ABSTRACT]ほとんどの畳み込みニューラルネットワーク（cnns）ベースのアルゴリズムは、浚渫を画像としての病変として扱います-レベルの注釈</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Purify Noisy Labels via Meta Soft Label Corrector -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_37.html">
      <font color="black">Learning to Purify Noisy Labels via Meta Soft Label Corrector</font>
    </a>
  </h2>
  <font color="black">最近のディープニューラルネットワーク（DNN）は、ノイズの多いラベルで偏ったトレーニングデータに簡単にオーバーフィットできます。ラベル修正戦略は、疑わしいノイズのあるラベルを特定して修正する方法を設計することにより、この問題を軽減するために一般的に使用されます。ラベル修正手順を表示するメタプロセスとして、ラベルを自動的に修正するメタラーナーを使用して、手動で事前設定されたハイパーパラメーターなしで、現在のトレーニング問題に従って修正されたソフトラベルを繰り返し適応的に取得できます。 
[ABSTRACT]ノイズの多いラベルを特定するために新しい方法を使用できます。疑わしいノイズのあるラベルを特定するために簡単に受け入れることができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Pixel-wise Crowd Understanding via Synthetic Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_38.html">
      <font color="black">Pixel-wise Crowd Understanding via Synthetic Data</font>
    </a>
  </h2>
  <font color="black">まず、無料のデータコレクターとラベラーを開発して、コンピューターゲームであるGrand Theft Auto Vで合成およびラベル付けされた群集シーンを生成します。次に、それを使用して、「GCC Dataset」という名前の大規模で多様な合成群集データセットを構築します。 「具体的には、1）監視された群衆の理解：合成データで群集分析モデルを事前トレーニングし、実際のデータとラベルを使用して微調整することで、現実世界でのモデルのパフォーマンスを向上させます。 2）ドメイン適応による群集の理解：合成データを写実的な画像に変換し、変換されたデータとラベルでモデルをトレーニングします。その結果、トレーニングされたモデルは実際の群集シーンでうまく機能します。 
[要約]群集理解は群集分析の最も基本的なタスクです。これは、他の分析タスクよりもビデオシーケンスまたは静止画像の結果が優れているためです。これにより、群集理解のパフォーマンスを向上させる方法が説明されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-30">
        <br><font color="black">2020-07-30</font>
      </time>
    </span>
</section>
<!-- paper0: Compact Global Descriptor for Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_39.html">
      <font color="black">Compact Global Descriptor for Neural Networks</font>
    </a>
  </h2>
  <font color="black">ベンチマーク実験は、提案された方法が、追加のコンピューティングコストを大幅に削減して、最先端の長期的なメカニズムを完成できることを示しています。このホワイトペーパーでは、異なる次元（チャネル、フレームなど）。この記述子により、後続のたたみ込みで、計算の複雑さとパラメーターを無視して、有益なグローバル機能にアクセスできます。 
[ABSTRACT]この記述子により、後続の畳み込みが有益なグローバル機能にアクセスできるようになります。このデスクマーカーにより、後続の情報に役立つグローバル機能にアクセスできます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-07-23">
        <br><font color="black">2019-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Temporally Coherent General Dynamic Scene Reconstruction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CV/paper_40.html">
      <font color="black">Temporally Coherent General Dynamic Scene Reconstruction</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、改良された非剛体オブジェクトセグメンテーションと形状再構成による完全に時間的にコヒーレントな4Dシーンモデルの教師なし再構成と、自由視点レンダリングおよび仮想現実へのその応用について説明します。作業の貢献は次のとおりです。共同推定;時間的コヒーレンスを導入するための、マルチビューの分割と再構成の統合と統合された疎密の時間対応。形状制約を導入することで、動的セグメンテーションの洗練と密集した再構成のための一般的なロバストなアプローチ。さまざまな複雑な屋内および屋外シーンでの最先端のアプローチとの比較により、マルチビューセグメンテーションの両方で精度が向上することが示されています。そして密な再建。 
[ABSTRACT]マルチビューワイド-ベースラインスタティックカメラまたは移動カメラから複雑なダイナミックシーンの4D表現を取得するための一般的なアプローチを発表</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-07-18">
        <br><font color="black">2019-07-18</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: DeLighT: Very Deep and Light-weight Transformer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CL/paper_0.html">
      <font color="black">DeLighT: Very Deep and Light-weight Transformer</font>
    </a>
  </h2>
  <font color="black">全体的に、DeLighTネットワークは、標準の変圧器モデルよりも2.5〜4倍の深さでありながら、パラメータと操作が少なくなっています。 （+0.4 BLEUスコア）ベースライントランスフォーマーよりもはるかに少ないパラメーターで、トランスフォーマーベースのモデルと同等以上のパフォーマンスを提供する、非常に深く軽量なトランスフォーマーDeLighTを導入します。 
[ABSTRACT] delightは、ベースラインモデルのパフォーマンスと大幅に少ないパラメーターで一致します。wmtのデータセットでは、delightは、ベースライントランスフォーマーよりも2倍少ない8のパラメーターで同様のパフォーマンスを実現します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Exploring Deep Hybrid Tensor-to-Vector Network Architectures for
  Regression Based Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CL/paper_1.html">
      <font color="black">Exploring Deep Hybrid Tensor-to-Vector Network Architectures for
  Regression Based Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">次に、エディンバラの騒々しい音声コーパスに関する実験的証拠を提供して、単一チャネルの音声強調では、CNNがモデルサイズの小さな増分を犠牲にしてDNNよりも優れていることを示します。ハイブリッドアーキテクチャ、つまりCNN-TT、モデルパラメーターサイズを小さくしても、高品質のパフォーマンスを維持できます。まず、畳み込みニューラルネットワーク（CNN）ベースのベクトル間回帰モデルの一般化力に新しい上限を導き出します。 
[ABSTRACT] cnn-ttのハイブリッドアーキテクチャは、モデルファクターサイズを小さくしても、高品質のパフォーマンスを維持できます。cnn-さらに、cnn-は、モデルパラメータ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-25">
        <br><font color="black">2020-07-25</font>
      </time>
    </span>
</section>
<!-- paper0: Matching Questions and Answers in Dialogues from Online Forums -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CL/paper_2.html">
      <font color="black">Matching Questions and Answers in Dialogues from Online Forums</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、相互注意と呼ばれる2つの同時注意メカニズムによって距離情報と対話履歴の両方を考慮したQAマッチングモデルを示します。Ubuntuデータセットなどの既存の対話データセットはQAマッチングタスクに適していないため、1,000のラベルが付いたデータセットをさらに作成します。特に、長距離QAペアのマッチングでは、提案されたモデルが最先端およびその他の強力なベースラインよりも優れていることを示しています。会話の2つのターン間の質問と回答の関係のマッチングは、対話を分析する最初のステップだけではありません構造だけでなく、対話システムのトレーニングにも役立ちます。 
[要約] qaマッチングモデルは、相互注意と呼ばれる2つの同時注意メカニズムによって距離情報と対話履歴の両方を考慮します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br><font color="black">2020-05-19</font>
      </time>
    </span>
</section>
<!-- paper0: Photon: A Robust Cross-Domain Text-to-SQL System -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CL/paper_3.html">
      <font color="black">Photon: A Robust Cross-Domain Text-to-SQL System</font>
    </a>
  </h2>
  <font color="black">私たちのシステムのライブデモは、http：//naturalsql.comで入手できます。質問コレクターは、入力質問の混乱スパンを検出し、ユーザーが翻訳可能な入力を与えるまで言い換えを提案する弁別的ニューラルシーケンスエディターです。またはシミュレーションの最大数が実行されます。シミュレートされたデータの実験は、提案された方法が翻訳不可能なユーザー入力に対するテキストからSQLシステムのロバスト性を効果的に改善することを示しています。 
[ABSTRACT] photonは、強力な神経意味論的parid、human-in-the loop質問修正プログラム、および応答ジェネレーターで構成されます。ユーザーにとって、システムにあいまいな質問や、メイン検索の意味論的範囲外の質問を発行することはよくあります言語</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-30">
        <br><font color="black">2020-07-30</font>
      </time>
    </span>
</section>
<!-- paper0: Lite Audio-Visual Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CL/paper_4.html">
      <font color="black">Lite Audio-Visual Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">この研究では、これらの問題に対処するためのLite AVSE（LAVSE）システムを提案します。さらに、実験結果は、視覚データ圧縮のための2つの技術の有効性を確認します。システムは、2つの視覚データ圧縮技術を含み、視覚データを削除しますトレーニングモデルからの特徴抽出ネットワークにより、オンライン計算の効率が向上します。 
[要約]システムには2つの視覚的データ圧縮技術が含まれています。結果は2つの技術の有効性を確認します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-24">
        <br><font color="black">2020-05-24</font>
      </time>
    </span>
</section>
<!-- paper0: SLEDGE: A Simple Yet Effective Baseline for COVID-19 Scientific
  Knowledge Search -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CL/paper_5.html">
      <font color="black">SLEDGE: A Simple Yet Effective Baseline for COVID-19 Scientific
  Knowledge Search</font>
    </a>
  </h2>
  <font color="black">TREC-COVIDチャレンジの強力なベースラインとしてSLEDGEの有効性を観察します（リーダーボードのnDCG @ 10が0.6844を上回ります）。 -IR-Lab / covid-neural-ir。詳細な分析によって提供される洞察は、日付によるフィルタリングの重要性や、カウント信号に大きく依存する神経方法の可能性など、探求する潜在的な将来の方向性を提供します。 
[ABSTRACT]モデルを一般的な-ドメイン回答ランキングデータセットでトレーニングします。関連性信号をsars-cov-2に転送して評価します。詳細な分析によって提供される洞察は、いくつかの潜在的な課題を提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-05">
        <br><font color="black">2020-05-05</font>
      </time>
    </span>
</section>
<!-- paper0: Multimodal Semi-supervised Learning Framework for Punctuation Prediction
  in Conversational Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CL/paper_6.html">
      <font color="black">Multimodal Semi-supervised Learning Framework for Punctuation Prediction
  in Conversational Speech</font>
    </a>
  </h2>
  <font color="black">さらに、ASR出力で最大2〜6％の改善を達成するN-bestリストでデータ拡張を実行することにより、ASRエラーに対するモデルの堅牢性をさらに改善します。1時間の音声およびテキストデータでトレーニングすると、提案されたモデルはベースラインモデルと比較して9〜18％の絶対改善。この作業では、ラベル付けされていない大量のオーディオおよびテキストデータから表現を学習することにより、句読点予測のためのマルチモーダル半教師あり学習アプローチを探索します。 
[ABSTRACT]従来のアプローチでは、通常、強制整列を使用して、フレームごとの音響特徴を単語レベルの特徴に符号化し、結果の音響表現と語彙表現のマルチモーダル融合を使用します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: SemEval-2020 Task 4: Commonsense Validation and Explanation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CL/paper_7.html">
      <font color="black">SemEval-2020 Task 4: Commonsense Validation and Explanation</font>
    </a>
  </h2>
  <font color="black">ただし、サブタスクCについては、システムと人間のパフォーマンスの間に比較的大きなギャップがあります。サブタスクAとサブタスクBについては、上位のシステムのパフォーマンスは人間のパフォーマンスに近いものです。 3つのサブタスクの1つ。 
[ABSTRACT]サブタスクaとサブタスクbの場合、上位ランクのシステムのパフォーマンスは人間のパフォーマンスに比較的近いです。このタスクで使用されるデータセットはwwwにあります。 github。 com</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br><font color="black">2020-07-01</font>
      </time>
    </span>
</section>
<!-- paper0: Audiovisual Speech Synthesis using Tacotron2 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CL/paper_8.html">
      <font color="black">Audiovisual Speech Synthesis using Tacotron2</font>
    </a>
  </h2>
  <font color="black">さらに、必要な韻律をエンコードして感情的な視聴覚音声を生成する感情埋め込みに対するエンドツーエンドとモジュール方式の両方の条件付けを行います。出力音響機能を使用して、WaveRNNを調整し、音声波形を再構築します。 2番目の視聴覚音声合成システムはモジュール式であり、従来のTacotron2を使用してテキストから音響音声が合成されます。 
[要約] 2つの生体視覚音声合成システムが顔認識と比較されています。これらは、感情的な声を作成するために使用される2つの顔の顔のフェイシャルと比較されます。2つのシステムを分析および分析して、人間が顔認識をより簡単に使用できるようにすることができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Discovery of Recurring Speech Patterns Using Probabilistic
  Adaptive Metrics -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CL/paper_9.html">
      <font color="black">Unsupervised Discovery of Recurring Speech Patterns Using Probabilistic
  Adaptive Metrics</font>
    </a>
  </h2>
  <font color="black">チャレンジの2020実装の一部として、ゼロリソーススピーチチャレンジ2017データセットでPDTWをテストします。この問題への1つの潜在的なアプローチは、ダイナミックタイムワーピング（DTW）を使用して、スピーチデータから適切に整列するパターンを見つけることです。ただし、 DTWアライメントの初期候補セグメントの自動選択と、それらの間の「十分に良好」なアライメントの検出には、事前定義された基準のタイプが必要であり、信号表現間のペアワイズ距離メトリックのしきい値パラメーターとして操作できることがよくあります。 
[要約]システムは、固定されたhyperparameters.pdtw-ベースのシステム-pdtwとして識別-を使用して、テストされた5つの言語すべてで一貫して動作し、同じシステムで一貫して動作することを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning based Topic Analysis on Financial Emerging Event Tweets -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CL/paper_10.html">
      <font color="black">Deep Learning based Topic Analysis on Financial Emerging Event Tweets</font>
    </a>
  </h2>
  <font color="black">次に、セマンティックワードクラスターが形成されました。ツイートベクトルは、ディープオートエンコーダーをトレーニングすることにより圧縮表現に変換されました。次に、各ツイートは、構成されている単語の用語頻度-逆ドキュメント頻度（TF-IDF）値を使用してベクトル化されました。 
[ABSTRACT] 28264財務つぶやき
[1]の分析は、株式市場で発生しているイベントを発見するために使用されました。財務比率epsは、投資家によって頻繁に議論されている指標です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: BURT: BERT-inspired Universal Representation from Twin Structure -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CL/paper_11.html">
      <font color="black">BURT: BERT-inspired Universal Representation from Twin Structure</font>
    </a>
  </h2>
  <font color="black">特に、これらのモデルで完全なトレーニングコンテキストとして使用されている文レベルの表現として、低レベルの言語単位（句と単語）のパフォーマンスが低下します。推論データセットと言い換えデータセットからの単語/フレーズレベルの表現。この作業では、任意の粒度の入力シーケンスのユニバーサルな固定サイズの表現を生成できるBURT（ツイン構造からのBERTに基づくユニバーサル表現）つまり、複数のトレーニング目標を持つ大規模な自然言語推論および言い換えデータを使用した単語、フレーズ、および文章。 
[ABSTRACT] burtは、大規模な自然言語知識と言い換えデータを使用して、任意の粒度の汎用的な固定サイズ表現を生成できます。これらには、単語、フレーズ、文が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-29">
        <br><font color="black">2020-04-29</font>
      </time>
    </span>
</section>
<!-- paper0: Falcon 2.0: An Entity and Relation Linking Tool over Wikidata -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/cs.CL/paper_12.html">
      <font color="black">Falcon 2.0: An Entity and Relation Linking Tool over Wikidata</font>
    </a>
  </h2>
  <font color="black">また、技術的な専門知識がなくても実行できるオンラインAPIのデモも行います。候補者はWikidataの国際化リソース識別子（IRI）で表されます。WikidataをバックグラウンドKGと考えると、ウィキデータへのテキスト。 
[要旨]テキスト内の知識をwikidata.falcon 2にリンクするためのツールは限られています。0は公開されており、コミュニティで再利用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-24">
        <br><font color="black">2019-12-24</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: Evolving Multi-Resolution Pooling CNN for Monaural Singing Voice
  Separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.AS/paper_0.html">
      <font color="black">Evolving Multi-Resolution Pooling CNN for Monaural Singing Voice
  Separation</font>
    </a>
  </h2>
  <font color="black">これらの問題に対処するために、MSVSのDNNの構造設計にニューラルアーキテクチャ検索（NAS）手法を導入します。 MRP-CNN）は、さまざまなサイズのプーリング演算子を使用してマルチ解像度機能を抽出します。ただし、既存のDNNは手動で設計されることが多く、時間がかかり、エラーが発生しやすくなります。 
[ABSTRACT] msvsは現在の状態です-cnn.mrpの最新のメソッドです-msvsの新しいシステムはマルチ解像度プーリングcnn（mrp-cnn）と呼ばれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Exploring Deep Hybrid Tensor-to-Vector Network Architectures for
  Regression Based Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.AS/paper_1.html">
      <font color="black">Exploring Deep Hybrid Tensor-to-Vector Network Architectures for
  Regression Based Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">次に、エディンバラの騒々しい音声コーパスに関する実験的な証拠を提供して、単一チャネルの音声強調では、CNNがモデルサイズの小さな増分を犠牲にしてDNNよりも優れていることを示します。さらに、CNN-TTは、 CNNモデルパラメータのわずか32 \％です。このホワイトペーパーでは、モデルパラメータの数と強化された音声品質との間のさまざまなトレードオフを、音声強調にいくつかの深いテンソルからベクトルへの回帰モデルを使用することによって調査します。 
[ABSTRACT] cnn-ttのハイブリッドアーキテクチャは、モデルファクターサイズを小さくしても、高品質のパフォーマンスを維持できます。cnn-さらに、cnn-は、モデルパラメータ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-25">
        <br><font color="black">2020-07-25</font>
      </time>
    </span>
</section>
<!-- paper0: Lite Audio-Visual Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.AS/paper_2.html">
      <font color="black">Lite Audio-Visual Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">以前の研究では、視覚情報を音声強調（SE）システムに組み込むことの有効性が確認されています。この研究では、これらの問題に対処するためのLite AVSE（LAVSE）システムを提案します。さらに、実験結果により、2つの有効性が確認されています視覚的なデータ圧縮の手法。 
[要約]システムには2つの視覚的データ圧縮技術が含まれています。結果は2つの技術の有効性を確認します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-24">
        <br><font color="black">2020-05-24</font>
      </time>
    </span>
</section>
<!-- paper0: Multimodal Semi-supervised Learning Framework for Punctuation Prediction
  in Conversational Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.AS/paper_3.html">
      <font color="black">Multimodal Semi-supervised Learning Framework for Punctuation Prediction
  in Conversational Speech</font>
    </a>
  </h2>
  <font color="black">さらに、ASR出力で最大2〜6％の改善を達成するN-bestリストでデータ拡張を実行することにより、ASRエラーに対するモデルの堅牢性をさらに向上させます。また、アブレーションスタディを実行することにより、半教師あり学習アプローチの有効性を示します。コーパスのさまざまなサイズで。1時間の音声およびテキストデータでトレーニングされた場合、提案されたモデルは、ベースラインモデルよりも9〜18％の絶対的な改善を達成しました。 
[ABSTRACT]従来のアプローチでは、通常、強制整列を使用して、フレームごとの音響特徴を単語レベルの特徴に符号化し、結果の音響表現と語彙表現のマルチモーダル融合を使用します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Exploiting Deep Sentential Context for Expressive End-to-End Speech
  Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.AS/paper_4.html">
      <font color="black">Exploiting Deep Sentential Context for Expressive End-to-End Speech
  Synthesis</font>
    </a>
  </h2>
  <font color="black">私たちのコンテキストエクストラクターは、最初に異なるSANレイヤーから韻律関連のセンテンスコンテキスト情報を収集し、それらを集約して包括的な文表現を学習して、最終的に生成される音声の表現力を高めます。この論文では、上に構築されたコンテキストエクストラクターを提案します。 SANベースのテキストエンコーダー。seq2seqベースのTTSの表現力のあるコーパスを介して、文のコンテキストを十分に活用します。表現力のモデリング。 
[要約] seq2seqフレームワークは、テキストエンコーダーのみから韻律情報を抽出します。これは、表現力豊かなコンテンツの平均的な表現に簡単に折りたたむことができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Audiovisual Speech Synthesis using Tacotron2 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.AS/paper_5.html">
      <font color="black">Audiovisual Speech Synthesis using Tacotron2</font>
    </a>
  </h2>
  <font color="black">2つのシステムのパフォーマンスを分析し、主観評価テストを使用してグラウンドトゥルースビデオと比較します。2番目の視聴覚音声合成システムはモジュール化されており、従来のTacotron2を使用してテキストから音響音声が合成されます。エンドツーエンドまた、モジュラーシステムは、専門家が記録したビデオから生成されたグラウンドトゥルースのMOS 4.1と比較して、平均オピニオンスコア（MOS）がそれぞれ4.1および3.9の人間に近い視聴覚スピーチを合成できます。 
[要約] 2つの生体視覚音声合成システムが顔認識と比較されています。これらは、感情的な声を作成するために使用される2つの顔の顔のフェイシャルと比較されます。2つのシステムを分析および分析して、人間が顔認識をより簡単に使用できるようにすることができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Speaker dependent articulatory-to-acoustic mapping using real-time MRI
  of the vocal tract -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.AS/paper_6.html">
      <font color="black">Speaker dependent articulatory-to-acoustic mapping using real-time MRI
  of the vocal tract</font>
    </a>
  </h2>
  <font color="black">客観的（正規化されたMSEおよびMCD）と主観的測定（知覚テスト）で結果を評価し、複数の画像を入力として取り込み、2.8〜4.5 dBのMCDスコアを達成するCNN-LSTMネットワークが好ましいことを示します。現在の論文では、rtMRIを入力としてスピーカー固有の方法で使用して、調音-音声変換用のさまざまなDNN（完全に接続された畳み込みおよびリカレントニューラルネットワーク）をトレーニングします。USC-TIMIT調音の男性と女性の2つのスピーカーを使用します。データベースでは、それぞれが460文を発声します。 
[ABSTRACT] mriは、他の技法では通常は不可能である、ベロムと咽頭領域をキャプチャできます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Multitask learning for instrument activation aware music source
  separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.AS/paper_7.html">
      <font color="black">Multitask learning for instrument activation aware music source
  separation</font>
    </a>
  </h2>
  <font color="black">結果は、提案されたマルチタスクモデルが、MUSDBデータセットで同等のパフォーマンスを維持しながら、ミキシングシークレットとMedleyDBデータセットの混合でベースラインのOpen-Unmixモデルよりも優れていることを示しています。さらに、MedleyDBとMixing Secretsデータセットの組み合わせを利用して、広く使用されているMUSDBデータセットに含まれる3つの機器よりも現実的なシナリオである6つの独立した機器でシステムを調査します。 
[ABSTRACT]既存のシステムのほとんどはソース分離自体の問題にのみ焦点を当てていますが、これらのシステムのほとんどは問題に焦点を当てていません。これは、musdbデータセットに含まれる3つの計測器よりも現実的です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Structure and Automatic Segmentation of Dhrupad Vocal Bandish Audio -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.AS/paper_8.html">
      <font color="black">Structure and Automatic Segmentation of Dhrupad Vocal Bandish Audio</font>
    </a>
  </h2>
  <font color="black">これにより、2人のパフォーマーの変化するリズミカルな相互作用を捉えるという観点から、コンサートセクションの完全な音楽的説明を取得できます。また、ボーカルの個々の表面密度を検出するための前処理ステップとして、オーディオソースの分離を採用しています。パーカッション..この作品は、音楽的に関連のあるリズミカルな密度が、バンディッシュ（作曲）パフォーマンス全体で時間とともに変化するときに、それらを自動的に検出することに関するものです。 
[ABSTRACT]曲のメトリックテンポに関連して、リズミカルなバウンシングバウンシングバウンシング密度のキャプチャを追跡します。これは、ボーカルとパーカッションの個々の表面密度の検出に役立ちます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Discovery of Recurring Speech Patterns Using Probabilistic
  Adaptive Metrics -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.AS/paper_9.html">
      <font color="black">Unsupervised Discovery of Recurring Speech Patterns Using Probabilistic
  Adaptive Metrics</font>
    </a>
  </h2>
  <font color="black">教師なし音声用語ディスカバリー（UTD）は、音響音声データのコーパスから繰り返しの音声セグメントを見つけることを目的としています。2020年のチャレンジの実装の一環として、PDTWをZero Resource Speech Challenge 2017データセットでテストします。このペーパーでは、 PDTWと名付けられたDTWベースのUTDへの新しい確率論的アプローチ。 
[要約]システムは、固定されたhyperparameters.pdtw-ベースのシステム-pdtwとして識別-を使用して、テストされた5つの言語すべてで一貫して動作し、同じシステムで一貫して動作することを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: MusiCoder: A Universal Music-Acoustic Encoder Based on Transformers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.AS/paper_10.html">
      <font color="black">MusiCoder: A Universal Music-Acoustic Encoder Based on Transformers</font>
    </a>
  </h2>
  <font color="black">結果は、MusiCoderが音楽ジャンルの分類と自動タグ付けタスクの両方で最先端のモデルよりも優れていることを示しています。従来のモデルは、音楽注釈タスクに教師あり学習を使用しています。しかし、教師あり機械学習アプローチは複雑さが増すにつれて、より多くの注釈付きのトレーニングデータの必要性が高まると、多くの場合、利用可能なデータと一致しなくなります。 
[ABSTRACT] musicoderはbertの成功に基づいており、自己注意双方向トランスフォーマーのアーキテクチャに基づいて構築されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: TutorNet: Towards Flexible Knowledge Distillation for End-to-End Speech
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.AS/paper_11.html">
      <font color="black">TutorNet: Towards Flexible Knowledge Distillation for End-to-End Speech
  Recognition</font>
    </a>
  </h2>
  <font color="black">LibriSpeechデータセットに対する多くの実験を通じて、提案された方法は、トポロジが異なるネットワーク間で知識を抽出するだけでなく、抽出された学生のワードエラーレート（WER）パフォーマンスの向上にも大きく貢献することが検証されています。具体的な実現のために、まず、初期化ステップ中に表現レベルの知識抽出（RKD）を適用し、次に、オリジナルのタスク学習と組み合わせたソフトマックスレベルの知識抽出（SKD）を適用します。以前のKDアプローチでは、一般的に、レイヤーごとの幅または教師モデルのレイヤー数。 
[ABSTRACT] kdモデルは、さまざまなタイプのニューラルネットワーク間で、非表示表現-レベルと出力-レベルで知識を転送できます。特定の場合に、生徒モデルは教師のパフォーマンスを上回ります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.AS/paper_12.html">
      <font color="black">One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech</font>
    </a>
  </h2>
  <font color="black">トレーニングでは、CSS10データセットと5つの言語のCommon Voice録音に基づく新しい小さなデータセットを使用しました。音声の複製を促進するために、モデルは、スピーカー固有の情報をエンコーダーから削除する勾配反転レイヤーを備えた敵対的なスピーカー分類子を使用します。 。評価するために、さまざまなレベルのクロスリンガルパラメータ共有を使用してモデルとベースラインを比較する2つの実験を準備しました：（1）少量のデータでトレーニングするときの安定性とパフォーマンス、（2）コードの発音精度と音声品質-スイッチング合成。 
[要約]私たちのモデルは、完全にたたみ込み入力テキストエンコーダーを備えたタコトロン2に基づいています。音声品質のコード-切り替え合成により、より自然で正確なコードが生成されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Intonation Pattern Embeddings for Arabic Dialect Identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.AS/paper_13.html">
      <font color="black">Learning Intonation Pattern Embeddings for Arabic Dialect Identification</font>
    </a>
  </h2>
  <font color="black">実験の結果は、アラビア語の方言のイントネーションパターンが、VarDial 17 ADIデータセットで最先端の結果を達成するのに十分な情報を提供し、単一機能システムよりも優れていることを示しています。十分な情報の重要性は、ディープラーニングADIタスクの最適性、より一般的には、音響モデリング問題へのその応用。この記事では、イントネーションパターンと音響表現を使用したアラビア語方言識別（ADI）の完全なエンドツーエンドパイプラインを紹介します。 
[ABSTRACT]これらには、言語や方言の音声の違いをキャプチャできるディープラーニングアーキテクチャが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: A fully recurrent feature extraction for single channel speech
  enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-04/eess.AS/paper_14.html">
      <font color="black">A fully recurrent feature extraction for single channel speech
  enhancement</font>
    </a>
  </h2>
  <font color="black">この目的のために、CNNレイヤーを抽出する特徴に反復係数を追加して、シングルチャネル音声強調のためのロバストなコンテキスト認識特徴抽出戦略を導入します。しかし、バニラCNNモジュールの特徴抽出力は、次の次元制約によって制限されています。統合された畳み込みカーネル-それにより、特徴抽出段階でノイズコンテキスト情報を適切にモデル化できませんでした。抽出された特徴でノイズ属性のローカル統計をキャプチャするのにロバストであるため、提案されたモデルは音声キューの区別に非常に効果的です。非常に騒々しい条件でも。 
[ABSTRACT] cnnモジュールの特徴抽出能力は、ネットワークのノイズコンテキストを適切にモデル化できませんでした。新しいモデルは、非常に騒々しい状況でも、音声キューの区別に非常に効果的です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-09">
        <br><font color="black">2020-06-09</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
