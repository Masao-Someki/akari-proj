<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-07-02の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: The NTT DCASE2020 Challenge Task 6 system: Automated Audio Captioning
  with Keywords and Sentence Length Estimation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.SD/paper_0.html">
      The NTT DCASE2020 Challenge Task 6 system: Automated Audio Captioning
  with Keywords and Sentence Length Estimation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちのモデルは、ベースラインシステムの5.4である20.7 SPIDErスコアを達成しました。開発テストデータセットを使用して、提出の簡略化されたモデルをテストしました。マルチタスク学習。 
[要旨]私たちの提出物は、自動音声字幕作成における2つの不確定性問題の解決に焦点を当てています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Private Speech Characterization with Secure Multiparty Computation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.SD/paper_1.html">
      Private Speech Characterization with Secure Multiparty Computation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      脅威モデルとして、パッシブセキュリティ、つまり人間の声のオーディオ信号分類などのオーディオ信号処理のディープラーニングは、機械学習の豊富なアプリケーション領域です。自動化された人間の音声分類には明らかな利点がありますが、アプリケーション開発者は、無防備なオーディオ信号処理から公認の範囲を超えた知識を得ることができます。 。 
[要約]このホワイトペーパーでは、人間のコミュニケーションのディープラーニングベースの音声分類に対する最初のプライバシー保護処理ソリューションを提案します。脅威の分類として、プロトコルの逸脱した悪意のある当事者によるセキュリティの両方の受動的な領域を検討します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Whole-Word Segmental Speech Recognition with Acoustic Word Embeddings -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.SD/paper_2.html">
      Whole-Word Segmental Speech Recognition with Acoustic Word Embeddings
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最終的なモデルは、同等のA2Wモデルよりも改善されます。さらに、筆記された単語ラベルの共同トレーニングされた音響単語埋め込み（AWE）と音響的に接地された単語埋め込み（AGWE）による事前トレーニングの使用を調査します。 AWEを使用して音響表現を事前トレーニングすることで、マージンを大幅に削減できます。AGWEを使用して単語予測レイヤーを事前トレーニングすることで、追加の（小さい）ゲインを取得できます。 
[ABSTRACT]モデルは、音響の単語の埋め込みに基づいています。これらは、音響の単語の埋め込みシステムに基づいています。さらに、驚異的な方法で音響表現を事前トレーニングします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Instantaneous PSD Estimation for Speech Enhancement based on Generalized
  Principal Components -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.SD/paper_3.html">
      Instantaneous PSD Estimation for Speech Enhancement based on Generalized
  Principal Components
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      マイクロフォン信号の一般化された固有ベクトルに基づく変換。ただし、この行列の時間的に滑らかな一般化された固有値から直接PSDを推定し、時間的に滑らかなPSD推定値を生成する代わりに、新しく定義された瞬間的な一般化固有値からPSDを推定し、瞬時のPSD推定..瞬時の一般化された固有値は、一般化された主成分から定義されます。つまり、
[要約]音声は非常に非psdであるため、psd推定の時間変化を維持することにより、パフォーマンスの向上が得られます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Exploring the time-domain deep attractor network with two-stream
  architectures in a reverberant environment -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.SD/paper_4.html">
      Exploring the time-domain deep attractor network with two-stream
  architectures in a reverberant environment
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本研究では、可変スピーカー数の条件下で残響除去と分離タスクの両方を効率的に実行する、2ストリーム畳み込みネットワークを備えた時間領域ディープアトラクタネットワーク（TD-DAN）を提案します。スピーカーエンコーディングストリーム（SES） TD-DANのモデルはスピーカー情報をモデル化し、さまざまな波形エンコーダーで探索されます。さらに、これらのモデルは反響環境でパフォーマンスの低下を被ります。 
[ABSTRACT] deep td-danの音声分離は、時間-周波数領域で行われますが、これは最適ではありません。これらのモデルは、残響環境でパフォーマンスが低下します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Unsupervised Interpretable Representation Learning for Singing Voice
  Separation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.SD/paper_5.html">
      Unsupervised Interpretable Representation Learning for Singing Voice
  Separation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの調査結果は、非負性、平滑性、時間周波数マスキングの対象となる再構成などの短時間フーリエ変換の便利さを維持しながら、音声分離で意味のある表現を学習できることを示唆しています。音楽ソースの分離..この作業では、波形信号から解釈可能な音楽信号表現を直接学習する方法を紹介します。この方法は、教師なしの目的を使用してトレーニングでき、デコードとして単純な正弦波モデルを使用するノイズ除去オートエンコーダモデルに依存します歌声を再現する機能。 
[ABSTRACT]私たちの方法は、教師なしの目的を使用してトレーニングできます。シンプルな正弦波モデルを使用して歌声を再構成するノイズ除去オートエンコーダモデルに依存しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-03">
        <br>2020-03-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Consistent Independent Low-Rank Matrix Analysis for Determined Blind
  Source Separation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.SD/paper_6.html">
      Consistent Independent Low-Rank Matrix Analysis for Determined Blind
  Source Separation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      スペクトログラムは重なり合うウィンドウによって計算されるため（そしてウィンドウ関数はメインローブとサイドローブと呼ばれるスペクトルの不鮮明さを誘発します）、時間-周波数ビンは互いに依存します。このようなスペクトルコンポーネント間の共起は、 ILRMAは、最近の研究で実証されている順列問題を解決します。ILRMAは、非負行列因数分解（NMF）を介してソース信号のパワースペクトログラムをモデル化することにより、優れた分離性能を実現します。 
[ABSTRACT] ilrmaは、非負行列因数分解を介してソース信号のパワースペクトログラムをモデル化することにより、優れた分離性能を実現しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Transformer-based Audio Captioning Model with Keyword Estimation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.SD/paper_7.html">
      A Transformer-based Audio Captioning Model with Keyword Estimation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      TRACKEは、入力オーディオのオーディオイベント/シーンに対応する単語セットで構成されるキーワードを推定し、推定されたキーワードを参照しながらキャプションを生成して、単語選択の不確定性を減らします。公開AACデータセットの実験結果は、TRACKEが状態を達成したことを示しています最先端のパフォーマンスとキャプションとそのキーワードの両方の推定に成功しました。この問題を解決するために、TRACKEと呼ばれるキーワード推定を備えたトランスフォーマーベースのオーディオキャプションモデルを提案します。 
[ABSTRACT] trackeは最先端のパフォーマンスを達成し、キャプションとそのキーワードの両方を正常に推定しました。trackeは、音響イベント検出のサブタスクを実行しながら、aacのメインタスクの問題を同時に解決します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: WaveNODE: A Continuous Normalizing Flow for Speech Synthesis -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.SD/paper_8.html">
      WaveNODE: A Continuous Normalizing Flow for Speech Synthesis
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      従来のモデルとは異なり、WaveNODEはフロー操作に使用される関数に制約を課さないため、より柔軟で複雑な関数の使用が可能になります。近年、さまざまなフローベースの生成モデルが実際に高忠実な波形を生成するために提案されています-time ..さらに、WaveNODEを最適化して、教師ネットワークや補助損失項を必要とせずに可能性を最大化できます。 
[ABSTRACT] wavenodeを使用すると、より柔軟で複雑な関数を作成できます。これらのモデルには、十分にトレーニングされた教育済みネットワークまたは多数のフローステップが必要です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-08">
        <br>2020-06-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Personalization of Hearing Aid Compression by Human-In-Loop Deep
  Reinforcement Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.SD/paper_9.html">
      Personalization of Hearing Aid Compression by Human-In-Loop Deep
  Reinforcement Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      開発されたパーソナライズされた圧縮の有効性を示すシミュレーションと被験者のテスト結果の両方が報告されます。補聴器ユーザーのほぼ半数は、一般的に規定されている設定とは異なる設定を好みます。補聴器の圧縮をパーソナライズして、聴覚を改善します。 
[ABSTRACT]開発されたアプローチは、ユーザーのフィードバックに基づいて圧縮を最適化するために、特定のユーザーの聴覚の好みを学習するように設計されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Psychoacoustically Motivated Audio Declipping Based on Weighted l1
  Minimization -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.SD/paper_10.html">
      Psychoacoustically Motivated Audio Declipping Based on Weighted l1
  Minimization
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験は、信号対歪み比（SDR）とPEMO-Q客観的差異グレード（ODG）に従って復元品質を比較し、適切に選択された重みを使用すると、提示された方法は現在の状態と競合するか、さらには現在の状態を上回ることができることを示しますスパース性に基づいたオーディオクリッピングの新しい方法が提示されます。重み付けは、アルゴリズムの複雑さを低く保ちながら、復元の品質を向上させます。 
[要約]この方法は、聴覚の絶対しきい値とグローバルマスキングしきい値に基づいています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-05-02">
        <br>2019-05-02
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: So What's the Plan? Mining Strategic Planning Document -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.CL/paper_0.html">
      So What's the Plan? Mining Strategic Planning Document
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このプロジェクトは、言語テクノロジーと電子政府の両方の観点から基礎づけられています。注釈付きテキストの量は、RuREBusからどのような洞察を得ることができるかを示すのに十分な大きさです。新しい言語ソースとツールだけでなく、そのアプリケーションも開発されています電子政府の研究。 
[要旨]新しいテキストコーパスを最初から作成するためのパイプラインを示しました。次のテキストは、人間のループ内戦略を使用してマークアップされています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Iterative Paraphrastic Augmentation with Discriminative Span Alignment -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.CL/paper_1.html">
      Iterative Paraphrastic Augmentation with Discriminative Span Alignment
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      文レベルの語彙的に制約された言い換えと差別的なスパンのアラインメントに基づく、新しいパラパラスティックな増強戦略を紹介します。20年以上の人の労働にまたがる大規模な言語理解の取り組みであるBerkeley FrameNet Projectのフレームワークを示します。アライメントモデルのトレーニングデータを約4日間収集し、並列計算を約1日間実行すると、コンテキストに注釈が付けられた495,300の一意の（フレーム、トリガー）組み合わせが自動的に生成されます（FrameNet v1.7の約50倍の拡張）。 
[ABSTRACT]私たちのアプローチは、既存のリソースの大規模な拡張、または手動で作成された小さなシードコーパスからの新しいリソースの迅速な作成を可能にします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Unsupervised Semantic Hashing with Pairwise Reconstruction -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.CL/paper_2.html">
      Unsupervised Semantic Hashing with Pairwise Reconstruction
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      PairRecを従来のアプローチと最先端のアプローチと実験的に比較し、ドキュメントの類似性検索のタスクで大幅なパフォーマンスの向上を実現します。PairRecは、まず、弱監視のトレーニングペア（クエリドキュメントと意味的に類似したドキュメント）を2つのハッシュにエンコードします。これらの両方のハッシュコードから同じクエリドキュメントを再構築することを学習します（つまり、ペアワイズ再構築）。このペアワイズ再構築により、モデルは、デコーダーを介してハッシュコード内のローカル近傍構造を直接エンコードできます。 
[ABSTRACT]ハッシュは、オートエンコーダベースのハッシュモデルです。ペアワイズ再構築（pairrec）に基づいています。モデルは、デコーダを介して直接使用できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Latent Compositional Representations Improve Systematic Generalization
  in Grounded Question Answering -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.CL/paper_3.html">
      Latent Compositional Representations Improve Systematic Generalization
  in Grounded Question Answering
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この作業では、CKYスタイルのパーサーを使用して、ボトムアップの構成的な方法ですべての質問スパンの表現と表記を計算するモデルを提案します。ツリー構造に対するこの帰納的バイアスにより、体系的な一般化が算術式のベンチマークと、グラウンデッドクエスチョンアンサーのモデルの体系的な一般化に焦点を当てたデータセットであるCLOSUREの強力なベースラインと比較した分布の例。この挑戦的なデータセットでは、モデルは92.8％の精度に達し、かなり高いランダムな分布内分割でタスクをほぼ完全に解決する以前のモデルよりも。 
[ABSTRACT]質問の回答では、分解を明示的に実行していないため、分布外の例への一般化が困難になっています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: SemEval-2020 Task 4: Commonsense Validation and Explanation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.CL/paper_4.html">
      SemEval-2020 Task 4: Commonsense Validation and Explanation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      3番目のサブタスクでは、参加システムが理由を自動的に生成する必要があります。このホワイトペーパーでは、SemEval-2020タスク4、常識の検証と説明（ComVE）を提示します。これには、3つのサブタスクが含まれ、システムが2番目のサブタスクは、システムに、特定のステートメントが意味をなさない3つのオプションから主要な理由を選択するように要求します。 
[ABSTRACT]最初のサブ区別では、参加しているシステムは同じような表現の2つの自然言語ステートメントから選択する必要があります。2番目のサブタスク、現在のシステムは、サブタスクへのそのキーを含み、理由を生成することを目的としています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Whole-Word Segmental Speech Recognition with Acoustic Word Embeddings -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.CL/paper_5.html">
      Whole-Word Segmental Speech Recognition with Acoustic Word Embeddings
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      単語全体（「音響対単語」）の音声認識のセグメントモデルを検討します。セグメント特徴ベクトルは、音響単語の埋め込みを使用して定義されます。パスの数は語彙サイズに比例するため、このようなモデルは計算上困難です。電話などのサブワード単位を使用する場合よりも桁違いに大きくなる可能性があります。最終的なモデルは、同等のA2Wモデルよりも優れています。 
[ABSTRACT]モデルは、音響の単語の埋め込みに基づいています。これらは、音響の単語の埋め込みシステムに基づいています。さらに、驚異的な方法で音響表現を事前トレーニングします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Transferability of Natural Language Inference to Biomedical Question
  Answering -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.CL/paper_6.html">
      Transferability of Natural Language Inference to Biomedical Question
  Answering
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      含意データから転送すると、以前のチャレンジレポート（BioASQ 7BフェーズB）と比較して、はい/いいえ（+ 5.59％）、ファクトイド（+ 0.53％）、リスト（+ 13.58％）タイプの質問で効果的なパフォーマンスを示していることがわかります。文のペアの含意に関する言語知識を学習すると、2つのタスク間の転送可能性を活用して、一般的なドメインQAでのパフォーマンスが向上します。順次転送学習では、タスクを微調整する順序が重要です。 
[ABSTRACT]転移学習を備えた言語モデルは、ある程度の問題に対処します。論文では、実験設定を統一することにより転移性を促進することに焦点を当てています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Overcoming Statistical Shortcuts for Open-ended Visual Counting -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.CL/paper_7.html">
      Overcoming Statistical Shortcuts for Open-ended Visual Counting
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      直感的には、奇数を数える適切なメカニズムを学習したモデルは、偶数でもうまく機能するはずです。これは、奇数と偶数のセットなど、同じカウントラベル分布に従わないトレーニングとテストのセットのペアに基づいています。最初に、統計的ショートカットに過度に依存するモデルにペナルティを課す修正カウント分布（MCD）プロトコルを提案します。 
[ABSTRACT]これは、奇数-偶数セットなど、同じカウントラベル分布に従わないトレーニングセットとテストセットに基づいています。そのため、視覚分析とカウントに基づく専用の空間カウントネットワーク（scn）を導入します。自然言語の質問
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-17">
        <br>2020-06-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: End-To-End Speech Synthesis Applied to Brazilian Portuguese -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.CL/paper_8.html">
      End-To-End Speech Synthesis Applied to Brazilian Portuguese
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      得られた結果は、より小さなデータセットを使用している場合でも、英語をカバーする関連研究と同等です。この作業は、データセットの形式でブラジルポルトガル語向けに公開されているリソースの作成と、エンドツーエンドの音声合成用のディープラーニングモデルで構成されています。提案されたシナリオでは、Mozilla TTSとRTISI-LAボコーダーに基づくモデルが提示されています。 4.03 MOS値を達成する最高のパフォーマンス。 
[ABSTRACT]データセットには1人の発言者から10.5時間あります。データセットには3.5時間の分析が必要です。モデルのパフォーマンスも分析しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-11">
        <br>2020-05-11
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Beneath the Tip of the Iceberg: Current Challenges and New Directions in
  Sentiment Analysis Research -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.CL/paper_9.html">
      Beneath the Tip of the Iceberg: Current Challenges and New Directions in
  Sentiment Analysis Research
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、多くの見落とされ、未回答の質問をカバーするこのフィールドの可能なコースを図表化しようとします。現在の関連性に責任がある重要な飛躍を分析します。この記事では、欠点と不十分な調査を指摘することにより、この認識について説明します、本当の感情を理解するために必要なこの分野の重要な側面。 
[ABSTRACT]感情は、マーケティング、リスク管理、極性、および政治において広まっています。それは、マーケティングやリスク管理などのさまざまな分野で広範な商用アプリケーションを持っています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-01">
        <br>2020-05-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: COVID-19 Literature Knowledge Graph Construction and Drug Repurposing
  Report Generation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.CL/paper_10.html">
      COVID-19 Literature Knowledge Graph Construction and Drug Repurposing
  Report Generation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      すべてのデータ、KG、リソース、共有サービスは公開されています。Googleのフレームワークは、詳細な文脈に基づく文章、部分図、知識のサブグラフも証拠として提供します。COVID-19と闘うには、臨床医と科学者全員が膨大な量の疾患のメカニズムと関連する生物学的機能を理解するための、関連する生物医学知識。 
[ABSTRACT]私たちは斬新で包括的な知識発見フレームワークcovid-kgを開発しました。新しいセマンティック表現と外部オントロジーを使用して、入力文献データのテキストと画像を表現します。次に、さまざまな抽出コンポーネントを実行して、きめの細かいマルチメディア知識を抽出します要素
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Predicting In-game Actions from Interviews of NBA Players -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.CL/paper_11.html">
      Predicting In-game Actions from Interviews of NBA Players
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、テキスト入力と過去のパフォーマンスメトリックの両方を使用するモデルは、最良の結果を生成しました。私たちのモデルは、テキスト信号のみ、または過去のパフォーマンスメトリックからの信号との組み合わせに基づいて予測を行うことができます。ニューラルモデルを設計します。オープンエンドのインタビューにおける言語信号のますます複雑な側面に基づくプレーヤーの行動予測。 
[ABSTRACT]最もパフォーマンスの高い文学モデルは、各予測タスクに直感的に関連するトピックに最も関連しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-24">
        <br>2019-10-24
      </time>
    </span>
  </h3>
</article>
<!-- paper0: WaveNODE: A Continuous Normalizing Flow for Speech Synthesis -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.CL/paper_12.html">
      WaveNODE: A Continuous Normalizing Flow for Speech Synthesis
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、WaveNODEを最適化して、教師ネットワークや補助損失項を必要とせずに可能性を最大化できます。従来のモデルとは異なり、WaveNODEはフロー操作に使用される関数に制約を課さないため、より柔軟で複雑な関数を使用できます。 WaveNODEは、従来のフローベースのボコーダーと比較して、少ないパラメーターで同等のパフォーマンスを実現することを実験的に示しています。 
[ABSTRACT] wavenodeを使用すると、より柔軟で複雑な関数を作成できます。これらのモデルには、十分にトレーニングされた教育済みネットワークまたは多数のフローステップが必要です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-08">
        <br>2020-06-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: RECAST: Interactive Auditing of Automatic Toxicity Detection Models -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.CL/paper_13.html">
      RECAST: Interactive Auditing of Automatic Toxicity Detection Models
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      予測の説明を視覚化し、検出された有毒な発話に代替の表現を提供することにより、毒性検出モデルを調べるインタラクティブツールであるRECASTを継続的に紹介します。有毒な言語がオンラインにほぼ浸透するにつれて、自然界の進歩を活用することに関心が高まっています。非常に大きなトランスフォーマーモデルから有毒なコメントの自動検出および削除まで、言語処理（NLP）。ディープラーニングシステムの公平性の懸念、敵対的な堅牢性の欠如、および限られた予測の説明可能性にもかかわらず、現在これらのシステムの監査と理解のための作業はほとんどありません。開発者とユーザーの両方にとってどのように機能するか。 
[要約]有毒な言語は堅牢な堅牢性の欠如のせいですが、現在これらのシステムを監査する作業はほとんどありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-07">
        <br>2020-01-07
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Characterizing Sociolinguistic Variation in the Competing Vaccination
  Communities -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.CL/paper_14.html">
      Characterizing Sociolinguistic Variation in the Competing Vaccination
  Communities
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      公衆衛生の専門家や政策立案者は、サイバーコミュニティで公衆衛生の誤った情報を暴くための効果的なメッセージベースの介入を考案するという課題に取り組みます。これらの社会言語の違いは、より良いメッセージ介入を考案するためにこれらのコミュニティを特徴付け、理解するためのプロキシとして使用できること。 
[ABSTRACT] Twitterで競合する2人のvaformerのソーシャルネットワーク分析では、2つのコミュニティの間に有意差があることが示されています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-08">
        <br>2020-06-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multimodal Text Style Transfer for Outdoor Vision-and-Language
  Navigation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/cs.CL/paper_15.html">
      Multimodal Text Style Transfer for Outdoor Vision-and-Language
  Navigation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、MTST学習アプローチがモデルにとらわれず、MTSTアプローチが屋外VLNタスクのベースラインモデルを大幅に上回っており、テストセットでタスクの完了率を22 \％向上させ、新しい状態を実現していることを示しています。アートパフォーマンス..最初に、Google Maps APIによって生成された指示のスタイルを転送してナビゲーションデータを充実させ、次に、拡張された外部の屋外ナビゲーションデータセットでナビゲーターを事前トレーニングします。ビジョンと言語ナビゲーション（VLN）タスクでは、エージェントは自然言語の指示に従い、視覚環境でナビゲートします。 
[ABSTRACT]実際の屋外環境でのナビゲーションは依然として重要な課題です。GoogleMaps APIから指示のスタイルを転送して、ナビゲーションデータを強化します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: The NTT DCASE2020 Challenge Task 6 system: Automated Audio Captioning
  with Keywords and Sentence Length Estimation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/eess.AS/paper_0.html">
      The NTT DCASE2020 Challenge Task 6 system: Automated Audio Captioning
  with Keywords and Sentence Length Estimation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このテクニカルレポートでは、音響シーンとイベントの検出と分類（DCASE）2020チャレンジ、タスク6：自動オーディオキャプションに参加するシステムについて説明します。複数のキーワードを使用してキーワードと文の長さを推定することで、メインキャプションの生成とサブ不確定性の問題を同時に解決します。 -タスク学習..私たちのモデルは20.7 SPIDErスコアを達成し、ベースラインシステムのスコアは5.4でした。 
[要旨]私たちの提出物は、自動音声字幕作成における2つの不確定性問題の解決に焦点を当てています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Private Speech Characterization with Secure Multiparty Computation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/eess.AS/paper_1.html">
      Private Speech Characterization with Secure Multiparty Computation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      脅威モデルとして、パッシブセキュリティ、つまり自動化された人間の音声分類には明らかな利点がありますが、アプリケーション開発者は、保護されていないオーディオ信号処理から公称範囲を超えた知識を得ることができます。ボブがアリスのスピーチ信号を暗号化されていない状態で見ることのない、別のパーティ（ボブ）の深いニューラルネットワークを使用します。 
[要約]このホワイトペーパーでは、人間のコミュニケーションのディープラーニングベースの音声分類に対する最初のプライバシー保護処理ソリューションを提案します。脅威の分類として、プロトコルの逸脱した悪意のある当事者によるセキュリティの両方の受動的な領域を検討します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Whole-Word Segmental Speech Recognition with Acoustic Word Embeddings -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/eess.AS/paper_2.html">
      Whole-Word Segmental Speech Recognition with Acoustic Word Embeddings
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最終的なモデルは、同等のA2Wモデルよりも改善されます。さらに、筆記された単語ラベルの共同トレーニングされた音響単語埋め込み（AWE）と音響的に接地された単語埋め込み（AGWE）による事前トレーニングの使用を調査します。 AWEを使用して音響表現を事前トレーニングすることで、マージンを大幅に削減できます。AGWEを使用して単語予測レイヤーを事前トレーニングすることで、追加の（小さい）ゲインを取得できます。 
[ABSTRACT]モデルは、音響の単語の埋め込みに基づいています。これらは、音響の単語の埋め込みシステムに基づいています。さらに、驚異的な方法で音響表現を事前トレーニングします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Instantaneous PSD Estimation for Speech Enhancement based on Generalized
  Principal Components -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/eess.AS/paper_3.html">
      Instantaneous PSD Estimation for Speech Enhancement based on Generalized
  Principal Components
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ただし、この行列の時間的に滑らかな一般化固有値から直接PSDを推定し、時間的に滑らかなPSD推定を生成する代わりに、新しく定義された瞬間的な一般化固有値からPSDを推定して、瞬間的なPSD推定を生成することを提案します。一般化された固有ベクトルに基づく変換マイク信号。この論文では、一般化された主成分に基づく瞬時PSD推定アプローチを提案します。 
[要旨]発話は非常に非psdであるため、時間を維持することでパフォーマンスを向上させることができます-psd推定の変動
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Exploring the time-domain deep attractor network with two-stream
  architectures in a reverberant environment -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/eess.AS/paper_4.html">
      Exploring the time-domain deep attractor network with two-stream
  architectures in a reverberant environment
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この研究では、2ストリーム畳み込みネットワークを備えた時間領域ディープアトラクタネットワーク（TD-DAN）を提案します。これは、可変数のスピーカーの条件下で残響除去と分離タスクの両方を効率的に実行します。スピーカーエンコーディングストリーム（SES） TD-DANモデルの話者情報のモデルであり、さまざまな波形エンコーダーで探索されます。実験結果は、TD-DANが10.40 / 9.78 dBおよび9.15 / 7.92のスケール不変ソース対歪み比（SI-SDR）ゲインを達成したことを示しました。反響2および3スピーカーの開発/評価セットでのdB、Conv-TasNet 1.55 / 1.33 dBおよび0.94 / 1.21 dBをそれぞれ超える。 
[ABSTRACT] deep td-danの音声分離は、時間-周波数領域で行われますが、これは最適ではありません。これらのモデルは、残響環境でパフォーマンスが低下します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: End-To-End Speech Synthesis Applied to Brazilian Portuguese -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/eess.AS/paper_5.html">
      End-To-End Speech Synthesis Applied to Brazilian Portuguese
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      得られた結果は、より小さなデータセットを使用している場合でも、英語をカバーする関連研究と同等です。提案されたシナリオでは、Mozilla TTSとRTISI-LAボコーダーに基づくモデルが最高のパフォーマンスを示し、4.03 MOS値を達成しました。また、さまざまなボコーダー（RTISI-LA、WaveRNN、Universal WaveRNN）に従ってモデルのパフォーマンスを分析しました。音声文字起こしの使用、転移学習（英語から）、ノイズ除去。 
[ABSTRACT]データセットには1人の発言者から10.5時間あります。データセットには3.5時間の分析が必要です。モデルのパフォーマンスも分析しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-11">
        <br>2020-05-11
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Unsupervised Interpretable Representation Learning for Singing Voice
  Separation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/eess.AS/paper_6.html">
      Unsupervised Interpretable Representation Learning for Singing Voice
  Separation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの方法の利点を実証するために、バイナリマスキングを介してインフォームドシンギング音声分離のタスクに取得した表現を採用し、スケール不変の信号対歪み比によって取得した分離品質を測定します。音声と音楽ソースの分離で望まれる非負性、滑らかさ、時間周波数マスキングの対象となる再構成などの短時間フーリエ変換の利便性を維持しながら、歌声分離の意味のある表現を学習できます。は、教師なしの目的を使用してトレーニングでき、歌声を再構築するためのデコード関数として単純な正弦波モデルを使用するノイズ除去オートエンコーダモデルに依存します。 
[ABSTRACT]私たちの方法は、教師なしの目的を使用してトレーニングできます。シンプルな正弦波モデルを使用して歌声を再構成するノイズ除去オートエンコーダモデルに依存しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-03">
        <br>2020-03-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Consistent Independent Low-Rank Matrix Analysis for Determined Blind
  Source Separation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/eess.AS/paper_7.html">
      Consistent Independent Low-Rank Matrix Analysis for Determined Blind
  Source Separation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      独立した低ランクマトリックス分析（ILRMA）は、特定の状況におけるブラインドソース分離（BSS）のための最先端のアルゴリズムです（マイクの数はソース信号の数以上です）。ILRMAは、非負行列因数分解（NMF）を介してソース信号のパワースペクトログラムをモデル化することによる優れた分離性能。スペクトログラムはオーバーラップウィンドウ（およびウィンドウ関数がメインローブおよびサイドローブと呼ばれるスペクトルスミアリングを誘発する）によって計算されるため、時間-周波数ビンは互いに依存しています。 
[ABSTRACT] ilrmaは、非負行列因数分解を介してソース信号のパワースペクトログラムをモデル化することにより、優れた分離性能を実現しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Transformer-based Audio Captioning Model with Keyword Estimation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/eess.AS/paper_8.html">
      A Transformer-based Audio Captioning Model with Keyword Estimation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      TRACKEは、入力オーディオのオーディオイベント/シーンに対応する単語セットで構成されるキーワードを推定し、推定されたキーワードを参照しながらキャプションを生成して、単語選択の不確定性を減らします。パブリックAACデータセットの実験結果は、TRACKEが状態を達成したことを示しています最先端のパフォーマンスとキャプションとそのキーワードの両方の推定に成功しました。自動オーディオキャプション（AAC）の問題の1つは、オーディオイベント/シーンに対応する単語選択の不確定性です。 
[ABSTRACT] trackeは最先端のパフォーマンスを達成し、キャプションとそのキーワードの両方を正常に推定しました。trackeは、音響イベント検出のサブタスクを実行しながら、aacのメインタスクの問題を同時に解決します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: WaveNODE: A Continuous Normalizing Flow for Speech Synthesis -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/eess.AS/paper_9.html">
      WaveNODE: A Continuous Normalizing Flow for Speech Synthesis
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      従来のモデルとは異なり、WaveNODEはフロー操作に使用される関数に制約を課さないため、より柔軟で複雑な関数を使用できます。さらに、WaveNODEを最適化して、教師ネットワークや補助損失項を必要とせずに可能性を最大化できます。ただし、これらのモデルには、十分にトレーニングされた教師ネットワークまたはいくつかのフローステップが必要であり、メモリの効率が悪くなります。 
[ABSTRACT] wavenodeを使用すると、より柔軟で複雑な関数を作成できます。これらのモデルには、十分にトレーニングされた教育済みネットワークまたは多数のフローステップが必要です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-08">
        <br>2020-06-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Personalization of Hearing Aid Compression by Human-In-Loop Deep
  Reinforcement Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/eess.AS/paper_10.html">
      Personalization of Hearing Aid Compression by Human-In-Loop Deep
  Reinforcement Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      補聴器ユーザーのほぼ半数は、一般に処方されている設定とは異なる設定を好みます。シミュレーションと被験者のテスト結果の両方が報告され、開発された個別圧縮の有効性を示しています。補聴器フィッティングで使用される既存の規範的圧縮戦略は、ゲインに基づいて設計されています特定のユーザーにとって必ずしも最適ではないユーザーのグループからの平均。 
[ABSTRACT]開発されたアプローチは、ユーザーのフィードバックに基づいて圧縮を最適化するために、特定のユーザーの聴覚の好みを学習するように設計されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Psychoacoustically Motivated Audio Declipping Based on Weighted l1
  Minimization -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/eess.AS/paper_11.html">
      Psychoacoustically Motivated Audio Declipping Based on Weighted l1
  Minimization
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      重み付けは、アルゴリズムの複雑さを低く保ちながら、復元の品質を向上させます。聴覚の絶対しきい値、グローバルマスキングしきい値、および2次曲線に基づいて、重みの3つの可能な構成が提案されています。この方法には、心理音響学が組み込まれています。 $ \ ell_1 $の最小化で変換係数を重み付けすることにより、情報。 
[要約]この方法は、聴覚の絶対しきい値とグローバルマスキングしきい値に基づいています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-05-02">
        <br>2019-05-02
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Mechanotransduction and dynamic outflow regulation in trabecular meshwork requires Piezo1 channels -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-02/biorxiv.physiology/paper_0.html">
      Mechanotransduction and dynamic outflow regulation in trabecular meshwork requires Piezo1 channels
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      Piezo1の発現は、転写産物とタンパク質の分析、およびYoda1を介した電流と一次ヒトTM細胞の
[Ca2 +] iの上昇を可視化することで確認されました。それを支える分子メカニズムの理解はまだ初期の段階にあります。圧力センサーとしてのPiezo1チャネルの重要性は、圧力によって引き起こされる電流と従来の流出機能のGsMTx4依存性によって示されました。 
[要約]機械刺激性piezo1チャネルは、従来の流出経路のカルシウムシグナル伝達とダイナミクスを介してtm圧力応答を調節します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br>2020-07-01
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
