<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-12-15の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Group Communication with Context Codec for Ultra-Lightweight Source
  Separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.SD/paper_0.html">
      <font color="black">Group Communication with Context Codec for Ultra-Lightweight Source
  Separation</font>
    </a>
  </h2>
  <font color="black">実験結果は、GC3が2.5％のモデルサイズで、幅広いベースラインアーキテクチャと同等またはそれ以上のパフォーマンスを達成できることを示しています。モデルは、大幅に狭い幅でグループに並列に適用できます。 、モデルのパフォーマンスを犠牲にすることなく、モデルのサイズと複雑さの両方を削減するために、コンテキストコーデック（GC3）設計を使用したグループ通信を提案します。 
[概要]近年、軽量モデル設計コンセプトのコンセプトが提案されていますが、多くのモデルは、モデルサイズ、モデルの複雑さ、およびモデルのパフォーマンスのバランスを見つけることに依然として苦労しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Audio Captioning using Pre-Trained Large-Scale Language Model Guided by
  Audio-based Similar Caption Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.SD/paper_1.html">
      <font color="black">Audio Captioning using Pre-Trained Large-Scale Language Model Guided by
  Audio-based Similar Caption Retrieval</font>
    </a>
  </h2>
  <font color="black">次に、音声入力のキャプションは、ガイダンスキャプションを参照しながら、事前にトレーニングされた言語モデルを使用して生成されます。音声キャプションの問題の1つは、音声キャプションのペアを収集するのが難しいため、トレーニングデータが不足していることです。 Webをクロールします。音声キャプションの目的は、入力音声を自然な言語を使用して説明に変換することです。 
[概要]音声の収集が難しいため、音声のキャプションが問題になります-ウェブをクロールして字幕のペアを作成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Detecting Parkinson's Disease From an Online Speech-task -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.SD/paper_2.html">
      <font color="black">Detecting Parkinson's Disease From an Online Speech-task</font>
    </a>
  </h2>
  <font color="black">このツールを使用すると、ビデオ/オーディオ対応デバイスを使用して、ほぼすべての人からデータを収集でき、神経学的ケアの公平性とアクセスに貢献します。標準的な音響機能（Mel Frequency Cepstral Coefficients（MFCC）、ジッターおよびシマーバリアント）の両方を抽出しました。音声データからの深層学習ベースの機能。さらに分析すると、広く使用されているMFCC機能と、パーキンソンの口頭発声タスク（「ahh」と発音）を検出するために設計された以前に検証された発声障害機能のサブセットに、最も明確な情報が含まれていることがわかります。 
[概要]研究者は726人のユニークな参加者からデータを収集しました。彼らは英語のアルファベットの文字を含む人気のパングラムを発するように言われました。これらの機能を使用して、いくつかの機械学習アルゴリズムをトレーニングしました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-02">
        <br><font color="black">2020-09-02</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: High-Contrast Reflection Tomography with Total-Variation Constraints -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_0.html">
      <font color="black">High-Contrast Reflection Tomography with Total-Variation Constraints</font>
    </a>
  </h2>
  <font color="black">私たちのフレームワークは、全変動と非負性の制約を課しながら、測定された散乱波動場との整合性を保証する一連の正規化された最小二乗サブ問題を解決します。結果として生じるサブ問題を解決し、自動パラメータ選択を考案するための近位準ニュートン法を提案します。各サブ問題の制約を決定するルーチン..オブジェクトの背景モデルからのサイド情報を必要としない制約付き増分周波数反転フレームワークを提案します。 
[概要]この論文では、高コントラストの物体の反射トモグラフィーについて考察します。これらには、物体の背景モデルから測定された側面を必要としないサブ問題が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-06">
        <br><font color="black">2020-05-06</font>
      </time>
    </span>
</section>
<!-- paper0: COVID-19 Image Data Collection: Prospective Predictions Are the Future -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_1.html">
      <font color="black">COVID-19 Image Data Collection: Prospective Predictions Are the Future</font>
    </a>
  </h2>
  <font color="black">出版物の図やさまざまなWebベースのリポジトリから、データローダーコードを伴う機械学習（ML）に適した形式に手動で集約されました。最初の症状からの経過時間、集中治療室（集中治療室）などの正面および側面の画像とメタデータを収集しました。 ICU）ステータス、生存ステータス、挿管ステータス、または病院の場所。ICUの必要性の予測、患者の生存の予測、治療中の患者の軌跡の理解など、データの複数の可能な使用例を示します。 
[要約]データセットには、何百もの正面図X線が含まれています。これは、covid-19画像および予後データの最大の公開リソースです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-22">
        <br><font color="black">2020-06-22</font>
      </time>
    </span>
</section>
<!-- paper0: D-LEMA: Deep Learning Ensembles from Multiple Annotations -- Application
  to Skin Lesion Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_2.html">
      <font color="black">D-LEMA: Deep Learning Ensembles from Multiple Annotations -- Application
  to Skin Lesion Segmentation</font>
    </a>
  </h2>
  <font color="black">この目的のために、複数のグラウンドトゥルースアノテーションの集約における2つの主要な要因を考慮して、セグメンテーションタスク用のベイズ完全畳み込みネットワーク（FCN）のアンサンブルを提案します。（1）アノテーター間の不一致に起因するトレーニングデータ内の矛盾するアノテーションの処理（2）ベースモデル予測の融合による信頼性キャリブレーションの改善..医療画像セグメンテーション注釈は、人間の注釈者の本質的な違いとあいまいな境界のために、専門家の間でも観察者間/観察者内のばらつきに悩まされます..注釈者の意見のコレクションを活用する画像の場合、ゴールドスタンダードを推定する興味深い方法です。 
[要約]この論文では、深いモデルをトレーニングするときにアノテーターの不一致を処理するアプローチを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: An efficient Quasi-Newton method for nonlinear inverse problems via
  learned singular values -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_3.html">
      <font color="black">An efficient Quasi-Newton method for nonlinear inverse problems via
  learned singular values</font>
    </a>
  </h2>
  <font color="black">これは、特異値分解を使用し、モデル出力から特異値へのマッピングを学習して、更新されたヤコビアンを計算することで実現します。計算上より効率的な代替方法は、ヤコビアンの繰り返し計算が近似値に置き換えられる準ニュートン法です。更新..実験データを使用して、電気インピーダンストモグラフィーの高度に非線形な逆問題の結果を示します。 
[要約]問題を解決するために、これには複数の開発エラーが必要です。これには、特異値分解の使用と、モデル出力から特異値へのマッピングの学習が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Progressive Domain Adaptation from a Source Pre-trained Model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_4.html">
      <font color="black">Progressive Domain Adaptation from a Source Pre-trained Model</font>
    </a>
  </h2>
  <font color="black">次に、クラスプロトタイプとの類似性スコアに基づいて、すべてのターゲットサンプルに疑似ラベルを割り当てます。これから、自己エントロピー基準を使用して信頼できるサンプルを選択し、これらをクラスプロトタイプとして定義します。コードはhttps：/で公開されています。 /github.com/youngryan1993/PrDA-Progressive-Domain-Adaptation-from-a-Source-Pre-trained-Model。 
[概要]私たちの重要なアイデアは、ソースドメインから事前にトレーニングされたモデルを活用し、自己学習方式でターゲットモデルを徐々に更新することです。代わりに、自己シンプソンを使用して信頼できるソースサンプルを選択します。したがって、セットを提案します。 -から-距離を設定します-調整可能なハイパーパラメータを必要としないベースのフィルタリング</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-03">
        <br><font color="black">2020-07-03</font>
      </time>
    </span>
</section>
<!-- paper0: DSM Refinement with Deep Encoder-Decoder Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_5.html">
      <font color="black">DSM Refinement with Deep Encoder-Decoder Networks</font>
    </a>
  </h2>
  <font color="black">この作業は、そのようなDSMを自動的に改良するアプローチを示します。ただし、計算されたDSMは、時間のかかるプロセスで手動でクリーンアップする必要があるノイズ、アーティファクト、およびデータホールに悩まされます。重要なアイデアはニューラルネットワークを教えることです。参照データからの都市部の特性。 
[概要]計算されたdsmsは、手動でクリーンアップする必要のあるノイズ、アーティファクト、およびデータホールに悩まされています。重要なアイデアは、参照データから都市部の特性をニューラルネットワークに教えることです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Models Genesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_6.html">
      <font color="black">Models Genesis</font>
    </a>
  </h2>
  <font color="black">この制限を克服するために、Generic Autodidactic Modelsと呼ばれる一連のモデルを構築しました。これらは、ex nihilo（手動ラベルなし）、自己学習（自己管理による学習）、および汎用（提供）で作成されるため、ModelsGenesisと呼ばれます。アプリケーション固有のターゲットモデルを生成するためのソースモデルとして）。さらに重要なことに、モデルを最初から3Dで学習するだけでは、ImageNetから2Dで転送学習するよりもパフォーマンスが向上するとは限りませんが、ModelsGenesisは常に2D / 2.5Dアプローチを上回っています。 ImageNetから事前にトレーニングされたモデルの微調整、モデルジェネシスの2Dバージョンの微調整、3D解剖学的情報の重要性、3D医療イメージングにおけるモデルジェネシスの重要性の確認などが含まれます。 Genesisは、セグメンテーションと分類の両方をカバーする5つのターゲット3Dアプリケーションすべてにおいて、ゼロからの学習および既存の事前トレーニング済み3Dモデルを大幅に上回っています。 
[概要]これは、シンプルでありながら強力な観察に基づいて構築された、教師あり学習フレームワークによるものです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-09">
        <br><font color="black">2020-04-09</font>
      </time>
    </span>
</section>
<!-- paper0: Moving Object Captured with Pink Noise Pattern in Computational Ghost
  Imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_7.html">
      <font color="black">Moving Object Captured with Pink Noise Pattern in Computational Ghost
  Imaging</font>
    </a>
  </h2>
  <font color="black">ホワイトノイズ法が失敗したときに、私たちの方法がオブジェクトを画像化できることを示します。その独自の能力と適用範囲を調べるために、オブジェクトは水平軸で可変振幅で振動し、一般的に使用されるホワイトノイズによる結果も測定されます。比較..さらに、私たちの方法は、使用するパターンの数が少なく、信号対雑音比（SNR）を大幅に向上させます。 
[概要]オブジェクトは、その固有の能力を調べるために可変持続時間で振動します。一般的に使用されるホワイトノイズによる結果も比較として測定されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: IPN-V2 and OCTA-500: Methodology and Dataset for Retinal Image
  Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_8.html">
      <font color="black">IPN-V2 and OCTA-500: Methodology and Dataset for Retinal Image
  Segmentation</font>
    </a>
  </h2>
  <font color="black">この作業では、水平方向のパーセプトロン能力を強化するために平面パーセプトロンを追加することによってIPNを拡張する画像投影ネットワークV2（IPN-V2）を提案します。その利点の1つは、セグメンテーション結果が元のボリュームから直接得られることです。任意の投影画像と網膜層セグメンテーションを使用します。実験結果は、提案されたIPN-V2がRVセグメンテーションとFAZセグメンテーションでIPNや他の深層学習法よりも優れていることを示しています。 
[概要]オクタ画像の網膜血管（rv）と中心窩無血管ゾーン（faz）のセグメンテーション。オクタ-500と呼ばれる新しいipnベースのデータセットには、360kを超える画像が含まれています。網膜血管には、サイズが約80gbの360kを超える画像が含まれています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Sub-Nyquist computational ghost imaging with orthonormalized colored
  noise pattern -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_9.html">
      <font color="black">Sub-Nyquist computational ghost imaging with orthonormalized colored
  noise pattern</font>
    </a>
  </h2>
  <font color="black">結果は、我々の方法が従来の方法よりも一次低いサンプリング比を使用しながら高品質の画像を提供できることを示唆している。我々はここで、サブナイキスト計算ゴーストイメージングを達成するためのカラードノイズパターンに基づく直交化アプローチを提案し、実験的に示す。 。計算ゴーストイメージングは、通常、高品質の画像を取得するために多くのパターンを必要とします。 
[概要]ホワイトノイズパターンの代わりにカラードノイズスペックルパターンが最近提案されました。パターンの数が限られている場合でも、信号対ノイズ比が大幅に向上します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: A Preliminary Comparison Between Compressive Sampling and Anisotropic
  Mesh-based Image Representation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_10.html">
      <font color="black">A Preliminary Comparison Between Compressive Sampling and Anisotropic
  Mesh-based Image Representation</font>
    </a>
  </h2>
  <font color="black">結果は、同じサンプル密度で、AMA表現がテストされたアルゴリズムに基づいてCSよりも優れた再構成品質を提供できることを示しています。この論文では、CSと最近開発されたMbIRメソッドであるAMA表現との予備比較を行います。 MbIRは画像ピクセルに直接作用し、三角形のメッシュを使用してより少ないポイントで画像を表現します。 
[ABSTRACT] csと最近開発されたmbirメソッドであるama表現は、貧弱な画像の例ですが、amaとして知られているメソッドはあまり注目されていません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: WDNet: Watermark-Decomposition Network for Visible Watermark Removal -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_11.html">
      <font color="black">WDNet: Watermark-Decomposition Network for Visible Watermark Removal</font>
    </a>
  </h2>
  <font color="black">公開グレースケールデータセットLVWおよびCLWDでの広範な実験は、提案されたWDNetが精度と効率の両方で最先端のアプローチよりも優れていることを一貫して示しています。目に見える透かしは、著作権の所有権を保護するために画像で広く使用されています。分解定式化により、WDNetは、透かしを単に削除するのではなく、画像から透かしを分離することができます。 
[概要]透かしのサイズ、形状、色、透明度の難しさは、これらの方法に大きな障壁を設定します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: AIDE: Annotation-efficient deep learning for automatic medical image
  segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_12.html">
      <font color="black">AIDE: Annotation-efficient deep learning for automatic medical image
  segmentation</font>
    </a>
  </h2>
  <font color="black">3つの医療センターからの872人の患者の11,852枚の乳房画像を含む3つの臨床データセットについて、AIDEは、完全に監視された対応者によって生成されたものに匹敵するセグメンテーションマップと、10％のトレーニング注釈のみを利用する独立した放射線科医の手動注釈を一貫して生成します。セグメンテーションは、医療画像アプリケーションにとって非常に重要です。一般的な深層学習アプローチは、通常、医療画像では利用できないことが多い高品質の手動注釈付きの非常に大規模なトレーニングデータセットに依存しています。 
[概要]一般的なディープラーニングアプローチは、通常、高品質の手動アノテーションを備えた非常に大規模なトレーニングデータセットに依存しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-09">
        <br><font color="black">2020-12-09</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Hybrid Representations for Automatic 3D Vessel Centerline
  Extraction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_13.html">
      <font color="black">Learning Hybrid Representations for Automatic 3D Vessel Centerline
  Extraction</font>
    </a>
  </h2>
  <font color="black">ただし、3D畳み込みは計算効率が悪いため、3D CNNが画像全体のグローバルキューをキャプチャするのに十分な大きさの受容野を使用できません。CTAデータセットで提案されたアプローチを検証し、従来のCNNベースと比較して優れたパフォーマンスを示します。ベースライン..主なアイデアは、CNNを使用して画像作物の血管の局所的な外観を学習し、別のポイントクラウドネットワークを使用して画像全体の血管のグローバルジオメトリを学習することです。 
[ABSTRACT]畳み込みニューラルネットワーク（cnns）に基づく既存の方法は、抽出された血管の不連続性に悩まされる可能性があります。ただし、3D畳み込みは非効率的であり、画像全体のグローバルキューをキャプチャするには3D表現が必要です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Biomechanical modelling of brain atrophy through deep learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_14.html">
      <font color="black">Biomechanical modelling of brain atrophy through deep learning</font>
    </a>
  </h2>
  <font color="black">このツールは、アルツハイマー病ニューロイメージングイニシアチブ（ADNI）データセットの縦断的脳萎縮データを使用して検証され、トレーニング済みモデルが最小限の残差で新しい脳変形を迅速にシミュレートできることを示しています。この方法は、データで使用できる可能性があります。増強または脳の成長と萎縮を反映するさまざまな原因となる仮説の調査のために..現実的な脳の変形の概念の証明、深層学習（DL）ベースの差別化可能な生体力学的モデルを提示します。 
[概要]局所萎縮と成長の所定のマップを入力として使用して、ネットワークは画像を変形することを学習します。これらのマップを使用して、組織の難易度のネオフックモデルを使用して画像を変形することを学習します。ツールは脳のモデルとして使用できます。成長と萎縮</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Cardiac Intervention Assistance: Hardware-aware Neural
  Architecture Exploration for Real-Time 3D Cardiac Cine MRI Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_15.html">
      <font color="black">Towards Cardiac Intervention Assistance: Hardware-aware Neural
  Architecture Exploration for Real-Time 3D Cardiac Cine MRI Segmentation</font>
    </a>
  </h2>
  <font color="black">リアルタイム心臓磁気共鳴画像法（MRI）は、さまざまな心臓介入を導く上でますます重要な役割を果たします。提案されたフレームワークは、基礎となるハードウェアを考慮して、リアルタイムの制約を処理するために損失関数に遅延正則化項を組み込みます。この作業では、リアルタイム3D心臓シネMRIセグメンテーションのための最初のハードウェア対応マルチスケールニューラルアーキテクチャ検索（NAS）フレームワークを提示します。 
[概要]目立つものを減らすために、シネmriフレームをオンザフライでセグメント化する必要があります-目立つlag.stateを回避するために-最先端の方法は主に精度のみに焦点を当てており、リアルタイムアプリケーションにはほとんど採用できませんまたはローカルハードウェア上</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-17">
        <br><font color="black">2020-08-17</font>
      </time>
    </span>
</section>
<!-- paper0: Reconstructing unseen modalities and pathology with an efficient
  Recurrent Inference Machine -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_16.html">
      <font color="black">Reconstructing unseen modalities and pathology with an efficient
  Recurrent Inference Machine</font>
    </a>
  </h2>
  <font color="black">$ T_1 $加重脳データおよび結合データでのトレーニングは、CSと比較して信号をわずかに強化しました。RIMは、$ T_2 $加重膝データでトレーニングした場合、CSよりも正確にトレーニング中に見えない病変を再構築できました。IndRNNは効率的な反復ユニットであり、パフォーマンスを維持しながら、CSと比較して推論時間を68 \％短縮します。 
[ABSTRACT]理論は、リムが加速されたmri再構成の逆問題を解決することを学習する一方で、可変のイメージング条件に対してロバストであるというものです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Uncertainty Analysis in 3D SPECT Reconstruction based on Probabilistic
  Programming -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_17.html">
      <font color="black">Uncertainty Analysis in 3D SPECT Reconstruction based on Probabilistic
  Programming</font>
    </a>
  </h2>
  <font color="black">単一光子放射型コンピュータ断層撮影（SPECT）は、動物および人間の臓器の機能分析に使用される核医学画像診断法の1つです。No-U-TurnSampler（NUTS）は、不確実性を考慮してスキャンされたオブジェクトシステムを推定するために使用されます。 、128x128x128ボクセル以上のファントムサイズの場合、再構成時間をさらに改善する必要があります。 
[概要]データの分析結果は、3Dスペクトル画像再構成における人間の作業が従来の再構成方法を上回っていることを示しています。結果は、再構成における提示された作業が従来の方法を上回っていることを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-19">
        <br><font color="black">2020-08-19</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic Open-World Reliability Assessment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_18.html">
      <font color="black">Automatic Open-World Reliability Assessment</font>
    </a>
  </h2>
  <font color="black">したがって、標準分類器またはオープンセット分類器のいずれについても、世界がいつ変化し、OOD入力が増加するとシステムの信頼性が低下するかを判断できることが重要です。このプロセスを自動化するために、ここでは、オープンワールド認識の信頼性を形式化します。問題を解決し、報告されたスコア/確率データの分布のみを使用してこの新しい問題に対処するための複数の自動信頼性評価ポリシーを提案します。オープンワールドでの画像分類は、分布外（OOD）画像を処理する必要があります。 
[ABSTRACT]システムは自動的にood画像を拒否する必要があります。そうしないと、既知のクラスの上にマッピングされます。ただし、softmaxとopen-set分類子はoodデータの頻度に依存します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-11">
        <br><font color="black">2020-11-11</font>
      </time>
    </span>
</section>
<!-- paper0: Open-World Learning Without Labels -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_19.html">
      <font color="black">Open-World Learning Without Labels</font>
    </a>
  </h2>
  <font color="black">研究者は、複雑なシナリオで動作する画像分類タスク用にいくつかのオープンワールド学習エージェントを提案しました。また、ラベルなしのオープンワールド学習用の新しいメトリックを導入します。自律エージェントがほぼリアルタイムで応答するか、通信インフラストラクチャが限られている地域では、データに人間によるラベルを付けることはできません。 
[ABSTRACT]自律的な真のオープン-世界の終わりのない学習エージェント。私たちの理論と方法は、自律的な真のオープン-世界の終わりのない学習を開発するための出発点になると期待しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-25">
        <br><font color="black">2020-11-25</font>
      </time>
    </span>
</section>
<!-- paper0: Accurate Cell Segmentation in Digital Pathology Images via Attention
  Enforced Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_20.html">
      <font color="black">Accurate Cell Segmentation in Digital Pathology Images via Attention
  Enforced Networks</font>
    </a>
  </h2>
  <font color="black">さらに、高レベルと低レベルの機能を橋渡しする機能融合ブランチを導入します。いくつかの以前の方法との定量的比較は、私たちのアプローチの優位性を示しています。最後に、マーカー制御流域アルゴリズムを適用して、予測を後処理します。断片化された領域を減らすためのセグメンテーションマップ。 
[概要]このサイクルの一部にセルが割り当てられるのはこれが初めてです。主なツールは、注意を強制するネットワーク（aenet）を開発することです。これは、空間的注意モジュールとチャネル注意モジュールに基づいて構築されます。ローカルに統合されます。グローバルな依存関係と重み効果のあるチャネルを適応的に備えた機能</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Effect of the regularization hyperparameter on deep learning-based
  segmentation in LGE-MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.IV/paper_21.html">
      <font color="black">Effect of the regularization hyperparameter on deep learning-based
  segmentation in LGE-MRI</font>
    </a>
  </h2>
  <font color="black">また、著者は、他の深層学習ハイパーパラメータの手動調整または調整を採用して、90％の検証精度を達成する前にすべてのエポックの10％に達した場合にのみ実行します。この作業では、著者は次の範囲を示すことを目的としています。 L2正則化ハイパーパラメータの任意の選択がLGE-MRIの深層学習ベースのセグメンテーションの結果に影響を与える可能性があります。実験的な比較は、小さなL2正則化値が心筋境界のより良いセグメンテーションにつながる可能性があることを示しています。 
[概要]これらは、さまざまな深層学習ベースのセグメンテーションネットワークを作成するために使用できます。これらは、l2正則化の長さに対する高血圧の例です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-10">
        <br><font color="black">2020-12-10</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: High-Contrast Reflection Tomography with Total-Variation Constraints -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_0.html">
      <font color="black">High-Contrast Reflection Tomography with Total-Variation Constraints</font>
    </a>
  </h2>
  <font color="black">合成低解像度ファントムでのアプローチのパフォーマンスを検証し、高解像度ファントムでの不一致フォワードモデルテストを使用します。結果として生じるサブ問題を解決するための近位準ニュートン法を提案し、決定するための自動パラメーター選択ルーチンを考案します。各サブ問題の制約..従来の透過トモグラフィーとは異なり、測定された波動場にはターゲットオブジェクトの空間周波数情報がはるかに少ないため、反射レジームはひどく不適切です。 
[概要]この論文では、高コントラストの物体の反射トモグラフィーについて考察します。これらには、物体の背景モデルから測定された側面を必要としないサブ問題が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-06">
        <br><font color="black">2020-05-06</font>
      </time>
    </span>
</section>
<!-- paper0: COVID-19 Image Data Collection: Prospective Predictions Are the Future -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_1.html">
      <font color="black">COVID-19 Image Data Collection: Prospective Predictions Are the Future</font>
    </a>
  </h2>
  <font color="black">これは、出版物の図やさまざまなWebベースのリポジトリから、データローダーコードを伴う機械学習（ML）に適した形式に手動で集約されました。このデータセットには、現在、数百の正面図X線が含まれており、COVID-19の最大の公開リソースです。画像と予測データ。COVID-19の治療に役立つツールを開発および評価するために必要なリソースになります。ICUの必要性の予測、患者の生存の予測、理解など、データの複数の可能な使用例を示します。治療中の患者の軌跡。 
[要約]データセットには、何百もの正面図X線が含まれています。これは、covid-19画像および予後データの最大の公開リソースです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-22">
        <br><font color="black">2020-06-22</font>
      </time>
    </span>
</section>
<!-- paper0: SPIN: Structure-Preserving Inner Offset Network for Scene Text
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_2.html">
      <font color="black">SPIN: Structure-Preserving Inner Offset Network for Scene Text
  Recognition</font>
    </a>
  </h2>
  <font color="black">この微分可能なモジュールは、認識アーキテクチャの前に挿入してダウンストリームタスクを容易にし、ニューラルネットワークに既存の空間整流ではなく入力強度をアクティブに変換する機能を提供します。既知の空間変換の補完モジュールとしても機能し、両方で機能します。それらとの独立した協調的な方法..広範な実験は、SPINの使用が、最先端のものと比較して、複数のテキスト認識ベンチマークの大幅な改善をもたらすことを示しています。 
[ABSTRACT]システムは、認識アーキテクチャの前に挿入して、ダウンストリームタスクを容易にすることができます。これにより、ニューラルネットワークは、既存の空間整流ではなく、入力強度をアクティブに変換できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-27">
        <br><font color="black">2020-05-27</font>
      </time>
    </span>
</section>
<!-- paper0: D-LEMA: Deep Learning Ensembles from Multiple Annotations -- Application
  to Skin Lesion Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_3.html">
      <font color="black">D-LEMA: Deep Learning Ensembles from Multiple Annotations -- Application
  to Skin Lesion Segmentation</font>
    </a>
  </h2>
  <font color="black">画像ごとに1つの注釈を使用して監視された設定で深いモデルをトレーニングすることは広く研究されていますが、画像ごとに複数の注釈を含むデータセットを操作するようにトレーニングを一般化することは、かなり未踏の問題です。この目的のために、ベイジアンのアンサンブルを完全に提案します。複数のグラウンドトゥルースアノテーションの集約における2つの主要な要因を考慮することによる、セグメンテーションタスクの畳み込みネットワーク（FCN）：（ 1）アノテーター間の不一致に起因するトレーニングデータ内の矛盾するアノテーションの処理、および（2）の融合による信頼性キャリブレーションの改善基本モデルの予測..医療画像セグメンテーションアノテーションは、人間のアノテーターの本質的な違いとあいまいな境界のために、専門家の間でも観察者間/観察者内の変動に悩まされています。 
[要約]この論文では、深いモデルをトレーニングするときにアノテーターの不一致を処理するアプローチを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Swapping Autoencoder for Deep Image Manipulation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_4.html">
      <font color="black">Swapping Autoencoder for Deep Image Manipulation</font>
    </a>
  </h2>
  <font color="black">その結果、テクスチャスワッピング、ローカルおよびグローバル編集、潜在コードベクトル演算など、さまざまな方法で実際の入力画像を操作するために使用できます。この方法はエンコーダーでトレーニングされているため、新しい入力の潜在コードを見つけることができます。画像は煩雑ではなく些細なものになります。複数のデータセットでの実験は、私たちのモデルが最近の生成モデルと比較してより良い結果を生み出し、実質的により効率的であることを示しています。 
[概要]スワッピングオートエンコーダは、画像操作専用に設計されたモデルです。複数の方法で実際の入力モデルを操作するために使用できます。その結果、さまざまな方法で画像を操作することもできます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-01">
        <br><font color="black">2020-07-01</font>
      </time>
    </span>
</section>
<!-- paper0: DUDE: Deep Unsigned Distance Embeddings for Hi-Fidelity Representation
  of Complex 3D Surfaces -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_5.html">
      <font color="black">DUDE: Deep Unsigned Distance Embeddings for Hi-Fidelity Representation
  of Complex 3D Surfaces</font>
    </a>
  </h2>
  <font color="black">DUDEは、表面への近接性を表すために符号なし距離フィールド（uDF）を使用し、表面の向きを表すために法線ベクトルフィールド（nVF）を利用する解きほぐされた形状表現です。DeepSDFなどの以前の作業とは対照的に、形状表現は次のようになります。ノイズの多い三角形のスープから直接学習し、水密メッシュを必要としません。この作業では、これらの両方の欠点を軽減するDUDE（深い符号なし距離埋め込み方法）を提案します。 
[概要]これらの単純な表現は、閉じた形状のみを表すためにのみ使用できます。これらには、点表面ネットワーク、ボクセル、メッシュが含まれます。これらは、これらのアプリケーションで使用した場合に低品質の結果を作成することを目的としています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Progressive Domain Adaptation from a Source Pre-trained Model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_6.html">
      <font color="black">Progressive Domain Adaptation from a Source Pre-trained Model</font>
    </a>
  </h2>
  <font color="black">最後に、事前にトレーニングされたソースモデルからの正則化を使用して、フィルター処理された疑似ラベルを使用してターゲットモデルをトレーニングします。これから、自己エントロピー基準を使用して信頼できるサンプルを選択し、これらをクラスプロトタイプとして定義します。ターゲットサンプルを観察します。事前にトレーニングされたソースモデルによって測定された自己エントロピーが低いほど、正しく分類される可能性が高くなります。 
[概要]私たちの重要なアイデアは、ソースドメインから事前にトレーニングされたモデルを活用し、自己学習方式でターゲットモデルを徐々に更新することです。代わりに、自己シンプソンを使用して信頼できるソースサンプルを選択します。したがって、セットを提案します。 -から-距離を設定します-調整可能なハイパーパラメータを必要としないベースのフィルタリング</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-03">
        <br><font color="black">2020-07-03</font>
      </time>
    </span>
</section>
<!-- paper0: Models Genesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_7.html">
      <font color="black">Models Genesis</font>
    </a>
  </h2>
  <font color="black">さらに重要なことに、モデルを最初から3Dで学習するだけでは、必ずしもImageNetから2Dで学習を転送するよりも優れたパフォーマンスが得られるとは限りませんが、Models Genesisは、ImageNetから事前にトレーニングされたモデルの微調整など、2D /2.5Dアプローチを常に上回っています。モデルジェネシスの2Dバージョンを微調整し、3D解剖学的情報の重要性と3D医用画像処理におけるモデルジェネシスの重要性を確認します。広範な実験により、モデルジェネシスはゼロからの学習や既存の事前トレーニング済み3Dモデルよりも大幅に優れていることがわかります。セグメンテーションと分類の両方をカバーする5つのターゲット3Dアプリケーションすべてで..この制限を克服するために、Generic Autodidactic Modelsと呼ばれる一連のモデルを構築しました。これらはexnihilo（手動ラベルなし）で作成されているため、ModelsGenesisと呼ばれます。教えられ（自己管理によって学習され）、一般的（アプリケーション固有のターゲットモデルを生成するためのソースモデルとして機能します）。 
[概要]これは、シンプルでありながら強力な観察に基づいて構築された、教師あり学習フレームワークによるものです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-09">
        <br><font color="black">2020-04-09</font>
      </time>
    </span>
</section>
<!-- paper0: Semantic Layout Manipulation with High-Resolution Sparse Attention -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_8.html">
      <font color="black">Semantic Layout Manipulation with High-Resolution Sparse Attention</font>
    </a>
  </h2>
  <font color="black">このパラダイムをレイアウト操作タスクに適合させるために、最大512x512の解像度で視覚的な詳細を新しいレイアウトに効果的に転送する高解像度のスパースアテンションモジュールを提案します。ADE20kおよびPlaces365データセットでの実験は、提案されたアプローチが大幅な改善を達成することを示しています既存の修復およびレイアウト操作方法に加えて..視覚的品質をさらに向上させるために、セマンティックエンコーダーと粗い合成から細かい合成のための2ステージデコーダーで構成される新しいジェネレーターアーキテクチャを紹介します。 
[概要]新しい検索エンジンは、新しいセマンティックセットで視覚品質を向上させるように機能します。新しいマッピングツールを使用して、画像から新しい情報に視覚情報を転送します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Information-Theoretic Segmentation by Inpainting Error Maximization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_9.html">
      <font color="black">Information-Theoretic Segmentation by Inpainting Error Maximization</font>
    </a>
  </h2>
  <font color="black">実験は、教師なしセグメンテーション品質で新しい最先端を達成する一方で、競合するアプローチよりも大幅に高速で一般的であることを示しています。簡単に計算できる損失は、貪欲な検索プロセスを駆動して、これらのパーティションの修復エラーを最大化します。この方法は、深いネットワークのトレーニングを必要とせず、計算コストが低く、クラスに依存せず、単一のラベルなし画像に単独で適用することもできます。 
[概要]私たちの方法は、深いネットワークのトレーニングを必要とせず、安価で、クラス-パパパです。一方のセットの他方からの予測可能性を最小限に抑えることができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Self-Supervised Tuning for Few-Shot Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_10.html">
      <font color="black">Self-Supervised Tuning for Few-Shot Segmentation</font>
    </a>
  </h2>
  <font color="black">具体的には、サポート画像から基礎となる意味的特徴を抽出するための基本学習者として、新しい自己教師あり内部ループが最初に考案されます。既存のメタ学習方法では、視覚的特徴が抽出されたときにカテゴリ固有の識別記述子を生成できない傾向があります。最後に、さまざまなエピソードから継続的に学習する機能を備えた最適化ベースのメタラーナーが、提案されたフレームワークの外部ループとして採用され、セグメンテーション結果が徐々に洗練されます。 
[要約]データセットは自己セグメンテーションスキームに基づいており、カテゴリを拡張します-ラベル予測のための特定の記述子</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-12">
        <br><font color="black">2020-04-12</font>
      </time>
    </span>
</section>
<!-- paper0: Deep N-ary Error Correcting Output Codes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_11.html">
      <font color="black">Deep N-ary Error Correcting Output Codes</font>
    </a>
  </h2>
  <font color="black">さらに、ディープN-ary ECOCに関する広範なアブレーション研究は、他のディープデータに依存しないアンサンブル手法よりも優れたパフォーマンスを示しています。具体的には、従来のECOCとその一般的な拡張N-ary ECOCは、元のマルチクラス分類問題を一連の独立したものに分解します。より単純な分類サブ問題..ディープN-aryECOCの一般化能力を検証するために、画像とテキストの両方の分類タスクについて、さまざまなディープニューラルネットワークアーキテクチャでバックボーンを変化させることによって実験を行います。 
[ABSTRACT] deep n-ary ecocは単純ではありませんが、基礎学習者のトレーニングに高い費用がかかるため、文献で十分に活用されています。画像とテキストの両方の分類タスクで、さまざまなディープニューラルネットワークアーキテクチャを使用してバックボーンを変更することで実験を行います。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-22">
        <br><font color="black">2020-09-22</font>
      </time>
    </span>
</section>
<!-- paper0: Anytime Inference with Distilled Hierarchical Neural Ensembles -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_12.html">
      <font color="black">Anytime Inference with Distilled Hierarchical Neural Ensembles</font>
    </a>
  </h2>
  <font color="black">私たちの2番目の貢献は、小さなアンサンブルの予測精度を高めるための新しい階層的蒸留法です。私たちの実験は、以前のいつでも推論モデルと比較して、HNEがCIFARで最先端の精度と計算上のトレードオフを提供することを示しています。 10/100およびImageNetデータセット..階層ツリー構造に複数のネットワークのアンサンブルを埋め込み、中間層を共有するための新しいフレームワークであるHierarchical Neural Ensembles（HNE）を提案します。 
[要約]では、アンサンブル内のモデルを多かれ少なかれ評価することにより、アンサンブルのメソッドを制御します。情報の量は、個々のモデル間で精度と多様性を割り当てるのに役立ちます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-03">
        <br><font color="black">2020-03-03</font>
      </time>
    </span>
</section>
<!-- paper0: SWIPENET: Object detection in noisy underwater images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_13.html">
      <font color="black">SWIPENET: Object detection in noisy underwater images</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、これら2つの問題に同時に対処するために、新しいSample-WeIghted hyPEr Network（SWIPENET）と、Curriculum Multi-Class Adaboost（CMA）という名前の堅牢なトレーニングパラダイムを提案します。複数の高解像度でセマンティックが豊富なハイパーフィーチャーマップ。これにより、小さなオブジェクトの検出が大幅に向上します。さらに、学習を簡単な概念から難しい概念へと駆り立てる人間の教育プロセスに触発されて、最初にクリーンな検出器をトレーニングするCMAトレーニングパラダイムを提案します。ノイズの多いデータの影響を受けません。 
[概要]提案されたスワイプネットcmaトレーニングコンセプトが提案されました。ノイズの多いデータの影響を受けないクリーンな検出器をトレーニングすることを目的としています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: IPN-V2 and OCTA-500: Methodology and Dataset for Retinal Image
  Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_14.html">
      <font color="black">IPN-V2 and OCTA-500: Methodology and Dataset for Retinal Image
  Segmentation</font>
    </a>
  </h2>
  <font color="black">私たちの知る限りでは、これは現在、豊富な情報を備えた最大のOCTAデータセットです。実験結果は、提案されたIPN-V2がRVセグメンテーションおよびFAZセグメンテーションにおいてIPNおよび他の深層学習方法よりも優れていることを示しています。 、画像投影ネットワークV2（IPN-V2）を提案し、平面パーセプトロンを追加してIPNを拡張し、水平方向のパーセプトロン能力を強化します。 
[概要]オクタ画像の網膜血管（rv）と中心窩無血管ゾーン（faz）のセグメンテーション。オクタ-500と呼ばれる新しいipnベースのデータセットには、360kを超える画像が含まれています。網膜血管には、サイズが約80gbの360kを超える画像が含まれています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: EventHands: Real-Time Neural 3D Hand Reconstruction from an Event Stream -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_15.html">
      <font color="black">EventHands: Real-Time Neural 3D Hand Reconstruction from an Event Stream</font>
    </a>
  </h2>
  <font color="black">したがって、学習に適した新しいイベントストリーム表現を受け入れる新しいニューラルアプローチを設計します。これは、新しく生成された合成イベントストリームでトレーニングされ、実際のデータに一般化できます。従来のカメラとはイベントカメラのデータモダリティが異なるため、既存のメソッドをイベントストリームに直接適用して再トレーニングすることはできません。私たちのメソッド、イベントストリームシミュレーター、およびデータセットは公開されます。 
[概要]新しい調査によると、イベントハンドは、カラー（または深度）カメラを使用した適切な単眼法よりも、精度と前例のない速度の手の動きをキャプチャする能力の点で優れています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-11">
        <br><font color="black">2020-12-11</font>
      </time>
    </span>
</section>
<!-- paper0: Dialog Policy Learning for Joint Clarification and Active Learning
  Queries -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_16.html">
      <font color="black">Dialog Policy Learning for Joint Clarification and Active Learning
  Queries</font>
    </a>
  </h2>
  <font color="black">対話システムに関するこれまでの研究は、明確化/情報探索の実行方法を専ら学習するか、能動学習を実行することに焦点を当てていました。インテリジェントシステムは、間違いから回復し、不確実性を解決し、トレーニング中に見られない新しい概念に適応できる必要があります。 。ダイアログの相互作用は、不確実性を修正および解決するための説明と、操作中に遭遇する新しい概念を学習するためのアクティブラーニングクエリを使用することによってこれを可能にします。 
[概要]この作業では、明確化と能動的学習の両方を実行するための社会的対話ポリシーをトレーニングします。彼らは、明確化のための共同学習ポリシーが、これらの機能の一方または両方に静的ダイアログポリシーを使用するよりも効果的であることを示しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-09">
        <br><font color="black">2020-06-09</font>
      </time>
    </span>
</section>
<!-- paper0: Dynamic Region-Aware Convolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_17.html">
      <font color="black">Dynamic Region-Aware Convolution</font>
    </a>
  </h2>
  <font color="black">このように、DRConvは、セマンティックバリエーションのモデリングにおいて標準の畳み込みよりも優れています。動的領域認識畳み込み（DRConv）と呼ばれる新しい畳み込みを提案します。これは、特徴が類似した表現を持つ対応する空間領域に複数のフィルターを自動的に割り当てることができます。特に効率的なネットワークの畳み込みレイヤーに電力を供給するための、プラグアンドプレイプロパティのための既存のネットワークでの畳み込み。 
[ABSTRACT] drconvは、モバイルネットワークに基づく標準の畳み込みよりも優れています。増加するチャネル単位のフィルターを空間次元に転送します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-27">
        <br><font color="black">2020-03-27</font>
      </time>
    </span>
</section>
<!-- paper0: SCOUTER: Slot Attention-based Classifier for Explainable Image
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_18.html">
      <font color="black">SCOUTER: Slot Attention-based Classifier for Explainable Image
  Recognition</font>
    </a>
  </h2>
  <font color="black">モデルの動作を制御して正と負の説明を切り替えるSCOUTERに合わせた新しい損失と、説明領域のサイズを設計します。説明可能な人工知能がここ数年注目を集めています。実験結果によると、SCOUTERは中小規模のデータセットで優れた精度を維持しながら、より良い視覚的説明を提供できます。 
[概要]ほとんどの既存の方法は、分類子の意思決定プロセスに直接関与しない否定的な機能に基づいています。スカウターは、中小規模のデータセットで優れた精度を維持しながら、より良い視覚的説明を提供できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-14">
        <br><font color="black">2020-09-14</font>
      </time>
    </span>
</section>
<!-- paper0: Weakly-supervised Temporal Action Localization by Uncertainty Modeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_19.html">
      <font color="black">Weakly-supervised Temporal Action Localization by Uncertainty Modeling</font>
    </a>
  </h2>
  <font color="black">弱教師あり時間アクションローカリゼーションは、ビデオレベルラベルのみでアクションクラスの時間間隔の検出を学習することを目的としています。弱教師あり設定での不確実性学習を実現するために、複数インスタンス学習の定式化を活用します。不整合に関して、分布外のサンプルとしてモデル化された背景フレームの新しい視点。 
[ABSTRACT]アクションクラスのフレームを背景フレームから分離することが可能です。フレームレベルのラベルなしで不確実性を直接学習することは不可能です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-12">
        <br><font color="black">2020-06-12</font>
      </time>
    </span>
</section>
<!-- paper0: GANSpace: Discovering Interpretable GAN Controls -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_20.html">
      <font color="black">GANSpace: Discovering Interpretable GAN Controls</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、生成的敵対的ネットワーク（GAN）を分析し、視点の変更、経年変化、照明、時刻など、画像合成のための解釈可能なコントロールを作成する簡単な手法について説明します。さまざまなデータセットでトレーニングされたさまざまなGANの結果を示します。以前の教師ありアプローチで見つかった方向を編集するための優れた定性的一致を示します。潜在空間または特徴空間のいずれかに適用された主成分分析（PCA）に基づいて、重要な潜在方向を識別します。 
[概要]主成分分析に基づいて重要な潜在的な方向を特定します。ビッグガンは層ごとのニューロンで制御できることを示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-06">
        <br><font color="black">2020-04-06</font>
      </time>
    </span>
</section>
<!-- paper0: A Preliminary Comparison Between Compressive Sampling and Anisotropic
  Mesh-based Image Representation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_21.html">
      <font color="black">A Preliminary Comparison Between Compressive Sampling and Anisotropic
  Mesh-based Image Representation</font>
    </a>
  </h2>
  <font color="black">この論文では、CSと最近開発されたMbIRメソッドであるAMA表現との予備的な比較を行います。結果は、同じサンプル密度で、AMA表現がテストされたアルゴリズムに基づいてCSよりも優れた再構成品質を提供できることを示しています。圧縮センシング（CS）は、信号自体よりもはるかに少ないサンプルでスパース信号を表現および再構築するために、過去20年間で人気のある分野になりました。 
[ABSTRACT] csと最近開発されたmbirメソッドであるama表現は、貧弱な画像の例ですが、amaとして知られているメソッドはあまり注目されていません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: DeepFakesON-Phys: DeepFakes Detection based on Heart Rate Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_22.html">
      <font color="black">DeepFakesON-Phys: DeepFakes Detection based on Heart Rate Estimation</font>
    </a>
  </h2>
  <font color="black">DeepFakesON-Physという名前の提案された偽の検出器は畳み込み注意ネットワーク（CAN）を使用します。これは、ビデオフレームから空間的および時間的情報を抽出し、両方のソースを分析および組み合わせて、偽のビデオをより適切に検出します。この作業では、生理学に基づく新しいDeepFake検出フレームワークを紹介します測定..rPPGメソッドは、ビデオシーケンスを分析して、人間の皮膚の微妙な色の変化を探し、組織の下に人間の血液が存在することを明らかにします。 
[概要]ディープフェイク動画は心拍数の変化を検出するために使用されます。最新の公開データベースと呼ばれ、これらの動画は検出できます。この方法は実験的に評価されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-01">
        <br><font color="black">2020-10-01</font>
      </time>
    </span>
</section>
<!-- paper0: Combining Visual and Textual Features for Semantic Segmentation of
  Historical Newspapers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_23.html">
      <font color="black">Combining Visual and Textual Features for Semantic Segmentation of
  Historical Newspapers</font>
    </a>
  </h2>
  <font color="black">ディープラーニング技術のおかげで、ドキュメント画像内の関心のあるセグメントの識別と分類が過去数年間で大幅に進歩した場合、特に、よりきめ細かいセグメンテーションの類型の使用や複雑で異種のドキュメントの検討など、多くの課題が残ります。歴史的な新聞など。過去数十年間に取得された大量のデジタル化された歴史的文書は、自然に自動処理と探索に役立ちます。さらに、ほとんどのアプローチは、テキスト信号を無視して視覚的特徴のみを考慮します。 
[概要]処理されるドキュメントの数を調べるために新しい調査が使用されています。ドキュメントのレイアウト分析の最初の重要なステップとして、ドキュメントがますます増えていると言われています。結果は、通時的なスイスとルクセンブルクの新聞での一連の実験に基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-14">
        <br><font color="black">2020-02-14</font>
      </time>
    </span>
</section>
<!-- paper0: Compressed Domain Image Classification Using a Dynamic-Rate Neural
  Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_24.html">
      <font color="black">Compressed Domain Image Classification Using a Dynamic-Rate Neural
  Network</font>
    </a>
  </h2>
  <font color="black">このトレーニングスキームは、トレーニングに選択された少数のMRのみを使用し、トレーニングされたニューラルネットワークは対象のMRの全範囲で有効です。MNIST、CIFAR-10、Fashion-のデータセットで動的レートニューラルネットワークのパフォーマンスを示します。 MNIST、COIL-100は、1つのMRに対してのみ有効なシングルレートニューラルネットワークとほぼ同等のパフォーマンスを各MRで生成することを示しています。動的レートモデルのノイズに対するロバスト性も示されています。動的レートトレーニングスキームは、さまざまなタイプのセンシングマトリックス、さまざまなニューラルネットワークアーキテクチャと互換性のある一般的なアプローチと見なすことができ、圧縮性の幅広い採用に向けた貴重なステップです。ニューラルネットワークを介した推論技術およびその他の圧縮センシング関連タスク。 
[ABSTRACT]ネットワークは、対象範囲内の任意のmrを分類できます。これを使用して、可視スペクトルを超えて高速検出と分類を拡張できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-01-28">
        <br><font color="black">2019-01-28</font>
      </time>
    </span>
</section>
<!-- paper0: Self-Supervised Representation Learning for Detection of ACL Tear Injury
  in Knee MR Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_25.html">
      <font color="black">Self-Supervised Representation Learning for Detection of ACL Tear Injury
  in Knee MR Videos</font>
    </a>
  </h2>
  <font color="black">この論文で提案された新しい畳み込みニューラルネットワークの効率は、下流のタスクで得られた実験結果に反映されています。私たちの知る限り、MRビデオから傷害分類タスクを実行する教師あり学習モデルのいずれも説明を提供しません。モデルによって行われた決定により、MRビデオデータに関するこの種の作業は初めてになります。口実タスクでの実験により、この提案されたアプローチにより、モデルは、ダウンストリームタスクで信頼性が高く説明可能なパフォーマンスに役立つ空間コンテキスト不変機能を学習できることがわかります。膝MRIによる前十字靭帯裂傷の分類のようなものです。 
[ABSTRACT]教師なし学習のサブセットである自己教師あり学習は、ラベルのない画像またはビデオデータから意味のある特徴を学習することでこの問題を処理します。口実タスクモデルは、mrビデオフレームが乱雑な画像パッチの正しい順序を予測するように設計されています。に分け</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: Domain Adaptation with Auxiliary Target Domain-Oriented Classifier -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_26.html">
      <font color="black">Domain Adaptation with Auxiliary Target Domain-Oriented Classifier</font>
    </a>
  </h2>
  <font color="black">具体的には、メモリメカニズムを採用し、2種類のノンパラメトリック分類器を開発します。最も一般的なSSL手法の1つは、ラベル付きデータによってトレーニングされた分類器を介してラベルなしデータごとに疑似ラベルを割り当てる疑似ラベル付けです。ただし、DA問題の分布シフトを無視し、必然的にソースデータにバイアスされます。 
[概要]最近のいくつかの方法は、一般的な半教師あり学習（ssl）手法を示しています。これにより、競争力のあるパフォーマンスを実現できます。ただし、da問題の分布シフトを無視し、必然的にソースデータにバイアスをかけます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-08">
        <br><font color="black">2020-07-08</font>
      </time>
    </span>
</section>
<!-- paper0: SSD-GAN: Measuring the Realness in the Spatial and Spectral Domains -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_27.html">
      <font color="black">SSD-GAN: Measuring the Realness in the Spatial and Spectral Domains</font>
    </a>
  </h2>
  <font color="black">具体的には、周波数認識分類器をディスクリミネーターに組み込んで、空間ドメインとスペクトルドメインの両方で入力の現実性を測定することを提案します。強化されたディスクリミネーターを使用すると、SSD-GANのジェネレーターは実際のデータと正確な詳細の生成..コードはhttps://github.com/cyq373/SSD-GANで入手できます。 
[概要] ssd-ganのジェネレーターは、実際のデータの高周波コンテンツを学習し、正確な詳細を生成することをお勧めします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-10">
        <br><font color="black">2020-12-10</font>
      </time>
    </span>
</section>
<!-- paper0: Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression
  Grounding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_28.html">
      <font color="black">Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression
  Grounding</font>
    </a>
  </h2>
  <font color="black">この目的のために、最初の段階で表現を意識した提案を生成する最初の方法であるRef-NMSを提案します。この論文では、これらの方法が2つの段階での提案の役割間の明らかな不一致を見落としていると主張します。それらは、プロポーザルが式内のすべての正しいインスタンスを含むことを期待して（つまり、式に依存しない）、検出の信頼性のみに基づいてプロポーザルを生成します（つまり、式を認識します）。参照式の根拠を解決するための一般的なフレームワークは、2つに基づいています。 -段階的なプロセス：1）オブジェクト検出器を使用して提案を検出し、2）提案の1つに指示対象を接地します。 
[概要]既存の2段階のソリューションは、主に、表現と提案を一致させることを目的とした接地ステップに焦点を当てています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-03">
        <br><font color="black">2020-09-03</font>
      </time>
    </span>
</section>
<!-- paper0: Multi Modal Adaptive Normalization for Audio to Video Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_29.html">
      <font color="black">Multi Modal Adaptive Normalization for Audio to Video Generation</font>
    </a>
  </h2>
  <font color="black">このアーキテクチャは、マルチモーダル適応正規化、キーポイントヒートマップ予測子、オプティカルフロー予測子、およびクラスアクティベーションマップ
[58]ベースのレイヤーを使用して、表情豊かな顔のコンポーネントの動きを学習し、特定の人物の非常に表情豊かなトーキングヘッドビデオを生成します。マルチモーダル適応正規化は、メルスペクトログラム、ピッチ、オーディオ信号からのエネルギー、予測されたキーポイントヒートマップ/オプティカルフローなどのオーディオとビデオのさまざまな機能と単一の画像を使用して、それぞれのアフィンパラメータを学習し、表現力の高いビデオを生成します。定性的評価とオンラインチューリングテストは、私たちのアプローチの有効性を示しています。 
[概要]マルチモーダル適応正規化（man）ベースのアーキテクチャは、メルスペクトログラム、ピッチ、オーディオ信号からのエネルギー、予測されるキーポイントヒートマップ/オプティカルフローなど、オーディオとビデオのさまざまな機能を使用します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Knowledge-Routed Visual Question Reasoning: Challenges for Deep
  Representation Embedding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_30.html">
      <font color="black">Knowledge-Routed Visual Question Reasoning: Challenges for Deep
  Representation Embedding</font>
    </a>
  </h2>
  <font color="black">知識推論の課題に加えて、アノテーターのバイアスに対処する方法も未解決のままであり、質問と回答の間に表面的な過剰な相関関係が生じることがよくあります。この問題に対処するために、Knowledge-RoutedVisualという名前の新しいデータセットを提案します。 VQAモデル評価の質問推論..これにより、VQAモデルに、与えられた質問のみに基づいて知識を推測する代わりに、画像を正しく認識するように強制できます。 ii）すべての質問は異なる知識に基づいていますが、候補者の回答はトレーニングセットとテストセットの両方で同じです。 
[要約]データセットは、現在のモデルで利用されているショートカット学習を遮断し、知識ベースの視覚的質問推論の研究境界を押し上げることを目的としています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Morphology on categorical distributions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_31.html">
      <font color="black">Morphology on categorical distributions</font>
    </a>
  </h2>
  <font color="black">この作業では、古典的な形態と確率的ビューを組み合わせることにより、カテゴリ分布の形態の一連の要件を確立します。カテゴリ分布は、マルチクラスセグメンテーションの不確実性の自然な表現です。次に、これらの要件を尊重する演算子を定義し、紹介します。カテゴリ分布に対する保護された操作と、2つのタスク例でのこれらの演算子の有用性を示します。脳腫瘍のセグメンテーションにおけるアノテーターバイアスのモデリングと、マルチクラスU-Netの予測からの小胞インスタンスのセグメント化です。 
[ABSTRACT]カテゴリカル分布は、カテゴリカル画像にとってそれほど単純ではありません。2クラスの場合、カテゴリカル分布はベルヌーイになります。カテゴリカル形状は、さまざまな便利な操作を提供します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: AIDE: Annotation-efficient deep learning for automatic medical image
  segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_32.html">
      <font color="black">AIDE: Annotation-efficient deep learning for automatic medical image
  segmentation</font>
    </a>
  </h2>
  <font color="black">3つの医療センターからの872人の患者の11,852枚の乳房画像を含む3つの臨床データセットについて、AIDEは、完全に監視されたカウンターパートによって生成されたものに匹敵するセグメンテーションマップと、10％のトレーニング注釈のみを利用する独立した放射線科医の手動注釈を一貫して生成します。セグメンテーションDiceスコアは、オープンデータセットの従来の深層学習モデルで最大30％の注釈が不足しているか、ノイズが多いです。正確な画像セグメンテーションは、医療画像アプリケーションにとって非常に重要です。 
[概要]一般的なディープラーニングアプローチは、通常、高品質の手動アノテーションを備えた非常に大規模なトレーニングデータセットに依存しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-09">
        <br><font color="black">2020-12-09</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Optimized Priors for 3D Shape Modeling and Reconstruction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_33.html">
      <font color="black">Deep Optimized Priors for 3D Shape Modeling and Reconstruction</font>
    </a>
  </h2>
  <font color="black">特に、テスト時に事前トレーニングされた事前確率を修正する一般的な方法とは異なり、トレーニング後の入力物理測定値に従って、学習された事前コードと潜在コードをさらに最適化することを提案します。多くの学習ベースのアプローチでは、目に見えないものにスケーリングすることが困難です。学習した事前分布の一般性はトレーニングサンプルの規模とバリエーションに限定されるため、データ。ディープジェネレータの一般化能力を大幅に向上させる3Dモデリングと再構成のための新しい学習フレームワークを紹介します。 
[概要]私たちのアプローチは、学習ベースの方法と分析ベースの方法の両方の良い目的を結びつけるよう努めています。提案された戦略は、事前に訓練された事前トレーニングによって制約された障壁を効果的に打ち破ります。しかし、実験結果は、私たちのアプローチが州と比べて遜色がないことを示しています-最先端の方法</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: INSPIRE: Intensity and Spatial Information-Based Deformable Image
  Registration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_34.html">
      <font color="black">INSPIRE: Intensity and Spatial Information-Based Deformable Image
  Registration</font>
    </a>
  </h2>
  <font color="black">INSPIREは、強度と空間情報を組み合わせた距離に基づく既存の対称登録フレームワークを、弾性Bスプラインベースの変換モデルに拡張します。INSPIREが優れた血管の薄いネットワークで構成される網膜画像から作成された合成データセットでメソッドを評価します。パフォーマンスは、参照メソッドを大幅に上回っています。また、高い計算効率を提供し、それによって幅広い実際のシナリオでフレームワークを適用できる、いくつかの理論的およびアルゴリズム的な改善を示します。 
[要約]提案された方法は、非常に正確であるだけでなく、安定した堅牢な登録結果を提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: One Point is All You Need: Directional Attention Point for Feature
  Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_35.html">
      <font color="black">One Point is All You Need: Directional Attention Point for Feature
  Learning</font>
    </a>
  </h2>
  <font color="black">このような点を方向性注意点（DAP）と呼びます。これは、トレーニングでタスクのパフォーマンスを最大化することによって学習されるオフセットベクトルを元の点に追加することによって検出されるためです。注意メカニズムを状態に簡単に組み込むことができることを示します。 -最先端の点群分類およびセグメンテーションネットワーク..私たちのメカニズムは、入力ポイントの特徴と関連する注意ポイントの特徴を組み合わせた、新しくシンプルな畳み込みによって特徴付けられます。 
[概要]注意メカニズムを最先端の点群分類に簡単に組み込むことができることを示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-11">
        <br><font color="black">2020-12-11</font>
      </time>
    </span>
</section>
<!-- paper0: Deep-Learning-Based Kinematic Reconstruction for DUNE -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_36.html">
      <font color="black">Deep-Learning-Based Kinematic Reconstruction for DUNE</font>
    </a>
  </h2>
  <font color="black">ただし、ディープラーニング手法によるニュートリノエネルギーと最終状態の粒子運動量の再構築は、完全なAIベースの再構築チェーンではまだ開発されていません。ディープラーニング手法、特に畳み込みニューラルネットワーク（CNN）は、分類問題で成功を収めています。 DUNEや他のニュートリノ実験での粒子識別など。DUNE遠方検出器モジュールは、液体アルゴンTPC（LArTPC）テクノロジーに基づいています。 
[概要]深部地下ニュートリノ実験（砂丘）は、次世代のロングベースラインニュートリノ振動実験です。$-`f ********** &#39;の振動パターンを測定することにより、これらの問題に対処することを目的としています。 1番目と2番目の振動の最大値にまたがるエネルギーの範囲にわたって</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-11">
        <br><font color="black">2020-12-11</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Category-level Shape Saliency via Deep Implicit Surface
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_37.html">
      <font color="black">Learning Category-level Shape Saliency via Deep Implicit Surface
  Networks</font>
    </a>
  </h2>
  <font color="black">技術的には、深い陰関数曲面ネットワークから同じカテゴリの形状インスタンスの顕著性マップを学習することを提案します。陰関数曲面フィールドのサンプリングされたポイントの適切な顕著性スコアは、入力潜在コードの容量を制約することによって予測されます。このような量を、カテゴリレベルの形状顕著性または略して形状顕著性と呼びます。また、顕著性予測を追加で強化します。対照的なトレーニングの喪失。 
[概要]たとえば、翼の平面の一般的な知識があり、椅子には脚があります。オブジェクトなどの量に名前を付けます：表面形状の顕著性または形状の顕著性。また、顕著性の予測を強化し、対照的なトレーニング</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Two-Stage Generative Adversarial Networks for Document Image
  Binarization with Color Noise and Background Removal -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_38.html">
      <font color="black">Two-Stage Generative Adversarial Networks for Document Image
  Binarization with Color Noise and Background Removal</font>
    </a>
  </h2>
  <font color="black">最初の段階では、4つの色に依存しない敵対的ネットワークがトレーニングされ、入力画像から色の前景情報を抽出してドキュメント画像を強調します。ドキュメント画像の強調と2値化の方法は、次のようなドキュメント画像分析タスクの精度と効率を向上させるためによく使用されます。テキスト認識..第2段階では、グローバルおよびローカル機能を備えた2つの独立した敵対的ネットワークが、可変サイズのドキュメントの画像2値化のためにトレーニングされます。 
[概要]従来の非機械学習手法は、教師なし方法で低レベルの機能に基づいて構築されますが、背景が大幅に劣化したドキュメントでの2値化が困難です。第2段階では、グローバル機能とローカル機能を備えた2つの独立した敵対的ネットワークがトレーニングされます。ドキュメントの画像の2値化</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Hybrid Representations for Automatic 3D Vessel Centerline
  Extraction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_39.html">
      <font color="black">Learning Hybrid Representations for Automatic 3D Vessel Centerline
  Extraction</font>
    </a>
  </h2>
  <font color="black">CTAデータセットで提案されたアプローチを検証し、従来のベースラインとCNNベースのベースラインの両方と比較して優れたパフォーマンスを示します。主なアイデアは、CNNを使用して画像作物の血管の局所的な外観を学習し、別のポイントクラウドネットワークを使用して画像全体における血管のグローバルジオメトリ..この作業では、この課題に対処するためのハイブリッド表現学習アプローチを提案します。 
[ABSTRACT]畳み込みニューラルネットワーク（cnns）に基づく既存の方法は、抽出された血管の不連続性に悩まされる可能性があります。ただし、3D畳み込みは非効率的であり、画像全体のグローバルキューをキャプチャするには3D表現が必要です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Non-Autoregressive Coarse-to-Fine Video Captioning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_40.html">
      <font color="black">Non-Autoregressive Coarse-to-Fine Video Captioning</font>
    </a>
  </h2>
  <font color="black">この論文では、これらの欠陥を軽減するために、粗いキャプション手順から細かいキャプション手順を備えた非自己回帰デコードベースのモデルを提案します。実装では、推論の高速化を実現するための言語モデルとして、双方向の自己注意ベースのネットワークを採用します。 、これに基づいて、キャプション手順を2つの段階に分解し、モデルの焦点を変えます。具体的には、視覚的な単語がキャプションの意味の正しさを決定することを前提として、シーンのトレーニングを促進するだけでなく、視覚的な単語を生成するメカニズムを設計します。関連する単語だけでなく、ビデオから関連する詳細をキャプチャして、大まかな文「テンプレート」を作成します。 
[概要]動画のキャプション方法は、自己回帰デコードが原因で結論が遅くなります。主流の動画主導の動画では、視覚言語言語のスキルが不足している可能性が低くなります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-27">
        <br><font color="black">2019-11-27</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Cardiac Intervention Assistance: Hardware-aware Neural
  Architecture Exploration for Real-Time 3D Cardiac Cine MRI Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_41.html">
      <font color="black">Towards Cardiac Intervention Assistance: Hardware-aware Neural
  Architecture Exploration for Real-Time 3D Cardiac Cine MRI Segmentation</font>
    </a>
  </h2>
  <font color="black">この作業では、リアルタイム3D心臓シネMRIセグメンテーションのための最初のハードウェア対応マルチスケールニューラルアーキテクチャ検索（NAS）フレームワークを提示します。ACDCMICCAI2017データセットの実験結果は、ハードウェア対応マルチスケールNASフレームワークを示しています。最先端のNASセグメンテーションフレームワークと比較して、競争力のあるセグメンテーション精度を達成しながら、遅延を最大3.5倍削減し、リアルタイムの制約を満たすことができます。提案されたフレームワークは、損失に遅延正規化項を組み込んでいます。基盤となるハードウェアを考慮して、リアルタイムの制約を処理する機能。 
[概要]目立つものを減らすために、シネmriフレームをオンザフライでセグメント化する必要があります-目立つlag.stateを回避するために-最先端の方法は主に精度のみに焦点を当てており、リアルタイムアプリケーションにはほとんど採用できませんまたはローカルハードウェア上</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-17">
        <br><font color="black">2020-08-17</font>
      </time>
    </span>
</section>
<!-- paper0: IoU-balanced Loss Functions for Single-stage Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_42.html">
      <font color="black">IoU-balanced Loss Functions for Single-stage Object Detection</font>
    </a>
  </h2>
  <font color="black">第一に、分類の標準クロスエントロピー損失はローカリゼーションタスクから独立しており、トレーニング中のローカリゼーション精度に関係なく、すべての肯定的な例が可能な限り高い分類スコアを学習するように駆動します。第二に、標準の滑らかなL1損失の場合、勾配は次のようになります。トレーニング中のローカリゼーション精度が低い異常値が支配的です。IoUバランスのとれたローカリゼーション損失は、IoUが低い例の勾配を減らし、IoUが高い例の勾配を増やします。これにより、モデルのローカリゼーション精度を向上させることができます。 
[ABSTRACT]単一のiou関連オブジェクトによって採用された損失関数は、ローカリゼーションの精度を損ないます。これらには、ローカリゼーションの精度を向上させるために使用される損失関数の検出が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-15">
        <br><font color="black">2019-08-15</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Training against Location-Optimized Adversarial Patches -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_43.html">
      <font color="black">Adversarial Training against Location-Optimized Adversarial Patches</font>
    </a>
  </h2>
  <font color="black">この作業では、最初に、画像内の位置を積極的に最適化しながら敵対パッチを取得するための実用的なアプローチを考案します。次に、これらの位置最適化された敵対パッチに敵対トレーニングを適用し、CIFAR10およびGTSRBで大幅に改善された堅牢性を示します。これらのパッチ物理的な世界で簡単に印刷して適用できます。 
[概要]敵対的なパッチははっきりと見えますが、敵対的なパッチは簡単に作成できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-05">
        <br><font color="black">2020-05-05</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic Open-World Reliability Assessment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_44.html">
      <font color="black">Automatic Open-World Reliability Assessment</font>
    </a>
  </h2>
  <font color="black">オープンワールドでの画像分類は、分布外（OOD）画像を処理する必要があります。このプロセスを自動化するために、ここでは、オープンワールド認識の信頼性問題を形式化し、この新しい問題に対処するための複数の自動信頼性評価ポリシーを提案します。報告されたスコア/確率データの分布。したがって、標準分類器またはオープンセット分類器のいずれについても、世界がいつ変化し、OOD入力が増加するとシステムの信頼性が低下するかを判断できることが重要です。 
[ABSTRACT]システムは自動的にood画像を拒否する必要があります。そうしないと、既知のクラスの上にマッピングされます。ただし、softmaxとopen-set分類子はoodデータの頻度に依存します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-11">
        <br><font color="black">2020-11-11</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Domain Multi-Task Rehearsal for Lifelong Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_45.html">
      <font color="black">Multi-Domain Multi-Task Rehearsal for Lifelong Learning</font>
    </a>
  </h2>
  <font color="black">これは、これらのメソッドが常に2つの重要な要素を無視するためです。次に、すべてのタスク間でタスクを分離すると、ドメインが予測できない方向にシフトします。予測不可能なドメインシフトに対処するために、このペーパーでは、マルチドメインマルチタスク（MDMT）リハーサルを提案して、古いタスクと新しいタスクを並行して均等にトレーニングし、タスク間の分離を解消します。リハーサル、モデルを思い出させるために生涯学習で古い知識を保存することは、壊滅的な忘却、つまり、新しいタスクに移動するときに以前の知識を偏って忘れることを軽減するための最も効果的な方法の1つです。 
[概要]以前のリハーサルベースの方法の古いタスクは、新しいタスクをトレーニングするときに予測できないドメインシフトに悩まされます。これには、新しいタスクと古いタスクの間のデータの不均衡が含まれ、古いタスクのドメインがシフトしやすくなります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Open-World Learning Without Labels -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_46.html">
      <font color="black">Open-World Learning Without Labels</font>
    </a>
  </h2>
  <font color="black">ここでは、エージェントが教師なし方法でラベルなしデータのストリームから新しいクラスを学習できるようにする新しいフレームワークを提案します。ただし、オープンワールド学習に関するこれまでのすべての作業では、画像ストリームから新しいクラスを学習するためにすべてのラベル付きデータがあります。 ..ラベルなしのオープンワールド学習のための新しいメトリックも導入します。 
[ABSTRACT]自律的な真のオープン-世界の終わりのない学習エージェント。私たちの理論と方法は、自律的な真のオープン-世界の終わりのない学習を開発するための出発点になると期待しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-25">
        <br><font color="black">2020-11-25</font>
      </time>
    </span>
</section>
<!-- paper0: Accurate Cell Segmentation in Digital Pathology Images via Attention
  Enforced Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_47.html">
      <font color="black">Accurate Cell Segmentation in Digital Pathology Images via Attention
  Enforced Networks</font>
    </a>
  </h2>
  <font color="black">いくつかの以前の方法との定量的比較は、私たちのアプローチの優位性を示しています。さらに、高レベルと低レベルの特徴を橋渡しする特徴融合ブランチを導入します。最後に、マーカー制御流域アルゴリズムを適用して、予測を後処理します。断片化された領域を減らすためのセグメンテーションマップ。 
[概要]このサイクルの一部にセルが割り当てられるのはこれが初めてです。主なツールは、注意を強制するネットワーク（aenet）を開発することです。これは、空間的注意モジュールとチャネル注意モジュールに基づいて構築されます。ローカルに統合されます。グローバルな依存関係と重み効果のあるチャネルを適応的に備えた機能</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: TDAF: Top-Down Attention Framework for Vision Tasks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_48.html">
      <font color="black">TDAF: Top-Down Attention Framework for Vision Tasks</font>
    </a>
  </h2>
  <font color="black">経験的証拠は、TDAFが効果的な層別注意情報をキャプチャしてパフォーマンスを向上できることを示しています。設計された再帰的二重方向ネスト構造は、再帰的パスと構造的パスの2セットの直交パスを形成し、ボトムアップの空間的特徴とトップダウンの注意があります。特徴はそれぞれ抽出されます。オブジェクト検出の場合、パフォーマンスはFCOSよりも2.7％AP向上します。 
[概要]このコンセプトは、トップダウンとボトムアップの混合方式で機能します。ほとんどの既存モデルで簡単に採用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Project RISE: Recognizing Industrial Smoke Emissions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_49.html">
      <font color="black">Project RISE: Recognizing Industrial Smoke Emissions</font>
    </a>
  </h2>
  <font color="black">これらの日中のクリップは、4つの季節すべてを含む2年間で30日間に及びます。私たちのデータセットには、3つの産業施設を監視したカメラからの19の異なるビューからの12,567のクリップが含まれています。 。 
[概要]産業用煙の排出を認識する最初の大規模なビデオ品質のビデオデータセットを紹介します。データセットには、3つの産業施設を監視したカメラからの19の異なるビューからの12、567のクリップが含まれています。ディープニューラルネットワークを使用して実験を実行し、強力なものを確立しました。パフォーマンスベースラインと煙認識の課題を明らかにする</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-13">
        <br><font color="black">2020-05-13</font>
      </time>
    </span>
</section>
<!-- paper0: Bi-directional Exponential Angular Triplet Loss for RGB-Infrared Person
  Re-Identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_50.html">
      <font color="black">Bi-directional Exponential Angular Triplet Loss for RGB-Infrared Person
  Re-Identification</font>
    </a>
  </h2>
  <font color="black">角度的に識別可能な特徴空間は、埋め込みベクトルに基づいて人間の画像を分類するために重要であるため、この論文では、角度的に分離可能な共通の特徴空間を学習するのに役立つ、双方向指数角度トリプレット損失という名前の新しいランキング損失関数を提案します。埋め込みベクトル間の包含角度を明示的に制約します。ただし、ユークリッド距離では埋め込みベクトル間の包含角度を効果的に測定できないため、これらの方法では角度識別特徴埋め込みを学習できません。提案された方法は、単一モダリティReのタスクに一般化できます。 -IDおよびランク1の精度/ mAPをMarket-1501データセットの92.0％/ 81.7％から94.7％/ 86.6％に、DukeMTMC-reIDデータセットの82.6％/ 70.6％から87.6％/ 77.1％に改善します。 
[概要]既存のほとんどの作業は、異なるモダリティからの画像の特徴間の不一致を解決するために使用されます。提案された方法は、57.1データセットを単一改造するタスクに簡略化できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-01">
        <br><font color="black">2020-06-01</font>
      </time>
    </span>
</section>
<!-- paper0: Source Data-absent Unsupervised Domain Adaptation through Hypothesis
  Transfer and Labeling Transfer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_51.html">
      <font color="black">Source Data-absent Unsupervised Domain Adaptation through Hypothesis
  Transfer and Labeling Transfer</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーは、ソースデータにアクセスするのではなく、トレーニングされた利用可能な分類モデルのみを使用して現実的な設定に取り組むことを目的としています。教師なしドメイン適応（UDA）は、関連しているが異なるラベルの付いたソースドメインから新しいソースドメインに知識を転送することを目的としています。ラベルのないターゲットドメイン..ほとんどの既存のUDAメソッドは、ソースデータへのアクセスを必要とするため、データが機密であり、プライバシーの懸念から共有できない場合は適用できません。 
[概要]ほとんどの既存のudaメソッドは、ソースデータへのアクセスを必要とします。これは、プライバシーの懸念から機密であり、共有できないためです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: DenserNet: Weakly Supervised Visual Localization Using Multi-scale
  Feature Aggregation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_52.html">
      <font color="black">DenserNet: Weakly Supervised Visual Localization Using Multi-scale
  Feature Aggregation</font>
    </a>
  </h2>
  <font color="black">私たちの方法は、計算上の制約を残しながら、困難な条件下で正確な大規模なローカリゼーションを実行できます。最後に、私たちのアーキテクチャは計算中に機能とパラメータを共有しているため、私たちの方法は計算効率が高くなります。次に、モデルはエンドツーエンドでトレーニングされます。正と負のGPSタグ付き画像ペア以外のピクセルレベルの注釈。 
[概要]私たちの仕事は3つの主要な貢献を提供します..密度の高い特徴マップは、より多くのキーポイント特徴を生成し、画像検索の精度を高めることができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-04">
        <br><font color="black">2020-12-04</font>
      </time>
    </span>
</section>
<!-- paper0: Effect of the regularization hyperparameter on deep learning-based
  segmentation in LGE-MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CV/paper_53.html">
      <font color="black">Effect of the regularization hyperparameter on deep learning-based
  segmentation in LGE-MRI</font>
    </a>
  </h2>
  <font color="black">また、著者は、他の深層学習ハイパーパラメータの手動調整または調整を採用して、90％の検証精度を達成する前にすべてのエポックの10％に達した場合にのみ実行します。この作業では、著者は次の範囲を示すことを目的としています。 L2正則化ハイパーパラメータの任意の選択がLGE-MRIの深層学習ベースのセグメンテーションの結果に影響を与える可能性があります。ここでは、任意のL2正則化値を使用して、さまざまな深層学習ベースのセグメンテーションネットワークを作成します。 
[概要]これらは、さまざまな深層学習ベースのセグメンテーションネットワークを作成するために使用できます。これらは、l2正則化の長さに対する高血圧の例です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-10">
        <br><font color="black">2020-12-10</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Explicitly Modeling Adaptive Depths for Transformer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_0.html">
      <font color="black">Explicitly Modeling Adaptive Depths for Transformer</font>
    </a>
  </h2>
  <font color="black">さらに、他の深度適応アプローチと比較すると、効率と堅牢性が大幅に向上しています。この論文では、停止ユニットを取り除き、必要な深度を事前に推定して、より高速な深度適応モデルを生成します。実験を行います。さまざまなサイズとドメインの24のデータセットを使用したテキスト分類タスクについて。 
[要約]停止ユニットが最適化されておらず、不正確である可能性があります。その結果、文をモデル化するときに最適ではなく不安定なパフォーマンスが発生します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-27">
        <br><font color="black">2020-04-27</font>
      </time>
    </span>
</section>
<!-- paper0: MICK: A Meta-Learning Framework for Few-shot Relation Classification
  with Small Training Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_1.html">
      <font color="black">MICK: A Meta-Learning Framework for Few-shot Relation Classification
  with Small Training Data</font>
    </a>
  </h2>
  <font color="black">実験結果は、私たちのフレームワークがほとんどの基礎となる分類モデルのパフォーマンスを向上させ、小さなトレーニングデータで最先端の結果を上回り、十分に大きなトレーニングデータで競争力のある結果を達成することを示しています。フレームワークには、クロスを集約する方法も含まれています。 -オープンソースタスクエンリッチメントによるモデルへのドメイン知識..このペーパーでは、トレーニング時に利用可能なデータの量をさらに制限することにより、さらに難しい問題に取り組みます。 
[概要]この能力は、大量のトレーニング中データを使用してトレーニングすることで得られます。これらの例には、オープンソースのタスク強化によってクロスドメイン知識をモデルに集約する方法が含まれています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-26">
        <br><font color="black">2020-04-26</font>
      </time>
    </span>
</section>
<!-- paper0: Self-supervised pre-training and contrastive representation learning for
  multiple-choice video QA -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_2.html">
      <font color="black">Self-supervised pre-training and contrastive representation learning for
  multiple-choice video QA</font>
    </a>
  </h2>
  <font color="black">メインステージでの対照学習のために、グラウンドトゥルース回答に対応する入力にマスキングノイズを追加し、グラウンドトゥルース回答の元の入力を正のサンプルと見なし、残りを負のサンプルとして扱います。ビデオ質問応答（ビデオQA）では、特定の質問に回答するために、ビデオと言語の両方のモダリティをきめ細かく理解する必要があります。自己管理型の事前トレーニング段階で、正解を予測する元の問題形式を予測するものに変換します。それ以上のデータセットや注釈なしで、より広範なコンテキスト入力をモデルに提供するための関連する質問。 
[概要]新しい論文では、多肢選択式のビデオ質問応答のための新しいトレーニングスキームを提案します。これらには、補助学習としてメインステージの教師あり対照学習が含まれます。以前の地面の入力-真実の回答には肯定的なものが必要ですサンプル、残りをネガティブサンプルとして扱いながら、特定の対応する字幕文に特に関連するビデオフレームにより効果的に焦点を合わせるために、ローカルに調整された注意をさらに採用します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-17">
        <br><font color="black">2020-09-17</font>
      </time>
    </span>
</section>
<!-- paper0: Reinforced Multi-Teacher Selection for Knowledge Distillation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_3.html">
      <font color="black">Reinforced Multi-Teacher Selection for Knowledge Distillation</font>
    </a>
  </h2>
  <font color="black">この論文では、トレーニング例の複雑さと学生モデルの能力の違いにより、教師モデルとは異なる学習が、蒸留された学生モデルのパフォーマンスの向上につながる可能性があることを観察します。私たちは、動的に重みを割り当てる強化された方法を体系的に開発します。さまざまなトレーニングインスタンスの教師モデルを作成し、学生モデルのパフォーマンスを最適化します。いくつかのNLPタスクに関する広範な実験結果により、アプローチの実現可能性と有効性が明確に検証されます。 
[要約]モデル圧縮の一般的な方法として、知識蒸留は知識を1つまたは複数の大きな教師モデルから小さな（学生）モデルに転送します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-11">
        <br><font color="black">2020-12-11</font>
      </time>
    </span>
</section>
<!-- paper0: Contrastive Triple Extraction with Generative Transformer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_4.html">
      <font color="black">Contrastive Triple Extraction with Generative Transformer</font>
    </a>
  </h2>
  <font color="black">3つのデータセット（つまり、NYT、WebNLG、およびMIE）での実験結果は、私たちのアプローチがベースラインよりも優れたパフォーマンスを達成することを示しています。さらに、モデルのパフォーマンスをさらに向上させる2つのメカニズム（つまり、バッチごとの動的注意マスキングとトリプルワイズキャリブレーション）。トリプル抽出は、自然言語処理と知識グラフ構築のための情報抽出に不可欠なタスクです。 
[概要]この論文では、シーケンス生成のトリプル抽出タスクを再検討します。これらには、エンコーダー-デコーダーベースの生成用の単一の共有トランスモジュールが含まれます。モデルのパフォーマンスをさらに向上させるための2つのメカニズムを紹介します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-14">
        <br><font color="black">2020-09-14</font>
      </time>
    </span>
</section>
<!-- paper0: Contrastive Learning with Adversarial Perturbations for Conditional Text
  Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_5.html">
      <font color="black">Contrastive Learning with Adversarial Perturbations for Conditional Text
  Generation</font>
    </a>
  </h2>
  <font color="black">提案した方法が、機械翻訳、テキスト要約、質問生成の3つのテキスト生成タスクでseq2seqの一般化を大幅に改善することを経験的に示します。この作業では、正のペアと負のペアを対比することにより、条件付きテキスト生成の問題を軽減することを提案します。一般化を改善するために、モデルが入力のさまざまな有効または不正確な摂動にさらされるようなペア。最近、Transformerアーキテクチャを使用したシーケンス間（seq2seq）モデルは、次のようなさまざまな条件付きテキスト生成タスクで優れたパフォーマンスを達成しました。機械翻訳として。 
[概要]これは「露出バイアス」問題として知られており、目に見えない入力への一般化を損ないます。これは、特に大きなテキストコーパスで事前トレーニングされたモデルでは、正しい出力と簡単に区別できるため、サブアーキテクチャです。 seq2seqモデルの対照学習のための正と負のサンプルを生成する方法</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Using multiple ASR hypotheses to boost i18n NLU performance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_6.html">
      <font color="black">Using multiple ASR hypotheses to boost i18n NLU performance</font>
    </a>
  </h2>
  <font color="black">ドイツ語とポルトガル語の2つの言語データセットの現状と比較した場合の5つの最良のASR仮説を利用した場合の、NLU関連タスクのパフォーマンスの変化を調査します。最良のASR仮説が、転写された発話と完全に一致しなかった場合（不一致）テストセット）、ドイツ語とポルトガル語で、それぞれ最大6.7％と8.8％のマイクロ平均F1スコアの改善が見られます。私たちの結果は、1つではなく複数のASR仮説を使用すると、大幅なパフォーマンスにつながる可能性があることを示唆しています。これらの英語以外のデータセットのDCタスクの改善。 
[概要]ドイツ語とポルトガル語の2つの言語データセットは、最良のasr仮説です。これらの5つの最良の仮説は、言語データセットの現状と比較されます。これらのテストは、icおよびnerタスクのパフォーマンスの大幅な向上につながる可能性があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-07">
        <br><font color="black">2020-12-07</font>
      </time>
    </span>
</section>
<!-- paper0: H-FND: Hierarchical False-Negative Denoising for Distant Supervision
  Relation Extraction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_7.html">
      <font color="black">H-FND: Hierarchical False-Negative Denoising for Distant Supervision
  Relation Extraction</font>
    </a>
  </h2>
  <font color="black">SemEval-2010とTACREDの実験は、トレーニングインスタンスと検証インスタンスの関係をランダムにネガティブに変換してFNインスタンスを生成する制御されたFN比率で実施されました。ここでは、堅牢な遠隔監視関係のための階層的なフォールスネガティブノイズ除去フレームワークであるH-FNDを提案します。 FNノイズ除去ソリューションとしての抽出..H-FNDは、トレーニングプロセス中に非関係（NA）インスタンスを保持、破棄、または改訂するかどうかを最初に決定する階層ポリシーを使用します。 
[概要]遠隔監視ノイズ除去に関するこれまでの研究は、fpノイズの抑制に重点を置いており、fn問題の解決には重点を置いていません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-07">
        <br><font color="black">2020-12-07</font>
      </time>
    </span>
</section>
<!-- paper0: Dialog Policy Learning for Joint Clarification and Active Learning
  Queries -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_8.html">
      <font color="black">Dialog Policy Learning for Joint Clarification and Active Learning
  Queries</font>
    </a>
  </h2>
  <font color="black">ダイアログシステムに関するこれまでの研究は、明確化/情報探索の実行方法を排他的に学習するか、アクティブラーニングを実行することに焦点を当てていました。ダイアログの相互作用は、修正と不確実性の解決のための説明と、新しい概念を学習するためのアクティブラーニングクエリを使用することでこれを可能にしますインテリジェントシステムは、ミスから回復し、不確実性を解決し、トレーニング中には見られない新しい概念に適応できる必要があります。 
[概要]この作業では、明確化と能動的学習の両方を実行するための社会的対話ポリシーをトレーニングします。彼らは、明確化のための共同学習ポリシーが、これらの機能の一方または両方に静的ダイアログポリシーを使用するよりも効果的であることを示しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-09">
        <br><font color="black">2020-06-09</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Summarization for Chat Logs with Topic-Oriented Ranking and
  Context-Aware Auto-Encoders -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_9.html">
      <font color="black">Unsupervised Summarization for Chat Logs with Topic-Oriented Ranking and
  Context-Aware Auto-Encoders</font>
    </a>
  </h2>
  <font color="black">RankAEは、中心性と多様性に応じてトピックの発話を同時に選択するトピック指向のランキング戦略と、選択した発話に基づいて簡潔でありながらコンテキストに役立つ要約を生成するように注意深く設計されたノイズ除去オートエンコーダで構成されます。そのRankAEは、他の教師なし手法を大幅に上回り、関連性とトピックカバレッジの点で高品質の要約を生成できます。提案された手法を評価するために、カスタマーサービス環境からチャットログの大規模なデータセットを収集し、注釈付きを作成します。モデル評価専用に設定します。 
[概要]提案された方法は、手動でラベル付けされたデータなしでチャットの要約を実行するように設計されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Empirical Analysis of Unlabeled Entity Problem in Named Entity
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_10.html">
      <font color="black">Empirical Analysis of Unlabeled Entity Problem in Named Entity
  Recognition</font>
    </a>
  </h2>
  <font color="black">中心的なアイデアは、負のサンプリングを使用して、ラベルのないエンティティでのトレーニングの確率を非常に低いレベルに保つことです。合成データセットと実際のデータセットでの実験は、モデルがラベルのないエンティティの問題に対して堅牢であり、以前のベースラインを上回っていることを示しています。 -注釈付きのデータセット、私たちのモデルは最先端の方法と競争力があります。 
[概要]パフォーマンス低下の最初の原因は、2番目の原因よりも影響が少ないです。これらの例は、ラベルのないエンティティを処理することで軽減できます。この概念は、合成データセットでの実験に基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-10">
        <br><font color="black">2020-12-10</font>
      </time>
    </span>
</section>
<!-- paper0: Rewriter-Evaluator Framework for Neural Machine Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_11.html">
      <font color="black">Rewriter-Evaluator Framework for Neural Machine Translation</font>
    </a>
  </h2>
  <font color="black">中国語-英語と英語-ドイツ語の2つの翻訳タスクで広範な実験を行い、提案されたフレームワークがNMTモデルのパフォーマンスを大幅に改善し、以前のベースラインを大幅に上回っていることを示しています。デコード..すべてのパスで、リライターは過去の翻訳を改善するために新しい翻訳を作成し、評価者は翻訳品質を推定して、リライトプロセスを終了するかどうかを決定します。 
[概要]デコードの複数のパスでリライターを改善するためにいくつかの方法が提案されています。これらには新しいフレームワークが含まれ、リライター-evaluator.rewriterとエバリュエーターも存在します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-10">
        <br><font color="black">2020-12-10</font>
      </time>
    </span>
</section>
<!-- paper0: Combining Visual and Textual Features for Semantic Segmentation of
  Historical Newspapers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_12.html">
      <font color="black">Combining Visual and Textual Features for Semantic Segmentation of
  Historical Newspapers</font>
    </a>
  </h2>
  <font color="black">ディープラーニング技術のおかげで、ドキュメント画像内の関心のあるセグメントの識別と分類が過去数年間で大幅に進歩した場合、特に、よりきめ細かいセグメンテーションの類型の使用や複雑で異種のドキュメントの検討など、多くの課題が残ります。歴史的な新聞など。結果は、強力な視覚的ベースラインと比較してマルチモーダルモデルの一貫した改善、および高い材料変動に対するより優れた堅牢性を示しています。過去数十年間に取得された大量のデジタル化された歴史的文書は、自然に自動処理に役立ちます。と探索。 
[概要]処理されるドキュメントの数を調べるために新しい調査が使用されています。ドキュメントのレイアウト分析の最初の重要なステップとして、ドキュメントがますます増えていると言われています。結果は、通時的なスイスとルクセンブルクの新聞での一連の実験に基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-14">
        <br><font color="black">2020-02-14</font>
      </time>
    </span>
</section>
<!-- paper0: Context-Guided BERT for Targeted Aspect-Based Sentiment Analysis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_13.html">
      <font color="black">Context-Guided BERT for Targeted Aspect-Based Sentiment Analysis</font>
    </a>
  </h2>
  <font color="black">2つの（T）ABSAデータセットで事前トレーニングされたBERTを使用して両方のモデルをトレーニングします：SentiHoodとSemEval-2014（タスク4）。異なるコンテキストで注意を分散することを学習するコンテキストガイドBERT（CG-BERT）の2つのバリアントを提案します。どちらのモデルも、最高のパフォーマンスを発揮するQACG-BERTモデルにより、新しい最先端の結果を実現しています。 
[概要]コンテキストcgの2つのバリエーションを提案します-異なる状況下で注意を分散することを学ぶバート。事前に訓練された自己苦しみを持つ同様のモデルも、新しい最先端の結果を達成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-15">
        <br><font color="black">2020-10-15</font>
      </time>
    </span>
</section>
<!-- paper0: Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression
  Grounding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_14.html">
      <font color="black">Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression
  Grounding</font>
    </a>
  </h2>
  <font color="black">この目的のために、最初の段階で式を意識した提案を生成する最初の方法であるRef-NMSを提案します。これらのスコアは、NMS操作をガイドして、式に関係のないボックスを除外し、重要なオブジェクトのリコールを増やすことができます。 、結果として大幅に改善された接地性能が得られます。コードはhttps://github.com/ChopinSharp/ref-nmsで入手できます。 
[概要]既存の2段階のソリューションは、主に、表現と提案を一致させることを目的とした接地ステップに焦点を当てています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-03">
        <br><font color="black">2020-09-03</font>
      </time>
    </span>
</section>
<!-- paper0: Segmenting Natural Language Sentences via Lexical Unit Analysis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_15.html">
      <font color="black">Segmenting Natural Language Sentences via Lexical Unit Analysis</font>
    </a>
  </h2>
  <font color="black">15のデータセットにわたって、構文チャンク、固有表現抽出（NER）、スロット充填、中国語の単語セグメンテーション、中国語の品詞（POS）タグ付けなど、5つのタスクについて広範な実験を実施しました。 LUAは、すべての有効なセグメンテーション候補をスコアリングし、動的プログラミング（DP）を利用して、最大スコアの候補を抽出します。LUAは、予測されたセグメンテーションが有効であることを本質的に保証し、グローバルに最適なトレーニングと推論を促進するなど、多くの魅力的な特性を備えています。 
[ABSTRACT] luaは、中国語のスピーチですべての有効なセグメンテーション候補をスコアリングします。また、競争の最大スコアを提供します。さらに、luaの実際の時間計算量を線形時間に減らすことができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-10">
        <br><font color="black">2020-12-10</font>
      </time>
    </span>
</section>
<!-- paper0: Entity Recognition and Relation Extraction from Scientific and Technical
  Texts in Russian -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_16.html">
      <font color="black">Entity Recognition and Relation Extraction from Scientific and Technical
  Texts in Russian</font>
    </a>
  </h2>
  <font color="black">このデータセットは、1600のラベルなしドキュメントと80のエンティティと意味関係でラベル付けされたもので構成されています（6つの関係タイプが考慮されました）。この論文では、ロシア語のメソッドのいくつかの変更が提案されています。キーワードを比較した実験の結果も含まれています。抽出法、語彙法、およびニューラルネットワークに基づくいくつかの方法。 
[要約]この論文では、ロシア語の科学テキストのコーパスを紹介します。データセットと研究データセットはwwwで入手できます。 github。 com / iis-研究チーム</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-19">
        <br><font color="black">2020-11-19</font>
      </time>
    </span>
</section>
<!-- paper0: Audio Captioning using Pre-Trained Large-Scale Language Model Guided by
  Audio-based Similar Caption Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_17.html">
      <font color="black">Audio Captioning using Pre-Trained Large-Scale Language Model Guided by
  Audio-based Similar Caption Retrieval</font>
    </a>
  </h2>
  <font color="black">次に、音声入力のキャプションは、ガイダンスキャプションを参照しながら、事前にトレーニングされた言語モデルを使用して生成されます。音声キャプションの目的は、入力音声を自然言語を使用して説明に翻訳することです。実験結果は、（i ）提案された方法は、音声キャプションに事前にトレーニングされた言語モデルを使用することに成功し、（ii）事前にトレーニングされたモデルベースのキャプションジェネレータのオラクルパフォーマンスは、ゼロからトレーニングされた従来の方法よりも明らかに優れていました。 
[概要]音声の収集が難しいため、音声のキャプションが問題になります-ウェブをクロールして字幕のペアを作成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Early Detection of Fake News by Utilizing the Credibility of News,
  Publishers, and Users Based on Weakly Supervised Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_18.html">
      <font color="black">Early Detection of Fake News by Utilizing the Credibility of News,
  Publishers, and Users Based on Weakly Supervised Learning</font>
    </a>
  </h2>
  <font color="black">3つの実世界のデータセットで実験を行った結果、SMANは4時間で91％を超える精度でフェイクニュースを検出できることがわかりました。これは、最先端のモデルよりもはるかに高速です。以前のモデルの必要な機能は、早期検出シナリオでは利用できないか不十分であることが多く、パフォーマンスが低下します。このペーパーでは、ニュースコンテンツ、公開、およびを組み合わせた新しい構造認識マルチヘッドアテンションネットワーク（SMAN）を提案します。偽のニュースの検出と信頼性の予測タスクを共同で最適化するために、発行者とユーザーの関係を再投稿します。 
[概要]大量のニュースから偽のニュースを簡単に見つけて、配布の初期段階で検出できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-08">
        <br><font color="black">2020-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Rumor Detection on Twitter Using Multiloss Hierarchical BiLSTM with an
  Attenuation Factor -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_19.html">
      <font color="black">Rumor Detection on Twitter Using Multiloss Hierarchical BiLSTM with an
  Attenuation Factor</font>
    </a>
  </h2>
  <font color="black">モデルは、ポストレベルとイベントレベルの2つのBiLSTMモジュールに分かれています。各モジュールには、二国間機能の学習とトレーニング時間の短縮に役立つ損失関数があります。多くの研究者が、従来の機械学習またはバニラディープを使用して噂を分類するモデルを開発しました。学習モデル。 
[概要]モデルは、ポストレベルとイベントレベルの2つのbilostmモジュールに分割されます。これには、二国間機能の学習とトレーニング時間の短縮に役立つ損失関数が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-31">
        <br><font color="black">2020-10-31</font>
      </time>
    </span>
</section>
<!-- paper0: Topic-Oriented Spoken Dialogue Summarization for Customer Service with
  Saliency-Aware Topic Modeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_20.html">
      <font color="black">Topic-Oriented Spoken Dialogue Summarization for Customer Service with
  Saliency-Aware Topic Modeling</font>
    </a>
  </h2>
  <font color="black">実際の中国のカスタマーサービスデータセットに関する包括的な研究により、いくつかの強力なベースラインに対する私たちの方法の優位性が実証されました。対話でトピックモデリングを効果的に実行し、複数の役割の情報をキャプチャするために、この作業では、新しいトピック拡張2段階を提案します。顧客サービスの対話のトピック指向の要約のための顕著性認識ニューラルトピックモデル（SATM）と共同の対話サマライザー（TDS）..音声対話では、豊富な対話ノイズと一般的なセマンティクスが基礎となる有益なコンテンツを覆い隠し、一般的なトピックモデリングを行う可能性があります適用するのが難しいアプローチ。 
[ABSTRACT]この作品では、非常に抽象的な要約を生成するトピック関連の対話要約に焦点を当てます。さらに、対話については、情報が重要であり、要約の不可欠な部分です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Exploring Explainable Selection to Control Abstractive Summarization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_21.html">
      <font color="black">Exploring Explainable Selection to Control Abstractive Summarization</font>
    </a>
  </h2>
  <font color="black">新しいペアワイズマトリックスは、文の相互作用、中心性、属性スコアをキャプチャし、調整可能な属性しきい値を備えたマスクにより、ユーザーは、抽出に含まれる可能性のある文を制御できます。さらに、エンコーダーは適応可能で、両方をサポートします。 TransformerベースおよびBERTベースの構成..アブストラクタの文で展開されたアテンションメカニズムにより、最終的な要約で目的のコンテンツが強調されます。 
[概要]ユーザーには、モデルが行っている選択へのウィンドウと、それらの選択をより望ましい方向に導く機会が与えられます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-24">
        <br><font color="black">2020-04-24</font>
      </time>
    </span>
</section>
<!-- paper0: The Struggles of Feature-Based Explanations: Shapley Values vs. Minimal
  Sufficient Subsets -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/cs.CL/paper_22.html">
      <font color="black">The Struggles of Feature-Based Explanations: Shapley Values vs. Minimal
  Sufficient Subsets</font>
    </a>
  </h2>
  <font color="black">さらに、2つの人気のあるクラスの説明者、Shapley説明者と最小限の十分なサブセット説明者が、説明者が1つの特定の機能ベースの説明を探す必要があるという暗黙の了解にもかかわらず、根本的に異なるタイプのグラウンドトゥルース説明を対象としていることを示します。説明者の開発と選択の両方で考慮すべき追加の側面。この作業では、機能ベースの説明が、些細なモデルを説明する場合でも問題を引き起こすことを示します。 
[概要]特定のケースでは、少なくとも2つの根拠（真実の特徴に基づく問題）があることを示します。しかし、場合によっては、どちらも意思決定の完全なビューを提供するのに十分ではありません-モデルの作成プロセス</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: REDAT: Accent-Invariant Representation for End-to-End ASR by Domain
  Adversarial Training with Relabeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.AS/paper_0.html">
      <font color="black">REDAT: Accent-Invariant Representation for End-to-End ASR by Domain
  Adversarial Training with Relabeling</font>
    </a>
  </h2>
  <font color="black">同等性の証明に動機付けられて、教師なしクラスタリングまたはソフトラベルのいずれかを使用してデータに再ラベル付けするDATに基づく新しい手法であるreDATを紹介します。また、DATで勾配反転を実行することは、イェンセンシャノン間の発散を最小化することと同等であることを証明します。ドメイン出力分布..23K時間のマルチアクセントデータの実験では、DATは、ネイティブおよび非ネイティブの両方の英語アクセントでアクセント固有のベースラインに対して競争力のある結果を達成しますが、目に見えないアクセントでは最大13％の相対WER削減を達成します。私たちのreDATは、アメリカ英語とイギリス英語の非ネイティブアクセントに対して、DATよりも3％と8％さらに改善されています。 
[概要]新しい調査によると、datはアクセントに対して達成可能な結果を達成します-ネイティブと非ネイティブの両方の英語アクセントの特定のベースライン。ただし、目に見えないアクセントの相対的な減少は最大13％です。redatはdatよりも3％と8％さらに改善されます。アメリカ英語とイギリス英語の非ネイティブアクセントに比較的</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Few Shot Adaptive Normalization Driven Multi-Speaker Speech Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.AS/paper_1.html">
      <font color="black">Few Shot Adaptive Normalization Driven Multi-Speaker Speech Synthesis</font>
    </a>
  </h2>
  <font color="black">さらに、正規化のアフィンパラメータが、エネルギーや基本周波数などの韻律的特徴を解きほぐしてキャプチャするのにどのように役立ち、モーフィングされた音声出力を生成するために使用できるかを示します。この論文では、新しい数ショットマルチスピーカーを紹介します。非自動回帰マルチヘッドアテンションモデルを使用した適応正規化アーキテクチャを活用する音声合成アプローチ（FSM-SS）。入力テキストと見えない人の参照音声サンプルが与えられると、FSM-SSはその人のスタイルで音声を生成できます。いくつかのショットの方法。 
[ABSTRACT]スタイルはsignal.fsmの韻律によって最適に撮影されます-nonは数ショットの方法でその人のスタイルで音声を生成できます。マルチスピーカーvctkおよびlibrittsデータセットで提案されたアーキテクチャの有効性を示しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Towards unsupervised phone and word segmentation using self-supervised
  vector-quantized neural networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.AS/paper_2.html">
      <font color="black">Towards unsupervised phone and word segmentation using self-supervised
  vector-quantized neural networks</font>
    </a>
  </h2>
  <font color="black">2つ目は、動的計画法を使用して、ペナルティ項を使用して二乗誤差を最適化し、より少ないがより長いセグメントを促進します。ペナルティ付きの方法は、一般的に最高のパフォーマンスを発揮します。電話セグメンテーション、ABX電話識別、同じ-異なる単語識別、およびシンボリック単語セグメンテーションアルゴリズムへの入力として。 
[概要] vqシステムは、さまざまなタスクを変更せずに使用できます。これらには、監視されていない電話セグメンテーション、abx電話識別、同じ-異なる単語識別、およびシンボリック単語セグメンテーションアルゴリズムへの貢献が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: AV Taris: Online Audio-Visual Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.AS/paper_3.html">
      <font color="black">AV Taris: Online Audio-Visual Speech Recognition</font>
    </a>
  </h2>
  <font color="black">これは、視聴覚音声統合とオンライン音声認識のために最近提案された2つのモデル、つまりAV AlignとTarisを接続することで実現します。音声の視覚モダリティは、これらの課題を部分的に克服し、話者のダイアリゼーションのサブタスクに貢献する可能性を秘めています。 、音声アクティビティの検出、調音部位の回復、平均で最大15dBのノイズを補正できます。この記事では、視聴覚音声をリアルタイムでデコードできる完全に区別可能なニューラルネットワークモデルであるAVTarisを開発します。 
[概要]この記事では、オーディオをデコードできる完全に微分可能なニューラルネットワークモデルであるavtarisを開発します-リアルタイムで視覚的な音声</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-SpectroGAN: High-Diversity and High-Fidelity Spectrogram
  Generation with Adversarial Style Combination for Speech Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.AS/paper_4.html">
      <font color="black">Multi-SpectroGAN: High-Diversity and High-Fidelity Spectrogram
  Generation with Adversarial Style Combination for Speech Synthesis</font>
    </a>
  </h2>
  <font color="black">本論文では、ジェネレータの自己監視非表示表現を条件付き弁別器に条件付けすることにより、敵対的フィードバックのみでマルチスピーカーモデルをトレーニングできるMulti-SpectroGAN（MSG）を提示します。さらに、敵対的スタイルも提案します。複数のメルスペクトログラムから埋め込まれた組み合わせスタイルの潜在的な表現を学習できる、目に見えない話し方とトランスクリプトのより良い一般化のための組み合わせ（ASC）..生成的敵対ネットワーク（GAN）ベースのニューラルテキストからスピーチ（TTS）システム神経音声合成の大幅な改善が示されていますが、敵対的なフィードバックのみでテキストシーケンスから音声を合成することを学習するTTSシステムはありません。 
[ABSTRACT] msgは、個々の話し方を制御および混合することにより、メル-スペクトログラムを合成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: A comparison of self-supervised speech representations as input features
  for unsupervised acoustic word embeddings -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.AS/paper_5.html">
      <font color="black">A comparison of self-supervised speech representations as input features
  for unsupervised acoustic word embeddings</font>
    </a>
  </h2>
  <font color="black">英語とXitsongaデータの単語識別タスクでは、3つの表現学習アプローチすべてがMFCCを上回り、CPCは一貫して最大の改善を示しています。最近のアプローチには、自己教師あり予測コーディングと対応オートエンコーダー（CAE）モデルが含まれます。ラベルのない音声が唯一の利用可能なリソースである処理では、最良のAWEアプローチのいくつかは、自動的に検出された単語のようなセグメントの形で弱いトップダウン制約に依存しています。 
[ABSTRACT]ゼロ-リソース調査では、表現学習を短期間で検討しました-時間枠レベル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Group Communication with Context Codec for Ultra-Lightweight Source
  Separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.AS/paper_6.html">
      <font color="black">Group Communication with Context Codec for Ultra-Lightweight Source
  Separation</font>
    </a>
  </h2>
  <font color="black">実験結果は、GC3が2.5％のモデルサイズで幅広いベースラインアーキテクチャと同等以上のパフォーマンスを達成できることを示しています。超軽量モデル設計は、既存の音声強調および音源分離技術の展開にとって重要なトピックです。低リソースプラットフォーム..コンテキストコーデックは、シーケンシャルフィーチャの長さを短縮するために適用されます。ここで、コンテキストエンコーダは、ローカルフィーチャの時間コンテキストを、コンテキストのグローバル特性を表す単一のフィーチャに圧縮し、コンテキストデコーダは、変換されたフィーチャを解凍します。グローバル機能をコンテキスト機能に戻します。 
[概要]近年、軽量モデル設計コンセプトのコンセプトが提案されていますが、多くのモデルは、モデルサイズ、モデルの複雑さ、およびモデルのパフォーマンスのバランスを見つけることに依然として苦労しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Classification of ALS patients based on acoustic analysis of sustained
  vowel phonations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.AS/paper_7.html">
      <font color="black">Classification of ALS patients based on acoustic analysis of sustained
  vowel phonations</font>
    </a>
  </h2>
  <font color="black">ただし、音声および音声の症状の初期の音響症状は非常に多様であるため、人間の専門家と自動システムの両方で検出が非常に困難になります。特に、この作業は母音/ a /および/ i /の持続発声の分析に焦点を当てています。 ALS患者の自動分類を実行するために..分析されました。 
[概要] alsの初期症状は嚥下と発話の困難です。これらの初期症状は嚥下と発話が困難です。これらは犯罪分析（lda）の一般的な初期症状です。これらの機能は、alsの患者によって生成された発声を分類するために使用されました。と健康な個人によるもの</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Bayesian Learning for Deep Neural Network Adaptation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.AS/paper_8.html">
      <font color="black">Bayesian Learning for Deep Neural Network Adaptation</font>
    </a>
  </h2>
  <font color="black">音声認識システムの重要なタスクは、話者の違いに起因することが多いトレーニングデータと評価データの不一致を減らすことです。この問題に対処するために、このペーパーでは、話者に依存するモデルを作成するための完全なベイズ学習ベースのDNN話者適応フレームワークを提案します。 SD）話者固有の適応データが限られている場合のパラメータの不確実性..話者レベルのデータの量が限られている場合、話者の適応は過剰に適合し、一般化が不十分になる傾向があります。 
[概要]話者の適応は、過剰適合や一般化が不十分になる傾向があります。この目的のために、話者の適応技術は効果的です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Audio Captioning using Pre-Trained Large-Scale Language Model Guided by
  Audio-based Similar Caption Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.AS/paper_9.html">
      <font color="black">Audio Captioning using Pre-Trained Large-Scale Language Model Guided by
  Audio-based Similar Caption Retrieval</font>
    </a>
  </h2>
  <font color="black">音声キャプションの目的は、入力音声を自然言語を使用して説明に翻訳することです。次に、音声入力のキャプションは、ガイダンスキャプションを参照しながら、事前にトレーニングされた言語モデルを使用して生成されます。音声の問題の1つキャプションとは、Webをクロールしてオーディオとキャプションのペアを収集することが難しいため、トレーニングデータが不足していることです。 
[概要]音声の収集が難しいため、音声のキャプションが問題になります-ウェブをクロールして字幕のペアを作成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Towards localisation of keywords in speech using weak supervision -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.AS/paper_10.html">
      <font color="black">Towards localisation of keywords in speech using weak supervision</font>
    </a>
  </h2>
  <font color="black">2つ目は、視覚的なコンテキストが、ラベルのない発話とペアになった画像の形式で提供されます。次に、ペアのデータを使用して、モデルを自己監視方式でトレーニングする必要があります。結果は、ローカリゼーションを可能にする信号があることを示していますが、これらの形式の弱い監視により適した他のローカリゼーション方法も必要です。キーワードのローカリゼーションでは、ビジョンドメインで通常使用される顕著性ベースの方法を採用しています。 
[概要]キーワードのローカリゼーションでは、ビジョンセルフで通常使用される顕著性ベースのモデルを採用します。このモデルはアーキテクチャ上の制限なしで適用できます。顕著性ベースの方法はより柔軟ですが、アーキテクチャ上の制限なしで使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-12-14">
        <br><font color="black">2020-12-14</font>
      </time>
    </span>
</section>
<!-- paper0: Detecting Parkinson's Disease From an Online Speech-task -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-12-15/eess.AS/paper_11.html">
      <font color="black">Detecting Parkinson's Disease From an Online Speech-task</font>
    </a>
  </h2>
  <font color="black">データのごく一部は、品質を比較するためにラボ設定で収集されました。さらに分析すると、広く使用されているMFCC機能と、パーキンソン病を口頭の発声タスク（「ahh」と発音）から検出するために設計された以前に検証された発声障害機能のサブセットに、最も明確な情報..音声データから、標準的な音響機能（Mel Frequency Cepstral Coefficients（MFCC）、ジッターおよびシマーバリアント）とディープラーニングベースの機能の両方を抽出しました。 
[概要]研究者は726人のユニークな参加者からデータを収集しました。彼らは英語のアルファベットの文字を含む人気のパングラムを発するように言われました。これらの機能を使用して、いくつかの機械学習アルゴリズムをトレーニングしました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-02">
        <br><font color="black">2020-09-02</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
