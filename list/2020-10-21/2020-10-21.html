<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-10-21の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: The Effect of Spectrogram Reconstruction on Automatic Music
  Transcription: An Alternative Approach to Improve Transcription Accuracy -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.SD/paper_0.html">
      <font color="black">The Effect of Spectrogram Reconstruction on Automatic Music
  Transcription: An Alternative Approach to Improve Transcription Accuracy</font>
    </a>
  </h2>
  <font color="black">U-netによって学習された特徴マップにはグリッド状の構造（ベースラインモデルには存在しません）が含まれています。これは、再構築損失が存在する場合、モデルが時間軸と周波数軸の両方に沿ってカウントしようとしていることを意味します。ノートレベルの転写精度..さらに、フレームレベルの精度を最新のモデルよりも高くすることもできます。提案されたモデルは2つのU-netで構成されています。最初のU-netはスペクトログラムをポストリアグラムに変換し、2番目のU-netがポストリアグラムをスペクトログラムに変換します。 
[概要]次に、予測が連結され、入力として使用されて、ピッチラベルを使用して別のモデルをトレーニングし、最終的な翻訳を取得します。これは、音楽の状態を達成することを目的としない、u-netによって監視された最初の論文です。代わりに、スペクトログラムの再構築がそのようなモデルに与える影響を調査します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Small-Footprint Keyword Spotting with Multi-Scale Temporal Convolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.SD/paper_1.html">
      <font color="black">Small-Footprint Keyword Spotting with Multi-Scale Temporal Convolution</font>
    </a>
  </h2>
  <font color="black">推論段階では、MTConvを基本畳み込みアーキテクチャに同等に変換できるため、基本モデルと比較して追加のパラメーターや計算コストが追加されることはありません。目的のモデルに基づいて、標準の時間畳み込みレイヤーを次のMTConvに置き換えます。パフォーマンスを向上させるためにトレーニングできます。マルチブランチ時間畳み込みモジュール（MTConv）を提案します。これは、時間的特徴空間を強化する、異なるカーネルサイズの複数の時間的畳み込みフィルターで構成されるCNNブロックです。 
[概要]複数のostrolフィルターで構成されるcnnブロックであるマルチブランチ時間畳み込みモジュール（mtconv）を提案します。ostrolostrackによると、kwsタスクのフットプリントが小さいことと精度が高いことのトレードオフを実現することは困難です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Phase recovery with Bregman divergences for audio source separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.SD/paper_2.html">
      <font color="black">Phase recovery with Bregman divergences for audio source separation</font>
    </a>
  </h2>
  <font color="black">音声強調タスクで実施された実験は、このアプローチがいくつかの代替損失についてMISIを上回っていることを示しており、オーディオソース分離アプリケーションとの関連性を強調しています。 ..このアルゴリズムは、マグニチュードスペクトログラム間の2次再構成エラーを最小限に抑えます。 
[概要]複数入力スペクトログラム反転（misi）アルゴリズムは、良好なパフォーマンスを示しています。この損失は、オーディオの一部の知覚特性を適切に説明していません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: An Audio-Video Deep and Transfer Learning Framework for Multimodal
  Emotion Recognition in the wild -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.SD/paper_3.html">
      <font color="black">An Audio-Video Deep and Transfer Learning Framework for Multimodal
  Emotion Recognition in the wild</font>
    </a>
  </h2>
  <font color="black">エンドツーエンドの深層学習を使用し、転移学習アプローチの恩恵を受けて、42.10％のテストセットチャレンジパフォーマンス測定に到達しました。この論文では、ABAW表情チャレンジへの貢献を示します。提案されたシステムとチャレンジプロトコルに準拠した公式のチャレンジ結果。 
[概要]提案システムと公式チャレンジ結果を報告します。公式結果も報告します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-07">
        <br><font color="black">2020-10-07</font>
      </time>
    </span>
</section>
<!-- paper0: Replacing Human Audio with Synthetic Audio for On-device Unspoken
  Punctuation Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.SD/paper_4.html">
      <font color="black">Replacing Human Audio with Synthetic Audio for On-device Unspoken
  Punctuation Prediction</font>
    </a>
  </h2>
  <font color="black">音響機能とテキスト機能を組み合わせた、英語用の新しいマルチモーダル無言句読点予測システムを紹介します。韻律認識テキスト読み上げシステムを使用して生成された合成データのみに依存することにより、初めて実証します。口に出さない句読点予測問題について、高価な人間の音声録音でトレーニングされたモデルよりも優れたパフォーマンスを発揮できます。これは、自動音声認識テキスト出力のハッシュベースの埋め込みを、準反復ニューラルネットワークへの入力として音響機能と組み合わせて活用することで実現されます。モデルサイズが小さく、待ち時間が短い。 
[概要]初めてデモンストレーションを行いましたが、高価な人間のオーディオ録音でトレーニングされたモデルよりも優れたパフォーマンスを発揮できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-End Text-to-Speech using Latent Duration based on VQ-VAE -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.SD/paper_5.html">
      <font color="black">End-to-End Text-to-Speech using Latent Duration based on VQ-VAE</font>
    </a>
  </h2>
  <font color="black">条件付きVQ-VAEに基づいてメソッドを定式化し、変分オートエンコーダーで離散期間を処理し、メソッドを正当化する理論的説明を提供します。結果は、システムがソフトアテンションベースのメソッド（Transformer-TTS、Tacotron2）の間で評価されたことを示しました。明示的持続時間モデリングベースの方法（Fastspeech）。提案された方法をリスニングテストで評価し、ソフトアテンションまたは明示的持続時間モデリングに基づく他のTTS方法と比較しました。 
[概要]明示的な期間モデリングを使用して新しいttsフレームワークを提案します。システムは接続主義分類（ctc）を使用します-ベースのフォースalignr.text-から-期間-はオートエンコーダーの優先事項です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: Leveraging the structure of musical preference in content-aware music
  recommendation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.SD/paper_6.html">
      <font color="black">Leveraging the structure of musical preference in content-aware music
  recommendation</font>
    </a>
  </h2>
  <font color="black">大規模なデータで実施された実験は、このアプローチが、コンパクトで意味のある音楽的特徴のセットを使用しながら、コールドスタートの問題に対処できることを示しています。この作品では、代わりに、音楽心理学の分野..低レベルの音響的特徴から、音楽の好みを正確に表す3つの要素（覚醒、価数、深さ）を抽出します。 
[概要]これらのアプローチは曲の内容に対してアギポクリティカルであるため、コールドスタートの問題に直面します。これは、履歴を聞いていない新しい曲を推奨できないためです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Investigating Cross-Domain Losses for Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.SD/paper_7.html">
      <font color="black">Investigating Cross-Domain Losses for Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">最近のモデルベースおよび深層学習SEアプローチに対する定量的比較分析を実行して、提案されたフレームワークのメリットを説明します。この研究では、音声了解度と品質への影響を個別に調べることにより、アプローチの各セットの利点を調査します。 。近年、音声強調（SE）と認識に利用できるフレームワークの数が急増しています。 
[概要]新しいモデルベースのseフレームワークにより、音声と時間-音声データの周波数（tf）表現が可能になります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Power pooling: An adaptive pooling function for weakly labelled sound
  event detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.SD/paper_8.html">
      <font color="black">Power pooling: An adaptive pooling function for weakly labelled sound
  event detection</font>
    </a>
  </h2>
  <font color="black">2つの公開データセットでは、提案された電力プーリング関数は、粗粒度と細粒度の両方のメトリックで最先端の線形ソフトマックスプーリングよりも優れています。特に、イベントベースのF1スコア（イベント開始の検出を評価する）が向上します。この論文はサウンドイベント検出アプリケーションに焦点を当てていますが、提案された方法は他のドメインのMILタスクに適用できます。 
[ABSTRACT]弱いラベルのサウンドイベントのタイプとタイムスタンプの両方を検出する機能は、タイプのみを指定します。この論文では、さまざまな音源に適応できる適応型パワープーリング機能を提案します。これにより、イベントベースも改善されます。 2つのデータセットに対して11.4％および10.2％のf1スコア</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Tongji University Undergraduate Team for the VoxCeleb Speaker
  Recognition Challenge2020 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.SD/paper_9.html">
      <font color="black">Tongji University Undergraduate Team for the VoxCeleb Speaker
  Recognition Challenge2020</font>
    </a>
  </h2>
  <font color="black">このレポートでは、Interspeech2020でのVoxCeleb話者認識チャレンジ（VoxSRC）2020のCLOSEトラックへの同済大学の学部チームの提出について説明します。CLOSEトラック用に選択された2つのシステムの融合により、0.2973DCFと4.9700 \％EERが達成されます。チャレンジ評価セットについて..RSBU-CWモジュールをResNet34フレームワークに適用して、ネットワークのノイズ除去能力を向上させ、複雑な環境での話者検証タスクをより適切に完了しました。スコアフュージョンとデータを使用して、ResNetの2つのバリアントをトレーニングしました。 -モデルのパフォーマンスを向上させるための拡張方法。 
[概要] resnetの2つのバリアントをトレーニングし、スコア融合とデータを使用しました。モデルのパフォーマンスを向上させるための拡張方法です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Adaptive Feature Selection for End-to-End Speech Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.SD/paper_10.html">
      <font color="black">Adaptive Feature Selection for End-to-End Speech Translation</font>
    </a>
  </h2>
  <font color="black">L0DROP（Zhang et al。、2020）をAFSのバックボーンとして採用し、時間的次元と特徴次元の両方に関して音声特徴をスパース化するように適合させます。LibriSpeechEn-FrおよびMuST-Cベンチマークの結果は、AFSが学習を容易にすることを示しています。 〜84％の時間的特徴を取り除くことにより、STの平均変換ゲインは〜1.3〜1.6 BLEU、デコード速度は〜1.4xになります。特に、AFSはカスケードベースラインと比較してパフォーマンスギャップを減らし、 BLEUスコアが18.56のLibriSpeechEn-Fr（データ拡張なし）
[要約]エンコーダー-デコーダーベースのe2e stの適応機能選択（afs）を提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-16">
        <br><font color="black">2020-10-16</font>
      </time>
    </span>
</section>
<!-- paper0: Pushing the Limits of Semi-Supervised Learning for Automatic Speech
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.SD/paper_11.html">
      <font color="black">Pushing the Limits of Semi-Supervised Learning for Automatic Speech
  Recognition</font>
    </a>
  </h2>
  <font color="black">そうすることで、LibriSpeechテスト/テスト-その他のセットで、現在の最先端のWER 1.7％/ 3.3％に対して単語誤り率（WER）1.4％/ 2.6％を達成できます。自動音声認識のための半教師あり学習の最近の開発の組み合わせを採用して、Libri-Lightデータセットのラベルなしオーディオを利用してLibriSpeechで最先端の結果を取得します。より正確には、SpecAugmentを使用して騒々しい学生トレーニングを実行します。 wav2vec2.0事前トレーニングを使用して事前トレーニングされた巨大なConformerモデルを使用します。 
[概要] speceeでノイズの多い学生トレーニングを実施します。また、巨大なコンフォーマーモデルを使用してノイズトレーニングを実施します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic multitrack mixing with a differentiable mixing console of
  neural audio effects -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.SD/paper_12.html">
      <font color="black">Automatic multitrack mixing with a differentiable mixing console of
  neural audio effects</font>
    </a>
  </h2>
  <font color="black">提案されたモデルは、限られた数の例でトレーニングでき、入力の順序に関して順列不変であり、入力ソースの数に制限はありません。これらの課題に対処するために、強力なドメインに触発されたモデルを提案します。ミキシングタスクの誘導バイアス..さらに、人間が読み取れるミキシングパラメータを生成し、ユーザーが生成されたミックスを手動で調整または調整できるようにします。 
[概要]この作業は、基礎となるミキシングパラメータの知識がなくても、波形レベルで実世界のデータからマルチトラックミキシング規則を学習する最初のアプローチを示しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Multi-structure bone segmentation in pediatric MR images with combined
  regularization from shape priors and adversarial network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.IV/paper_0.html">
      <font color="black">Multi-structure bone segmentation in pediatric MR images with combined
  regularization from shape priors and adversarial network</font>
    </a>
  </h2>
  <font color="black">小児磁気共鳴（MR）画像をセグメント化するという困難なタスクのために、正規化された畳み込みエンコーダ-デコーダネットワークを提案します。この目的のために、損失関数への追加の正規化項を含むセグメンテーションネットワークの新しい最適化スキームを考案しました。小児の筋骨格系の形態学的および診断的評価は、臨床診療において非常に重要です。 
[ABSTRACT]調査によると、セグメンテーションモデルの一般化を改善するために正則化戦略を採用しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-15">
        <br><font color="black">2020-09-15</font>
      </time>
    </span>
</section>
<!-- paper0: Towards an Automatic Analysis of CHO-K1 Suspension Growth in
  Microfluidic Single-cell Cultivation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.IV/paper_1.html">
      <font color="black">Towards an Automatic Analysis of CHO-K1 Suspension Growth in
  Microfluidic Single-cell Cultivation</font>
    </a>
  </h2>
  <font color="black">結果：新しい機械学習アーキテクチャと特殊なトレーニング手順を提案します。これにより、データレベルで人力による抽象化をディープニューラルネットワークに注入できるため、必要なのはごくわずかな回帰モデルになります。ラベル付けされたデータの量..動機：革新的なマイクロ流体システムは、明確に定義された環境条件下で単一細胞の時空間分析を大幅に促進し、集団の不均一性に対する新しい洞察を可能にし、基本的および応用バイオテクノロジーの新しい機会を開くことを約束します。画像処理技術はこの分野で満足のいく結果をもたらさないため、畳み込みネットワークなどの最新の深層学習技術は、自動セル追跡や自動カウント、成長率などの重要なパラメーターの抽出など、さまざまなタスクに十分に対応できます。 
[概要]マイクロフルイディクス実験には、顕微鏡画像の時系列など、手動での評価が不可能な膨大な量のデータが伴います。これらは、一連の画像ごとに細胞を追跡するために必要です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Reinventing 2D Convolutions for 3D Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.IV/paper_2.html">
      <font color="black">Reinventing 2D Convolutions for 3D Images</font>
    </a>
  </h2>
  <font color="black">事前トレーニングがなくても、ACS畳み込みは、標準の3D畳み込みのプラグアンドプレイの代替として使用でき、モデルサイズが小さく、計算が少なくて済みます。概念実証データセットといくつかの医療ベンチマークに関する広範な実験により、の一貫した優位性が検証されます。事前トレーニングあり/なしの2D / 3D CNN対応物を超える、事前トレーニング済みACS CNN ..この研究では、2D畳み込みを再発明することにより、2Dと3D畳み込みの間のギャップを埋めます。 
[概要]これらは大規模な2D事前トレーニングの恩恵を受ける可能性がありますが、一般に大規模な3D環境のキャプチャには弱いです。さらに、acsコンボリューションはプラグアンドプレイとして標準の3Dコンボリューションの代わりに使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-24">
        <br><font color="black">2019-11-24</font>
      </time>
    </span>
</section>
<!-- paper0: Synthesis of COVID-19 Chest X-rays using Unpaired Image-to-Image
  Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.IV/paper_3.html">
      <font color="black">Synthesis of COVID-19 Chest X-rays using Unpaired Image-to-Image
  Translation</font>
    </a>
  </h2>
  <font color="black">このデータセットから収集された洞察は、COVID-19パンデミックとの闘いにおける予防措置に使用できます。さらに、提案されたデータ生成フレームワークは、特にCOVID-19検出、および医用画像分類タスクに対する実行可能なソリューションを提供します。一般的に..次に、合成データのみでトレーニングした場合に同等の検出パフォーマンスを達成することにより、画像合成方法がデータ匿名化ツールとしてどのように機能するかを示します。 
[概要]合成データのみでトレーニングした場合に同等の検出パフォーマンスを実現することにより、画像合成方法がデータ匿名化ツールとしてどのように機能するかを示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Linear-Model-inspired Neural Network for Electromagnetic Inverse
  Scattering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.IV/paper_4.html">
      <font color="black">Linear-Model-inspired Neural Network for Electromagnetic Inverse
  Scattering</font>
    </a>
  </h2>
  <font color="black">効率的なエンドツーエンドの学習を達成するために、ネットワークアーキテクチャとハイパーパラメータ推定が提示されます。実験結果は、いくつかの最先端技術に対するその優位性を検証します。それはしばしば非常に非線形であり、問題を引き起こします。解決するのは非常に難しいです。 
[要約]これは多くの場合、非常に線形であるため、問題の解決が非常に困難になります。多くの場合、非常に線形であるため、学習が困難になります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-03">
        <br><font color="black">2020-03-03</font>
      </time>
    </span>
</section>
<!-- paper0: A Graph Attention Spatio-temporal Convolutional Network for 3D Human
  Pose Estimation in Video -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.IV/paper_5.html">
      <font color="black">A Graph Attention Spatio-temporal Convolutional Network for 3D Human
  Pose Estimation in Video</font>
    </a>
  </h2>
  <font color="black">また、重要なことに、相乗効果を実現するために、空間セマンティクスと時間依存性のインターリーブを慎重に設計します。2つの挑戦的なベンチマークデータセット（Human3.6MとHumanEva-I）とYouTubeビデオでの実験は、私たちのアプローチが深さのあいまいさと自己を効果的に軽減することを示しています-オクルージョン、上半身の半分の推定に一般化し、2Dから3Dへのビデオポーズ推定で競争力のあるパフォーマンスを実現します。この作業では、人間の骨格における運動学的制約の学習を改善します：姿勢、局所運動学的接続、および対称性注意メカニズムを介したローカルおよびグローバルな空間情報のモデリング。 
[要約]空間-またはグローバルアーキテクチャは、固定長の空間-一時性を埋め込むために使用されています。これらには、人間の骨格の不道徳と不道徳を没頭させることが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-11">
        <br><font color="black">2020-03-11</font>
      </time>
    </span>
</section>
<!-- paper0: COVID-19 Screening Using Residual Attention Network an Artificial
  Intelligence Approach -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.IV/paper_6.html">
      <font color="black">COVID-19 Screening Using Residual Attention Network an Artificial
  Intelligence Approach</font>
    </a>
  </h2>
  <font color="black">この論文では、人工知能を使用してCOVID-19をスクリーニングする手法を紹介します。このプロジェクトで使用されるコードとデータセットは、https：//github.com/vishalshar/covid-19-screening-using-RAN-で入手できます。 on-X線画像..COVID-19の効果的な検査戦略は、発生を制御するために重要ですが、検査の需要は、逆転写ポリメラーゼ連鎖反応（RT-PCR）を使用する検査キットの入手可能性を上回っています。 
[要約]ウイルスは急速に伝染します;基本再生産数rは2.2-2。7.covid-19は、現在200か国以上に影響を及ぼしており、600万件の活動があります。人工知能を使用してウイルスをスクリーニングする技術をテストしています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-26">
        <br><font color="black">2020-06-26</font>
      </time>
    </span>
</section>
<!-- paper0: Micro CT Image-Assisted Cross Modality Super-Resolution of Clinical CT
  Images Utilizing Synthesized Training Dataset -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.IV/paper_7.html">
      <font color="black">Micro CT Image-Assisted Cross Modality Super-Resolution of Clinical CT
  Images Utilizing Synthesized Training Dataset</font>
    </a>
  </h2>
  <font color="black">したがって、臨床CTボリュームの超解像は肺癌の診断に役立つ可能性があります。この論文は、臨床CTのSRをマイクロCTの解像度レベルに実行するための新しい監視されていない超解像（SR）アプローチを提案します（ $ \ mu $ CT）..一般的なSR法では、トレーニング用に低解像度（LR）画像と高解像度（HR）画像のペアを揃える必要があります。 
[要約]肺がんの正確な侵襲前診断は、通常、臨床ctデータを使用します。この方法では、診断の低解像度（lr）画像と高解像度（hr）画像のペアを揃える必要があります。この方法は、と呼ばれる臨床ct画像に基づいています。 sr-cyclegan</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: A versatile anomaly detection method for medical images with a
  flow-based generative model in semi-supervision setting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.IV/paper_8.html">
      <font color="black">A versatile anomaly detection method for medical images with a
  flow-based generative model in semi-supervision setting</font>
    </a>
  </h2>
  <font color="black">この方法を使用すると、事後確率を任意の画像の正規性メトリックとして計算できます。この研究では、2つのトレーニング済みフローベースの生成モデルに基づく異常検出方法を示します。この結果は、方法の多様性を示しています。 
[要約]この方法は、2種類の医用画像で検証されました。以前の研究では、ヨーク大学による研究の一部として検証されたことがわかりました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-22">
        <br><font color="black">2020-01-22</font>
      </time>
    </span>
</section>
<!-- paper0: A Loss Function for Generative Neural Networks Based on Watson's
  Perceptual Model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.IV/paper_9.html">
      <font color="black">A Loss Function for Generative Neural Networks Based on Watson's
  Perceptual Model</font>
    </a>
  </h2>
  <font color="black">モデルをカラー画像に拡張し、フーリエ変換を使用して変換に対するロバスト性を高め、画像をブロックに分割することによるアーティファクトを除去し、微分可能にします。実験では、新しい損失関数でトレーニングされたVAEが現実的で高高品質の画像サンプル..ユークリッド距離と構造類似性指数を使用した場合と比較して、画像のぼやけが少なくなりました。ディープニューラルネットワークベースの損失と比較して、新しいアプローチでは、必要な計算リソースが少なく、アーティファクトの少ない画像が生成されました。 
[概要]新しい損失関数は、ワトソンの知覚モデルに基づいています。新しいモデルからの詳細で詳細なデータを提供します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-26">
        <br><font color="black">2020-06-26</font>
      </time>
    </span>
</section>
<!-- paper0: Video Reconstruction by Spatio-Temporal Fusion of Blurred-Coded Image
  Pair -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.IV/paper_10.html">
      <font color="black">Video Reconstruction by Spatio-Temporal Fusion of Blurred-Coded Image
  Pair</font>
    </a>
  </h2>
  <font color="black">従来のコード化された露出フレームワークの方が適切ですが、時空間ボリュームの一部しかサンプリングしません。これは、時空間ボリュームのせいぜい50％です。ここでは、完全に存在する補足情報を使用することを提案します。露出された（ぼやけた）画像とコード化された露出画像を組み合わせて、動きのあいまいさのない高忠実度のビデオを復元します。これに加えて、動きの方向が失われ、動きのあいまいさが生じます。 
[概要]システムは、ぼやけたコード化された画像を使用して高解像度のビデオを作成します。単一のモーションを再現するために使用できます。ぼやけた画像ですが、シーンの静的な部分の情報を完全に保存するという利点があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Federated Learning for Breast Density Classification: A Real-World
  Implementation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.IV/paper_11.html">
      <font color="black">Federated Learning for Breast Density Classification: A Real-World
  Implementation</font>
    </a>
  </h2>
  <font color="black">結果は、FLを使用してトレーニングされたモデルは、研究所のローカルデータのみでトレーニングされたモデルよりも平均6.3％優れていることを示しています。さらに、他の参加サイトのテストデータで評価すると、モデルの一般化可能性が45.8％向上していることがわかります。 ..すべてのサイトのデータセット（マンモグラフィーシステム、クラス分布、データセットサイズ）が大幅に異なるにもかかわらず、データを一元化することなく、フェデレーションでAIモデルを正常にトレーニングできることを示します。 
[概要]研究者は、フェデレーション学習（fl）を使用して、実世界の共同設定で医用画像分類モデルをトレーニングします。彼らは、大幅な違いにもかかわらず、フェデレーションでaiモデルを正常にトレーニングできることを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-03">
        <br><font color="black">2020-09-03</font>
      </time>
    </span>
</section>
<!-- paper0: SpecNet: Spectral Domain Convolutional Neural Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.IV/paper_12.html">
      <font color="black">SpecNet: Spectral Domain Convolutional Neural Network</font>
    </a>
  </h2>
  <font color="black">SpecNetのパフォーマンスは、3つの競合するオブジェクト認識ベンチマークタスク（CIFAR-10、SVHN、およびImageNet）で評価され、いくつかの最先端の実装と比較されます。全体として、SpecNetはメモリ消費を約60削減できます。 ％テストしたすべてのネットワークのパフォーマンスを大幅に低下させることなく..スペクトルドメインにCNNアーキテクチャを組み込むことは、トレーニングプロセスを加速するために広く利用されていますが、スペクトルドメインを使用してメモリフットプリントを削減することも可能であることを示しています。スペクトルドメインで畳み込み操作とアクティブ化操作の両方を実行するSpectralDomain Convolutional Neural Network（SpecNet）を呼び出します。 
[概要]多くの調査によると、機能マップがメモリの主なボトルネックです。これらには、畳み込み層の後に生成されるマップが含まれます。これらはトレーニングを加速するために使用されますが、赤外線ドメインを使用してメモリフットプリントを削減することもできます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-05-27">
        <br><font color="black">2019-05-27</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Multi-structure bone segmentation in pediatric MR images with combined
  regularization from shape priors and adversarial network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_0.html">
      <font color="black">Multi-structure bone segmentation in pediatric MR images with combined
  regularization from shape priors and adversarial network</font>
    </a>
  </h2>
  <font color="black">小児磁気共鳴（MR）画像をセグメント化するという困難なタスクのために、正則化された畳み込みエンコーダ-デコーダネットワークを提案します。さらに、弁別器によって計算された敵対的な正則化が統合され、もっともらしい描写を促進します。この目的のために、私たちは新しい損失関数への追加の正則化項を含むセグメンテーションネットワークの最適化スキーム。 
[ABSTRACT]調査によると、セグメンテーションモデルの一般化を改善するために正則化戦略を採用しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-15">
        <br><font color="black">2020-09-15</font>
      </time>
    </span>
</section>
<!-- paper0: Teacher-Student Competition for Unsupervised Domain Adaptation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_1.html">
      <font color="black">Teacher-Student Competition for Unsupervised Domain Adaptation</font>
    </a>
  </h2>
  <font color="black">広範な実験により、提案されたTSCフレームワークは、Office-31およびImageCLEF-DAベンチマークでの最先端のドメイン適応方法を大幅に上回っています。既存の従来のUDA方法の構造を備えた教師ネットワークを紹介し、教師と学生ネットワークは、学生ネットワークでのすべてのターゲットサンプルのトレーニングを制約するために、ターゲット疑似ラベルを提供するために競合します。特に、ターゲット固有の特徴空間を学習するために学生ネットワークが導入され、より信頼性の高い疑似を選択するための新しい競合メカニズムが設計されています。学生ネットワークのトレーニングのためのラベル。 
[概要]この論文は、教師との教師なしドメイン適応アプローチを提案します-学生の競争（tsc）</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded
  Dialogues -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_2.html">
      <font color="black">BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded
  Dialogues</font>
    </a>
  </h2>
  <font color="black">この欠点に対処するために、テキストキューに基づくビデオの高解像度クエリのための視覚言語ニューラルフレームワークである双方向時空間学習（BiST）を提案します。具体的には、私たちのアプローチは空間レベルと時間レベルの両方を活用するだけではありません。情報だけでなく、空間から時間および時間から空間への推論を通じて、2つの特徴空間間の動的な情報拡散も学習します。双方向戦略は、ダイアログ設定でのユーザークエリの進化するセマンティクスに取り組むことを目的としています。 
[概要]ビデオトラッキングビデオトラップされたビデオトラッキングは4,000回以上視聴されています。これには、ビデオトラッキング、ビデオトラッキング、ビデオトラッキングが含まれます。ただし、ビデオトラッキングとビデオトラッキングのビデオトラッキング技術はすでに使用されています。発展した</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Leveraging SLIC Superpixel Segmentation and Cascaded Ensemble SVM for
  Fully Automated Mass Detection In Mammograms -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_3.html">
      <font color="black">Leveraging SLIC Superpixel Segmentation and Cascaded Ensemble SVM for
  Fully Automated Mass Detection In Mammograms</font>
    </a>
  </h2>
  <font color="black">マンモグラムでの胸のしこりの識別とセグメンテーションは、その形状、輪郭、テクスチャ、および向きに関して悪性密度の性質が非常に変動するため、複雑な課題に直面します。サポートベクトルマシン（SVM）の新しいカスケードアンサンブルを使用して効果的に取り組みます。クラスの不均衡と重要な予測を提供します。0.35、0.69、0.82の真陽性率（TPR）の場合、システムはそれぞれ0.1、0.5、1.0の偽陽性/画像（FPI）のみを生成します。 
[概要]単純な単純な複雑な複雑な複雑なシステムを使用して、クラスの不均衡に取り組み、重要な予測を提供します。結果には、0 00.00000000000000000が識別される可能性が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Claw U-Net: A Unet-based Network with Deep Feature Concatenation for
  Scleral Blood Vessel Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_4.html">
      <font color="black">Claw U-Net: A Unet-based Network with Deep Feature Concatenation for
  Scleral Blood Vessel Segmentation</font>
    </a>
  </h2>
  <font color="black">具体的には、残差構造を使用して、特徴抽出段階でネットワーク層の数を増やし、より深い特徴を学習します。小さな血管を効果的に抽出するために、注意メカニズムを使用して、画像内の各位置の注意係数を計算します。 UNetは、強膜血管画像データセットで他のUNetベースのネットワークよりも優れています。 
[ABSTRACT] swsは、強膜血管の特性に基づいて2つのタイプに分類できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Attention Augmented ConvLSTM for Environment Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_5.html">
      <font color="black">Attention Augmented ConvLSTM for Environment Prediction</font>
    </a>
  </h2>
  <font color="black">以前に使用されたConvLSTMベースのフレームワークは、多くの場合、移動するオブジェクトの大幅なぼやけと消失をもたらし、セーフティクリティカルなアプリケーションでの使用への適用を妨げます。ロボットシステムでの安全でプロアクティブな計画には、通常、環境の正確な予測が必要です。予測は、占有グリッドなどの鳥瞰図環境表現にビデオフレーム予測技術を適用しました。 
[概要] environmenttmでの以前の作業では、占有グリッドなどの鳥瞰図環境表現にビデオフレーム予測手法を適用しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: Reinventing 2D Convolutions for 3D Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_6.html">
      <font color="black">Reinventing 2D Convolutions for 3D Images</font>
    </a>
  </h2>
  <font color="black">事前トレーニングがなくても、ACS畳み込みは、標準の3D畳み込みのプラグアンドプレイの代替として使用でき、モデルサイズが小さく、計算が少なくて済みます。この研究では、2D畳み込みを再発明することにより、2D畳み込みと3D畳み込みの間のギャップを埋めます。 ..3D医療画像での2Dおよび3D表現の学習についてはかなりの議論がありました。 
[概要]これらは大規模な2D事前トレーニングの恩恵を受ける可能性がありますが、一般に大規模な3D環境のキャプチャには弱いです。さらに、acsコンボリューションはプラグアンドプレイとして標準の3Dコンボリューションの代わりに使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-24">
        <br><font color="black">2019-11-24</font>
      </time>
    </span>
</section>
<!-- paper0: Wasserstein K-Means for Clustering Tomographic Projections -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_7.html">
      <font color="black">Wasserstein K-Means for Clustering Tomographic Projections</font>
    </a>
  </h2>
  <font color="black">合成データセットで、この方法が$ L_2 $ベースラインと比較して優れた結果をもたらすことを示します。さらに、Wasserstein-1メトリックの高速線形時間近似を使用しているため、計算のオーバーヘッドがほとんどありません。 Earthmoverの距離..単一粒子クライオ電子顕微鏡法（cryo-EM）の2Dクラス平均化問題に動機付けられて、画像の回転不変ワッサースタインメトリックに基づくk-meansアルゴリズムを提示します。 
[概要]ワッサースタイン計量が-2ドルのアウトにうまく対応できることを証明します。ただし、高速線形時間のアナロジーを使用しているため、計算のオーバーヘッドはほとんどありません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Tilting at windmills: Data augmentation for deep pose estimation does
  not help with occlusions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_8.html">
      <font color="black">Tilting at windmills: Data augmentation for deep pose estimation does
  not help with occlusions</font>
    </a>
  </h2>
  <font color="black">私たちの広範な実験は、人間の姿勢推定方法が閉塞に対してロバストではなく、データ増強が閉塞の問題を解決しないことを示しています。閉塞は人間の姿勢推定のパフォーマンスを低下させます。この論文では、ターゲットキーポイントと身体部分の閉塞攻撃を紹介します。 
[概要]本稿では、ターゲットを絞ったキーポイントと身体部分のオクルージョン攻撃を紹介します。さらに、オクルージョン固有のデータ拡張手法を提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Asynchronous Edge Learning using Cloned Knowledge Distillation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_9.html">
      <font color="black">Asynchronous Edge Learning using Cloned Knowledge Distillation</font>
    </a>
  </h2>
  <font color="black">特に、知識蒸留方式を「クローン蒸留」と名付け、他の知識蒸留法との違いを説明します。データ需要の高まりとともに、利用性の高い連合学習（FL）法トレーニングプロセスでデバイス上に分散されたローカルデータが提案されています。ただし、スタートアップ企業が提供する駆け出しのサービスは、クライアントの数が限られているだけでなく、サーバーと複数のクライアント間の継続的な通信のための最小限のリソースしかありません。 、フェデレーション学習シナリオをより柔軟な非同期エッジ学習に修正します。 
[ABSTRACT] flシステムは、ユーザーの迅速な流入と流出を使用すると同時に、複数のユーザーのネットワーク遅延によるボトルネックを最小限に抑えます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Visual Localization for Autonomous Driving: Mapping the Accurate
  Location in the City Maze -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_10.html">
      <font color="black">Visual Localization for Autonomous Driving: Mapping the Accurate
  Location in the City Maze</font>
    </a>
  </h2>
  <font color="black">ただし、GPSを適用すると、さまざまな種類の構造物がGPS信号をシャドウイングし、位置の結果が不正確になる可能性がある都心部を車両が走行する場合、深刻な問題が発生する可能性があります。最先端の視覚的ローカリゼーションネットワークとそのアーキテクチャを適切に変更して、車両の操作に適用できるようにします。広範なフィールドテストの結果は、私たちのアプローチが、困難な都心部の設定でもロバストに位置を予測できることを示しています。 
[概要]ミシガン大学の研究者は、視覚的位置特定のための新しい機能投票技術を提案しました。この方法は、最先端の視覚的視覚視覚ネットワークでテストする必要があります。自動運転車が正確な位置情報を見つけるのに役立ちます。望ましい時間の制約内での都市の迷路</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Stage Fusion for One-Click Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_11.html">
      <font color="black">Multi-Stage Fusion for One-Click Segmentation</font>
    </a>
  </h2>
  <font color="black">現在のディープラーニングベースのインタラクティブセグメンテーションアプローチは、アーリーフュージョンを使用し、画像入力レイヤーにユーザーキューを組み込みます。提案されたフレームワークでは、アーリーフュージョンフレームワークと比較してパラメーター数の増加はごくわずかです。標準のインタラクティブインスタンスセグメンテーションで広範な実験を実行します。ワンクリックのセグメンテーションベンチマークを実行し、最先端のパフォーマンスを報告します。 
[ABSTRACT] cnnsには多くのレイヤーがあり、初期の融合は最終的な予測結果に対するユーザーの操作の影響を弱める可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: Two-Stage Generative Adversarial Networks for Document Image
  Binarization with Color Noise and Background Removal -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_12.html">
      <font color="black">Two-Stage Generative Adversarial Networks for Document Image
  Binarization with Color Noise and Background Removal</font>
    </a>
  </h2>
  <font color="black">第1段階では、4つの色に依存しない敵対的ネットワークがトレーニングされ、入力画像から色の前景情報が抽出されてドキュメント画像が強調されます。第2段階では、グローバルおよびローカル機能を備えた2つの独立した敵対的ネットワークがトレーニングされ、ドキュメントの画像の2値化が行われます。可変サイズ..畳み込みニューラルネットワークベースの方法は、グレースケール画像とローカルテキストの特徴にのみ焦点を当てています。 
[概要]従来の非機械学習手法は、教師なし方法で低レベルの特徴に基づいて構築されますが、背景が大幅に劣化したドキュメントでの2値化が困難です。第2段階では、グローバル機能とローカル機能を備えた2つの独立した敵対的ネットワークがトレーニングされます。ドキュメントの画像の2値化</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: UCSG-Net -- Unsupervised Discovering of Constructive Solid Geometry Tree -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_13.html">
      <font color="black">UCSG-Net -- Unsupervised Discovering of Constructive Solid Geometry Tree</font>
    </a>
  </h2>
  <font color="black">2Dおよび3D自動エンコードタスクでメソッドを評価します。ただし、これらのメソッドは非凸形状の再構築に苦労します。予測された解析ツリー表現が解釈可能であり、CADソフトウェアで使用できることを示します。 
[概要]提案された3D形状再構成品質が必要です。ただし、既存のモデルにはcsgパリティツリー全体が必要です。モデルはプリミティブのパラメーターを予測し、微分可能なインジケーター関数を介してそれらのsdf表現を2値化します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-16">
        <br><font color="black">2020-06-16</font>
      </time>
    </span>
</section>
<!-- paper0: Deep N-ary Error Correcting Output Codes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_14.html">
      <font color="black">Deep N-ary Error Correcting Output Codes</font>
    </a>
  </h2>
  <font color="black">深層学習ベースの学習者によるN項ECOCのトレーニングを容易にするために、深層N項ECOCのパラメーター共有アーキテクチャの3つの異なるバリアントをさらに提案します。さらに、深層N項ECOCに関する広範なアブレーション研究は、他よりも優れたパフォーマンスを示しています。ディープデータに依存しないアンサンブル手法..ディープN項ECOCの一般化能力を検証するために、画像とテキストの両方の分類タスクに対して、さまざまなディープニューラルネットワークアーキテクチャでバックボーンを変化させることによって実験を行います。 
[ABSTRACT] deep n-ary ecocは単純ではありませんが、基礎学習者のトレーニングに高い費用がかかるため、文献で十分に活用されています。画像とテキストの両方の分類タスクで、さまざまなディープニューラルネットワークアーキテクチャを使用してバックボーンを変更することで実験を行います。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-22">
        <br><font color="black">2020-09-22</font>
      </time>
    </span>
</section>
<!-- paper0: HCNet: Hierarchical Context Network for Semantic Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_15.html">
      <font color="black">HCNet: Hierarchical Context Network for Semantic Segmentation</font>
    </a>
  </h2>
  <font color="black">具体的には、最初に、特徴マップ全体を異なるクラスベースの同種領域に分割するマルチスケールガイド付き事前セグメンテーションモジュールを提案します。各同種領域内で、ピクセルレベルの相関をキャプチャするようにピクセルコンテキストモジュールを設計します。グローバルピクセルレベルの相関行列は、無差別に自己注意メカニズムで非常に冗長です。 
[ABSTRACT]モデルは、ピクセルレベルの相関行列を組み合わせて「弱い不均一領域」を作成します。したがって、弱い不均一相関をモデル化する自己注意メカニズムとは異なります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-10">
        <br><font color="black">2020-10-10</font>
      </time>
    </span>
</section>
<!-- paper0: Region-specific Dictionary Learning-based Low-dose Thoracic CT
  Reconstruction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_16.html">
      <font color="black">Region-specific Dictionary Learning-based Low-dose Thoracic CT
  Reconstruction</font>
    </a>
  </h2>
  <font color="black">提案手法は、シミュレーションと人間のCTイメージングの両方で構造を回復し、ノイズを抑制する単一の辞書に基づく従来の再構成よりも優れたパフォーマンスをもたらします。この論文は、ユーティリティを最大化するための地域固有の画像パッチを使用した辞書学習ベースの手法を示しますCT画像再構成のための強力なスパースデータ処理技術の概要..CTにおける画像特徴とノイズの不均一な分布を考慮して、辞書の領域固有のカスタマイズが反復再構成で利用されます。 
[概要]提案された方法は、他の解剖学的領域のctイメージングに容易に拡張できます。これは、椎骨周辺のノイズを効果的に低減しながら、肺と心臓の構造をより適切に回復できることを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Synthesis of COVID-19 Chest X-rays using Unpaired Image-to-Image
  Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_17.html">
      <font color="black">Synthesis of COVID-19 Chest X-rays using Unpaired Image-to-Image
  Translation</font>
    </a>
  </h2>
  <font color="black">このデータセットから収集された洞察は、COVID-19パンデミックとの闘いにおける予防措置に使用できます。さらに、提案されたデータ生成フレームワークは、特にCOVID-19検出、および医用画像分類タスクに対する実行可能なソリューションを提供します。一般的に..私たちの貢献は2つあります。 
[概要]合成データのみでトレーニングした場合に同等の検出パフォーマンスを実現することにより、画像合成方法がデータ匿名化ツールとしてどのように機能するかを示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Convolutional neural networks for automatic detection of Focal Cortical
  Dysplasia -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_18.html">
      <font color="black">Convolutional neural networks for automatic detection of Focal Cortical
  Dysplasia</font>
    </a>
  </h2>
  <font color="black">このモデルでは、15人中11人の被験者でFCDの検出に成功しています。限局性皮質異形成症（FCD）は、皮質発達奇形に関連する最も一般的なてんかん原性病変の1つです。ただし、FCDの正確な検出は放射線科医に依存しています。プロフェッショナリズム、そして多くの場合、病変は見落とされる可能性があります。 
[概要] fcd検出は、15人のラベル付きfcd患者のデータセットに基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Query Attack via Opposite-Direction Feature:Towards Robust Image
  Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_19.html">
      <font color="black">Query Attack via Opposite-Direction Feature:Towards Robust Image
  Retrieval</font>
    </a>
  </h2>
  <font color="black">私たちの知る限り、私たちは画像検索専用の攻撃方法を設計する初期の試みの1つです。攻撃された画像をクエリとして展開すると、真の一致は低いランクを受け取る傾向があります。これらの制限に対処するために、敵対的なクエリを生成するための新しいホワイトボックス攻撃アプローチであるOpposite-DirectionFeature Attack（ODFA）。 
[ABSTRACT] quedfaは反対方向のフィーチャ攻撃（odfa）です。これは、フィーチャレベルの敵対地形を効果的に活用し、表現空間のフィーチャ距離を利用します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-09-07">
        <br><font color="black">2018-09-07</font>
      </time>
    </span>
</section>
<!-- paper0: Cross-Scale Internal Graph Neural Network for Image Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_20.html">
      <font color="black">Cross-Scale Internal Graph Neural Network for Image Super-Resolution</font>
    </a>
  </h2>
  <font color="black">広範な実験により、標準ベンチマークでの既存の非ローカルネットワークを含む最先端のSISRメソッドに対するIGNNの有効性が実証されています。さらに、これらの内部画像固有のLR / HRエグザンプラは、以下から学習した外部情報を大幅に補完します。トレーニングデータセット..次に、LR画像内の対応するk HR隣接パッチを取得し、作成されたグラフのエッジラベルに従ってそれらを適応的に集約します。 
[概要]以前のディープ非ローカルメソッドは、低解像度（lr）入力イメージの同じスケール内の同様のパッチのみを利用します。これは、ほとんどの既存のディープ非ローカルメソッドが同じ量の同様のパッチを利用しているためです。標準ベンチマークの非ローカルネットワーク</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-30">
        <br><font color="black">2020-06-30</font>
      </time>
    </span>
</section>
<!-- paper0: Preventing Personal Data Theft in Images with Adversarial ML -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_21.html">
      <font color="black">Preventing Personal Data Theft in Images with Adversarial ML</font>
    </a>
  </h2>
  <font color="black">これは、FaceNetなどの最新の顔認識ツールによって簡単になります。エンコーダーを使用して低次元の埋め込みを生成し、クラスター化してこれまで未知の顔を学習できます。特に問題なのは、教師なし学習を活用して、ユーザーが顔を認識している場合でも顔を認識する機能です。この論文では、新しく導入されたユーザーの顔画像に適用する非侵襲的ノイズマスクを生成し、敵対的な例を示し、埋め込みスペースでの識別可能なクラスターの形成を防ぐ戦略を提案します。 
[概要]新しく導入されたユーザーの顔画像に適用する非侵襲的ノイズマスクを生成する戦略を提案します。これにより、敵対的な例が生成され、ウェビングスペースでの識別可能なクラスターの形成が防止される可能性があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: AutoBSS: An Efficient Algorithm for Block Stacking Style Search -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_22.html">
      <font color="black">AutoBSS: An Efficient Algorithm for Block Stacking Style Search</font>
    </a>
  </h2>
  <font color="black">提案された方法であるAutoBSSは、ブロックスタッキングスタイルコード（BSSC）を繰り返し改良およびクラスタリングすることにより、ベイズ最適化に基づく新しいAutoMLアルゴリズムであり、偏った評価なしに数回の試行で最適なBSSを見つけることができます。さらに重要なのは、モデル圧縮に関する実験結果です。 、オブジェクト検出とインスタンスセグメンテーションは、提案されたAutoBSSの強力な一般化可能性を示し、ニューラルネットワークに対するBSSの無視できない影響をさらに検証します。ImageNet分類タスクでは、ResNet50 / MobileNetV2 / EfficientNet-B0と検索されたBSSが79.29％/ 74.5％を達成します。 /77.79％、これは元のベースラインを大幅に上回っています。 
[概要]検索エンジンを使用して、ネットワークへの無視できない影響を検索できます。したがって、自動的に検索する効率的なアルゴリズムを作成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Localized Interactive Instance Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_23.html">
      <font color="black">Localized Interactive Instance Segmentation</font>
    </a>
  </h2>
  <font color="black">現在のインタラクティブなインスタンスセグメンテーション作業では、オブジェクトをセグメント化するためのクリックを提供するときに、ユーザーにフリーハンドが付与されます。クリックは、ターゲットオブジェクトから遠く離れた背景ピクセルやその他のオブジェクトインスタンスで許可されます。いくつかの標準的なインタラクティブセグメンテーションベンチマークで最先端を上げる詳細な実験を通じて、提案されたクリックスキームとローカリゼーション戦略の有効性を示します。 。この形式の対話は、対象のオブジェクトを効率的に分離するという最終目標とは非常に矛盾しています。 
[概要]この形式の相互作用は、対象のオブジェクトを効率的に分離するという最終目標とは非常に矛盾しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-18">
        <br><font color="black">2020-10-18</font>
      </time>
    </span>
</section>
<!-- paper0: Joint Device-Edge Inference over Wireless Links with Pruning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_24.html">
      <font color="black">Joint Device-Edge Inference over Wireless Links with Pruning</font>
    </a>
  </h2>
  <font color="black">分類タスクでのアプローチを評価し、エッジデバイスでのエンドツーエンドの信頼性とワークロード削減の両方で改善された結果を示します。ワイヤレスネットワークエッジでの効率的な推論のための共同機能圧縮および送信スキームを提案します。チャネルコーディングの計算コストを無視して、主に機能の圧縮に焦点を当てた作業。 
[概要]これは、deepjsccとネットワークプルーニングを組み合わせた最初の作業です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br><font color="black">2020-03-04</font>
      </time>
    </span>
</section>
<!-- paper0: Image Captioning with Visual Object Representations Grounded in the
  Textual Modality -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_25.html">
      <font color="black">Image Captioning with Visual Object Representations Grounded in the
  Textual Modality</font>
    </a>
  </h2>
  <font color="black">パフォーマンスのわずかな変化だけで、接地されたモデルは、制約のないモデルよりも速くトレーニング中に停止基準に到達し、トレーニングの更新が約2〜3倍少なくて済みます。さらに、学習されたオブジェクトのベクトル空間投影とその影響の分析を提供します。 ICシステムのパフォーマンス..オブジェクト検出ラベルのテキストの性質と抽出された視覚オブジェクト表現の仮想的な表現力を活用して、現在の傾向とは逆のアプローチを提案します。つまり、接地ではなく、キャプションシステムの単語埋め込みスペースに表現を接地します。関連する画像内の単語または文。 
[概要]現在の傾向とは逆のアプローチを提案します。関連する画像の単語や文を固定するのではなく、キャプションシステムの単語埋め込みスペースの表現を固定します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: A Two-stage Unsupervised Approach for Low light Image Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_26.html">
      <font color="black">A Two-stage Unsupervised Approach for Low light Image Enhancement</font>
    </a>
  </h2>
  <font color="black">最初の段階では、従来のRetinexベースの方法で低照度画像を事前に強調します。ただし、それらのほとんどには、次の問題があります。1）トレーニング用の低照度画像と通常光画像のペアの必要性。2）暗い画像のパフォーマンスの低下、3）ノイズの増幅。実験結果は、4つのベンチマークデータセットで、私たちの方法が以前の方法よりも優れていることを示しています。 
[ABSTRACT]ディープラーニングパー法は、ピクセルにペナルティを課すことによって低光画像を強化するために提案されています-低光と通常光の画像の賢明な損失。この方法は、低光条件での特徴点のマッチングと同時ローカリゼーションおよびマッピングを大幅に改善できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: A Graph Attention Spatio-temporal Convolutional Network for 3D Human
  Pose Estimation in Video -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_27.html">
      <font color="black">A Graph Attention Spatio-temporal Convolutional Network for 3D Human
  Pose Estimation in Video</font>
    </a>
  </h2>
  <font color="black">シングルフレームおよびマルチフレームの推定に適応するために、拡張された時間モデルを使用して、さまざまなスケルトンシーケンスを処理します。2つの挑戦的なベンチマークデータセット（Human3.6MおよびHumanEva-I）とYouTubeビデオでの実験は、私たちのアプローチが深さのあいまいさを効果的に軽減することを示しています自己オクルージョンは、上半身の半分の推定に一般化され、2Dから3Dへのビデオポーズ推定で競争力のあるパフォーマンスを実現します。時空間情報は、3Dポーズ推定におけるオクルージョンと深度のあいまいさを解決するための鍵です。 
[要約]空間-またはグローバルアーキテクチャは、固定長の空間-一時性を埋め込むために使用されています。これらには、人間の骨格の不道徳と不道徳を没頭させることが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-11">
        <br><font color="black">2020-03-11</font>
      </time>
    </span>
</section>
<!-- paper0: Fast Implementation of 4-bit Convolutional Neural Networks for Mobile
  Devices -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_28.html">
      <font color="black">Fast Implementation of 4-bit Convolutional Neural Networks for Mobile
  Devices</font>
    </a>
  </h2>
  <font color="black">量子化された低精度ニューラルネットワークは、推論に必要な計算リソースが少なく、リアルタイムおよび組み込み認識システムに不可欠な高性能を提供できるため、非常に人気があります。量子化ニューラルネットワークの4ビットマトリックス乗算の効率的な実装を紹介します。ネットワークを構築し、モバイルARMプロセッサで時間測定を実行します。また、MIDV-500データセットでのOCR認識のための4ビット量子化ニューラルネットワークのデモも行います。 
[要約] 4-ビット量子化により、95。0％の精度と48％の全体的な結論が得られます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-14">
        <br><font color="black">2020-09-14</font>
      </time>
    </span>
</section>
<!-- paper0: VINNAS: Variational Inference-based Neural Network Architecture Search -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_29.html">
      <font color="black">VINNAS: Variational Inference-based Neural Network Architecture Search</font>
    </a>
  </h2>
  <font color="black">私たちのアプローチは、事前に自動関連性決定を伴う変分ドロップアウトを使用して、パラメーターが過剰なスーパーグラフで候補操作をドロップアウトすることによって最適なニューラルアーキテクチャを見つけます。これにより、アルゴリズムは、モードの崩壊のリスクを冒すことなく、不要な操作と接続を徐々に削除します。ゼロ以外のパラメータが最大で約2分の1になり、最先端の精度を示します。ただし、これらの方法では、アルゴリズムが選択するために検出されたアーキテクチャの品質が低下するモード崩壊が発生することがよくあります。ネットワーク全体の単一の操作タイプ、またはさまざまなデータセットまたは検索スペースのローカル最小値で停滞します。 
[概要]特に、勾配ベースのnasアプローチは、アルゴリズムのおかげで最も人気のあるアーキテクチャの1つになりました。この方法は、さまざまな画像データセットを分類するためのニューラルネットワークを形成するさまざまなタイプの畳み込みセルを検索するために使用されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-12">
        <br><font color="black">2020-07-12</font>
      </time>
    </span>
</section>
<!-- paper0: COVID-19 Screening Using Residual Attention Network an Artificial
  Intelligence Approach -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_30.html">
      <font color="black">COVID-19 Screening Using Residual Attention Network an Artificial
  Intelligence Approach</font>
    </a>
  </h2>
  <font color="black">この論文では、人工知能を使用してCOVID-19をスクリーニングする手法を紹介します。COVID-19の効果的な検査戦略は、発生を制御するために重要ですが、検査の需要は、逆転写ポリメラーゼ連鎖を使用する検査キットの可用性を上回ります。反応（RT-PCR）..コロナウイルス病2019（COVID-19）は、重度の急性呼吸器症候群コロナウイルス2ウイルス（SARS-CoV-2）によって引き起こされます。 
[要約]ウイルスは急速に伝染します;基本再生産数rは2.2-2。7.covid-19は、現在200か国以上に影響を及ぼしており、600万件の活動があります。人工知能を使用してウイルスをスクリーニングする技術をテストしています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-26">
        <br><font color="black">2020-06-26</font>
      </time>
    </span>
</section>
<!-- paper0: Pedestrian Intention Prediction: A Multi-task Perspective -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_31.html">
      <font color="black">Pedestrian Intention Prediction: A Multi-task Perspective</font>
    </a>
  </h2>
  <font color="black">JAADデータセットでの実験は、意図予測のための以前の作業と比較して、私たちの方法のパフォーマンスの優位性を示しています。それは、歩行者の将来の位置のそれぞれの意図を予測する1つのヘッドと、歩行者..この方法は、マルチタスク学習アプローチにおけるリカレントニューラルネットワークです。 
[概要] 1つのヘッドで、将来の位置ごとに歩行者の意図を予測し、もう1つのヘッドで歩行者の視覚状態を予測します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Tracking from Patterns: Learning Corresponding Patterns in Point Clouds
  for 3D Object Tracking -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_32.html">
      <font color="black">Tracking from Patterns: Learning Corresponding Patterns in Point Clouds
  for 3D Object Tracking</font>
    </a>
  </h2>
  <font color="black">また、パイプラインにシンプルで効果的な速度平滑化モジュールを装備して、一貫したオブジェクトの動きを推定します。この論文では、時間ポイントクラウドデータから3Dオブジェクトの対応を直接学習し、対応パターンから動き情報を推測することを提案します。堅牢な3D周囲の物体を継続的に追跡し、その軌道を推定するオブジェクトトラッカーは、自動運転車両の鍵となります。 
[概要]ほとんどの既存の追跡方法は、追跡による検出戦略を採用しています。標準の3Dオブジェクト検出器を変更して、2つの対応を同時に処理します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Robust Neural Networks inspired by Strong Stability Preserving
  Runge-Kutta methods -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_33.html">
      <font color="black">Robust Neural Networks inspired by Strong Stability Preserving
  Runge-Kutta methods</font>
    </a>
  </h2>
  <font color="black">提案されたネットワークが防御方法なしで敵対的な例に対するロバスト性を改善することを経験的に示します。さらに、SSPネットワークは最先端の敵対的なトレーニングスキームを補完します。最後に、私たちの実験はSSPネットワークが敵対的な摂動の爆発。 
[概要]数値離散化のオイラー法が提案されています。これらには、強力な安定性保存ネットワーク（sspネットワーク）が含まれます。これらは、敵対攻撃に対する堅牢性を向上させます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: TTPLA: An Aerial-Image Dataset for Detection and Segmentation of
  Transmission Towers and Power Lines -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_34.html">
      <font color="black">TTPLA: An Aerial-Image Dataset for Detection and Segmentation of
  Transmission Towers and Power Lines</font>
    </a>
  </h2>
  <font color="black">他の関連データセットとは異なり、TTPLAは、検出とセマンティックセグメンテーションに加えて、インスタンスセグメンテーションの評価をサポートします。このペーパーでは、解像度3,840 $の1,100枚の画像で構成される新しいTT / PL空中画像（TTPLA）データセットを収集してリリースします。 \ times $ 2,160ピクセル、およびTTとPLの手動でラベル付けされた8,987インスタンス。TTPLAで画像を収集、注釈付け、ラベル付けするための新しいポリシーを開発します。 
[概要] ttsとplsの航空写真は、コンピュータービジョンの研究者に多くの新しい課題をもたらします。コンピュータービジョンチームは、オブジェクトの検出とセグメンテーションに取り組んでいます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: SDF-SRN: Learning Signed Distance 3D Object Reconstruction from Static
  Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_35.html">
      <font color="black">SDF-SRN: Learning Signed Distance 3D Object Reconstruction from Static
  Images</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、この問題に対処し、トレーニング時にオブジェクトの単一のビューのみを必要とするアプローチであるSDF-SRNを提案します。これにより、実際のシナリオでの有用性が高まります。この目的のために、次の新しい微分可能なレンダリング定式化を導き出します。 2Dシルエットから符号付き距離関数（SDF）を学習します。単一の画像からの高密度3Dオブジェクトの再構築は最近目覚ましい進歩を遂げていますが、ペアの画像形状データセットを作成する面倒なプロセスのため、グラウンドトゥルース3D形状でニューラルネットワークを監視することは実用的ではありません。 。 
[概要]最近の取り組みは、注釈付きの2Dシルエットを持つRGB画像から3D監視なしで3D再構成を学習することに向けられています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Micro CT Image-Assisted Cross Modality Super-Resolution of Clinical CT
  Images Utilizing Synthesized Training Dataset -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_36.html">
      <font color="black">Micro CT Image-Assisted Cross Modality Super-Resolution of Clinical CT
  Images Utilizing Synthesized Training Dataset</font>
    </a>
  </h2>
  <font color="black">したがって、臨床CTボリュームの超解像は肺癌の診断に役立つ可能性があります。この論文は、臨床CTのSRをマイクロCTの解像度レベルに実行するための新しい監視されていない超解像（SR）アプローチを提案します（ $ \ mu $ CT）..実験結果は、提案された方法が$ \ mu $ CTレベルの解像度で肺癌患者の臨床CT画像のSRを正常に実行でき、定量的および定性的に従来の方法（SR-CycleGAN）を上回り、改善することを示しています。 SSIM（構造類似性）は0.40から0.51を形成します。 
[要約]肺がんの正確な侵襲前診断は、通常、臨床ctデータを使用します。この方法では、診断の低解像度（lr）画像と高解像度（hr）画像のペアを揃える必要があります。この方法は、と呼ばれる臨床ct画像に基づいています。 sr-cyclegan</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Item Promotion: Vulnerabilities at the Core of Top-N
  Recommenders that Use Images to Address Cold Start -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_37.html">
      <font color="black">Adversarial Item Promotion: Vulnerabilities at the Core of Top-N
  Recommenders that Use Images to Address Cold Start</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、悪意のある商人が商品を人為的に宣伝してランキングを向上させるアイテム画像を作成する方法を示します。トップNの推奨者の中核を直接攻撃する、新しいタイプの攻撃であるAdversarial Item Promotion（AIP）について説明します。 ：ランキングメカニズム自体..3つの連続するより現実的な攻撃モデルに関して定義された3つのAIP攻撃インサイダー攻撃、エキスパート攻撃、およびセマンティック攻撃を紹介します。 
[概要] eコマースなどの新しいサービスは、顧客にリストのトップnのトップスポットを提供するように設計されています。新しいシステムシステムシステムは、画像を使用してコールドスタートの問題に対処します。このセキュリティリスクに対して脆弱です。敵対者に関する既存の作業レコメンダーシステムの画像は、従来の攻撃の影響を調査します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-02">
        <br><font color="black">2020-06-02</font>
      </time>
    </span>
</section>
<!-- paper0: A versatile anomaly detection method for medical images with a
  flow-based generative model in semi-supervision setting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_38.html">
      <font color="black">A versatile anomaly detection method for medical images with a
  flow-based generative model in semi-supervision setting</font>
    </a>
  </h2>
  <font color="black">この方法では、事後確率を任意の画像の正規性メトリックとして計算できます。この研究では、2つのトレーニング済みフローベースの生成モデルに基づく異常検出方法を示します。最近、深層学習に基づいて構築された異常検出方法メソッドの人気は急速に高まっており、これらのメソッドは問題に対する合理的な解決策を提供しているようです。 
[要約]この方法は、2種類の医用画像で検証されました。以前の研究では、ヨーク大学による研究の一部として検証されたことがわかりました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-22">
        <br><font color="black">2020-01-22</font>
      </time>
    </span>
</section>
<!-- paper0: Learned Fine-Tuner for Incongruous Few-Shot Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_39.html">
      <font color="black">Learned Fine-Tuner for Incongruous Few-Shot Learning</font>
    </a>
  </h2>
  <font color="black">MAMLを、異なるが関連する数ショット学習の問題がモデルパラメーターを共有しない可能性がある不調和なメタ学習に拡張します。モデルに依存しないメタ学習（MAML）は、すべてが数ショット学習のモデルパラメーターの初期化を効果的にメタ学習します。学習問題は、モデルパラメーターの同じ形式（一致するメタ学習）を共有します。ここで、MAMLは、代わりに、モデルが微調整されるように、最適化学習（L2O）フレームワークを活用して、不一致なタスク全体でこのLFTのパラメーターをメタ学習します。 LFT（ランダムな初期化からでも）は、新しいタスクにすばやく適応します。 
[概要] mamlを不調和なメタに拡張します-異なるが関連する少数の学習-ショット学習の問題はモデルパラメータを共有しない可能性があります。したがって、 `maml &#39;とl2o&#39;の違いを定量化します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-29">
        <br><font color="black">2020-09-29</font>
      </time>
    </span>
</section>
<!-- paper0: Representation Sharing for Fast Object Detector Search and Beyond -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_40.html">
      <font color="black">Representation Sharing for Fast Object Detector Search and Beyond</font>
    </a>
  </h2>
  <font color="black">Region Proposal Network（RPN）は、2段階のオブジェクト検出でオブジェクトのスケール変動を処理するための強力なサポートを提供します。FADは、設計された検索スペースと効率的なアーキテクチャ検索アルゴリズムで構成されます。検索スペースには、設計された多様な変換の豊富なセットが含まれます。特にオブジェクト検出用。 
[概要] rpnを持たない1ステージ検出器の場合、強力なサブネットワークが必要になります。設計された検索空間に対処するために、表現共有（repshare）と呼ばれる新しい検索アルゴリズムが提案され、最適なものを効果的に識別します。定義された変換の組み合わせ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Proof of Concept: Automatic Type Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_41.html">
      <font color="black">Proof of Concept: Automatic Type Recognition</font>
    </a>
  </h2>
  <font color="black">タイプ分類については、フォントグループ分類に元々使用されていた深い畳み込みニューラルネットワーク（CNN）に依存し、検索の場合は一般的なライター識別方法を使用します。どちらのシナリオでも、簡単なタイプを分類/検索できることを示します。困難なケースは確かに困難ですが、高精度で..初期の印刷された本で使用される簡単なタイプと難しいタイプで構成される新しく作成されたデータセットを使用して、タイプ分類とタイプ検索のパフォーマンスを調査します。 
[ABSTRACT]タイプは現在、「m」または「qu」の文字形状とタイプ全体のサイズの両方を使用して行われ、大規模な参考書で検索されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-15">
        <br><font color="black">2020-07-15</font>
      </time>
    </span>
</section>
<!-- paper0: ICFHR 2020 Competition on Image Retrieval for Historical Handwritten
  Fragments -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_42.html">
      <font color="black">ICFHR 2020 Competition on Image Retrieval for Historical Handwritten
  Fragments</font>
    </a>
  </h2>
  <font color="black">ほとんどのチームが畳み込みニューラルネットワークに基づいてメソッドを提出しましたが、優勝エントリは40％未満のmAPを達成します。以前のコンテストと比較して、サンプルの粒度の問題に対処し、ライターからページフラグメントの取得に移行することで、結果をより意味のあるものにします..したがって、120000を超えるフラグメントで構成される大規模なデータセットを作成しました。 
[概要]これは以前のコンテストと比較して、サンプルの粒度の問題に対処することで結果をより意味のあるものにします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: AAG: Self-Supervised Representation Learning by Auxiliary Augmentation
  with GNT-Xent Loss -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_43.html">
      <font color="black">AAG: Self-Supervised Representation Learning by Auxiliary Augmentation
  with GNT-Xent Loss</font>
    </a>
  </h2>
  <font color="black">補助増強は、画像の多様性を高めることにより、対照学習のパフォーマンスを促進することができます。提案されたGNT-Xent損失は、安定した高速のトレーニングプロセスを可能にし、競争力のある精度をもたらします。自己監視表現学習は、ラベルのないデータを使用した学習におけるその強力な能力。 
[要約]主流の自己教師あり学習方法として、拡張ベースの方法は、手動の注釈がないさまざまなコンピュータービジョンタスクで大きな成功を収めています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-17">
        <br><font color="black">2020-09-17</font>
      </time>
    </span>
</section>
<!-- paper0: L-RED: Efficient Post-Training Detection of Imperceptible Backdoor
  Attacks without Access to the Training Set -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_44.html">
      <font color="black">L-RED: Efficient Post-Training Detection of Imperceptible Backdoor
  Attacks without Access to the Training Set</font>
    </a>
  </h2>
  <font color="black">BAに対するリバースエンジニアリングベースの防御（RED）は、トレーニングセットへのアクセスを必要とせず、独立したクリーンなデータセットへのアクセスのみを必要とします。バックドア攻撃（BA）は、通常、ディープニューラルネットワーク画像分類子に対する敵対的攻撃の新たな形態です。この仮定に依存しないものは、多くの場合、クリーンな画像の大規模なセットと大量の計算を必要とします。 
[要約]ほとんどの既存の赤は、ターゲットクラスを除くすべてのクラスが攻撃のソースクラスであるという非現実的な仮定に依存しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Explorable Tone Mapping Operators -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_45.html">
      <font color="black">Explorable Tone Mapping Operators</font>
    </a>
  </h2>
  <font color="black">トーンマッピングは、ハイダイナミックレンジ（HDR）イメージングで重要な役割を果たします。HDR画像からトーンマッピングされた結果を提供するために多くの作品が提案されていますが、それらのほとんどは、事前に設計された単一の方法でのみトーンマッピングを実行できます。この論文では、優れた視覚的品質を達成するだけでなく、スタイルの多様性を探求する、学習ベースのマルチモーダルトーンマッピング法を提案します。 
[要約]提案された方法は、潜在的なコードを操作することにより、さまざまな専門家レベルのトーンマッピングされた結果を提供することができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Convolutional-LSTM for Multi-Image to Single Output Medical Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_46.html">
      <font color="black">Convolutional-LSTM for Multi-Image to Single Output Medical Prediction</font>
    </a>
  </h2>
  <font color="black">実験結果は、人間の医師の診断プロセスを模倣するマルチイメージからシングルの診断モデルを取得できることを示しています。患者の画像のコレクションを評価し、メモリ内の重要な情報を使用して、患者の単一の診断を決定します。開発途上国で非常に一般的なシナリオは、画像のフォーマット変換（.dicomからjpgなど）などの複数の理由でボリュームメタデータが失われることです。このシナリオでは、医師が画像のコレクションを分析してから、患者に単一の診断を送信します。 （患者ごとに固定されていない可変数の画像がある可能性があります）、これにより、最先端の3Dモデルを使用できなくなりますが、（1画像、1診断）で監視対象の問題に変換することもできません。 1人の患者の画像の角度や位置が異なると、病気や病変が含まれない可能性があるため、設定します。このための最先端の分類モデルとアルゴリズムsタスクは通常、教師あり学習設定での体積データの3D畳み込みレイヤー（1つの入力ボリューム、患者ごとに1つの予測）または教師あり設定での2d畳み込みレイヤー（1つの入力画像、画像ごとに1つの予測）に基づいています。 
[ABSTRACT]スキャンモデルは通常、体積データの3D畳み込みレイヤーまたは2D畳み込みレイヤーに基づいています。これを使用して、患者の画像が処理される単一の診断設定を作成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Deep learning to achieve clinically applicable segmentation of head and
  neck anatomy for radiotherapy -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_47.html">
      <font color="black">Deep learning to achieve clinically applicable segmentation of head and
  neck anatomy for radiotherapy</font>
    </a>
  </h2>
  <font color="black">モデルの臨床適用性を示すために、臨床診療からの21のCTスキャンのテストセットでのパフォーマンスを評価します。各スキャンは、2人の独立した専門家によってセグメント化された21のOARを使用します。臓器描写の比較。ボリュームではなくOAR表面輪郭間の偏差を定量化し、自動臓器セグメンテーションのエラーを修正する臨床タスクをより適切に反映します。モデルは、で取得された663の匿名化されたコンピューター断層撮影（CT）スキャンのデータセットでトレーニングされました。日常的な臨床診療と、臨床診療から得られたセグメンテーションと、この研究の一部として経験豊富な放射線技師によって作成されたセグメンテーションの両方を、すべてコンセンサスOAR定義に従って行います。 
[概要]自動セグメンテーションはこの病気の重要な治療法ですが、無線に敏感な臓器での手動のタイムリカ描写が必要です。このモデルは、663の匿名化されたctスキャンのデータセットでトレーニングされました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-09-12">
        <br><font color="black">2018-09-12</font>
      </time>
    </span>
</section>
<!-- paper0: DARTS+: Improved Differentiable Architecture Search with Early Stopping -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_48.html">
      <font color="black">DARTS+: Improved Differentiable Architecture Search with Early Stopping</font>
    </a>
  </h2>
  <font color="black">さらに、「早期停止」の概念は、「早期停止」の{\ em明示的}基準を与える一方で、少数の検索エポックを手動で設定することにより、一部の既存のDARTSバリアントに暗黙的に含まれていることに注意してください。 「DARTS +」という名前のシンプルで効果的なアルゴリズムは、特定の基準を満たしたときに検索手順を「早期停止」することで、崩壊を回避し、元のDARTSを改善します。また、ベンチマークデータセットとさまざまな検索スペースで包括的な実験を行い、 DARTS +アルゴリズムの有効性、およびDARTS +は、CIFAR10で$ 2.32 \％$、CIFAR100で$ 14.87 \％$、ImageNetで$ 23.7 \％$を達成します。 
[概要]検索エポック数が多くなるとダーツのパフォーマンスが低下することがよくあります。この記事では、過剰適合が存在することが原因であると主張しています。ベンチマークデータセットやさまざまな検索スペースでも実験を行っています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-13">
        <br><font color="black">2019-09-13</font>
      </time>
    </span>
</section>
<!-- paper0: Stronger, Faster and More Explainable: A Graph Convolutional Baseline
  for Skeleton-based Action Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_49.html">
      <font color="black">Stronger, Faster and More Explainable: A Graph Convolutional Baseline
  for Skeleton-based Action Recognition</font>
    </a>
  </h2>
  <font color="black">2つの大規模データセット（NTU RGB + D 60および120）での広範な実験により、提案されたベースラインが他のSOTAモデルをわずかに上回り、トレーニングおよび推論手順中に必要なパラメーターがはるかに少ないことが検証されます。たとえば、DGNNの最大34分の1です。 、これは最良のSOTAメソッドの1つです。この作業では、グラフ畳み込みネットワーク（GCN）に基づいて、効率的で強力なベースラインを提案します。ここでは、3つの主要な改善点、つまり、初期融合複数入力ブランチ（MIB）、残差が集約されます。ボトルネック構造とPart-wiseAttention（PartAtt）ブロックを備えたGCN（ResGCN）..まず、MIBは、有益なスケルトン機能を強化し、融合の初期段階でコンパクトな表現を維持するように設計されています。 
[要約]モデルのトレーニングと結論の欠如が、特に大規模なアクションデータセットの場合、フィールドでの開発を妨げてきました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Real-time Localized Photorealistic Video Style Transfer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_50.html">
      <font color="black">Real-time Localized Photorealistic Video Style Transfer</font>
    </a>
  </h2>
  <font color="black">さまざまなスタイルの画像とターゲットビデオで、さまざまなスタイルを複数のオブジェクトに同時に転送し、時間内にスタイル間をスムーズに移行する機能など、この方法を示します。最近の作業に触発されたディープニューラルネットワークアーキテクチャに基づく方法フォトリアリスティックなスタイル転送はリアルタイムであり、アーティスティックスタイルの多様なデータセットでトレーニングされると、ランタイム最適化なしで任意の入力で機能します。ノイズの多いセマンティックラベルでビデオデータセットを拡張し、スタイル、コンテンツ、マスク、時間的損失を共同で最適化することにより、私たちの方法は、入力のさまざまな欠陥に対処し、視覚的なアーティファクトのない時間的にコヒーレントなビデオを生成できます。 
[概要]私たちの方法は、入力のさまざまな欠陥に対処し、視覚的なアーティファクトのない意味のあるビデオを生成できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Bit Error Robustness for Energy-Efficient DNN Accelerators -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_51.html">
      <font color="black">Bit Error Robustness for Energy-Efficient DNN Accelerators</font>
    </a>
  </h2>
  <font color="black">この論文では、ロバストな固定小数点量子化、重みクリッピング、およびランダムビットエラートレーニング（RandBET）の組み合わせにより、（量子化された）DNN重みのランダムビットエラーに対するロバスト性が大幅に向上することを示します。これにより、両方のエネルギーを大幅に節約できます。低電圧動作と低精度量子化..プロファイルされたSRAMアレイからのビットエラーで示されているように、私たちのアプローチは動作電圧とアクセラレータ全体で一般化されています。 
[ABSTRACT]ウェイトクリッピングだけでも、ビット精度に対する堅牢性を実現するための非常に効果的な方法です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-24">
        <br><font color="black">2020-06-24</font>
      </time>
    </span>
</section>
<!-- paper0: SOrT-ing VQA Models : Contrastive Gradient Learning for Improved
  Consistency -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_52.html">
      <font color="black">SOrT-ing VQA Models : Contrastive Gradient Learning for Improved
  Consistency</font>
    </a>
  </h2>
  <font color="black">これらのサブ質問は、モデルが高レベルの質問に正しく答えるために理想的に理解する必要がある画像内の低レベルの視覚的概念に関連しています。これに対処するために、最初に勾配ベースの解釈可能性アプローチを提示して、最も強く相関する質問を決定します。画像に推論の質問を追加し、これを使用して、推論の質問に回答するために必要な関連するサブ質問を特定する能力についてVQAモデルを評価します。SOrTが既存のベースラインよりも最大6.5％ポイント、モデルの一貫性を向上させることを示します。また、視覚的な接地を改善します。 
[ABSTRACT] sub-質問vvqaは、モデルが間違った質問をランク付けする必要があることを意味します。これは、サブ解決の質問に関する一連の調査の最新の調査です。sub-question-qaは、新しい形式の調査と調査です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: ivadomed: A Medical Imaging Deep Learning Toolbox -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_53.html">
      <font color="black">ivadomed: A Medical Imaging Deep Learning Toolbox</font>
    </a>
  </h2>
  <font color="black">ivadomedのメインプロジェクトページはhttps://ivadomed.orgで入手できます。全体として、ivadomedを使用すると、医療画像アプリケーションのディープラーニングの最新の進歩を簡単かつ迅速に探索できます。ivadomedの元の機能には、画像メタデータを解析できるデータローダーが含まれます。トレーニングおよび評価中のカスタムデータ分割または追加情報のための（たとえば、取得パラメータ、画像コントラスト、解像度）および対象メタデータ（たとえば、病理学、年齢、性別）。 
[概要]パッケージには、API、コマンドラインツール、ドキュメント、チュートリアルが含まれています。元の機能には、画像メタデータを解析できるデータローダーが含まれています。コードは、アーキテクチャの追加または変更を可能にするように高度にトレーニングされています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: A Loss Function for Generative Neural Networks Based on Watson's
  Perceptual Model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_54.html">
      <font color="black">A Loss Function for Generative Neural Networks Based on Watson's
  Perceptual Model</font>
    </a>
  </h2>
  <font color="black">ユークリッド距離と構造類似性指数を使用する場合と比較して、画像のぼやけは少なくなりました。ディープニューラルネットワークベースの損失と比較して、新しいアプローチでは、必要な計算リソースが少なく、アーティファクトの少ない画像が生成されます。モデルをカラー画像に拡張し、フーリエ変換を使用して変換に対するロバスト性を高め、画像をに分割することによるアーティファクトを削除します。ブロックし、微分可能にします。実験では、新しい損失関数でトレーニングされたVAEが、リアルで高品質の画像サンプルを生成しました。 
[概要]新しい損失関数は、ワトソンの知覚モデルに基づいています。新しいモデルからの詳細で詳細なデータを提供します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-26">
        <br><font color="black">2020-06-26</font>
      </time>
    </span>
</section>
<!-- paper0: Video Reconstruction by Spatio-Temporal Fusion of Blurred-Coded Image
  Pair -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_55.html">
      <font color="black">Video Reconstruction by Spatio-Temporal Fusion of Blurred-Coded Image
  Pair</font>
    </a>
  </h2>
  <font color="black">従来のコード化された露出フレームワークはより適切なポーズですが、時空間ボリュームの一部のみをサンプリングします。これは、時空間ボリュームのせいぜい50％です。私たちのフレームワークは、共有エンコーダーとそれに続くアテンションモジュールで構成されています。完全に露光された画像からの空間情報をコード化された画像からの時間情報と組み合わせ、それを超解像して、曖昧さのない高品質のビデオを復元します。ここでは、完全に存在する補足情報を使用することを提案します。 -露出された（ぼやけた）画像とコード化された露出画像により、動きの曖昧さなしに忠実度の高いビデオを復元します。 
[概要]システムは、ぼやけたコード化された画像を使用して高解像度のビデオを作成します。単一のモーションを再現するために使用できます。ぼやけた画像ですが、シーンの静的な部分の情報を完全に保存するという利点があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Resolution Dependent GAN Interpolation for Controllable Image Synthesis
  Between Domains -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_56.html">
      <font color="black">Resolution Dependent GAN Interpolation for Controllable Image Synthesis
  Between Domains</font>
    </a>
  </h2>
  <font color="black">これにより、まったく新しいドメインから画像を生成し、出力の性質をある程度制御してこれを行うことができます。ここでは、解像度に依存する方法でStyleGANアーキテクチャの生成モデル間を補間する方法を示します。トレーニングデータのドメインからフォトリアリスティックな画像を生成します。 
[ABSTRACT]画像の創造的な使用は、スタイルガンアーキテクチャの画像を作成するために使用できます。これらは、解像度に依存する方法と組み合わせることができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-11">
        <br><font color="black">2020-10-11</font>
      </time>
    </span>
</section>
<!-- paper0: Depth Guided Adaptive Meta-Fusion Network for Few-shot Video Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_57.html">
      <font color="black">Depth Guided Adaptive Meta-Fusion Network for Few-shot Video Recognition</font>
    </a>
  </h2>
  <font color="black">さらに、数ショット認識プロセスをより適切に模倣するために、モデルはメタ学習方法でトレーニングされます。具体的には、3つの側面から数ショット認識問題に取り組みます。まず、この非常にデータ不足の問題を導入することで軽減します。シーンのキャリアとしての深度情報。これにより、モデルに追加の視覚情報がもたらされます。次に、元のRGBクリップの表現を、機能レベルで新しいインスタンスを合成する時間的非同期拡張メカニズムによってサンプリングされた、厳密に対応していない複数の深度クリップと融合します。第三に、新しい深度誘導適応インスタンス正規化（DGAdaIN）融合モジュールを提案して、2ストリームモダリティを効率的に融合します。この論文では、数ショットビデオ認識のための深度誘導適応メタ融合ネットワークを提案します。 AMeFu-Net。 
[概要] 3つの側面からいくつかのショットビデオアクション認識問題に取り組みます。これらには、シーンのキャリアとして深度情報を導入することが含まれます。これにより、モデルに追加の視覚情報がもたらされます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Fast Video Salient Object Detection via Spatiotemporal Knowledge
  Distillation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_58.html">
      <font color="black">Fast Video Salient Object Detection via Spatiotemporal Knowledge
  Distillation</font>
    </a>
  </h2>
  <font color="black">具体的には、空間的側面では、顕著性ガイダンス機能の埋め込み構造と空間的知識の蒸留を組み合わせて、空間的特徴を洗練します。ビデオの顕著なオブジェクトの検出にディープラーニングフレームワークが広く採用されているため、最近のアプローチの精度は驚くべき進歩を遂げています。 ..時間的側面では、時間的知識蒸留戦略を提案します。これにより、ネットワークは、隣接するフレームから情報をエンコードおよび抽出する推論フレーム特徴を通じて、ロバストな時間的特徴を学習できます。 
[概要]これらのアプローチは、主にオプティカルフローまたはリカレントニューラルネットワーク（rnn）に基づくモジュールを採用して、堅牢な時空間特徴を学習します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: FishNet: A Unified Embedding for Salmon Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_59.html">
      <font color="black">FishNet: A Unified Embedding for Salmon Recognition</font>
    </a>
  </h2>
  <font color="black">個々のサケのタグ付けと追跡の現在の方法は、魚との物理的な相互作用に依存しています。個々のサケを特定することは、魚の行動と福祉を監視および分析できるため、養殖業界にとって非常に有益です。比較的小さなニューラルネットワークモデルで達成：FishNetは1 \％の偽陽性率と96 \％の真陽性率を達成します。 
[概要]養殖研究者は個々のサケを特定できます。このプロセスは非効率的であり、サケに身体的危害やストレスを引き起こす可能性があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Federated Learning for Breast Density Classification: A Real-World
  Implementation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_60.html">
      <font color="black">Federated Learning for Breast Density Classification: A Real-World
  Implementation</font>
    </a>
  </h2>
  <font color="black">さらに、他の参加サイトのテストデータで評価すると、モデルの一般化可能性が45.8％向上していることを示しています。世界中の7つの臨床機関がこのFLの取り組みに参加し、乳房イメージングに基づく乳房密度分類のモデルをトレーニングしました。 Reporting＆Data System（BI-RADS）..結果は、FLを使用してトレーニングされたモデルは、機関のローカルデータのみでトレーニングされたモデルよりも平均6.3％優れていることを示しています。 
[概要]研究者は、フェデレーション学習（fl）を使用して、実世界の共同設定で医用画像分類モデルをトレーニングします。彼らは、大幅な違いにもかかわらず、フェデレーションでaiモデルを正常にトレーニングできることを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-03">
        <br><font color="black">2020-09-03</font>
      </time>
    </span>
</section>
<!-- paper0: BYOL works even without batch statistics -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_61.html">
      <font color="black">BYOL works even without batch statistics</font>
    </a>
  </h2>
  <font color="black">BYOLは、画像の拡張ビューから、オンラインネットワークをトレーニングして、同じ画像の別の拡張ビューのターゲットネットワーク表現を予測します。したがって、最近、BYOLの崩壊を防ぐために、バッチ正規化（BN）が重要であるとの仮説が立てられました。 ..それでも、それは些細で一定の表現への崩壊を回避します。 
[ABSTRACT] byolは、同じ画像の異なるバージョンの異なる拡張ビューを予測するようにオンラインネットワークをトレーニングします。これとは異なり、些細な一定の表現とのコントラストを回避します。ただし、bnpの使用に対する重大な反応を防ぐことがわかっています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: A Generalized Zero-Shot Framework for Emotion Recognition from Body
  Gestures -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_62.html">
      <font color="black">A Generalized Zero-Shot Framework for Emotion Recognition from Body
  Gestures</font>
    </a>
  </h2>
  <font color="black">この問題を解決するために、3つのブランチで構成される一般化ゼロショット学習（GZSL）フレームワークを導入します。これは、セマンティック記述のみで新しいボディジェスチャの感情状態を推測します。ただし、表情からの自動感情認識と音声は目覚ましい進歩を遂げ、身体ジェスチャーからの感情認識は十分に検討されていません。最初のブランチは、サンプルが見られた身体ジェスチャーカテゴリに属するかどうかを判断し、予測結果を取得するために使用されるプロトタイプベースの検出器（PBD）です。見られたカテゴリーからのサンプルの。 
[概要]新しいシステムでは、感情的なジェスチャーを使用して感情を表現します。すべての感情的な身体のジェスチャーを列挙し、カテゴリごとに十分なサンプルを収集することは困難です。しかし、新しい方法では、どの感情的なフレームワークが属するかを正確に判断できません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-13">
        <br><font color="black">2020-10-13</font>
      </time>
    </span>
</section>
<!-- paper0: SpecNet: Spectral Domain Convolutional Neural Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_63.html">
      <font color="black">SpecNet: Spectral Domain Convolutional Neural Network</font>
    </a>
  </h2>
  <font color="black">SpecNetのパフォーマンスは、3つの競合するオブジェクト認識ベンチマークタスク（CIFAR-10、SVHN、およびImageNet）で評価され、いくつかの最先端の実装と比較されます。ほとんどの畳み込みニューラルネットワーク（CNN）アーキテクチャのメモリ消費量ネットワークの深さが増すにつれて急速に成長します。これは、メモリ、組み込みシステム、モバイルデバイスが限られている最新のGPUでの効率的なネットワークトレーニングの主な制約です。いくつかの研究によると、機能マップ（畳み込み層の後に生成される）はこのメモリ問題の主なボトルネック。 
[概要]多くの調査によると、機能マップがメモリの主なボトルネックです。これらには、畳み込み層の後に生成されるマップが含まれます。これらはトレーニングを加速するために使用されますが、赤外線ドメインを使用してメモリフットプリントを削減することもできます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-05-27">
        <br><font color="black">2019-05-27</font>
      </time>
    </span>
</section>
<!-- paper0: Measuring Human and Economic Activity from Satellite Imagery to Support
  City-Scale Decision-Making during COVID-19 Pandemic -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_64.html">
      <font color="black">Measuring Human and Economic Activity from Satellite Imagery to Support
  City-Scale Decision-Making during COVID-19 Pandemic</font>
    </a>
  </h2>
  <font color="black">このCNNアンサンブルフレームワークは、米国国防総省のxViewチャレンジで3位にランクされました。これは、衛星画像でのオブジェクト検出の最先端のベンチマークです。また、COVID-19の発生前後のさまざまなサイトの実際の例の結果を示し、さまざまな測定可能なものを示します。指標..この作業では、戦略的位置サンプリングと軽量畳み込みニューラルネットワーク（CNN）のアンサンブルを組み合わせた深層学習アプローチを使用して、衛星画像内の特定の要素を認識し、それに基づいて経済指標を自動的に計算します。 
[概要]経済活動は社会的行動に影響を与え、分類可能な衛星画像に署名を残します。このシステムを使用して、それに基づいて経済指標を自動的に計算し、分類することができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-16">
        <br><font color="black">2020-04-16</font>
      </time>
    </span>
</section>
<!-- paper0: Contextual Heterogeneous Graph Network for Human-Object Interaction
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_65.html">
      <font color="black">Contextual Heterogeneous Graph Network for Human-Object Interaction
  Detection</font>
    </a>
  </h2>
  <font color="black">この作業では、人間とオブジェクトを異なる種類のノードとしてモデル化し、同種ノード間のクラス内メッセージと異種ノード間のクラス間メッセージを組み込む異種グラフネットワークを提案することにより、HOIタスクのこのような問題に対処します。ベンチマークデータセットV-COCOおよびHICO-DETは、クラス内およびクラス間メッセージがHOI検出において非常に重要であることを示し、メソッドの有効性を検証します。さらに、クラス内コンテキストに基づくグラフアテンションメカニズムクラス間のコンテキストは、学習を改善するために活用されます。 
[概要]人間とオブジェクトは同じ種類のノードにリンクされています。以前の研究では、メッセージが同じように同じであるとは見なされていません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Mind2Mind : transfer learning for GANs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_66.html">
      <font color="black">Mind2Mind : transfer learning for GANs</font>
    </a>
  </h2>
  <font color="black">トレーニング時間を大幅に短縮するGANの転移学習手法を提案します。批評家と生成者の内部表現の互換性を確保するためにオートエンコーダの制約を想定しています。私たちの方法をベースラインと比較し、トレーニング。 
[概要]私たちのアプローチは、元のganの批評家と生成者の両方の低レベルのレイヤーを凍結することで構成されています。styleganと比較すると、hqデータセットで2桁に達する可能性があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-27">
        <br><font color="black">2019-06-27</font>
      </time>
    </span>
</section>
<!-- paper0: IPN Hand: A Video Dataset and Benchmark for Real-Time Continuous Hand
  Gesture Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_67.html">
      <font color="black">IPN Hand: A Video Dataset and Benchmark for Real-Time Continuous Hand
  Gesture Recognition</font>
    </a>
  </h2>
  <font color="black">さらに、3D-CNNモデルのリアルタイムパフォーマンスを維持しながら、RGBフレームから派生した複数のモダリティ、つまりオプティカルフローとセマンティックセグメンテーションを追加することにより、認識精度を向上させる可能性を分析します。データセットと事前トレーニング済みモデル評価に使用されたものは、https：//github.com/GibranBenitez/IPN-handで公開されています。実験結果は、最新のResNext-101モデルを使用すると、精度が約30％低下することを示しています。 IPN Handデータセットをベンチマークとして使用でき、コミュニティが継続的なHGRで前進するのに役立つ可能性があることを示す世界のデータセット。 
[ABSTRACT]特に、遷移状態なしで連続ジェスチャが実行され、被験者が手で自然な動きを非ジェスチャアクションとして実行するシナリオを考慮します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-20">
        <br><font color="black">2020-04-20</font>
      </time>
    </span>
</section>
<!-- paper0: Domain Adaptation by Class Centroid Matching and Local Manifold
  Self-Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_68.html">
      <font color="black">Domain Adaptation by Class Centroid Matching and Local Manifold
  Self-Learning</font>
    </a>
  </h2>
  <font color="black">本論文では、ターゲットドメインのデータ分布構造を徹底的に調査できる新しいドメイン適応アプローチを提案します。具体的には、ターゲットドメイン内の同じクラスター内のサンプルを個人ではなく全体として見なし、疑似ラベルを割り当てます。クラス重心マッチングによるターゲットクラスター..ドメイン適応は、ソースドメインからターゲットドメインに知識を転送するための基本的なテクノロジーです。効率的な反復最適化アルゴリズムは、理論的な収束保証を使用して提案の目的関数を解決するように設計されています。 
[概要]ドメイン適応の重要な問題は、2つのドメイン間の分布の不一致をどのように減らすかです。さらに、ターゲットサンプルの固有のローカル接続を適応的にキャプチャするためのローカル多様体自己学習戦略を提案に導入します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-20">
        <br><font color="black">2020-03-20</font>
      </time>
    </span>
</section>
<!-- paper0: A Flatter Loss for Bias Mitigation in Cross-dataset Facial Age
  Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CV/paper_69.html">
      <font color="black">A Flatter Loss for Bias Mitigation in Cross-dataset Facial Age
  Estimation</font>
    </a>
  </h2>
  <font color="black">既存の損失関数と比較して、その低い勾配は、より良い最適化の発見と収束を容易にし、その結果、より良い一般化を促進します。この目的のために、ニューラルネットワークトレーニングにより効果的な新しい損失関数を提案します。 -データセットの実験結果は、精度と一般化機能の点で、提案された方法が最先端のアルゴリズムよりも優れていることを示しています。 
[概要]以前の研究では、提案された方法が最先端技術よりも優れていることが示されています。ただし、これは、トレーニングセットとテストセットの特性が通常異なる実際のアプリケーションではほとんど有効ではありません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Comparison of Interactive Knowledge Base Spelling Correction Models for
  Low-Resource Languages -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_0.html">
      <font color="black">Comparison of Interactive Knowledge Base Spelling Correction Models for
  Low-Resource Languages</font>
    </a>
  </h2>
  <font color="black">自然データと合成データの両方、および2つの危機に瀕した言語（アイヌ語とグリコ語）のデータで実験を行います。複数の言語での実験結果は、モデルが少量のデータで有効になる可能性があることを示しています。低リソース言語でのスペル修正のための知識ベースと予測モデルの組み込みシステムを設計します。 
[ABSTRACT]研究は、ターゲット言語データの量が異なるニューラルモデルと文字言語モデルの比較を示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: A Benchmark for Lease Contract Review -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_1.html">
      <font color="black">A Benchmark for Lease Contract Review</font>
    </a>
  </h2>
  <font color="black">エンティティとそれに含まれる危険信号で手動で注釈を付けた179のリース契約文書の新しいベンチマークデータセットをリリースします。これは、関連する抽出アルゴリズムのトレーニングとテストに使用できます。最後に、ALeaseBERTと呼ばれる新しい言語モデルをリリースします。 、このデータセットで事前にトレーニングされ、前述の要素の検出用に微調整されており、さらなる調査のベースラインを提供します。後者は、1つまたは複数の署名者に何らかの危険またはその他の潜在的に問題のある状況があることを示す用語または文です。 
[概要]新しい言語モデルは、契約のレビューで重要な役割を果たす2種類の要素を検出できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Inference Strategies for Machine Translation with Conditional Masking -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_2.html">
      <font color="black">Inference Strategies for Machine Translation with Conditional Masking</font>
    </a>
  </h2>
  <font color="black">条件付きマスク言語モデル（CMLM）トレーニングは、機械翻訳などの非自己回帰および半自己回帰シーケンス生成タスクで成功することが証明されています。部分シーケンスの条件付き確率の因数分解としてマスク推論を定式化し、これがパフォーマンスに悪影響を与えないことを示します。 、およびこの観点から動機付けられたいくつかの単純なヒューリスティックを調査します。標準の「マスク予測」アルゴリズムよりも優れているしきい値戦略を特定し、機械翻訳タスクでの動作の分析を提供します。 
[要約]最も可能性の高い戦略が何であるかは明確ではありません。たとえば、それがどれほど効果的であるかは明確ではありません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-05">
        <br><font color="black">2020-10-05</font>
      </time>
    </span>
</section>
<!-- paper0: Incorporating Commonsense Knowledge into Abstractive Dialogue
  Summarization via Heterogeneous Graph Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_3.html">
      <font color="black">Incorporating Commonsense Knowledge into Abstractive Dialogue
  Summarization via Heterogeneous Graph Networks</font>
    </a>
  </h2>
  <font color="black">この論文では、大規模な常識知識が対話の理解と要約の生成をどのように促進できるかを示すために、新しいマルチスピーカー対話サマライザーを提示します。詳細には、発話と常識知識を2つの異なるタイプのデータと見なし、対話の異種を設計します。両方の情報をモデル化するためのグラフネットワーク（D-HGN）。SAMSumデータセットの実験結果は、モデルがさまざまな方法よりも優れていることを示しています。 
[概要]この論文では、新しいマルチスピーカー対話サマライザーを紹介します。また、情報の流れを容易にするために、スピーカーを異種ノードとして追加します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded
  Dialogues -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_4.html">
      <font color="black">BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded
  Dialogues</font>
    </a>
  </h2>
  <font color="black">この欠点に対処するために、テキストの手がかりに基づくビデオの高解像度クエリのための視覚言語神経フレームワークである双方向時空間学習（BiST）を提案します。ただし、ビデオに基づく対話への既存のアプローチは、しばしば表面的なものに焦点を当てています。時間レベルの視覚的手がかりですが、ビデオからのよりきめ細かい空間信号を無視します。双方向戦略は、ダイアログ設定でのユーザークエリの進化するセマンティクスに取り組むことを目的としています。 
[概要]ビデオトラッキングビデオトラップされたビデオトラッキングは4,000回以上視聴されています。これには、ビデオトラッキング、ビデオトラッキング、ビデオトラッキングが含まれます。ただし、ビデオトラッキングとビデオトラッキングのビデオトラッキング技術はすでに使用されています。発展した</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Bi-directional Cognitive Thinking Network for Machine Reading
  Comprehension -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_5.html">
      <font color="black">Bi-directional Cognitive Thinking Network for Machine Reading
  Comprehension</font>
    </a>
  </h2>
  <font color="black">DuReaderデータセットで競合の改善が見られ、双方向の知識がQAタスクに役立つという仮説が確認されました。フレームワークの有効性を検証するために、対応する双方向認知思考ネットワーク（BCTN）を設計して、パッセージをエンコードし、質問（回答）に回答（質問）を与え、双方向の知識を切り離します。モデルには、より正確な回答を生成するための慣性的思考を支援できる推論質問を逆にする機能があります。 
[ABSTRACT]モデルは、逆思考と慣性思考を含む、質問に答えるための脳内の2つの考え方をシミュレートすることを目的としています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Stay Hungry, Stay Focused: Generating Informative and Specific Questions
  in Information-Seeking Conversations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_6.html">
      <font color="black">Stay Hungry, Stay Focused: Generating Informative and Specific Questions
  in Information-Seeking Conversations</font>
    </a>
  </h2>
  <font color="black">実用的な質問を生成するために、強化学習を使用して、提案する有益性メトリックを最適化し、より具体的な質問を促進するように設計された報酬関数と組み合わせます。主に答えが何であるかについての知識を前提とする質問生成に関する以前の作業とは異なり、私たちは興味を持っています質問者に回答が得られるコンテキストが与えられていないが、共有された会話履歴を考慮して、新しい情報を取得する方法について実用的に推論する必要があるシナリオで。情報非対称会話で有益な質問を生成する問題を調査します。 
[概要]質問者に回答が得られるコンテキストが与えられていないシナリオに関心がありますが、新しい情報を取得する方法を説明する必要があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-30">
        <br><font color="black">2020-04-30</font>
      </time>
    </span>
</section>
<!-- paper0: Fine-Tuning Pre-trained Language Model with Weak Supervision: A
  Contrastive-Regularized Self-Training Approach -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_7.html">
      <font color="black">Fine-Tuning Pre-trained Language Model with Weak Supervision: A
  Contrastive-Regularized Self-Training Approach</font>
    </a>
  </h2>
  <font color="black">対照的な正則化と信頼性に基づく再重み付けに支えられたこの対照的な自己トレーニングフレームワークは、エラーの伝播を効果的に抑制しながら、モデルの適合を徐々に改善できます。LMの容量が大きいため、弱い監視によって生成されたノイズの多いラベルが過剰適合しやすいため、この問題は困難です。 ..シーケンス、トークン、および文のペアの分類タスクに関する実験では、6つのタスクの7つのベンチマークで、モデルが最強のベースラインを大幅に上回り、完全に監視された微調整方法で競争力のあるパフォーマンスを達成していることが示されています。 
[概要] fine-msの問題は、ラベル付けされたデータがなく、弱い監視に基づいています。これは問題のためであり、対照的な自己トレーニングフレームワークであるコサインを開発します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-15">
        <br><font color="black">2020-10-15</font>
      </time>
    </span>
</section>
<!-- paper0: Natural Language Inference with Mixed Effects -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_8.html">
      <font color="black">Natural Language Inference with Mixed Effects</font>
    </a>
  </h2>
  <font color="black">\ textit {アノテーター変量効果}を既存のニューラルモデルに組み込むことによって\ textit {混合効果モデル}の概念を一般化するこの方法が、そのような効果を組み込んでいないモデルよりもパフォーマンスを向上させることを示します。証拠が増えています。自然言語推論データセットを構築するために使用される生の注釈の不一致の蔓延により、それらの注釈を単一のラベルに集約する一般的な慣行が問題になること。集約ステップをスキップして生の注釈をトレーニングできる一般的な方法を提案します。アノテーターの応答バイアスから発生する可能性のある不要なノイズにモデルをさらすことなく、直接。 
[概要]モデルを不要なノイズにさらすことなく、集計手順をスキップしてアノテーションを直接トレーニングできる一般的な方法を提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Individual corpora predict fast memory retrieval during reading -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_9.html">
      <font color="black">Individual corpora predict fast memory retrieval during reading</font>
    </a>
  </h2>
  <font color="black">1〜2か月後のその後の視線追跡調査では、回帰分析により、コーパスベースの単語確率ではなく、個々の単語の確率が初回通過持続時間と初回通過注視持続時間を説明できることが明らかになりました。個人の長期記憶構造は、通常のコーパスよりも読み取りパフォーマンスをよりよく説明でき、最近取得した情報は字句的に迅速にアクセスされます。個人のコーパスが長期記憶検索の認知タスクをより適切に予測できるかどうかをテストするために、刺激資料を生成しました。相関のない個人および規範ベースの単語確率を持つ134文で構成されます。 
[概要] 2人の参加者の毎日の読書をタブレットで2か月間記録し、300 / 500kトークンの個別のコーパスサンプルを生成しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Cue Me In: Content-Inducing Approaches to Interactive Story Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_10.html">
      <font color="black">Cue Me In: Content-Inducing Approaches to Interactive Story Generation</font>
    </a>
  </h2>
  <font color="black">自動評価と人間による評価の両方の実験結果は、これらの方法がベースライン方法と比較して、よりトピック的に一貫性のあるパーソナライズされたストーリーを生成することを示しています。ここでは、代わりに、ユーザーがモデルの中レベルの文の抽象化を提供するインタラクティブなストーリー生成のタスクに焦点を当てます。生成プロセス中のキューフレーズの形式。ストーリーの自動生成は、トピックに関するイベントの因果関係のある論理的なシーケンスを生成する必要がある難しい問題です。 
[概要]このドメインの以前のアプローチは、主にワンショット生成に焦点を当てていました。これは、人間のユーザーがストーリーレベルをガイドするためのインターフェイスを提供します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Data and Representation for Turkish Natural Language Inference -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_11.html">
      <font color="black">Data and Representation for Turkish Natural Language Inference</font>
    </a>
  </h2>
  <font color="black">これらのデータセットを使用して、トルコ語NLIの表現の主要な問題に対処します。言語内の埋め込みが不可欠であり、トレーニングセットが大きい場合は形態学的解析を回避できることがわかります。2つの大きな英語のNLIデータセットをトルコ語に翻訳し、専門家のチームが、翻訳の品質と元のラベルへの忠実度を検証します。 
[概要]これは他の言語での進歩の障害ですが、言語言語の言語システムは現在堅牢です。商用機械翻訳システムが堅牢になったのはこれが初めてです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-30">
        <br><font color="black">2020-04-30</font>
      </time>
    </span>
</section>
<!-- paper0: Complete Multilingual Neural Machine Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_12.html">
      <font color="black">Complete Multilingual Neural Machine Translation</font>
    </a>
  </h2>
  <font color="black">最後に、cMNMTを大規模にストレステストし、すべての言語ペアに競争力のある翻訳品質を提供する最大111 * 112 = 12,432の言語ペアでcMNMTモデルをトレーニングできることを示します。条件付きの新しいトレーニングデータサンプリング戦略と組み合わせてターゲット言語のみで、cMNMTはすべての言語ペアに対して競争力のある翻訳品質をもたらします。元の英語中心の並列コーパスを充実させるために、多方向に整列された例の使用を研究することに着手しました。 
[概要]一般的に使用されるバイリンガルコーパス（wmt）を確認します。すべてのソース言語とターゲット言語間で多方向に整列されたコーパスからこの直接並列データを再導入します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Language Representation in Multilingual BERTand its applications to
  improve Cross-lingual Generalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_13.html">
      <font color="black">Language Representation in Multilingual BERTand its applications to
  improve Cross-lingual Generalization</font>
    </a>
  </h2>
  <font color="black">言語の表現は、言語のトークンの埋め込みを平均するだけで得られることがわかります。さらに、観察に基づいて、m-BERTの言語間能力を向上させるための計算上安価で効果的なアプローチを提案します。言語表現では、トークンの埋め込みを操作することで多言語BERTの出力言語を制御し、教師なしトークン翻訳を実現できます。 
[ABSTRACT]言語の代表は、言語のトークンの埋め込みを平均するだけで取得できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Context-aware Memory Enhanced Transformer for End-to-end Task-Oriented
  Dialogue Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_14.html">
      <font color="black">Context-aware Memory Enhanced Transformer for End-to-end Task-Oriented
  Dialogue Systems</font>
    </a>
  </h2>
  <font color="black">この問題を解決するために、知識ベースをモデル化する新しいコンテキストアウェアメモリ生成モジュールを設計します。これにより、関連するエンティティを認識してコンテキストアウェアエンティティ表現を生成できます。さらに、このモジュールをTransformerに組み込み、コンテキストアウェアメモリ拡張を提案します。対話履歴と知識ベースからの情報を集約してより良い応答を生成できるTransformer（CMET）。広範な実験を通じて、私たちの方法は最先端の方法よりも優れたパフォーマンスを達成できます。 
[概要]このモジュールをトランスフォーマーに組み込み、コンテキストアウェアなメモリ拡張トランスフォーマーを提案します。ダイアログ履歴と知識ベースからの情報を集約して、より良い応答を生成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-12">
        <br><font color="black">2020-10-12</font>
      </time>
    </span>
</section>
<!-- paper0: An Empirical Study for Vietnamese Constituency Parsing with Pre-training -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_15.html">
      <font color="black">An Empirical Study for Vietnamese Constituency Parsing with Pre-training</font>
    </a>
  </h2>
  <font color="black">この作業では、ベトナムの構成要素の解析にスパンベースのアプローチを使用します。ベトナムのデータセットVietTreebankとNIIVTB1の両方で事前トレーニングモデルXLM-RobertaとPhoBERTを使用した経験的手法の比較の実験結果の分析を示します。結果は、XLM-Robertaを使用したモデルが、他の事前トレーニングモデル（VietTreebankが81.19％、NIIVTB1が85.70％）よりも大幅にF1スコアをアーカイブしたことを示しています。 
[ABSTRACT]私たちの方法は、自己注意エンコーダアーキテクチャとckyスタイルのアルゴリズムを使用したチャートデコーダに従います</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: Text Classification of COVID-19 Press Briefings using BERT and
  Convolutional Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_16.html">
      <font color="black">Text Classification of COVID-19 Press Briefings using BERT and
  Convolutional Neural Networks</font>
    </a>
  </h2>
  <font color="black">BERTなどのトランスフォーマーと組み合わせたCNNは、他の埋め込み（Word2Vec、Glove、ELMo）と組み合わせたCNNよりも優れていること、および事前にトレーニングされた分類器を使用して、追加のトレーニングなしでさまざまな政治テキストの自動分類を実行できることを示します。マニフェストプロジェクト（Volkens et al。、2020a）からの既存の人間の専門家による政治マニフェストの注釈付きコーパスを使用し、それらをCOVID-19Press Briefings（Chatsiou、2020）のコーパスに適用するレベルの政治談話分類子。ローカルトピックConvolutionalNeuralNetwork（CNN）分類器をトレーニングするためのトレーニングデータ。次に、それをCOVID-19PressBriefings Corpusに適用して、テストコーパス内の文を自動的に分類します。文レベルの分類タスク用に事前にトレーニングされた埋め込みの上にトレーニングされたCNNを使用した一連の実験について報告します。 
[ABSTRACT] cnnの選挙マニフェストは、政治マニフェストを使用して人々を訓練するために使用されます。cnnの埋め込みは、人々が自分の信念を理解するのを助けるために使用されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Word Shape Matters: Robust Machine Translation with Visual Embedding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_17.html">
      <font color="black">Word Shape Matters: Robust Machine Translation with Visual Embedding</font>
    </a>
  </h2>
  <font color="black">経験的に、私たちの方法は、モデルがトレーニングフェーズ中に利用可能なものを超えるノイズでテストされるテストシナリオでも、標準以下の入力に対するモデルの堅牢性を向上させます。ニューラルマシン変換は、標準のベンチマークデータセットよりも優れた経験的パフォーマンスを達成しました。しかし最近の証拠は、スペルミスのある単語などの標準以下の入力をモデルが簡単に処理できない可能性があることを示唆しています。この問題を克服するために、文字レベルのNLPモデルの入力シンボルの新しいエンコードヒューリスティックを導入します。印刷時の文字を描いた画像..この新しい戦略を視覚的埋め込みと名付け、人間も機械のワンホットベクトルではなく印刷された文字を介してコーパスを視覚的に処理するため、NLPモデルの堅牢性の向上が期待されます。 
[概要]この新しい戦略をビジュアル埋め込みと名付けます。nlpモデルの堅牢性が向上することが期待されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Open Question Answering over Tables and Text -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_18.html">
      <font color="black">Open Question Answering over Tables and Text</font>
    </a>
  </h2>
  <font color="black">2番目の手法は、クロスブロックリーダーを使用して、グローバルローカルのまばらな注意を払って複数の取得された証拠間の相互依存関係をモデル化することです。オープン質問応答（QA）では、質問への回答は、ドキュメントを取得して分析することによって生成されます。質問に対する回答が含まれている可能性があります。ほとんどのオープンQAシステムは、構造化されていないテキストから情報を取得することのみを検討しています。 
[要約]ほとんどのオープンqaシステムは、非構造化テキストから情報を取得することを検討しています。ほとんどのオープン回答では、表形式のデータと非構造化テキスト全体でマルチホップの結論が必要であり、質問に回答するために必要な証拠はさまざまな方法で配布できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Image Captioning with Visual Object Representations Grounded in the
  Textual Modality -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_19.html">
      <font color="black">Image Captioning with Visual Object Representations Grounded in the
  Textual Modality</font>
    </a>
  </h2>
  <font color="black">オブジェクト検出ラベルのテキストの性質と抽出された視覚オブジェクト表現の仮想的な表現力を活用して、現在の傾向とは逆のアプローチを提案します。関連する単語や文を接地するのではなく、キャプションシステムの単語埋め込みスペースに表現を接地します。画像..さらに、学習したオブジェクトのベクトル空間投影とICシステムのパフォーマンスへの影響の分析を提供します。パフォーマンスのわずかな変化だけで、接地されたモデルは、制約のないモデルよりも速く停止基準に到達します。トレーニングの更新が2〜3分の1になります。 
[概要]現在の傾向とは逆のアプローチを提案します。関連する画像の単語や文を固定するのではなく、キャプションシステムの単語埋め込みスペースの表現を固定します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: Simulated Chats for Task-oriented Dialog: Learning to Generate
  Conversations from Instructions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_20.html">
      <font color="black">Simulated Chats for Task-oriented Dialog: Learning to Generate
  Conversations from Instructions</font>
    </a>
  </h2>
  <font color="black">私たちの知る限り、クラウドソーシングによるデータ収集プロセスをシミュレートすることにより、会話全体を生成するためのモデルを最初に提示しました。このようなダイアログデータセットの生成に関連するコストを削減するために、最近の研究では、小さなサンプルからより大きなデータセットを自動的に作成する方法が検討されています。このホワイトペーパーでは、事前にトレーニングされた言語モデルであるGPT2を使用するデータ作成戦略を紹介します（Radford etal。 。2018）、ユーザーボットとエージェントボットを作成することにより、クラウドソースのワーカー間の相互作用をシミュレートします。 
[ABSTRACT]群集ベースのワーカーは、実行するタスクを説明する目標命令を提供することによって作成されます。これらは、ユーザーボットとエージェントボットに基づいています。シミュレーションデータを使用することで、両方の低レベルで大幅な改善が達成されることを実証しました。 -リソース設定以上-すべてのタスクのパフォーマンス</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Replacing Human Audio with Synthetic Audio for On-device Unspoken
  Punctuation Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_21.html">
      <font color="black">Replacing Human Audio with Synthetic Audio for On-device Unspoken
  Punctuation Prediction</font>
    </a>
  </h2>
  <font color="black">私たちのモデルアーキテクチャは、デバイスでの使用に最適です。韻律を意識したテキスト読み上げシステムを使用して生成された合成データのみに依存することで、高価なヒューマンオーディオでトレーニングされたモデルよりも優れたパフォーマンスを発揮できることを初めて実証しました。暗黙の句読点予測問題に関する録音..これは、自動音声認識テキスト出力のハッシュベースの埋め込みを、準反復ニューラルネットワークへの入力として音響機能と組み合わせて活用し、モデルサイズを小さくして待ち時間を低く抑えることによって実現されます。 
[概要]初めてデモンストレーションを行いましたが、高価な人間のオーディオ録音でトレーニングされたモデルよりも優れたパフォーマンスを発揮できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually
  Grounded Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_22.html">
      <font color="black">Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually
  Grounded Speech</font>
    </a>
  </h2>
  <font color="black">このような情報をRNNベースのモデルに導入し、どのタイプの境界が最も効率的かを調査する簡単な方法を示します。最後に、階層構造で複数の境界タイプを一度に使用することで、低レベルのセグメントが高レベルのセグメントを再構成するために使用され、低レベルまたは高レベルのセグメントを単独で使用するよりも有益であり、より良い結果が得られます。また、パフォーマンスを最大化するために、ネットワークのアーキテクチャのどのレベルでそのような情報を導入する必要があるかについても検討します。 
[概要]電話、単語の境界情報が提供された場合、そのようなネットワークが信頼できる音声からタイプへのマッピングをどれだけうまく学習できるかを評価しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-15">
        <br><font color="black">2020-06-15</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-End Text-to-Speech using Latent Duration based on VQ-VAE -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_23.html">
      <font color="black">End-to-End Text-to-Speech using Latent Duration based on VQ-VAE</font>
    </a>
  </h2>
  <font color="black">私たちのフレームワークでは、コネクティビスト時間分類（CTC）ベースのフォースアライナーがおおよその後方として機能し、テキストから持続時間までが変分オートエンコーダーの事前として機能します。結果は、システムがソフトアテンションベースの間で評価されたことを示しました。メソッド（Transformer-TTS、Tacotron2）および明示的な期間モデリングベースのメソッド（Fastspeech）。条件付きVQ-VAEに基づいてメソッドを定式化し、変分オートエンコーダーで離散期間を処理し、メソッドを正当化する理論的な説明を提供します。 
[概要]明示的な期間モデリングを使用して新しいttsフレームワークを提案します。システムは接続主義分類（ctc）を使用します-ベースのフォースalignr.text-から-期間-はオートエンコーダーの優先事項です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: CoRT: Complementary Rankings from Transformers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_24.html">
      <font color="black">CoRT: Complementary Rankings from Transformers</font>
    </a>
  </h2>
  <font color="black">MS MARCOデータセットを使用して、CoRTがBM25を欠落している候補で補完することにより、第1段階のランキング品質とリコールを大幅に向上させることを示します。このようなシナリオの候補は、通常、BM25などのスケーラブルなbag-of-words検索モデルによって検索されます。このコンテキストでは、CoRTを提案します。これは、トランスフォーマーベースの言語モデルからのコンテキスト表現を活用して、用語ベースのランキング関数からの候補を補完し、大幅な遅延を発生させないフレームワークおよびニューラル第1段階ランキングモデルです。 
[要約] ms marcoデータセットのモデルは、bm25を欠落している候補で補完することにより、cortが第1段階のランキングとリコールを大幅に増加させることを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary
  Representations From Characters -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_25.html">
      <font color="black">CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary
  Representations From Characters</font>
    </a>
  </h2>
  <font color="black">この新しいモデルは、さまざまな医療分野のタスクでBERTのパフォーマンスを向上させると同時に、堅牢で単語レベルのオープンボキャブラリー表現を生成することを示します。BERTによってもたらされる魅力的な改善により、最近の多くの表現モデルが採用されました。 Transformerアーキテクチャを主要な構成要素として使用するため、Transformerの概念に本質的にリンクされていなくても、ワードピーストークン化システムを継承します。さらに、ワードピーストークン化を採用すると、フォーカスがワードレベルからサブワードレベルにシフトし、モデルが作成されます。概念的にはより複雑で、実際にはおそらくあまり便利ではありません。 
[概要] `characterbert &#39;の概念は、ワードピースシステムを完全に削除するbertの新しいバリアントです。新しいモデルを使用して、文字を参照して単語全体を表現します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Single headed attention based sequence-to-sequence model for
  state-of-the-art results on Switchboard -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_26.html">
      <font color="black">Single headed attention based sequence-to-sequence model for
  state-of-the-art results on Switchboard</font>
    </a>
  </h2>
  <font color="black">クロス発話言語モデルを使用すると、シングルパススピーカーに依存しないシステムは、Hub5&#39;00のSwitchboardサブセットとCallHomeサブセットで発音辞書なしで6.4％と12.5％の単語誤り率（WER）に達します。全体として、さまざまな組み合わせ正規化とシンプルだがかなり大きなモデルは、外部データリソースなしでSWB-2000を使用して、スイッチボードとCallHomeセットで4.7％と7.8％のWERという新しい最先端の結果をもたらします。 -シーケンス（seq2seq）音声認識モデルは、トレーニングに少なくとも1000時間の大量のデータが利用できる場合にのみ、ハイブリッドモデルと競合します。 
[概要]新しい調査によると、すべてのモデルの7％が、注意を認識できる単一のシステムを使用できます。新しいテストでは、より多くのデータほど有用なものはないことが示されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-20">
        <br><font color="black">2020-01-20</font>
      </time>
    </span>
</section>
<!-- paper0: Are All Good Word Vector Spaces Isomorphic? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_27.html">
      <font color="black">Are All Good Word Vector Spaces Isomorphic?</font>
    </a>
  </h2>
  <font color="black">言語ペア間のパフォーマンスの違いは、類型の違いだけでなく、利用可能な単一言語リソースのサイズ、および単一言語トレーニングの特性と期間に主に起因する可能性があることを示す、さまざまな言語にわたる一連の実験を提示します（たとえば、このような非同形性は、言語間の類型の違いに起因すると仮定されています。言語間の単語ベクトル空間を整列させるための既存のアルゴリズムは、ベクトル空間がほぼ同形であることを前提としています。
[要約]これらの作業は、非同形性では機能しません。スペース。これらは、パフォーマンスが低いか、完全に失敗するためと考えられます。また、パフォーマンスが悪いか、非同形スペースで失敗するためです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-08">
        <br><font color="black">2020-04-08</font>
      </time>
    </span>
</section>
<!-- paper0: Bootleg: Chasing the Tail with Self-Supervised Named Entity
  Disambiguation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_28.html">
      <font color="black">Bootleg: Chasing the Tail with Self-Supervised Named Entity
  Disambiguation</font>
    </a>
  </h2>
  <font color="black">シンプルなTransformerアーキテクチャで推論パターンをエンコードすることで、Bootlegは3つのNEDベンチマークで最先端を満たしているか上回っています。これらのパターンに触発されて、Bootlegを紹介します。Bootlegは、曖昧さ回避..曖昧さ回避のコア推論パターンを定義し、自己監視モデルがパターンを学習するように促す学習手順を作成し、弱い監視を使用してトレーニングデータの信号を強化する方法を示します。 
[概要]曖昧さ回避のコア推論パターンを定義します。自己曖昧化モデルがパターンを学習するように促す学習手順を作成し、弱い監視を使用してトレーニングデータの信号を強化する方法を示します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Back to the Future: Unsupervised Backprop-based Decoding for
  Counterfactual and Abductive Commonsense Reasoning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_29.html">
      <font color="black">Back to the Future: Unsupervised Backprop-based Decoding for
  Counterfactual and Abductive Commonsense Reasoning</font>
    </a>
  </h2>
  <font color="black">この論文では、DeLoreanを提案します。これは、既製の左から右への言語モデルのみを使用し、監視なしで過去と未来の両方のコンテキストを柔軟に組み込むことができる新しい教師なしデコードアルゴリズムです。 、DeLoreanは、左と右の両方のコンテキストを反映する出力表現をデコードできます。私たちのアプローチは一般的であり、2つの非単調な推論タスクに適用できることを示します。誘拐テキストの生成と反事実的なストーリーの改訂です。自動および人間による評価に基づく方法。 
[要約]私たちのアプローチは一般的であり、2つの非単調推論タスクに適用されることを示しました：アブダクションテキスト生成と反事実的ストーリー改訂</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-12">
        <br><font color="black">2020-10-12</font>
      </time>
    </span>
</section>
<!-- paper0: Extracting Procedural Knowledge from Technical Documents -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_30.html">
      <font color="black">Extracting Procedural Knowledge from Technical Documents</font>
    </a>
  </h2>
  <font color="black">この作業では、この分野の一部を次のように説明します。1）ドキュメントの構造的および言語的特性をグループ化して手順のタイプを定義する方法に関する洞察を提供する。2）ドキュメントを分析して関連する言語的および構造的特性を抽出する。3）上記の分析から導き出された文書の特徴を活用する分類問題として手順の識別を定式化する。さまざまな使用例での評価に基づいて、監視されていないアプローチの弱点を理解した。手順は、文書の重要な知識要素である。自動化、質問への回答、または会話の促進のために、認知アシスタントが活用できます。 
[概要]大きな密度の高いドキュメントを解析するのは難しい問題です。ユーザーガイドは、どの部分が手順について話しているのかを理解し、その後それらを抽出します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual
  Embeddings Using the Unified Medical Language System Metathesaurus -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_31.html">
      <font color="black">UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual
  Embeddings Using the Unified Medical Language System Metathesaurus</font>
    </a>
  </h2>
  <font color="black">これらの2つの戦略を適用することにより、UmlsBERTは、臨床ドメイン知識を単語埋め込みにエンコードし、一般的な固有表現抽出（NER）および臨床自然言語推論臨床NLPタスクで既存のドメイン固有モデルよりも優れたパフォーマンスを発揮します。BioBERTやBioBERTなどのコンテキスト単語埋め込みモデルBio_ClinicalBERTは、事前トレーニングプロセスをドメイン固有のコーパスに集中させることにより、生物医学的自然言語処理タスクで最先端の結果を達成しました。ただし、このようなモデルでは、専門家のドメイン知識は考慮されていません。 
[要約]統一された医療言語システムによるumsbertの拡張は2つの方法で実行されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: BLEU might be Guilty but References are not Innocent -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_32.html">
      <font color="black">BLEU might be Guilty but References are not Innocent</font>
    </a>
  </h2>
  <font color="black">私たちの方法は、WMT 2019英語からドイツ語への提出だけでなく、標準参照を使用した自動メトリックとの相関が低いことが示されている逆翻訳およびAPE拡張MT出力についても人間の判断との相関が高くなります。さまざまなシステムや指標について、人間の評価との相関関係を報告することにより、参照を収集し、自動評価でそれらの値を比較するさまざまな方法。典型的な参照は多様性が低く、翻訳言語に集中しているという発見に動機付けられて、言語学者向けの言い換えタスクを開発します。このバイアスを打ち消す既存の参照翻訳を実行します。 
[要約]ある研究では、典型的な参照は多様性が低いことがわかりました。言語学者が既存の参照翻訳に対して実行する言い換えタスクを開発しました。これは、このバイアスに対抗します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-13">
        <br><font color="black">2020-04-13</font>
      </time>
    </span>
</section>
<!-- paper0: SOrT-ing VQA Models : Contrastive Gradient Learning for Improved
  Consistency -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_33.html">
      <font color="black">SOrT-ing VQA Models : Contrastive Gradient Learning for Improved
  Consistency</font>
    </a>
  </h2>
  <font color="black">これに対処するために、最初に勾配ベースの解釈可能性アプローチを提示して、画像上の推論質問と最も強く相関する質問を決定し、これを使用して、推論に答えるのに必要な関連サブ質問を識別する能力についてVQAモデルを評価します。質問..SOrTが既存のベースラインよりも最大6.5％ポイントだけモデルの一貫性を改善すると同時に、視覚的根拠も改善することを示します。視覚的質問回答（VQA）の最近の研究により、最先端のモデルの一貫性が失われていることが明らかになりました。世界の理解-彼らは正しく推論を必要とする一見難しい質問に答えますが、より単純な関連するサブ質問は間違っています。 
[ABSTRACT] sub-質問vvqaは、モデルが間違った質問をランク付けする必要があることを意味します。これは、サブ解決の質問に関する一連の調査の最新の調査です。sub-question-qaは、新しい形式の調査と調査です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Human-Paraphrased References Improve Neural Machine Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_34.html">
      <font color="black">Human-Paraphrased References Improve Neural Machine Translation</font>
    </a>
  </h2>
  <font color="black">候補翻訳を参照翻訳の人間が生成した言い換えと比較する自動評価は、最近Freitag et al。によって提案されました。この効果は、さまざまな異なる自動メトリックに当てはまり、より文字通りの（翻訳）ものよりも自然な定式化を好む傾向があります。最先端の英語-ドイツ語NMTコンポーネントでは、言い換えられた参照にチューニングすると、人間の判断によれば大幅に優れたシステムが生成されますが、標準の参照でテストすると5BLEUポイント悪化します。 
[要約]標準および言い換えられた参照は、人間の判断とよりよく相関するメトリックスコアを生成します。初めて、言い換えられた参照は、人間の判断とより相関するメトリックスコアを生成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Modeling Content and Context with Deep Relational Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_35.html">
      <font color="black">Modeling Content and Context with Deep Relational Learning</font>
    </a>
  </h2>
  <font color="black">私たちのフレームワークは、表現力豊かな言語エンコーダーとの簡単な統合をサポートし、表現、推論、学習の間の相互作用を研究するためのインターフェースを提供します。ただし、ニューラル表現とシンボリック表現を組み合わせるための既存のフレームワークのほとんどは、機能する古典的なリレーショナル学習タスク用に設計されています。シンボリックエンティティと関係の宇宙..ニューラルシンボリック表現は、シンボリックメソッドの推論機能とニューラルネットワークの表現力を組み合わせる方法として登場しました。 
[ABSTRACT] drailは、深い世界を指定するためのオープンソースツールです。ニューラルネットワークとのさまざまな創造的な相互作用を提供します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Neural Language Modeling for Contextualized Temporal Graph Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_36.html">
      <font color="black">Neural Language Modeling for Contextualized Temporal Graph Generation</font>
    </a>
  </h2>
  <font color="black">その理由の一部は、人間が注釈を付けたイベントと時間的リンクを備えた大規模なトレーニングコーパスを取得するのが難しいことです。これらの戦略により、グラフ生成タスクのシステム誘導トレーニングデータで事前トレーニングされた言語モデルを活用および微調整できます。 。私たちの実験は、私たちのアプローチが構造的および意味的に有効なグラフを生成するのに非常に効果的であることを示しています。 
[概要]事前トレーニング方法は十分に検討されていません。私たちのアプローチは、構造的および意味的にトレーニングされたものを生成するのに非常に効果的です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Bridging the Gap between Conversational Reasoning and Interactive
  Recommendation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_37.html">
      <font color="black">Bridging the Gap between Conversational Reasoning and Interactive
  Recommendation</font>
    </a>
  </h2>
  <font color="black">統一されたフレームワーク内の2つの別々のシステムを表示するために、階層的なダイアログ動作とマルチホップ知識グラフ推論の間の高レベルのマッピングを求めます。モデルは大規模な知識グラフ上を歩き、各ターンで推論ツリーを形成します。 、次にダイアログにマッピングされ、応答の生成をガイドします。推奨と会話の間のブリッジなどのマッピングメカニズムにより、フレームワークは2つのシステム間の相互利益を最大化します。推奨の品質と説明可能性の強化としてのダイアログ、目標としての推奨、ダイアログセマンティクスの強化。 
[概要]推奨事項と会話は通常、既存の作品では情報交換が制限された2つの別個のモジュールとして扱われていました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Enhancing Keyphrase Extraction from Microblogs using Human Reading Time -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_38.html">
      <font color="black">Enhancing Keyphrase Extraction from Microblogs using Human Reading Time</font>
    </a>
  </h2>
  <font color="black">2番目のモデルでは、外部機能として人間の読書時間を使用します。さらに、キーフレーズ抽出で注視時間をより効果的にするための戦略を提案します。この記事では、人間の読書時間を活用してマイクロブログの投稿からキーフレーズを抽出することを目指します。 。 
[概要]私たちが読むとき、より重要な単語はより長い読書時間を占めます。この研究では、オープンソースの目から抽出された視線固定時間を使用します-追跡コーパス（osec）。これらはまた、人間の読書時間をキーフレーズに統合することを目的としています。抽出モデル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Factual Completeness and Consistency of Image-to-Text
  Radiology Report Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_39.html">
      <font color="black">Improving Factual Completeness and Consistency of Image-to-Text
  Radiology Report Generation</font>
    </a>
  </h2>
  <font color="black">この作業では、生成された放射線レポートの事実の完全性と一貫性を促進する2つの新しいメトリックを提案することにより、この問題を克服することを目指しています。2番目のメトリックであるEntailing Entity Matchスコアは、自然言語推論モデルを導入することによって最初のメトリックを補強します。参照から含意できる一貫した世代を促進するためのエンティティ一致プロセス。これを達成するために、放射線医学テキストでのパフォーマンスを向上させるために、弱い監視を介してドメイン内NLIモデルも開発しました。 
[概要]既存のレポート生成システムは、依然として不完全で一貫性のない生成に悩まされています。これらのシステムは実際には使用できず、使用できません。これにより、臨床所見メトリックのf1スコアが向上します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Adaptive Feature Selection for End-to-End Speech Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_40.html">
      <font color="black">Adaptive Feature Selection for End-to-End Speech Translation</font>
    </a>
  </h2>
  <font color="black">特に、AFSはカスケードベースラインと比較してパフォーマンスギャップを減らし、LibriSpeech En-FrでBLEUスコア18.56（データ拡張なし）でそれを上回ります。 L0DROP（Zhang et al。、2020）をAFSのバックボーンとして採用し、時間的次元と特徴次元の両方に関して音声特徴をスパース化するように適合させます。LibriSpeechEn-FrおよびMuST-Cベンチマークの結果は、AFSが学習を容易にすることを示しています。 〜84％の時間的特徴を取り除くことにより、STの平均変換ゲインは〜1.3〜1.6 BLEU、デコード速度は〜1.4xになります。 
[概要]エンコーダーの適応特徴選択（afs）を提案します-デコーダーベースのe2e st</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-16">
        <br><font color="black">2020-10-16</font>
      </time>
    </span>
</section>
<!-- paper0: Generating Strategic Dialogue for Negotiation with Theory of Mind -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_41.html">
      <font color="black">Generating Strategic Dialogue for Negotiation with Theory of Mind</font>
    </a>
  </h2>
  <font color="black">一次心の理論の確率論的定式化を導入し、CraigslistBargainデータセットでアプローチをテストします。実験によると、ToM推論を使用した方法では、対戦相手の混合母集団のベースラインと比較して、40 \％高い対話合意率が達成されます。 。心の理論（ToM）の概念を、タスク指向の対話のための発話の生成に統合するためのフレームワークを提案します。 
[概要]私たちのアプローチは、対戦相手の性格タイプをモデル化して推測する能力を探求します。私たちの方法は、この情報を使用して、エージェントの高レベルの戦略を適応させます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: ConjNLI: Natural Language Inference Over Conjunctive Sentences -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_42.html">
      <font color="black">ConjNLI: Natural Language Inference Over Conjunctive Sentences</font>
    </a>
  </h2>
  <font color="black">RoBERTaのような大規模な事前トレーニング済み言語モデルは、接続詞のセマンティクスを十分に理解しておらず、浅いヒューリスティックに頼ってそのような文を推論していることがわかります。 、前提条件が、接続詞の削除、追加、または置換によって仮説と異なる場合。パフォーマンスの向上は見られますが、ConjNLIは現在の方法に挑戦しているため、接続詞をよりよく理解するための興味深い将来の作業を奨励しています。 
[ABSTRACT] cnnのアダム・ソーベルは、新しいストレステストは接続詞の非ブール使用法を考慮しないと言います。彼は接続詞の理論をテストすることが重要であると言います。しかし彼は新しいシステムがに基づいていないと言います。それがうまく機能するという事実</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Optimal Subarchitecture Extraction For BERT -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_43.html">
      <font color="black">Optimal Subarchitecture Extraction For BERT</font>
    </a>
  </h2>
  <font color="black">「Bort」と呼ばれるこの最適なサブセットは、明らかに小さく、元のBERT-largeアーキテクチャの有効な（つまり、埋め込み層を数えない）サイズが$ 5.5 \％$で、ネットサイズ..CPU上で$ 7.9 $ x速く、アーキテクチャの他の圧縮バリアントや一部の非圧縮バリアントよりもパフォーマンスが優れています。パフォーマンスが$ 0.3 \％$から$ 31向上します。 \％$、絶対、BERT-largeに関して、複数の公的自然言語理解（NLU）ベンチマークで..Bortは$ 288 $ GPU時間で事前トレーニングすることもできます。これは、事前トレーニングに必要な時間の$ 1.2 \％$です。最高性能のBERTパラメトリックアーキテクチャバリアントであるRoBERTa-large（Liu et al。、2019）、および同じハードウェアでBERT-largeをトレーニングするために必要な、GPU時間での世界記録の約$ 33 \％$。 
[ABSTRACT] wertは$ 288 $ gpu時間の事前トレーニングを行うことができます。これは、最高の事前トレーニングに必要な時間の$ 1.2％です。$ $-パフォーマンスの高いbertパラメトリックアーキテクチャバリアント、roberta-大</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Local Knowledge Powered Conversational Agents -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_44.html">
      <font color="black">Local Knowledge Powered Conversational Agents</font>
    </a>
  </h2>
  <font color="black">フレームワークとデータセットを使用して、ローカルの知識を組み込むことで、人間の評価を使用して情報量、一貫性、現実性の測定値を大幅に改善できることを示します。モデルのサイズを117Mから8.3Bのパラメーターにスケーリングすると、検証の複雑さも一貫して改善されることもわかりました。人間が評価した指標として..特に、私たちのアプローチは、3つの測定値すべてにわたってRedditデータセットの最先端の会話モデルを一貫して上回っています。 
[概要] redditの会話に基づいてデータセットを構築するアプローチを紹介します。アウトバウンドリンクは会話で広く利用可能であり、ハイパーリンクされたドキュメントはローカルの外部知識として自然に含めることができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Reliable Part-of-Speech Tagging of Historical Corpora through Set-Valued
  Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_45.html">
      <font color="black">Reliable Part-of-Speech Tagging of Historical Corpora through Set-Valued
  Prediction</font>
    </a>
  </h2>
  <font color="black">私たちの実験的研究では、最先端のPOSタガーを設定値の予測に拡張すると、特に未知の単語、つまりトレーニングデータに出現しない単語に対して、より正確で堅牢なタグ付けが得られることがわかりました。コーパスの構文注釈品詞（POS）タグの形式は、言語研究とそれに続く自動自然言語処理（NLP）タスクの両方にとって重要な要件です。これらの不規則性により、自動POSタグ付けのタスクがより困難になりエラーが発生しやすくなります。 
[概要]問題は通常、機械学習手法を使用して対処されます。postaggerに自然なタグを予測してコミットさせる代わりに、その不確実性を表現できる必要があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-04">
        <br><font color="black">2020-08-04</font>
      </time>
    </span>
</section>
<!-- paper0: Elaborative Simplification: Content Addition and Explanation Generation
  in Text Simplification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_46.html">
      <font color="black">Elaborative Simplification: Content Addition and Explanation Generation
  in Text Simplification</font>
    </a>
  </h2>
  <font color="black">大規模な事前トレーニング済み言語モデルを使用して詳細生成のベースラインを確立し、生成中にコンテキストの特異性を考慮するとパフォーマンスが向上することを示します。ただし、コンテンツの追加は、難しい概念や理由を説明する必要がある場合に役立つことがよくあります。 、ドキュメントの簡素化におけるコンテンツ追加の最初のデータ駆動型研究を紹介します。これを詳細な簡素化と呼びます。 
[概要]詳細な簡略化の1.3kインスタンスの新しい注釈付きデータセットを紹介します。コンテキストの特異性のレンズを通してエンティティ、アイデア、および概念がどのように概説されるかを説明します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Supertagging-based Parsing with Linear Context-free Rewriting Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_47.html">
      <font color="black">Supertagging-based Parsing with Linear Context-free Rewriting Systems</font>
    </a>
  </h2>
  <font color="black">これは、M \ &quot;orbitz and Ruprecht（2020）による以前の作業を応用したものです。さらに、私たちの結果は、最高の（一般的な）不連続パーサーに追いつきます。特に、不連続構成要素のスコアは優れています。最初のスーパータグ付けを提示します- LCFRS用のベースのパーサー。
[要約]ドイツ語ベースのスーパータグ付きスーパータグ付けは、標準の標準lcfrs解析に新しく追加されたものです。また、字句lcfrsのインカネーションを元のツリーバンクの同等の解析ツリーに変換する手順も紹介します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Fluent and Low-latency Simultaneous Speech-to-Speech Translation with
  Self-adaptive Training -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_48.html">
      <font color="black">Fluent and Low-latency Simultaneous Speech-to-Speech Translation with
  Self-adaptive Training</font>
    </a>
  </h2>
  <font color="black">同様のレベルの翻訳品質（BLEUで測定）で、私たちの方法は、両方のZh &lt;-&gt; En方向で、ベースラインよりも大幅に低い遅延で、より流暢なターゲット音声（自然性メトリックMOSで測定）を生成します。問題については、翻訳の長さを柔軟に調整してさまざまなソースの発話速度に対応する自己適応型翻訳（SAT）を提案します。その結果、現在のアプローチでは、話者が速く話すとレイテンシが徐々に蓄積され、話者が遅く話すと不自然な一時停止が発生します。 。 
[要約]さらに、文のストリームを自動的に翻訳する必要があります。ただし、最近のすべてのソリューションは、単一の文のシナリオにのみ焦点を当てています。この方法は、より遅い発話速度に対応するために使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Topic-Aware Abstractive Text Summarization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_49.html">
      <font color="black">Topic-Aware Abstractive Text Summarization</font>
    </a>
  </h2>
  <font color="black">この研究では、潜在的なトピックによって表されるドキュメントの基礎となるセマンティック構造を活用することにより、トピックを意識した抽象要約（TAAS）フレームワークを提案します。具体的には、TAASは、ニューラルトピックモデリングをエンコーダ-デコーダベースのシーケンス生成手順にシームレスに組み込みます。要約の注意..これらのモデルは、ドキュメント内の単語間のコンテキスト情報を適切にキャプチャしますが、ダウンストリームの抽象要約タスクをより適切に微調整するためにグローバルセマンティクスを組み込むことにはほとんど注意が払われていません。 
[要約]新しい研究によると、要約要約は単語ごとに要約を生成します。モデルは、ドキュメント内の単語間のコンテキスト情報をキャプチャします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: JUNLP@Dravidian-CodeMix-FIRE2020: Sentiment Classification of Code-Mixed
  Tweets using Bi-Directional RNN and Language Tags -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/cs.CL/paper_50.html">
      <font color="black">JUNLP@Dravidian-CodeMix-FIRE2020: Sentiment Classification of Code-Mixed
  Tweets using Bi-Directional RNN and Language Tags</font>
    </a>
  </h2>
  <font color="black">ソーシャルメディアのテキストは1つの言語ではなく、本質的にコードが混在しているため、従来の感情分類モデルでは許容できる結果が得られません。感情分析は、過去20年間、そして最近では、活発な研究分野となっています。ソーシャルメディアの出現により、ソーシャルメディアテキストの感情分析に対する需要が高まっています。提示されたアルゴリズムは、テストデータで評価すると、それぞれ0.59、0.66、および0.58の精度、想起、およびF1スコアを獲得しました。 
[概要]ソーシャルメディアテキストは1つの言語ではなく、大部分がコードです-本質的に混合されています。データをテストするために、ソーシャルメディアテキスト分析の結果は混合されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: The Effect of Spectrogram Reconstruction on Automatic Music
  Transcription: An Alternative Approach to Improve Transcription Accuracy -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.AS/paper_0.html">
      <font color="black">The Effect of Spectrogram Reconstruction on Automatic Music
  Transcription: An Alternative Approach to Improve Transcription Accuracy</font>
    </a>
  </h2>
  <font color="black">ピッチラベルのみを使用して（スペクトログラム再構成損失とともに）、監視対象のサブタスクを導入せずにこのモデルがどこまで進むことができるかを調べます。元のスペクトログラムと再構成されたスペクトログラムの間に再構成損失を適用して、2番目のU-を制約します。ネットは再構築のみに焦点を当てています。最先端の自動音楽トランスクリプション（AMT）モデルのほとんどは、メインのトランスクリプションタスクを開始予測やオフセット予測などのサブタスクに分解し、開始ラベルとオフセットラベルを使用してトレーニングします。 。 
[概要]次に、予測が連結され、入力として使用されて、ピッチラベルを使用して別のモデルをトレーニングし、最終的な翻訳を取得します。これは、音楽の状態を達成することを目的としない、u-netによって監視された最初の論文です。代わりに、スペクトログラムの再構築がそのようなモデルに与える影響を調査します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Small-Footprint Keyword Spotting with Multi-Scale Temporal Convolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.AS/paper_1.html">
      <font color="black">Small-Footprint Keyword Spotting with Multi-Scale Temporal Convolution</font>
    </a>
  </h2>
  <font color="black">さらに、時間的および深さ方向の畳み込みを利用して、時間効率の高いニューラルネットワーク（TENet）がKWSシステム用に設計されています。マルチブランチ時間畳み込みモジュール（MTConv）、異なるカーネルを持つ複数の時間畳み込みフィルターで構成されるCNNブロックを提案します。サイズは、時間的特徴空間を豊かにします。GoogleSpeechCommand Datasetの結果は、MTConvでトレーニングされたモデルの1つが、わずか100Kのパラメーターで96.8％の精度を実行することを示しています。 
[概要]複数のostrolフィルターで構成されるcnnブロックであるマルチブランチ時間畳み込みモジュール（mtconv）を提案します。ostrolostrackによると、kwsタスクのフットプリントが小さいことと精度が高いことのトレードオフを実現することは困難です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Phase recovery with Bregman divergences for audio source separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.AS/paper_2.html">
      <font color="black">Phase recovery with Bregman divergences for audio source separation</font>
    </a>
  </h2>
  <font color="black">音声強調タスクで実施された実験は、このアプローチがいくつかの代替損失についてMISIを上回っていることを示しており、オーディオソース分離アプリケーションとの関連性を強調しています。 ..結果の目的を最適化するために、投影勾配降下アルゴリズムを導出します。 
[概要]複数入力スペクトログラム反転（misi）アルゴリズムは、良好なパフォーマンスを示しています。この損失は、オーディオの一部の知覚特性を適切に説明していません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: An Audio-Video Deep and Transfer Learning Framework for Multimodal
  Emotion Recognition in the wild -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.AS/paper_3.html">
      <font color="black">An Audio-Video Deep and Transfer Learning Framework for Multimodal
  Emotion Recognition in the wild</font>
    </a>
  </h2>
  <font color="black">エンドツーエンドの深層学習を使用し、転移学習アプローチの恩恵を受けて、42.10％のテストセットチャレンジパフォーマンス測定に到達しました。この論文では、ABAW表情チャレンジへの貢献を示します。提案されたシステムとチャレンジプロトコルに準拠した公式のチャレンジ結果。 
[概要]提案システムと公式チャレンジ結果を報告します。公式結果も報告します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-07">
        <br><font color="black">2020-10-07</font>
      </time>
    </span>
</section>
<!-- paper0: Replacing Human Audio with Synthetic Audio for On-device Unspoken
  Punctuation Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.AS/paper_4.html">
      <font color="black">Replacing Human Audio with Synthetic Audio for On-device Unspoken
  Punctuation Prediction</font>
    </a>
  </h2>
  <font color="black">私たちのモデルアーキテクチャは、デバイス上での使用に最適です。これは、自動音声認識テキスト出力のハッシュベースの埋め込みを、準リカレントニューラルネットワークへの入力として音響機能と組み合わせて活用し、モデルサイズを小さくしてレイテンシを維持することで実現されます。低..音響機能とテキスト機能を組み合わせた、英語用の新しいマルチモーダル音声なしパンクチャレーション予測システムを紹介します。 
[概要]初めてデモンストレーションを行いましたが、高価な人間のオーディオ録音でトレーニングされたモデルよりも優れたパフォーマンスを発揮できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-End Text-to-Speech using Latent Duration based on VQ-VAE -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.AS/paper_5.html">
      <font color="black">End-to-End Text-to-Speech using Latent Duration based on VQ-VAE</font>
    </a>
  </h2>
  <font color="black">条件付きVQ-VAEに基づいてメソッドを定式化し、変分オートエンコーダーで離散期間を処理し、メソッドを正当化する理論的な説明を提供します。フレームワークでは、接続主義時間分類（CTC）ベースのフォースアライナーがおおよその後方として機能します。結果は、私たちのシステムがソフトアテンションベースの方法（Transformer-TTS、Tacotron2）と明示的な期間モデリングベースの方法（Fastspeech）の間で評価されたことを示しました。 
[概要]明示的な期間モデリングを使用して新しいttsフレームワークを提案します。システムは接続主義分類（ctc）を使用します-ベースのフォースalignr.text-から-期間-はオートエンコーダーの優先事項です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-19">
        <br><font color="black">2020-10-19</font>
      </time>
    </span>
</section>
<!-- paper0: Single headed attention based sequence-to-sequence model for
  state-of-the-art results on Switchboard -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.AS/paper_6.html">
      <font color="black">Single headed attention based sequence-to-sequence model for
  state-of-the-art results on Switchboard</font>
    </a>
  </h2>
  <font color="black">クロス発話言語モデルを使用すると、シングルパススピーカーに依存しないシステムは、Hub5&#39;00のSwitchboardサブセットとCallHomeサブセットで発音辞書なしで6.4％と12.5％の単語誤り率（WER）に達します。全体として、さまざまな組み合わせ正規化とシンプルだがかなり大きなモデルは、外部データリソースなしでSWB-2000を使用して、スイッチボードとCallHomeセットで4.7％と7.8％のWERという新しい最先端の結果をもたらします。 -シーケンス（seq2seq）音声認識モデルは、トレーニングに少なくとも1000時間の大量のデータが利用できる場合にのみ、ハイブリッドモデルと競合します。 
[概要]新しい調査によると、すべてのモデルの7％が、注意を認識できる単一のシステムを使用できます。新しいテストでは、より多くのデータほど有用なものはないことが示されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-20">
        <br><font color="black">2020-01-20</font>
      </time>
    </span>
</section>
<!-- paper0: Leveraging the structure of musical preference in content-aware music
  recommendation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.AS/paper_7.html">
      <font color="black">Leveraging the structure of musical preference in content-aware music
  recommendation</font>
    </a>
  </h2>
  <font color="black">低レベルの音響的特徴から、音楽の好みを正確に表す3つの要素（覚醒、価数、深さ）を抽出します。この作品では、代わりに音楽心理学の分野に由来する音楽の好みのモデルを活用することを提案します。これらを、コンテンツを意識した音楽の推奨のための共同フィルタリングフレームワークに統合します。 
[概要]これらのアプローチは曲の内容に対してアギポクリティカルであるため、コールドスタートの問題に直面します。これは、履歴を聞いていない新しい曲を推奨できないためです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Investigating Cross-Domain Losses for Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.AS/paper_8.html">
      <font color="black">Investigating Cross-Domain Losses for Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">最近のモデルベースおよび深層学習SEアプローチに対する定量的比較分析を実行して、提案されたフレームワークのメリットを説明します。この研究では、音声了解度と品質への影響を個別に調べることにより、アプローチの各セットの利点を調査します。 。近年、音声強調（SE）と認識に利用できるフレームワークの数が急増しています。 
[概要]新しいモデルベースのseフレームワークにより、音声と時間-音声データの周波数（tf）表現が可能になります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Power pooling: An adaptive pooling function for weakly labelled sound
  event detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.AS/paper_9.html">
      <font color="black">Power pooling: An adaptive pooling function for weakly labelled sound
  event detection</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーではサウンドイベント検出アプリケーションに焦点を当てていますが、提案された方法は他のドメインのMILタスクに適用できます。2つの公開データセットでは、提案された電力プーリング関数は、粗粒度と粗粒度の両方で最先端の線形ソフトマックスプーリングよりも優れています。きめ細かいメトリック..特に、イベントベースのF1スコア（イベントの開始とオフセットの検出を評価する）が、2つのデータセットに対して11.4％と10.2％向上します。 
[ABSTRACT]弱いラベルのサウンドイベントのタイプとタイムスタンプの両方を検出する機能は、タイプのみを指定します。この論文では、さまざまな音源に適応できる適応型パワープーリング機能を提案します。これにより、イベントベースも改善されます。 2つのデータセットに対して11.4％および10.2％のf1スコア</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Tongji University Undergraduate Team for the VoxCeleb Speaker
  Recognition Challenge2020 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.AS/paper_10.html">
      <font color="black">Tongji University Undergraduate Team for the VoxCeleb Speaker
  Recognition Challenge2020</font>
    </a>
  </h2>
  <font color="black">CLOSEトラック用に選択された2つのシステムの融合により、チャレンジ評価セットで0.2973DCFと4.9700 \％EERが達成されます。このレポートでは、VoxCeleb話者認識チャレンジ（VoxSRC）のCLOSEトラックへの同済大学の学部チームの提出について説明します。 ）2020 at Interspeech 2020 .. RSBU-CWモジュールをResNet34フレームワークに適用して、ネットワークのノイズ除去能力を向上させ、複雑な環境での話者検証タスクをより適切に完了しました。 -モデルのパフォーマンスを向上させるための拡張方法。 
[概要] resnetの2つのバリアントをトレーニングし、スコア融合とデータを使用しました。モデルのパフォーマンスを向上させるための拡張方法です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Adaptive Feature Selection for End-to-End Speech Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.AS/paper_11.html">
      <font color="black">Adaptive Feature Selection for End-to-End Speech Translation</font>
    </a>
  </h2>
  <font color="black">L0DROP（Zhang et al。、2020）をAFSのバックボーンとして採用し、時間的次元と特徴次元の両方に関して音声特徴をスパース化するように適合させます。特に、AFSはカスケードベースラインと比較してパフォーマンスギャップを減らし、パフォーマンスを上回ります。 LibriSpeech En-Frで、BLEUスコアが18.56（データ拡張なし）です。 LibriSpeech En-FrおよびMuST-Cベンチマークの結果は、AFSが約84％の時間的特徴を取り除くことにより、STの学習を容易にし、平均変換ゲインが約1.3〜1.6 BLEU、デコード速度が約1.4倍になることを示しています。 
[概要]エンコーダーの適応特徴選択（afs）を提案します-デコーダーベースのe2e st</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-16">
        <br><font color="black">2020-10-16</font>
      </time>
    </span>
</section>
<!-- paper0: Pushing the Limits of Semi-Supervised Learning for Automatic Speech
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.AS/paper_12.html">
      <font color="black">Pushing the Limits of Semi-Supervised Learning for Automatic Speech
  Recognition</font>
    </a>
  </h2>
  <font color="black">そうすることで、LibriSpeechテスト/テスト-その他のセットで、現在の最先端のWER 1.7％/ 3.3％に対して単語誤り率（WER）1.4％/ 2.6％を達成できます。自動音声認識のための半教師あり学習の最近の開発の組み合わせを採用して、Libri-Lightデータセットのラベルなしオーディオを利用してLibriSpeechで最先端の結果を取得します。より正確には、SpecAugmentを使用して騒々しい学生トレーニングを実行します。 wav2vec2.0事前トレーニングを使用して事前トレーニングされた巨大なConformerモデルを使用します。 
[概要] speceeでノイズの多い学生トレーニングを実施します。また、巨大なコンフォーマーモデルを使用してノイズトレーニングを実施します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic multitrack mixing with a differentiable mixing console of
  neural audio effects -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-10-21/eess.AS/paper_13.html">
      <font color="black">Automatic multitrack mixing with a differentiable mixing console of
  neural audio effects</font>
    </a>
  </h2>
  <font color="black">さらに、人間が読み取れる混合パラメーターを生成し、ユーザーが生成された混合を手動で調整または調整できるようにします。提案されたモデルは、限られた数の例でトレーニングでき、入力順序に関して順列不変であり、制限はありません。入力ソースの数..これは、事前にトレーニングされたサブネットワークと重み共有のアプリケーション、および和/差ステレオ損失関数を使用して実現します。 
[概要]この作業は、基礎となるミキシングパラメータの知識がなくても、波形レベルで実世界のデータからマルチトラックミキシング規則を学習する最初のアプローチを示しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-20">
        <br><font color="black">2020-10-20</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
