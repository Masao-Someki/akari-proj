<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-04-22の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Vector Quantized Contrastive Predictive Coding for Template-based Music
  Generation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.SD/paper_0.html">
      Vector Quantized Contrastive Predictive Coding for Template-based Music
  Generation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの貢献は2つあります。最初に、情報の制御を可能にするメカニズムとともに、コードの個別のセットに対する基本単位の意味のある割り当てを学習できる、ベクトル量子化対比予測コーディングと呼ばれる自己監視エンコーディング技術を提案します。これらの学習した離散表現の内容..次に、これらの圧縮表現を使用して、Transformerアーキテクチャの適切な注意パターンを使用してテンプレートシーケンスのバリエーションを生成する方法を示します。学習した離散表現の音楽的意味について説明するバッハの合唱コードを示し、提案された方法により、所定のテンプレートの一貫した高品質のバリエーションを生成できることを示します。 
[要約]バリエーションを生成するという私たちの問題は、監督なしで関連する高レベルの表現を学習する問題と密接に関連しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: The CORAL+ Algorithm for Unsupervised Domain Adaptation of PLDA -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.SD/paper_1.html">
      The CORAL+ Algorithm for Unsupervised Domain Adaptation of PLDA
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      提案された方法は、相関アラインメント（CORAL）として知られている機能ベースのドメイン適応技術に関する以前の研究に触発されました。最先端の話者認識システムは、x-ベクトル（またはi-ベクトル）話者埋め込みフロントを備えています。 -endの後に確率的線形判別分析（PLDA）バックエンドが続きます。このドキュメントで提案されているモデルベースの適応手法をCORAL +と呼びます。 
[要約]この方法には、ラベルの付いていない少量のドメイン内データから学習することが含まれます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-12-26">
        <br>2018-12-26
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Leveraging Cognitive Search Patterns to Enhance Automated Natural
  Language Retrieval Performance -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_0.html">
      Leveraging Cognitive Search Patterns to Enhance Automated Natural
  Language Retrieval Performance
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      また、言語および関連性モデルと比較した場合、単語の埋め込みに基づくモデルのインスタンス化よりも平均平均精度の点で優れたパフォーマンスを示します。遺伝的アルゴリズムに基づく重み付けプロセスにより、概念的な役割タイプに従って用語を強調できます。 ..このために、ユーザーの検索動作を模倣する認知的再定式パターンが強調表示され、検索プロセスで採用された元の用語と統計的に同じまたは語彙的に関連する拡張用語が強調表示されます。 
[ABSTRACT]自動検索システムは人間の検索動作を完全にエミュレートする必要があるという意味で、これらの取り組みは結合されるべきであると考えています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: TAPAS: Weakly Supervised Table Parsing via Pre-training -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_1.html">
      TAPAS: Weakly Supervised Table Parsing via Pre-training
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      3つの異なるセマンティック解析データセットを実験し、TAPSがSQAの最先端の精度を55.1から67.2に改善し、WIKISQLの最先端と同等に実行することにより、セマンティック解析モデルよりも優れているか、競合していることを確認します。とWIKITQ、しかしより単純なモデルアーキテクチャで.. TAPASは弱い監視からトレーニングし、テーブルセルを選択し、オプションで対応する集計演算子をそのような選択に適用することで、表記を予測します。論理フォームを生成せずにテーブルを介して。 
[ABSTRACT] tapasは、論理フォームを生成せずにテーブルを介して質問応答するアプローチです。彼は、ウィキペディアからクロールされたテキストセグメントとテーブルの効果的な共同事前トレーニングから予測し、エンドツーエンド-エンド-エンドでトレーニングされます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-05">
        <br>2020-04-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Knowledge Distillation for Multilingual Unsupervised Neural Machine
  Translation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_2.html">
      Knowledge Distillation for Multilingual Unsupervised Neural Machine
  Translation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      経験的調査結果に基づいて、多言語UNMTのパフォーマンスをさらに向上させる2つの知識抽出方法を提案します。教師なし神経機械翻訳（UNMT）は、最近、いくつかの言語ペアで驚くべき結果を達成しました。他の12の言語（3つの言語ファミリと6つの言語ブランチを含む）からは、驚くべき結果が示され、教師なしの強力な個々のベースラインを上回りながら、ゼロショット翻訳シナリオで英語以外の言語ペア間で有望なパフォーマンスを達成し、リソースの少ない言語ペアのパフォーマンス低下を軽減しています。 
[要約]教師なしの研究により、unmtは単一の言語ペア間でしか翻訳できず、同時に複数の言語ペアの結果しか生成できないことがわかった
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: User Generated Data: Achilles' Heel of BERT -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_3.html">
      User Generated Data: Achilles' Heel of BERT
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、このパフォーマンス低下の原因となっているBERTパイプラインの欠点を特定します。実験では、3つのよく知られたデータセット（IMDBムービーレビュー、SST-2およびSTS-B）を使用してパフォーマンスを測定します。BERTのおかげでさまざまなNLPタスクとベンチマークデータセットで驚異的な成功を収めた業界の専門家は、BERTを組み込んでアプリケーションを構築し、業界のユースケースを解決する実験を始めました。 
[要約]産業用nlpアプリケーションは、ベンチマークデータセットと比較して、はるかにノイズの多いデータを処理することが知られています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-29">
        <br>2020-03-29
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Combining Pretrained High-Resource Embeddings and Subword
  Representations for Low-Resource Languages -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_4.html">
      Combining Pretrained High-Resource Embeddings and Subword
  Representations for Low-Resource Languages
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの調査では、事前学習済みと形態学的情報の両方の単語埋め込みを組み合わせたメタ埋め込みアプローチが、コーサ語英語翻訳の下流のタスクで最高に機能することを示しています。この問題を回避するために、形態学的に豊富な言語の品質を利用する手法を探ります（MRL）、リソースの豊富な言語で事前トレーニングされた単語ベクトルを活用します。現在の自然言語処理（NLP）技術では大量のデータが必要なことと、不足していることの違いは、アフリカの言語の場合に強調されます。そのほとんどは低リソースと見なされます。 
[ABSTRACT]生態学的に豊かな言語の品質を活用する手法を探ります。また、十分にリソースのある言語で事前トレーニング済みの単語センターを使用します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-09">
        <br>2020-03-09
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Neural Abstractive Summarization with Structural Attention -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_5.html">
      Neural Abstractive Summarization with Structural Attention
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      注意、RNNベースのエンコーダー/デコーダーアーキテクチャは、ニュース記事の抽象的要約で印象的なパフォーマンスを達成しています。人気のあるポインタージェネレーターアーキテクチャーとそれから派生した一部のアーキテクチャーをベースラインとして設定し、それらが良い要約を生成できないことを示しています。マルチドキュメント設定..ただし、これらのメソッドは、ドキュメントの文内の長期的な依存関係を説明できません。 
[ABSTRACT]これらの方法は、ドキュメントの文内の長期的な依存関係を説明できません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Train No Evil: Selective Masking for Task-guided Pre-training -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_6.html">
      Train No Evil: Selective Masking for Task-guided Pre-training
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      具体的には、トークンの重要性を最終的な分類結果への影響として定義し、ニューラルモデルを使用して暗黙的な選択ルールを学習します。タスク固有の言語パターンを効率的に学習するために、従来のランダムではなく選択的マスキング戦略を採用しますマスキング、つまり下流のタスクにとって重要なトークンのみをマスキングすることを意味します。このホワイトペーパーでは、選択的なマスキングタスクに基づく事前トレーニング方法を提案し、それを一般的な事前トレーニングと微調整の間に追加します。 
[ABSTRACT] 2ステージモデルは通常、事前特定およびタスク固有の言語パターンをキャプチャできません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: The Ivory Tower Lost: How College Students Respond Differently than the
  General Public to the COVID-19 Pandemic -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_7.html">
      The Ivory Tower Lost: How College Students Respond Differently than the
  General Public to the COVID-19 Pandemic
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      3月7日にワシントン大学が閉鎖された後、米国の1000を超えるカレッジや大学が直接授業やキャンパス活動をキャンセルし、数百万人の学生に影響を与えました。これは私たちの知る限り、最初のソーシャルメディアです。大規模な危機の際に大学生のコミュニティの人口統計と一般的な社会問題への対応に焦点を当てたベースの研究。パンデミックに関連する最も中心的な問題を表す多数のCOVID-19ツイートに埋め込まれたいくつかのトピックを発見しました。大学生と一般市民の両方にとって大きな懸念の。 
[ABSTRACT]アメリカ合衆国、covid-19で最大の感染例が確認された国では、大統領によって社会的隔離プロトコルが実施されました。公共の場やサービスの大多数が運営を停止しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Discrete Variational Attention Models for Language Generation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_8.html">
      Discrete Variational Attention Models for Language Generation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちのアプローチは、言語生成の潜在空間を強化できる観測からの順次依存性をキャプチャする前に、自動回帰と組み合わされます。さらに、共通のガウス分布を使用して、連続空間に対する離散潜在空間の優位性を慎重に分析します。さらに、離散性の性質のおかげで、提案されたアプローチのトレーニングは、後方崩壊の影響を受けません。 
[要約]提案されたアプローチは、エンコーダーからの最後の非表示状態のみが潜在空間に変換されるという事実に基づいています。これは、データを要約するには不十分です。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: AriEL: volume coding for sentence generation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_9.html">
      AriEL: volume coding for sentence generation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ただし、予測と表現の滑らかさを同時に最適化しているため、2つの間のトレードオフが強制されます。次に、人間の対話の実際のデータセットでベンチマークを行います。離散データのシーケンスのマッピング連続空間内のポイントは、ランダムサンプリングによってこれらのシーケンスを取得することを困難にします。 
[ABSTRACT]私たちの方法arielは、潜在的な空間をランダムにサンプリングすることで、さまざまな正しい言語を生成できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-30">
        <br>2020-03-30
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Adaptive Interaction Fusion Networks for Fake News Detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_10.html">
      Adaptive Interaction Fusion Networks for Fake News Detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この論文では、偽のニュース検出のための機能間の相互相互作用融合を実現するために、適応相互作用融合ネットワーク（AIFN）を提案します。機能の関連付けを確立するために、意味相関を強化する意味レベルの融合自己注意ネットワーク（SFSN）を考案し、機能間の融合..偽ニュースの検出のための既存の方法の大部分は、検出のためにさまざまな機能の学習と融合に普遍的に焦点を当てています。 
[ABSTRACT] aifnでは、ゲーテッドアダプティブインタラクションネットワーク（ゲイン）を設計します。これらは、ニュースと、投稿とコメントの間の競合するニュースパターンを組み合わせます。これにより、ソーシャルメディア上の機能間の相互作用の融合が不足します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Considering Likelihood in NLP Classification Explanations with Occlusion
  and Language Modeling -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_11.html">
      Considering Likelihood in NLP Classification Explanations with Occlusion
  and Language Modeling
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最近、最先端のNLPモデルは言語の構文的および意味論的な理解を深めるようになり、説明方法はその決定を理解するために非常に重要です。 NLPの他の説明方法のこれらの弱点を緩和し、オクルージョンベースの説明でデータの可能性を考慮することの重要性を強調する結果を提供する理論的基礎をレイアウトします。 
[要旨] olmを提案します。これは、オクルージョンモデルと言語モデルを組み合わせて、有効で構文的に正しい構文サンプルを抽出する新しい説明方法です。現在のオクルージョンベースの方法では、無効または構文的に正しくない言語データが生成されることが多く、最近のnlpモデルの改善された機能を無視しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Contextual Neural Machine Translation Improves Translation of Cataphoric
  Pronouns -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_12.html">
      Contextual Neural Machine Translation Improves Translation of Cataphoric
  Pronouns
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      また、ターゲット化されたカタフォラテストスイートの評価を行い、BLEUの観点から、コンテキストにとらわれないトランスフォーマーを大幅に上回る結果を報告します。コンテキスト認識NMTの出現により、全体的な翻訳品質、特に代名詞などの談話現象。この作業では、未来の文脈で訓練された文脈NMTモデルと過去の文脈で訓練されたもののパフォーマンスを比較することにより、文脈としての将来の文の効果を調査します。 
[要約]将来のコンテキストの使用により、コンテキストが大幅に改善されました。また、過去のコンテキストでトレーニングされた対応物よりも改善されました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: TD-GIN: Token-level Dynamic Graph-Interactive Network for Joint Multiple
  Intent Detection and Slot Filling -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_13.html">
      TD-GIN: Token-level Dynamic Graph-Interactive Network for Joint Multiple
  Intent Detection and Slot Filling
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      包括的な分析により、SLUのパフォーマンスを向上させるために、フレームワークが複数の関連するインテント情報を正常にキャプチャすることが実証的に示されています。グラフの相互作用メカニズムにより、フレームワークには、関連するインテント情報を自動的に抽出して、各トークンスロットの予測を導き、きめの細かいインテントを作成するという利点があります。トークンレベルのスロット予測のための情報の統合。2つのマルチインテントデータセットでの実験により、このモデルは最先端のパフォーマンスを達成し、他の以前の方法を大幅に上回っています。 
[ABSTRACT] sluデータセットは、モデルが最先端のパフォーマンスを達成し、他の以前の方法を大幅に上回っていることを示しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Enhancing Machine Translation with Dependency-Aware Self-Attention -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_14.html">
      Enhancing Machine Translation with Dependency-Aware Self-Attention
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      WMTの英語-ドイツ語、英語-トルコ語、およびWATの英語-日本語の翻訳タスクに対する各アプローチの有効性を示します。ほとんどのニューラル機械翻訳モデルは、構文情報が注意メカニズムによって自動的に学習されると仮定して、対の文にのみ依存しています。 。この作業では、Transformerモデルに構文知識を組み込むためのさまざまなアプローチを調査し、特に長い文やリソース不足のシナリオで、翻訳の品質を向上させる、新しいパラメーターのない依存関係認識自己注意メカニズムを提案します。 。 
[ABSTRACT]これは、構文知識と呼ばれるトランスフォーマーモデルの働きです。また、特に長い文やリソースが少ない状況で、翻訳品質を向上させる新しいリソースメカニズムを提案しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-06">
        <br>2019-09-06
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Experience Grounds Language -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_15.html">
      Experience Grounds Language
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      自然言語処理は多様な分野であり、その開発全体の進歩は、新しい表現理論、モデリング手法、データ収集パラダイム、およびタスクからもたらされています。コンテキスト情報が私たちの表現にどのように影響するかについて、簡単な歴史と可能な進行について説明します。この統合がフィールドをどのように前進させ、現在開拓しているところに目を向けています。この記事では、言語のコンテキストの基礎である、グラウンディング、具現化、社会的相互作用についての作業を検討します。 
[ABSTRACT]言語処理モデルは、言語を新しい世界に結び付けることができませんでした。しかし、今日の最高のシステムは、言語を関連付けることに失敗することから生じる間違いを犯しています。大きなテキストコーパスでトレーニングされた表現学習アプローチの成功は、深いものになると信じています言語の社会的性質に関する研究の平行した伝統から豊かになった
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Knowledge-Driven Distractor Generation for Cloze-style Multiple Choice
  Questions -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_16.html">
      Knowledge-Driven Distractor Generation for Cloze-style Multiple Choice
  Questions
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このデータセットは、将来のディストラクタ生成のベンチマークとしても使用できます。4つのドメインにまたがるデータセットの実験結果は、私たちのフレームワークが以前の方法よりももっともらしく信頼できるディストラクタを生成することを示しています。このホワイトペーパーでは、新しい構成可能なオープンドメインクローズスタイルの多肢選択式質問の注意散漫な選択肢を自動的に生成するフレームワーク。汎用的な知識ベースを組み込んで効果的に小さな注意散漫候補セットを作成し、機能豊富な学習からランク付けモデルを使用して、もっともらしくて信頼できる。 
[ABSTRACT]調査によると、texting.distractedorsは以前の方法よりも混乱している可能性が高い
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Relabel the Noise: Joint Extraction of Entities and Relations via
  Cooperative Multiagents -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_17.html">
      Relabel the Noise: Joint Extraction of Entities and Relations via
  Cooperative Multiagents
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ノイズの多いインスタンスをきめ細かく処理するために、協力グループの各エージェントは、独自の観点から継続的な信頼スコアを計算することによってインスタンスを評価します。これら2つの抽出タスク間の相関関係を活用するために、信頼性コンセンサスモジュールは、すべてのエージェントの知恵を収集し、信頼性スコア付きラベルでノイズの多いトレーニングセットを再配布するように設計されています。2つの実際のデータセットの実験結果は、ノイズの多いインスタンスを再ラベル付けし、提案されたモデルが最先端のエンティティおよび関係抽出方法を大幅に上回っていることを示します。さらに、信頼度は、抽出器のトレーニング損失を調整するために使用されます。 
[ABSTRACT]調査により、提案されたモデルは最先端のエンティティおよび関係抽出方法を大幅に上回っています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Unsupervised Opinion Summarization with Noising and Denoising -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_18.html">
      Unsupervised Opinion Summarization with Noising and Denoising
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      いくつかの言語的に動機付けられたノイズ生成関数と、入力のノイズを除去し、元のレビューを生成することを学習する要約モデルを紹介します。このホワイトペーパーでは、利用可能なドキュメントのみがある設定（例：〜productまたはビジネスレビュー）。グラウンドトゥルーサマリーなし。テスト時に、モデルは本物のレビューを受け入れ、コンセンサスに達しないものをノイズとして扱い、顕著な意見を含むサマリーを生成します。 
[ABSTRACT]レビューをサンプリングすることにより、ユーザーレビューのコーパスから合成データセットを作成します。これは事実です。データのいくつかの例は、さまざまなテスト方法を使用して作成されます。モデルは、本物のレビューを受け入れ、要約を生成し、それらを処理しますノイズとしてコンセンサスに達しない
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Attention Module is Not Only a Weight: Analyzing Transformers with
  Vector Norms -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_19.html">
      Attention Module is Not Only a Weight: Analyzing Transformers with
  Vector Norms
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      BERTとTransformerベースのニューラル機械翻訳システムの自己注意モジュールの分析では、注意モジュールが以前の調査結果とは逆に、非常に直感的に動作することが示されています。特別なトークンに多くの注意を払い、（2）Transformerの注意モジュールは単語の配置を非常によくキャプチャします。この研究では、注意の重みだけが自己注意モジュールの出力を決定する2つの要因の1つにすぎないことを指摘し、他の因子、つまり変換された入力ベクトルを分析に組み込むこと。 
[要約]これまでの研究では、主に注意の重みを分析して、注意モジュールが各入力から収集して出力を生成する情報量を確認していました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Making Monolingual Sentence Embeddings Multilingual using Knowledge
  Distillation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_20.html">
      Making Monolingual Sentence Embeddings Multilingual using Knowledge
  Distillation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      トレーニングは、翻訳された文を元の文と同じベクトル空間の場所にマッピングする必要があるという考えに基づいています。文の埋め込みモデルを400を超える言語に拡張するコードが公開されています。さまざまな言語ファミリの10言語のアプローチ。 
[要旨]元のモデルを使用してソース言語の文の埋め込みを生成します。次に、翻訳された文で新しいシステムをトレーニングして、元のモデルを模倣します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Learning Relation Ties with a Force-Directed Graph in Distant Supervised
  Relation Extraction -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_21.html">
      Learning Relation Ties with a Force-Directed Graph in Distant Supervised
  Relation Extraction
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この問題を解決するために、この論文では、関係の結びつきを包括的に学習するための新しい力指向グラフベースの関係抽出モデルを提案します。得られた関係表現は、相互依存関係分類子として適用されます。 
[ABSTRACT]既存のモデルは、ローカルの依存関係を反発することによって定義されます。ただし、ローカルの最適なソリューションに簡単に該当する場合があります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Keyphrase Generation with Cross-Document Attention -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_22.html">
      Keyphrase Generation with Cross-Document Attention
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      提案されたTransformer +クロスドキュメントアテンションアーキテクチャに加えて、コピーメカニズムを採用して、ドキュメントから適切な単語を選択し、キーフレーズ内の語彙外の単語を処理することでモデルを拡張します。5つのベンチマークデータセットの実験結果は、すべてのデータセットで最先端のパフォーマンスを実現するモデルの有効性と有効性。さらなる分析により、提案されたモデルは、十分な多様性を保ちながら、参照と一致するキーフレーズを生成できることが確認されています。 
[要約]提案されたモデルは、入力ドキュメントのキーフレーズを生成するように設計されています。これにより、ドキュメント間の依存関係や潜在的なトピックなど、他の同様のドキュメントによって伝達される重要なコーパスレベルの情報が省略されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Beyond Optimizing for Clicks: Incorporating Editorial Values in News
  Recommendation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_23.html">
      Beyond Optimizing for Clicks: Incorporating Editorial Values in News
  Recommendation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちは、1か月半にわたる1,200人を超えるユーザーを対象とするニュースレコメンダーシステムを使用して、2つのオンライン調査を実施し、提示します。 。私たちは、（i）非パーソナライズされた編集ランキングと比較して、推奨システムがより多様な読書行動をもたらし、記事のカバレッジを高くすること、（ii）再ランキング方法として推奨システムにダイナミズムをうまく組み込めること、推奨システムの精度を損なうことなく、より動的な記事に読者を効果的に誘導します。 
[ABSTRACT]論文では、報道機関の編集値に照らして自動化されたニュースレコメンダーシステムを研究しています。自動化されたニュースレコメンダーは、より多様な読書行動を生み出すことがわかりました。ランキング方式、推奨システムの精度を損なうことなく、より動的な記事に読者を効果的に誘導します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Learning Goal-oriented Dialogue Policy with Opposite Agent Awareness -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_24.html">
      Learning Goal-oriented Dialogue Policy with Opposite Agent Awareness
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      動作から反対エージェントのポリシーを推定し、この推定を使用して、ターゲットポリシーの一部と見なしてターゲットエージェントを改善します。目標指向の対話ポリシー学習のほとんどの既存のアプローチは、ターゲットエージェントポリシーに焦点を当てた強化学習を使用しました環境の一部として反対のエージェントのポリシーを単に扱います。したがって、目標指向の対話におけるポリシー学習のための反対の行動を意識したフレームワークを提案します。 
[要約]反対側のエージェントの動作は、特定のパターンを示すか、隠れたポリシーの基礎となることがよくあります。これは、ターゲットエージェントが推測して使用し、独自の意思決定を容易にすることができます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Reducing Gender Bias in Neural Machine Translation as a Domain
  Adaptation Problem -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_25.html">
      Reducing Gender Bias in Neural Machine Translation as a Domain
  Adaptation Problem
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      推論中に、一般的なテストセットBLEUの低下なしに、WinoMTのStanovsky et al（2019）で評価されたすべてのシステムよりも優れた格子リスコアリングスキームを提案し、このスキームを適用して、 `black box`オンライン商用MTシステム..新しいドメインでの転移学習の既知の落とし穴は「破局的な忘却」です。これは、適応と推論の両方で対処します。英語から3つの言語へのさまざまな言語特性とデータへの変換のアプローチを示します可用性。 
[ABSTRACT]ジェンダーバイアスは、特にターゲット言語に文法上の性別がある場合、翻訳の品質を低下させることが示されています。トレーニングの前にすべてのデータのバイアスを解除するだけで、システムバイアスを軽減する必要がありますが、これを効果的に達成すること自体が課題です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-09">
        <br>2020-04-09
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Word Embedding-based Text Processing for Comprehensive Summarization and
  Distinct Information Extraction -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_26.html">
      Word Embedding-based Text Processing for Comprehensive Summarization and
  Distinct Information Extraction
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      収集された回答は効果的にクラスター化され、顧客が尋ねる可能性のある単一の質問に対する複数の異なる回答を見つけます。これは、文章を数値ベクトルに変換し、類似度に基づくコミュニティー検出アルゴリズムを使用してそれらをクラスター化することによって実行されます。フレームワークは、既存のレビュー処理ソリューションよりも包括的であることが示されています。 
[ABSTRACT]数値的な最初のフレームワークは、重要な文を抽出してレビューデータセットを要約することです。これは、レビューに基づく一連のカテゴリに基づいています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Curriculum Pre-training for End-to-End Speech Translation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_27.html">
      Curriculum Pre-training for End-to-End Speech Translation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これに触発され、筆記学習の初級コースと、2つの言語での発話とマッピング単語を理解するための2つの上級コースを含むカリキュラムの事前トレーニング方法を提案します。これらのコースの難易度は徐々に高まっています。エンドスピーチ翻訳は、クロスリンガルセマンティクスを同時に転記、理解、学習する必要があるため、エンコーダに大きな負担をかけます。 
[要約]私たちは、2つの言語で発話を理解し、単語をマッピングするために、初級コースと2つの言語コースを含むカリキュラムの事前トレーニング方法を提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: BERT-ATTACK: Adversarial Attack Against BERT Using BERT -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_28.html">
      BERT-ATTACK: Adversarial Attack Against BERT Using BERT
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このホワイトペーパーでは、BERTに代表される事前トレーニング済みのマスクされた言語モデルを使用して敵対サンプルを生成する高品質で効果的な方法である\ textbf {BERT-Attack}を提案します。成功率と摂動パーセンテージの両方における最新の攻撃戦略。生成された敵対的なサンプルは流暢で、意味的に保存されます。BERTは、微調整されたモデルと他のダウンストリームタスク用のディープニューラルモデルに反しています。 
[ABSTRACT]現在、テキストに対する攻撃手法は、通常、文字または単語レベルでヒューリスティック置換戦略を採用しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: DIET: Lightweight Language Understanding for Dialogue Systems -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_29.html">
      DIET: Lightweight Language Understanding for Dialogue Systems
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの最高のパフォーマンスモデルはBERTの微調整よりもパフォーマンスが高く、トレーニングが約6倍高速です。エンティティトランスフォーマー（DIET）アーキテクチャ、および2つの一般的な対話言語理解タスクである、意図とエンティティ予測に関するさまざまな事前トレーニング済み表現の有効性を研究します。 
[ABSTRACT]私たちは、二重の意図とエンティティトランスフォーマー（ダイエット）アーキテクチャを紹介します。意図とエンティティの予測に関する2つの事前トレーニング済みモデルの有効性を調査します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Learning to Encode Evolutionary Knowledge for Automatic Commenting Long
  Novels -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_30.html">
      Learning to Encode Evolutionary Knowledge for Automatic Commenting Long
  Novels
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      広範な実験結果は、私たちのEKGベースの方法が自動評価と人間評価の両方でいくつかの強力なベースラインより優れていることを示しています。コメントする特定のパッセージが与えられた場合、逐次モデリングを使用して、コンテキスト表現の履歴と将来の埋め込みを組み込みます。 -to-sequenceモデルは、EKGをコメント生成に利用するように設計されています。 
[ABSTRACT]静的知識グラフは、動的ストーリーラインをモデル化するために必要な知識の進化を表すことができませんでした
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Logic-Guided Data Augmentation and Regularization for Consistent
  Question Answering -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/cs.CL/paper_31.html">
      Logic-Guided Data Augmentation and Regularization for Consistent
  Question Answering
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      特に、この方法により、データセット全体でRoBERTaベースのモデルのパフォーマンスが1〜5％向上します。WIQAおよびQuaRelで最先端の技術を5〜8％向上させ、HotpotQAで一貫性違反を58％削減します。 。私たちの方法は、論理的および言語学的知識を活用して、ラベル付けされたトレーニングデータを補強し、一貫性ベースの正規化機能を使用してモデルをトレーニングします。 
[要旨]私たちのアプローチは、以前の方法に比べて大幅な改善を実現しています。wiqaとquarelで最先端の技術を約5〜8％向上させます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: MatchboxNet: 1D Time-Channel Separable Convolutional Neural Network
  Architecture for Speech Commands Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/eess.AS/paper_0.html">
      MatchboxNet: 1D Time-Channel Separable Convolutional Neural Network
  Architecture for Speech Commands Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      MatchboxNetはフットプリントが小さいため、計算リソースが限られているデバイスにとって魅力的な候補になります。モデルは非常にスケーラブルであるため、適度なメモリと計算を追加することでモデルの精度を向上させることができます。MatchboxNetは、 Google音声コマンドのデータセットで、類似モデルよりもパラメーターが大幅に少ない。 
[ABSTRACT] matchboxnetは、1d時間のブロックで構成される深い残余ネットワークです。matchboxの小さなフットプリントは、限られたコンピューティングリソースを持つデバイスの魅力的な候補になります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-18">
        <br>2020-04-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Speaker Verification By Partial AUC Optimization With Mahalanobis
  Distance Metric Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/eess.AS/paper_1.html">
      Speaker Verification By Partial AUC Optimization With Mahalanobis
  Distance Metric Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、提案されたバックエンドが7つの評価指標の点で最先端のスピーカー検証バックエンドよりも優れていることを示しています。マハラノビス距離メトリック学習ベースのバックエンドは、pAUCを最適化するために適用され、マハラノビス距離計量学習は、バックエンドの最適化目標が凸状のものであることを保証するため、全体的な最適解が達成可能です。しかし、実際の話者検証システムは、通常、与えられたROC曲線全体ではなく、ROC曲線の一部で機能します。アプリケーション。 
[要約] roc曲線の一部の下にあるシステム（pauc）は、話者検証のためのより効率的な評価指標として使用されます。提案されたバックエンドは、7つの評価指標の点で、話者検証バックエンドの状態よりも優れています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-02-03">
        <br>2019-02-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Vector Quantized Contrastive Predictive Coding for Template-based Music
  Generation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/eess.AS/paper_2.html">
      Vector Quantized Contrastive Predictive Coding for Template-based Music
  Generation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      JSのコーパスに対するアプローチを示します。次に、これらの圧縮表現を使用して、Transformerアーキテクチャで適切な注意パターンを使用することにより、テンプレートシーケンスのバリエーションを生成する方法を示します。私たちの貢献は2つあります。最初に、Vectorという名前の自己監視エンコーディング手法を提案します量子化されたコントラスト予測コーディング。これにより、コードの個別のセットに対する基本単位の意味のある割り当てを、これらの学習された離散表現の情報コンテンツを制御できるメカニズムとともに学習できます。 
[要約]バリエーションを生成するという私たちの問題は、監督なしで関連する高レベルの表現を学習する問題と密接に関連しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: The CORAL+ Algorithm for Unsupervised Domain Adaptation of PLDA -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/eess.AS/paper_3.html">
      The CORAL+ Algorithm for Unsupervised Domain Adaptation of PLDA
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      提案された手法の有効性は、最近のNIST 2016および2018話者認識評価（SRE&#39;16、SRE&#39;18）データセットで実験的に検証されています。このホワイトペーパーで提案されているモデルベースの適応手法をCORAL +と呼びます。この方法は、相関アラインメント（CORAL）として知られている機能ベースのドメイン適応技術に関する以前の研究に触発されました。 
[要約]この方法には、ラベルの付いていない少量のドメイン内データから学習することが含まれます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-12-26">
        <br>2018-12-26
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: A thymus tumour impairing hibernation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-22/biorxiv.physiology/paper_0.html">
      A thymus tumour impairing hibernation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この方法を使用して、胸腺腫瘍を持つ動物の冬眠からの再加温が損なわれることを示しました。その原因として考えられるのは、肺と心臓の閉塞であり、換気と循環が非効率的です。結果：BATで温度が並行して監視されました（IPTTタグ）と冬眠の覚醒覚醒サイクリング中の腹腔（iButtons）。コアの体温の再加温は6.2 * h-1、BATの再加温は12 * h-1減少しました。 
[ABSTRACT]これは、代謝低下と体温の低下（不眠）および再加温（覚醒）のサイクルを通じて達成されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br>2020-04-21
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
