<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
<title>Akari-2019-11-15の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
  <div class="header-logo">
    <a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
  </div>
</header>
<input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">

<div class="menubar">
<span class="bar"></span>
<span class="bar"></span>
<span class="bar"></span>
</div>

<ul>
<li><a id="home" href="../../index.html">Home</a></li>
<li><a id="about" href="../../teamAkariとは.html">About</a></li>
<li><a id="contact" href="../../contact.html">Contact</a></li>
<li><a id="contact" href="../../list/newest.html">New Papers</a></li>
<li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
</ul>

</label>
<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Listening while Speaking and Visualizing: Improving ASR through
  Multimodal Chain -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.SD/paper_0.html">
      Listening while Speaking and Visualizing: Improving ASR through
  Multimodal Chain
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      上記の音声チェーンにより、ペアデータの全量の要件が緩和されますが、この場合、未ペアデータが大量に必要になります。フレームワークにより、多数の並列マルチモーダルデータを必要とせずに各コンポーネントをトレーニングできます。また、実験結果は、音声とテキストデータなしでASRをさらにトレーニングでき、提案されたチェーンを通じてクロスモーダルデータの拡張が可能であり、ASRのパフォーマンスが向上することも示しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-03">
        <br>2019-06-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Learning with Learned Loss Function: Speech Enhancement with Quality-Net
  to Improve Perceptual Evaluation of Speech Quality -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.SD/paper_1.html">
      Learning with Learned Loss Function: Speech Enhancement with Quality-Net
  to Improve Perceptual Evaluation of Speech Quality
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、学習したサロゲート関数が強化モデルを導き、PESQスコアをさらに向上させ（MSE損失でトレーニングした結果と比較して0.18ポイント増加）、音声明瞭度を維持できることを示しています。 -知覚に関連するメトリックは、音声品質の知覚的評価（PESQ）であり、人間によって評価された品質スコアと高い相関関係を提供することが証明されています。最近人気のあるトピックになっています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-05-06">
        <br>2019-05-06
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Using musical relationships between chord labels in automatic chord
  extraction tasks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.SD/paper_2.html">
      Using musical relationships between chord labels in automatic chord
  extraction tasks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      結果の詳細な分析を実行することにより、統計モデルに基づいてACEタスクに関する一連の関連する洞察を明らかにし、またいくつかの分類エラーの音楽的意味を定式化します。ただし、ほとんどのモデルは以前のラベリングアルファベット（コードラベル）の基礎となる知識。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-12">
        <br>2019-11-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Speaker independence of neural vocoders and their effect on parametric
  resynthesis speech enhancement -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.SD/paper_3.html">
      Speaker independence of neural vocoders and their effect on parametric
  resynthesis speech enhancement
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらのボコーダーはどちらもスピーカーに依存しています。次に、これら2つのボコーダーと新しいボコーダーLPCNetを使用して、目に見えないスピーカーでのPRのノイズ低減品質を評価し、客観的な信号と全体の品質が最新技術よりも高いことを示します音声強調システムWave-U-Net、Wavenet-denoise、およびSEGAN ..ここでは、十分なスピーカーからのデータを学習すると、これらのボコーダーが男性と女性の両方の目に見えないスピーカーから見たスピーカーと同じ品質のスピーチを生成できることを最初に示します研修中。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-14">
        <br>2019-11-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Coincidence, Categorization, and Consolidation: Learning to Recognize
  Sounds with Minimal Supervision -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.SD/paper_4.html">
      Coincidence, Categorization, and Consolidation: Learning to Recognize
  Sounds with Minimal Supervision
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この動機により、（i）ユニモーダルおよびクロスモーダル一致の一般的な概念に基づく自己監視型目標、（ii）カテゴリを課す必要性を反映するクラスタリング目標を組み合わせた音声表現および認識の学習フレームワークを提示します（iii）対象を絞った弱い監視を要求して、関連するセマンティッククラスにカテゴリを統合するクラスターベースのアクティブな学習手順。人間は、機械のトレーニング方法で知覚能力を獲得しません。 / clustering / classification network、これらの基準に従って、新しい最先端の教師なし音声表現を実現し、目的の分類パフォーマンスに到達するために必要なラベルの数を最大20倍削減します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-14">
        <br>2019-11-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Scene-Aware Audio Rendering via Deep Acoustic Analysis -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.SD/paper_5.html">
      Scene-Aware Audio Rendering via Deep Acoustic Analysis
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      インタラクティブな幾何学的な音の伝搬を使用したオーディオレンダリングに推定された音響材料特性を使用し、多くの実際のシナリオでのパフォーマンスを強調します。これらの推定値は、新しい材料最適化目標を使用して部屋の残響に関連する材料特性を計算するために使用されます。録音されたオーディオから部屋の残響時間と等化を推定するディープニューラルネットワークに基づいています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-14">
        <br>2019-11-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Parametric Resynthesis with neural vocoders -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.SD/paper_6.html">
      Parametric Resynthesis with neural vocoders
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、WaveNetとWaveGlowは、オラクルWienerマスクよりも大幅に優れた主観品質評価を達成します。さらに、WaveNetとWaveGlowの間で、WaveNetは最高の主観品質スコアを達成しますが、波形生成は非常に遅くなります。 WaveGlowは、ソース分離モデルChimera ++よりも優れた主観的および客観的な品質スコアを達成します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-16">
        <br>2019-06-16
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Iterative Answer Prediction with Pointer-Augmented Multimodal
  Transformers for TextVQA -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.CL/paper_0.html">
      Iterative Answer Prediction with Pointer-Augmented Multimodal
  Transformers for TextVQA
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この作業では、画像内のテキストの豊富な表現を伴うマルチモーダルトランスフォーマアーキテクチャに基づくTextVQAタスクの新しいモデルを提案します。さらに、動的なポインタネットワークを使用した反復的な回答のデコードが可能になり、モデルが1段階の分類ではなく多段階の予測を通じて回答を形成できるようになります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-14">
        <br>2019-11-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Ethanos: Lightweight Bootstrapping for Ethereum -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.CL/paper_1.html">
      Ethanos: Lightweight Bootstrapping for Ethereum
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      イーサリアム（geth）クライアントを装備してエタノを実装し、イーサリアムの7Mブロックから8Mブロックまでの1,400万口座からの実際の1億1,300万トランザクションで評価しました。イーサリアムブロックチェーンが普及するにつれて、トランザクションが急増し、そのデータサイズが爆発的に増加しました。これは、通常のクライアントが個人のデバイスでブートストラップするのに十分なものです。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-14">
        <br>2019-11-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Deep Patent Landscaping Model using Transformer and Graph Embedding -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.CL/paper_2.html">
      A Deep Patent Landscaping Model using Transformer and Graph Embedding
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      提案されたモデルは、特許文書に存在するテキストデータを分析するための修正されたトランスフォーマー構造と、特許メタデータを分析するためのDiff2Vecと呼ばれる拡散グラフを使用したグラフ埋め込み方法を含みます。特許造園のプロセスにはいくつかの高度なリソースが必要であり、退屈になる可能性があるため、自動化された特許造園の需要は徐々に増加しています。しかし、明確に定義されたベンチマークの不足データセットと比較可能なモデルにより、関連する調査研究を見つけることが難しくなります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-03-14">
        <br>2019-03-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Listening while Speaking and Visualizing: Improving ASR through
  Multimodal Chain -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.CL/paper_3.html">
      Listening while Speaking and Visualizing: Improving ASR through
  Multimodal Chain
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この研究では、さらに一歩踏み込んでマルチモーダルチェーンを構築し、ASR、TTS、画像キャプション、および画像生成モデルを単一のフレームワークに結合する緊密なチェーンアーキテクチャを設計します。この場合、まだ大量のペアリングされていないデータが必要です。フレームワークにより、多数の並列マルチモーダルデータを必要とせずに各コンポーネントのトレーニングが可能になります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-03">
        <br>2019-06-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Semi-Supervised Natural Language Approach for Fine-Grained
  Classification of Medical Reports -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.CL/paper_4.html">
      Semi-Supervised Natural Language Approach for Fine-Grained
  Classification of Medical Reports
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      機械学習は臨床分析で医師を強化する強力なツールになりましたが、教師付き学習アプローチのトレーニングに必要な膨大な量のラベル付きデータは、時間とリソースを集中的に使用するため、各開発タスクに負担をかけます。画像データを使用して、多数の異なるモダリティを処理できるモデルを作成します。標準モデル開発に自然言語データを使用する際の課題は、モダリティの複雑な性質によるものです。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-29">
        <br>2019-10-29
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Generating Persona Consistent Dialogues by Exploiting Natural Language
  Inference -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.CL/paper_5.html">
      Generating Persona Consistent Dialogues by Exploiting Natural Language
  Inference
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      一貫性は、対話エージェントが直面する主要な課題の1つです。具体的には、ジェネレーターは注意ベースのエンコーダーデコーダーを使用してペルソナベースの応答を生成します。モデルベースの一貫性評価を含む、人間と自動の両方のメトリックに関する実験結果、特に生成された応答のペルソナ一貫性において、提案されたアプローチが強力な生成ベースラインより優れていることを実証します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-14">
        <br>2019-11-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Training a code-switching language model with monolingual data -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.CL/paper_6.html">
      Training a code-switching language model with monolingual data
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      提案されたアプローチは、人工的に生成されたCSデータでCS言語モデルをトレーニングするのと同等またはそれ以上です。RNNベースの言語モデルで出力投影行列を制約および正規化することにより、異なる言語の埋め込みを互いに近づけます。コードスイッチングデータは、コードスイッチング（CS）言語モデルのトレーニングを複雑にします。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-14">
        <br>2019-11-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Neural Naturalist: Generating Fine-Grained Image Comparisons -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.CL/paper_7.html">
      Neural Naturalist: Generating Fine-Grained Image Comparisons
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちは、鳥の写真間のきめの細かい違いを記述する41k文の新しいBirds-to-Wordsデータセットを紹介します。我々の結果は、自然言語を使用した視覚埋め込み空間の違いを説明する神経モデルの有望な可能性と、市民科学者が生物多様性を守る努力を支援するための機械学習
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-09">
        <br>2019-09-09
      </time>
    </span>
  </h3>
</article>
<!-- paper0: The Power of Communities: A Text Classification Model with Automated
  Labeling Process Using Network Community Detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.CL/paper_8.html">
      The Power of Communities: A Text Classification Model with Automated
  Labeling Process Using Network Community Detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この方法は、より正確な会話インテリジェンスおよび他のテキスト分類システムの開発に役立つ可能性があります。ネットワークリンクの重み。テキスト分類は、機械学習と人工知能の研究で最も重要な分野の1つです。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-25">
        <br>2019-09-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Text classification with pixel embedding -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.CL/paper_9.html">
      Text classification with pixel embedding
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      後続の1次元max-over-timeプーリングがこの機能マップに適用され、最終的にテキスト分類を行うために3つの完全に接続されたレイヤーが使用されます。つまり、単語シーケンス上で3次元カーネルをスライドするたびに、畳み込みは$ n $の単語画像をカバーし、スカラーを出力します。いくつかのテキスト分類データセットの実験は、既存の方法と比較して提案モデルを使用すると驚くほど優れたパフォーマンスを示します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-11">
        <br>2019-11-11
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with
  Minimal Resources -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.CL/paper_10.html">
      Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with
  Minimal Resources
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この目的のために、メタ学習アルゴリズムを提示して、特定のテストケースに迅速に適応できる適切なモデルパラメーターの初期化を見つけ、文の類似性を計算することにより、メタトレーニング用の複数の擬似NERタスクを構築することを提案します。豊富なリソースの言語から知識を転送する注釈付きリソースは、名前付きエンティティ認識（NER）の効果的なソリューションです。5言語以上の最小限のリソースで、言語間の名前付きエンティティ認識に関する広範な実験を行います。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-14">
        <br>2019-11-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Answering questions by learning to rank -- Learning to rank by answering
  questions -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.CL/paper_11.html">
      Answering questions by learning to rank -- Learning to rank by answering
  questions
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この記事の貢献は2つあります。ランク付けされたドキュメントは、既製のダウンストリーム意思決定モデルを改善するために使用できるように公開されています。これを達成するために、潜在的に自己注意ベースのニューラルネットワークを導入します正しい答えを予測する目的を最適化しながら、特定の質問に関連する重要度によってドキュメントをランク付けすることを学習します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-02">
        <br>2019-09-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: FAQ-based Question Answering via Knowledge Anchors -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.CL/paper_12.html">
      FAQ-based Question Answering via Knowledge Anchors
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この論文では、質問をよりよく理解し、より適切な回答を取得するために、FAQベースのQAの新しい知識アンカーベースの質問回答（KAQA）フレームワークを提案します。質問回答（QA）は、ユーザーの質問を理解し、適切な回答を見つけることを目的としています。具体的には、KAQAは主に3つの部分で構成されています：ナレッジグラフの構築、クエリのアンカー、クエリとドキュメントのマッチング。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-14">
        <br>2019-11-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Contextual Recurrent Units for Cloze-style Reading Comprehension -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.CL/paper_13.html">
      Contextual Recurrent Units for Cloze-style Reading Comprehension
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、提案されたCRUモデルが、双方向の条件を含む従来のCNNまたはRNNモデル、および両方のタスクのさまざまな最先端システムを大幅に改善できることを示し、他のNLPタスクへの拡張性の将来性を示しています。 ..センテンスレベルとドキュメントレベルのモデリングNLPタスクでCRUモデルをテストしました。センチメント分類と読解力。提案されたCRUは、コンボリューショナルニューラルネットワーク（CNN）をリカレントユニットに注入して、ローカルコンテキストをモデル化する能力を高めます。双方向RNNでも単語のあいまいさを減らします。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-14">
        <br>2019-11-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Prevalence of code mixing in semi-formal patient communication in low
  resource languages of South Africa -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/cs.CL/paper_14.html">
      Prevalence of code mixing in semi-formal patient communication in low
  resource languages of South Africa
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      南アフリカの全国規模の公衆衛生プラットフォームの一部であるMomConnectヘルプデスクのユーザーによって生成された18万2,000の固有の質問で構成されるデータを調べます。このペーパーでは、リソース不足の言語設定でのコード混合の問題に対処します。このデータセット内の約10％のレベルでのコード切り替えの証拠-将来のサービスに課題をもたらす可能性のあるレベル。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-13">
        <br>2019-11-13
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Listening while Speaking and Visualizing: Improving ASR through
  Multimodal Chain -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/eess.AS/paper_0.html">
      Listening while Speaking and Visualizing: Improving ASR through
  Multimodal Chain
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この研究では、さらに一歩踏み込んでマルチモーダルチェーンを構築し、ASR、TTS、画像キャプション、および画像生成モデルを単一のフレームワークに結合する緊密なチェーンアーキテクチャを設計します。認識（ASR）とテキスト音声合成（TTS）を組み合わせて、ペアのないデータを受け取ったときに、半教師あり学習で互いに教え合うことができます。フレームワークにより、多数の並列マルチモーダルを必要とせずに各コンポーネントをトレーニングできますデータ。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-03">
        <br>2019-06-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Learning with Learned Loss Function: Speech Enhancement with Quality-Net
  to Improve Perceptual Evaluation of Speech Quality -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/eess.AS/paper_1.html">
      Learning with Learned Loss Function: Speech Enhancement with Quality-Net
  to Improve Perceptual Evaluation of Speech Quality
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      人間の知覚に関連する目的関数を使用して音声強調モデルをトレーニングすることは、最近人気のあるトピックになりました。実験結果は、学習したサロゲート関数が強化モデルを導き、PESQスコアをさらに高めることができることを示しています（MSE損失でトレーニングされた結果と比較して0.18ポイントの増加） ）音声明瞭度を維持します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-05-06">
        <br>2019-05-06
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Using musical relationships between chord labels in automatic chord
  extraction tasks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/eess.AS/paper_2.html">
      Using musical relationships between chord labels in automatic chord
  extraction tasks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この論文では、統計的なACEモデルの学習を改善するために、コードラベル間の特定のプロパティと関係を活用することを提案します。結果の詳細な分析を行うことにより、ACEタスクに関する一連の関連する洞察を明らかにします統計モデル、またいくつかの分類エラーの音楽的意味を定式化します。さらに、音楽理論に基づいた新しいトレーニング損失を提案します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-12">
        <br>2019-11-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Speaker independence of neural vocoders and their effect on parametric
  resynthesis speech enhancement -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/eess.AS/paper_3.html">
      Speaker independence of neural vocoders and their effect on parametric
  resynthesis speech enhancement
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ここでは、より良い音声強調のために、ニューラルボコーダーの高品質音声生成機能を使用することを提案します。品質は、最先端の音声強調システムWave-U-Net、Wavenet-denoise、SEGANよりも高くなっています。これらのボコーダーはどちらも、伝統的にスピーカーに依存しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-14">
        <br>2019-11-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Coincidence, Categorization, and Consolidation: Learning to Recognize
  Sounds with Minimal Supervision -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/eess.AS/paper_4.html">
      Coincidence, Categorization, and Consolidation: Learning to Recognize
  Sounds with Minimal Supervision
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この動機により、（i）ユニモーダルおよびクロスモーダル一致の一般的な概念に基づく自己監視型目標、（ii）カテゴリを課す必要性を反映するクラスタリング目標を組み合わせた音声表現および認識の学習フレームワークを提示します（iii）対象を絞った弱い監視を要求して、関連するセマンティッククラスにカテゴリを統合するクラスターベースのアクティブな学習手順。人間は、機械のトレーニング方法で知覚能力を獲得しません。 / clustering / classification network、これらの基準に従って、新しい最先端の教師なし音声表現を実現し、目的の分類パフォーマンスに到達するために必要なラベルの数を最大20倍削減します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-14">
        <br>2019-11-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Scene-Aware Audio Rendering via Deep Acoustic Analysis -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/eess.AS/paper_5.html">
      Scene-Aware Audio Rendering via Deep Acoustic Analysis
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      インタラクティブな幾何音響伝播を使用したオーディオレンダリングに推定された音響材料特性を使用し、多くの現実世界のシナリオでのパフォーマンスを強調します。キャプチャされた音声と現実世界の部屋の近似幾何モデルを考えると、新しい学習ベースを提示します音響材料特性を推定する方法。我々のアプローチは、録音されたオーディオから部屋の残響時間と等化を推定するディープニューラルネットワークに基づいています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-14">
        <br>2019-11-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Parametric Resynthesis with neural vocoders -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-11-15/eess.AS/paper_6.html">
      Parametric Resynthesis with neural vocoders
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、WaveNetとWaveGlowの間で、WaveNetは最高の主観品質スコアを達成しますが、波形生成は非常に遅くなります。システムは一般に、品質が低下した出力音声を生成します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-16">
        <br>2019-06-16
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="">
      
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本日更新された論文はありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="">
        <br>
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
