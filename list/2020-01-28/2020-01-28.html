<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
<title>Akari-2020-01-28の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
  <div class="header-logo">
    <a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
  </div>
</header>
<input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">

<div class="menubar">
<span class="bar"></span>
<span class="bar"></span>
<span class="bar"></span>
</div>

<ul>
<li><a id="home" href="../../index.html">Home</a></li>
<li><a id="about" href="../../teamAkariとは.html">About</a></li>
<li><a id="contact" href="../../contact.html">Contact</a></li>
<li><a id="contact" href="../../list/newest.html">New Papers</a></li>
<li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
</ul>

</label>
<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: ML Estimation and CRBs for Reverberation, Speech and Noise PSDs in
  Rank-Deficient Noise-Field -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/cs.SD/paper_0.html">
      ML Estimation and CRBs for Reverberation, Speech and Noise PSDs in
  Rank-Deficient Noise-Field
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ただし、実際の音響シナリオでは、ノイズPSDマトリックスは不明であり、音声および残響PSDとともに推定する必要があります。音声、残響、およびノイズパワースペクトル密度（PSD）。ノイズの多い音響環境。 
[要約]残響除去およびノイズ低減アルゴリズムは通常、いくつかのモデルパラメーターを必要とします。一般的に使用される仮定は、ノイズpsdマトリックスが既知であるということです
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-07-22">
        <br>2019-07-22
      </time>
    </span>
  </h3>
</article>
<!-- paper0: 3-D Feature and Acoustic Modeling for Far-Field Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/cs.SD/paper_1.html">
      3-D Feature and Acoustic Modeling for Far-Field Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらの実験では、提案された3D機能と音響モデリングアプローチにより、ビームフォーミングオーディオでトレーニングされたASRシステムに比べて大幅な改善が提供されます（CHiME-3およびREVERB Challengeデータセットのワードエラー率がそれぞれ10％および9％の相対的な改善が平均されています）。 3-D CNNアーキテクチャにより、エンハンスメントタスクに焦点を合わせた従来のビームフォーミングモデルと比較して、音声認識コストを最適化するマルチチャネル機能の組み合わせが可能になります。多変量自己回帰（MAR）モデリングアプローチを使用した信号、時間、周波数、およびチャネルの3次元すべての相関が活用されます
[要約]マルチチャネル音声信号のチャネル再検証ベースの拡張は、スペクトログラムベースの抽出に使用されますニューラルネットワークの音響モデルの機能。3月の機能は畳み込みニューラルネットワークアーキテクチャに供給され、三次元の共同音響モデリング
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-13">
        <br>2019-11-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Online Spectrogram Inversion for Low-Latency Audio Source Separation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/cs.SD/paper_2.html">
      Online Spectrogram Inversion for Low-Latency Audio Source Separation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、MISIはオフラインで動作しますが、ここでは低遅延ソース分離に適したoMISIと呼ばれるMISIのオンラインバージョンを提案します。これは補聴器アプリケーションなどの重要な要件です。音声信号の時間構造。音声分離タスクで行われた実験は、oMISIがオフラインの対応物と同様に機能することを示し、リアルタイムソース分離の可能性を示しています。 
[要約]複数入力スペクトログラム反転（misi）アルゴリズムは、最近のいくつかの作品でうまく利用されています。元々はヒューリスティックな方法で導入されました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-08">
        <br>2019-11-08
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: What's happened in MOOC Posts Analysis, Knowledge Tracing and Peer
  Feedbacks? A Review -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/cs.CL/paper_0.html">
      What's happened in MOOC Posts Analysis, Knowledge Tracing and Peer
  Feedbacks? A Review
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      使用されたペーパーとリソースの統合セットは、https：//github.com/manikandan-ravikiran/cs6460-Surveyでリリースされています。頭字語の拡張は、付録セクション4.1に追加されています。それぞれ図1および表1に示すとおり。 
[概要]データの分析は、データ分析とデータデータ分析に基づいています。データは、edmの3つの主要なタスクに関する文献レビューで提示されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-27">
        <br>2020-01-27
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Towards a Human-like Open-Domain Chatbot -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/cs.CL/paper_1.html">
      Towards a Human-like Open-Domain Chatbot
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      エンドツーエンドで訓練された最高のパープレキシティトレーニングされたMeenaスコアがSSAで高い（マルチターン評価で72％）という事実は、パープレキシティをより最適化できる場合、86％の人間レベルのSSAが潜在的に到達可能であることを示唆しています。この判断では、良好な会話の重要な要素をキャプチャする、Sensibleness and Specificity Average（SSA）と呼ばれる人間の評価指標を提案します。さらに、Meenaのフルバージョン（フィルタリングメカニズムと調整されたデコード）は79％SSA、23％評価した次にスコアの高いチャットボットよりも高い。 
[要約]私たちの実験は、当惑とssaの間の強い相関関係を示しています。meenaの完全版は79％ssaを記録しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-27">
        <br>2020-01-27
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A (Simplified) Supreme Being Necessarily Exists -- Says the Computer! -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/cs.CL/paper_2.html">
      A (Simplified) Supreme Being Necessarily Exists -- Says the Computer!
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      したがって、この論文は、現代のシンボリックAIテクノロジーが公式の哲学と神学に新しい知識をどのように貢献できるかを示しています。G \ &quot;odel&#39;sの一部、それぞれ。修正された議論は、定量化されたモーダルロジックKで既に有効であることが示されています。高次論理に基づく最新の知識表現と推論技術を活用
[要約]修正された引数は、定量化されたモーダル論理kで既に有効であることが示されている
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-14">
        <br>2020-01-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Scaling Up Online Speech Recognition Using ConvNets -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/cs.CL/paper_3.html">
      Scaling Up Online Speech Recognition Using ConvNets
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      システムには、適切に調整されたハイブリッドASRベースラインのほぼ3倍のスループットがあり、レイテンシーが低く、ワードエラー率も優れています。設計の選択の影響を示すために、スループット、レイテンシー、精度を分析し、これらのメトリックを議論しますユーザー要件に基づいて調整できます。認識機能の効率にとって重要なのは、高度に最適化されたビーム検索デコーダーです。 
[ABSTRACT]将来のコンテキストを制限し、精度を維持しながら待機時間を短縮するために、コアtdsアーキテクチャを改善します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-27">
        <br>2020-01-27
      </time>
    </span>
  </h3>
</article>
<!-- paper0: The POLAR Framework: Polar Opposites Enable Interpretability of
  Pre-Trained Word Embeddings -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/cs.CL/paper_4.html">
      The POLAR Framework: Polar Opposites Enable Interpretability of
  Pre-Trained Word Embeddings
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      一緒に、これらの結果は、パフォーマンスを損なうことなく、単語の埋め込みに解釈可能性を追加できることを示しています。フレームワークの有効性を、さまざまなダウンストリームタスクに展開することで実証します。 ..このフレームワークでは、オラクル、つまり外部ソースによって提供される一連の極次元から最も識別可能な次元を選択することもできます。 
[概要]フレームワークによって選択された解釈可能な次元は、人間の判断と一致します。これらのタスクは、2つの正反対のスケール間の言語の位置に基づいています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-27">
        <br>2020-01-27
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Towards Quantifying the Distance between Opinions -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/cs.CL/paper_5.html">
      Towards Quantifying the Distance between Opinions
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      トピックに関する多数の意見を収集することが容易になりましたが、意見の空間をナビゲートするための自動化ツールが必要になります。ますます、公共政策、ガバナンス、およびビジネス戦略における重要な決定は、より深い理解に依存しています構成メンバーのニーズと意見の比較（例えば、我々は微妙な観察を活用する意見間の類似性をとらえるための新しい距離尺度を提案します。類似した意見は特定の関連する関心のあるエンティティに対する類似した感情の極性を表します。新しい尺度は、意見間の類似性をキャプチャするように設計されています。ニュアンスのある観察を使用します-類似の意見は、特定のエンティティに関する類似の感情の極性を表します-関心のある
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-27">
        <br>2020-01-27
      </time>
    </span>
  </h3>
</article>
<!-- paper0: PMIndia -- A Collection of Parallel Corpora of Languages of India -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/cs.CL/paper_6.html">
      PMIndia -- A Collection of Parallel Corpora of Languages of India
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この論文では、インドの13の主要言語と英語をペアにした並列文で構成される新しい公開コーパス（PMIndia）について説明しました。高品質の機械翻訳（MT）システムやその他の多言語NLPアプリケーション。コーパスには、言語ペアごとに最大56000の文が含まれます。 
[要約]南アジアの多くの言語では、そのようなデータは不足しています。コーパスには、各言語ペアの最大13文が含まれます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-27">
        <br>2020-01-27
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Emotion Recognition for Vietnamese Social Media Text -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/cs.CL/paper_7.html">
      Emotion Recognition for Vietnamese Social Media Text
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      何よりもまず、標準的なベトナム語ソーシャルメディア感情コーパス（UIT-VSMEC）を構築し、正確に6,927の感情注釈付き文を使用して、自然言語処理（NLP）の低リソース言語であるベトナム語の感情認識研究に貢献しています。この研究では、2つの目標を達成しました。次に、UIT-VSMECコーパスで機械学習とディープニューラルネットワークモデルを評価および測定しました。 
[概要]結果は、悲しみ、楽しさ、怒り、嫌悪感、恐怖、驚きなどの表現で示されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-21">
        <br>2019-11-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Retrospective Reader for Machine Reading Comprehension -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/cs.CL/paper_8.html">
      Retrospective Reader for Machine Reading Comprehension
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      人間がどのように読解の質問を解決するかに触発されて、私たちは2段階の読解と検証戦略を統合するレトロスペクティブリーダー（レトロリーダー）を提案しました。 2）答えを検証し、最終的な予測を与える集中的な読み..機械読解（MRC）は、特定のパッセージに基づいて質問に対する正しい答えを機械が判断することを機械に要求するAIチャレンジです。強力なALBERTベースラインよりも優れています。 
[要旨]提案された読者は、2つのベンチマークmrcチャレンジデータセットsquad2で評価されます。 0およびnewsqa、新しい最先端の結果を達成
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-27">
        <br>2020-01-27
      </time>
    </span>
  </h3>
</article>
<!-- paper0: ChID: A Large-scale Chinese IDiom Dataset for Cloze Test -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/cs.CL/paper_9.html">
      ChID: A Large-scale Chinese IDiom Dataset for Cloze Test
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      結果は、機械の精度が人間の精度よりも大幅に悪いことを示しており、さらなる研究の余地が大きいことを示しています。このコーパスでは、パッセージのイディオムが空白記号に置き換えられ、適切に設計された候補イディオムから正しい答えを選択する必要があります。 
[概要]中国語のクローゼテストデータセットchid研究イディオムの理解
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-04">
        <br>2019-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Thieves on Sesame Street! Model Extraction of BERT-based APIs -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/cs.CL/paper_10.html">
      Thieves on Sesame Street! Model Extraction of BERT-based APIs
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      したがって、私たちの研究は、NLPコミュニティ内での転移学習法への移行によってのみ実現可能になったエクスプロイトを強調しています。数百ドルのクエリ予算に対して、攻撃者は被害者モデルよりもわずかに劣るモデルを抽出できます。自然言語処理におけるモデル抽出の問題。被害者モデルへのクエリアクセスのみを持つ敵がそのモデルのローカルコピーを再構築しようとします。実際、攻撃者は文法的または意味的に意味のあるクエリを使用する必要さえありません。タスク固有のヒューリスティックと組み合わされた単語のランダムシーケンスは、自然言語の推論や質問への回答など、さまざまなNLPタスクのセットでモデルを抽出するための効果的なクエリを形成します。 
[要約]単語のランダムシーケンスとタスク固有のヒューリスティックが、nlp被害者の多様なセットに対するモデル抽出のための効果的なクエリを形成することを示します。洗練されたもの
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-27">
        <br>2019-10-27
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: ML Estimation and CRBs for Reverberation, Speech and Noise PSDs in
  Rank-Deficient Noise-Field -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/eess.AS/paper_0.html">
      ML Estimation and CRBs for Reverberation, Speech and Noise PSDs in
  Rank-Deficient Noise-Field
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      両方の推定量が分析的に比較および分析され、平均二乗誤差（MSE）の式が導出されます。提案された推定量は、シミュレーションと実際の残響およびノイズのある信号の両方を使用して調べられ、競合する推定量と比較した提案された方法の利点を示しています。 、残響およびノイズパワースペクトル密度（PSD）。 
[要約]残響除去およびノイズ低減アルゴリズムは通常、いくつかのモデルパラメーターを必要とします。一般的に使用される仮定は、ノイズpsdマトリックスが既知であるということです
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-07-22">
        <br>2019-07-22
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Source coding of audio signals with a generative model -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/eess.AS/paper_1.html">
      Source coding of audio signals with a generative model
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      SampleRNNを生成モデルとして使用して、提案されたコーディング構造が、オーディオ信号の特定のカテゴリの最先端のソースコーディングツールと競合するパフォーマンスを提供することを示します。生成モデルの助けを借りて、オーディオ信号のソースコーディングを検討します..提案されたコーディングスキームは理論的に分析されます。 
[要旨]提案された提案された提案された開発は提案に使用することができます。提案された開発は提案されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-27">
        <br>2020-01-27
      </time>
    </span>
  </h3>
</article>
<!-- paper0: 3-D Feature and Acoustic Modeling for Far-Field Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/eess.AS/paper_2.html">
      3-D Feature and Acoustic Modeling for Far-Field Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この論文では、多変量自己回帰（MAR）モデリングアプローチを使用して、マルチチャネル音声信号から特徴を直接抽出することを提案します。このアプローチでは、時間、周波数、チャネルの3次元すべての相関が活用されます。 3D CNNアーキテクチャにより、強化タスクに焦点を当てた従来のビームフォーミングモデルと比較して、音声認識コストを最適化するマルチチャネル機能の組み合わせが可能になります。 
[概要]マルチチャネル音声信号のチャネル再変換ベースの強化を使用して、ニューラルネットワーク音響モデルのスペクトログラムベースの特徴を抽出します。3月の特徴は、3次元で共同音響モデリングを実行する畳み込みニューラルネットワークアーキテクチャに供給されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-13">
        <br>2019-11-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Online Spectrogram Inversion for Low-Latency Audio Source Separation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/eess.AS/paper_3.html">
      Online Spectrogram Inversion for Low-Latency Audio Source Separation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、MISIはオフラインで動作しますが、ここでは低遅延ソース分離に適したoMISIと呼ばれるMISIのオンラインバージョンを提案します。これは補聴器アプリケーションなどの重要な要件です。オーディオソース分離は通常、各ソースの時間フーリエ変換（STFT）の大きさ、およびスペクトログラム反転アルゴリズムを適用して時間領域信号を取得します。ただし、このアルゴリズムには2つの欠点があります。 
[要約]複数入力スペクトログラム反転（misi）アルゴリズムは、最近のいくつかの作品でうまく利用されています。元々はヒューリスティックな方法で導入されました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-08">
        <br>2019-11-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Noise dependent Super Gaussian-Coherence based dual microphone Speech
  Enhancement for hearing aid application using smartphone -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/eess.AS/paper_4.html">
      Noise dependent Super Gaussian-Coherence based dual microphone Speech
  Enhancement for hearing aid application using smartphone
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      提案された方法の客観的および主観的な測定値は、-5 dB、0 dB、および5 dBの信号対雑音比レベルでのいくつかのノイズの多い条件について、この論文で検討された標準技術と比較して効果的な改善を示します。提案されたSE方法は、この記事では、音声信号とノイズ信号間のコヒーレンスを使用して、スピーチエンハンスメント（SE）ゲイン関数を、スーパーガウスジョイントマキシマムアポステリオリ（SGJMAP）と組み合わせて使用します。シングルマイクSEゲイン機能。 
[概要]提案されたse比は、補聴器の補助デバイスとして動作するスマートフォンに実装できます。ただし、sgjmapを使用するとseは追加のミュージカルノイズで音声品質を向上させます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-27">
        <br>2020-01-27
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Phase-Aware Speech Enhancement with a Recurrent Two Stage Net work -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/eess.AS/paper_5.html">
      Phase-Aware Speech Enhancement with a Recurrent Two Stage Net work
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、TSNフレームワークは位相再構成を考慮しませんでしたが、位相情報は知覚品質に影響します。したがって、Griffin-Limアルゴリズムに基づく位相再構成法の採用を提案しました。最後に、TSNなどのベースラインでrTSNを評価しました知覚品質関連の指標と電話認識エラー率で。 
[概要] rtsnは、以前に提案された2フェーズネットワーク（tsn）フレームワークの拡張です。ネットワークは、さまざまな最新の手法を上回りましたが、pri-nnとして単純なディープニューラルネットワークを採用しました。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-27">
        <br>2020-01-27
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Audio Codec Enhancement with Generative Adversarial Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/eess.AS/paper_6.html">
      Audio Codec Enhancement with Generative Adversarial Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      提案されたGANベースのコード化されたオーディオエンハンサーの主な利点は、デコードされたオーディオサンプルで直接エンドツーエンドで動作することです。手動で作成されたフロントエンドを設計する必要がなくなります。さらに、このホワイトペーパーで説明する拡張アプローチは、既存の標準準拠のエンコーダを変更することなく、低ビットレートの符号化オーディオの音質を改善できます。 
[概要]コーディングノイズからオーディオを復元する技術が開発されています。この方法は、低ビットレートのコーディングされたオーディオの音質を改善することです。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-27">
        <br>2020-01-27
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Development and Evaluation of Video Recordings for the OLSA Matrix
  Sentence Test -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/eess.AS/paper_7.html">
      Development and Evaluation of Video Recordings for the OLSA Matrix
  Sentence Test
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      28人の通常聴覚参加者は、オーディオおよびビジュアルモダリティ、静かでノイズのあるスピーチ、オープンセットおよびクローズドセットの応答形式などの条件でテストおよび再テストセッションを完了しました。-1.5dB ..静かで、オーディオのみと比較したオーディオビジュアルの利点音圧レベル（SPL）は7 dBでした。 
[ABSTRACT]このテストのほとんどのバージョンは、％stimuli.speechreading能力で設計されています。音声読み上げ能力は、音声のみのsrtsよりも標準偏差が大きい視聴覚音声受信しきい値（srts）に反映されていました。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-10">
        <br>2019-12-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Frame-based overlapping speech detection using Convolutional Neural
  Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-01-28/eess.AS/paper_8.html">
      Frame-based overlapping speech detection using Convolutional Neural
  Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この研究では、畳み込みニューラルネットワークを使用して、25msという短いセグメントで重複する音声の検出を調査します。この現象は、個々の話者の追跡と認識の複雑さにより、音声技術のパフォーマンスを低下させます。複数のスピーカーからの信号。 
[ABSTRACT]さまざまな色の機能を使用して検出パフォーマンスをチェックします。pyknogram機能は、他の一般的に使用される音声機能よりも優れています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-27">
        <br>2020-01-27
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="">
      
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本日更新された論文はありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="">
        <br>
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
