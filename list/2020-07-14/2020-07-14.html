<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-07-14の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Generative Modelling for Controllable Audio Synthesis of Expressive
  Piano Performance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.SD/paper_0.html">
      <font color="black">Generative Modelling for Controllable Audio Synthesis of Expressive
  Piano Performance</font>
    </a>
  </h2>
  <font color="black">想定されるユースケースの1つは、既存のピアノ曲の創造的で真新しい解釈を刺激することです。これは、前のサンプルからサンプリングしたり、他の曲から推論したりできる潜在変数である条件に基づいています。は、オーディオの合成中に細粒度のモーフィングを適用できます。 
[ABSTRACT]モデルは、オーディオを合成する過程で細粒度のモーフィングを適用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-16">
        <br><font color="black">2020-06-16</font>
      </time>
    </span>
</section>
<!-- paper0: Drum Beats and Where To Find Them: Sampling Drum Patterns from a Latent
  Space -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.SD/paper_1.html">
      <font color="black">Drum Beats and Where To Find Them: Sampling Drum Patterns from a Latent
  Space</font>
    </a>
  </h2>
  <font color="black">私たちの知る限り、これはACAIのドラムパターン生成への最初の適用です。このペーパーでは、ドラムパターンの大規模なデータセットを提示し、潜在的な探索可能なスペースと認識可能なジャンルエリアを生成する人工ニューラルネットワークの2つの異なるアーキテクチャを比較します。オートエンコーダ補間（ACAI）は、標準の変分オートエンコーダと比較してより良い結果を示します。 
[ABSTRACT]オートエンコーダネットワークは、標準の変分オートエンコーダと比較してより良い結果を示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: CheXphoto: 10,000+ Smartphone Photos and Synthetic Photographic
  Transformations of Chest X-rays for Benchmarking Deep Learning Robustness -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_0.html">
      <font color="black">CheXphoto: 10,000+ Smartphone Photos and Synthetic Photographic
  Transformations of Chest X-rays for Benchmarking Deep Learning Robustness</font>
    </a>
  </h2>
  <font color="black">スマートフォンの写真のデータセットであるCheXphotoとCheXpertデータセットからサンプリングされた胸部X線の合成写真変換を紹介します。CheXphotoを生成するには、（1）さまざまな照明条件を含むさまざまな設定の下でデジタルX線の写真を自動および手動でキャプチャしますと場所、および（2）デジタルX線の合成変換をターゲットにして、それらをデジタルX線やX線フィルムの写真のように見せることを目的としています。ただし、胸部X線アルゴリズムの胸部Xの写真への適用光線には、機械学習モデルのトレーニングに使用されるデジタルX線では通常発生しない画面のまぶしさや視野角の悪さなど、スマートフォンの写真のアーティファクトが存在する場合に信頼できる分類が必要です。 
[ABSTRACT]世界のいくつかの地域では、臨床医や放射線科医が胸部X線の写真をキャプチャして、他の専門家や臨床医と共有しています。新しいテクノロジーは、スマートフォンの既存のユビキタスを使用することです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: ReActNet: Towards Precise Binary Neural Network with Generalized
  Activation Functions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_1.html">
      <font color="black">ReActNet: Towards Precise Binary Neural Network with Generalized
  Activation Functions</font>
    </a>
  </h2>
  <font color="black">この重要な観察に基づいて、それぞれの一般化された関数に対してRSignおよびRPReLUとして示される従来のSignおよびPReLU関数を一般化して、ゼロに近い追加コストで分布の再形成およびシフトの明示的な学習を可能にすることを提案します。コードとモデルはhttps://github.com/liuzechun/ReActNet。で入手できます。最後に、分布損失を採用して、バイナリネットワークをさらに強化し、実数値ネットワークと同様の出力分布を学習させます。 
[要旨]まず、コンパクトな実数値ネットワークを変更して2値化することにより、ベースラインネットワークを構築します。これは、実数値からバイナリネットおよびmeliusnet29よりも、それぞれ4％、3％優れています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-07">
        <br><font color="black">2020-03-07</font>
      </time>
    </span>
</section>
<!-- paper0: Exploiting context dependence for image compression with upsampling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_2.html">
      <font color="black">Exploiting context dependence for image compression with upsampling</font>
    </a>
  </h2>
  <font color="black">この記事では、この目的のためのシンプルで安価な一般的な手法について説明します。これにより、48の標準$ 512 \ times 512 $グレースケール8ビット画像の最後のアップスケーリングで、平均$ 0.645 $ビット/差（$ 0.138 $と$ 1.489 $の間）を節約できました-想定と比較して提示された単純な安価な一般的な方法論は、損失のある画像圧縮におけるDCT係数のようなさまざまなタイプのデータにも使用できます。ラプラス分布の中心を予測するコンテキストの最小二乗線形回帰を使用すると、平均$ 0.393 $ビット/違いの節約。 
[ABSTRACT]シンプルな一般的な方法は、さまざまなタイプのデータに使用できます。たとえば、圧縮は平均0ドルでした。rgb画像の場合、393。色変換の強化だけで平均1.69ドルが得られました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-06">
        <br><font color="black">2020-04-06</font>
      </time>
    </span>
</section>
<!-- paper0: Compressed Sensing with Signal Averaging for Improved Sensitivity and
  Motion Artifact Reduction in Fluorine-19 MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_3.html">
      <font color="black">Compressed Sensing with Signal Averaging for Improved Sensitivity and
  Motion Artifact Reduction in Fluorine-19 MRI</font>
    </a>
  </h2>
  <font color="black">この目的のために、数値シミュレーションとファントム実験を実行して、アンダーサンプリングパターンの点像分布関数（PSF）と、x信号平均と加速係数x（NAx-AFx）のペアの番号を使用した取得-再構築戦略のノイズに対する脆弱性を特徴付けました。結論として、19F MRIでは、アンダーサンプリングと平均化の組み合わせにより、非圧縮の完全にサンプリングされたデータセットと比較して、モーションアーチファクトに対する感度と堅牢性の両方が圧縮センシングで再構築される場合の両方が改善されるという仮説を検証しました。買収-再建戦略は、ペルフルオロポリエーテルを注入したマウス（n = 2）のin vivoで検証されました。 
[要約]特定の比率と信号の平均化を組み合わせた19f mriパルスシーケンスにより、モーションアーティファクトに対する感度と堅牢性が向上</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-10">
        <br><font color="black">2019-09-10</font>
      </time>
    </span>
</section>
<!-- paper0: Accelerated FBP for computed tomography image reconstruction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_4.html">
      <font color="black">Accelerated FBP for computed tomography image reconstruction</font>
    </a>
  </h2>
  <font color="black">畳み込みを高速化するために、ランプフィルターは、因果的および反因果的再帰フィルターのペアによって近似されます。これは、無限インパルス応答フィルターとも呼ばれます。このアルゴリズムの古典的な直接実装では、$ \ Theta（N ^ 3）$演算の実行が必要です。ここで、$ N $は2Dスライスの線形サイズです。シミュレーションデータの実験結果は、提案されたアプローチの効率を示しています。 
[要約]このアルゴリズムの従来の直接バージョンでは、実行する必要があります。これらには、このデータの現在のバージョンの詳細バージョンが含まれます。このメソッドは、アルゴリズムに対する一連の劇的な変更に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Validation of image-guided cochlear implant programming techniques -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_5.html">
      <font color="black">Validation of image-guided cochlear implant programming techniques</font>
    </a>
  </h2>
  <font color="black">キーワード：人工内耳、グラウンドトゥルース、セグメンテーション、検証。臨床研究は、IGCIPがCI受信者の聴覚転帰を改善できることを示しています。私たちのグループは、画像分析手法を使用して、着床前または着床後のCT画像で内耳構造をセグメント化するための画像誘導CIプログラミング（IGCIP）技術を開発しました。着床後のCT画像でCI電極を特定します。 
[ABSTRACT]聴覚の結果は、蝸牛内の解剖学と電極の配置と相関しています。これにより、連絡先のどれを無効にするかを提案することで、聴覚専門家がciプログラミングを支援できるようになります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-23">
        <br><font color="black">2019-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: DeepHAZMAT: Hazardous Materials Sign Detection and Segmentation with
  Restricted Computational Resources -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_6.html">
      <font color="black">DeepHAZMAT: Hazardous Materials Sign Detection and Segmentation with
  Restricted Computational Resources</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、Hazmatを4つのステップで検出およびセグメント化するために、DeepHAZMATと呼ばれるCNNベースのパイプラインを提案します。 1）CNNネットワークに入力される入力画像の数を最適化する、2）危険領域から必要な視覚情報を収集するためにYOLOv3-tiny構造を使用する、3）GrabCut技術を使用して背景からHazmatサインのセグメント化と分離を行う、および4）モルフォロジー演算子とコンベックスホールアルゴリズムを使用して結果を後処理します。非常に限られたメモリとCPUリソースを使用しているにもかかわらず、実験結果は、提案された方法が検出速度と最先端の方法と比較した検出精度。各Hazmat標識には、救急ロボットが安全な行動をとるためにそれを検出して解釈する必要があるという特定の意味があります。 
[要旨] hazmatの記号は、安全な行動をとるためにレスキューロボットが検出して解釈する必要があるという特定の意味を持っています。防護標識は異なるタイプのコミュニケーションに基づいています。防護認識は、画像の歪みなどの他の障害物を支援するために使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Free-running SIMilarity-Based Angiography (SIMBA) for simplified
  anatomical MR imaging of the heart -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_7.html">
      <font color="black">Free-running SIMilarity-Based Angiography (SIMBA) for simplified
  anatomical MR imaging of the heart</font>
    </a>
  </h2>
  <font color="black">このような生理学的制約を回避するために取得したデータの固有の類似性を活用する、ゲートのないフリーランニングシーケンスに適用可能な、新しい高速再構成アルゴリズムを提案します。画像と血液心筋層の境界の鮮明度、コントラスト比、および冠動脈口の可視性を比較しました.. SIMBAとFRFの両方を使用すると、すべてのデータ（すべてのデータ：4/36、SIMBA：30/36、FRF：33/36、どちらもP &lt;0.001）よりも冠動脈口を視覚化できますが、最初の2つ。 
[要約]新しい方法は、心臓と呼吸のセルフゲーティングと圧縮センシングを使用します。メソッドは、ゲートなしのフリーランニングシーケンスに基づいています。メソッドを使用して、取得したデータの類似性を回避できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Data-driven geophysics: from dictionary learning to deep learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_8.html">
      <font color="black">Data-driven geophysics: from dictionary learning to deep learning</font>
    </a>
  </h2>
  <font color="black">地球物理現象の原理を理解することは不可欠で挑戦的なタスクです。コーディングチュートリアルと、初心者や興味のある地球物理学の読者がディープラーニングをすばやく探索するためのヒントの概要を示します。 。 
[ABSTRACT]データ-ドリブンアプローチは地球物理学の発展を支えてきました。しかし、そのような方法は次元の呪いに悩まされ、地下を不正確にモデル化する可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Extracting the fundamental diagram from aerial footage -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_9.html">
      <font color="black">Extracting the fundamental diagram from aerial footage</font>
    </a>
  </h2>
  <font color="black">導出された方法論は、車両検出、車両追跡、交通状態推定の3つのフェーズで構成されます。このホワイトペーパーでは、ドローンプラットフォームから取得した空中映像から基本図を取得する革新的な方法を考案します。輻輳は、2つの測定可能な特性と強く相関しています。システム全体の動作に影響を与える需要とネットワーク密度。 
[ABSTRACT]トラフィックの混雑は、システムの全体的な動作に影響を与える需要とネットワーク密度と強く相関しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-07">
        <br><font color="black">2020-07-07</font>
      </time>
    </span>
</section>
<!-- paper0: DeU-Net: Deformable U-Net for 3D Cardiac MRI Video Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_10.html">
      <font color="black">DeU-Net: Deformable U-Net for 3D Cardiac MRI Video Segmentation</font>
    </a>
  </h2>
  <font color="black">実験結果は、DeU-Netが一般的に使用される評価指標、特に心臓周辺情報（ASSDおよびHD）で最先端のパフォーマンスを実現することを示しています。心臓磁気共鳴画像（MRI）の自動セグメンテーションにより、効率的かつ正確に臨床アプリケーションでのボリューム測定。最初に、TDAMは、オフセット予測ネットワークによって抽出された時間情報を含む入力として心臓MRIビデオクリップを受け取ります。 
[ABSTRACT] 3D心臓MRIビデオビデオセグメンテーションの精度が向上しました。これは、異方性解像度と曖昧な境界が原因です。しかし、既存の方法では精度が低下します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: A Convolutional Approach to Vertebrae Detection and Labelling in Whole
  Spine MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_11.html">
      <font color="black">A Convolutional Approach to Vertebrae Detection and Labelling in Whole
  Spine MRI</font>
    </a>
  </h2>
  <font color="black">最後に、腰椎と脊椎全体のMRスキャンで脊柱側弯症の自動検出に使用するこの方法の臨床的適用性を示します。結果として得られるシステムは、脊椎全体のスキャンの挑戦的な臨床データセットで98.1％の検出率と96.5％の識別率を実現します。腰椎のみのスキャンで以前のシステムのパフォーマンスに匹敵するか、それを超えます。脊椎全体のMRIで椎骨を検出および識別するための新しい畳み込み手法を提案します。 
[要約]システムを使用して、椎体の椎骨の角を特定できます。次に、システムを使用して、自己整合的な方法で椎骨のレベルを特定できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-06">
        <br><font color="black">2020-07-06</font>
      </time>
    </span>
</section>
<!-- paper0: Robotized Ultrasound Imaging of the Peripheral Arteries -- a Phantom
  Study -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_12.html">
      <font color="black">Robotized Ultrasound Imaging of the Peripheral Arteries -- a Phantom
  Study</font>
    </a>
  </h2>
  <font color="black">さらに、これは非常にユーザーに依存する操作です。このシステムは、動脈に沿ってスキャンしている間、脚のファントムの血管内腔をUS画像内に維持する可能性をチェックすることによって評価されました。結論として、このシステムは完全に放射線を使わないアプローチを使用した、ヒトの自動化された末梢動脈イメージング。 
[要約]末梢動脈を自律的にスキャンするロボット化された私たちのシステムには、これらの制限を克服する可能性があります。システムは、動脈に沿ってスキャンしながら、足のファントムの血管腔を米国の画像内に維持する可能性をチェックすることによって評価されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Synthetic Aperture Radar Image Formation with Uncertainty Quantification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_13.html">
      <font color="black">Synthetic Aperture Radar Image Formation with Uncertainty Quantification</font>
    </a>
  </h2>
  <font color="black">結果の事後のサンプルと、スペックルおよびノイズを管理するパラメーターは、ギブスサンプラーを使用して取得されます。後者の情報は、合成で作成された例でさえグラウンドトゥルースイメージが通常不明であるSARで特に重要です。ほとんどの既存のSARイメージの形成メソッドは、未知の地上シーンの反射率を近似する最大事後画像をもたらします。 
[ABSTRACT]ほとんどの既存のsar画像形成方法では、最大の事後画像が得られます。ただし、モードを見つけることは、事後を調査するための最良の方法ではありません。これらのサンプルを使用して推定値を計算し、不確実性を助けることもできます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Seeing Eye-to-Eye? A Comparison of Object Recognition Performance in
  Humans and Deep Convolutional Neural Networks under Image Manipulation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_14.html">
      <font color="black">Seeing Eye-to-Eye? A Comparison of Object Recognition Performance in
  Humans and Deep Convolutional Neural Networks under Image Manipulation</font>
    </a>
  </h2>
  <font color="black">そのため、計算神経科学と機械学習の分野では、人工および生物学的視覚に多くの類似点と相違点があるとされ始めています。精度の分析により、人間はすべての条件でDCNNよりも優れているだけでなく、形状および最も注目すべきは、色の変更です。7つの異なるサルのカテゴリの典型的な学習プロセスに基づいて設計されたアプローチには、自然な例を使用したトレーニングと検証フェーズ、および新しい形状と色の操作を使用したテストフェーズが含まれていました。 
[ABSTRACT]人間の参加者は、さまざまなフィードフォワードdcnetnetnetnetsに対してオンライン実験に参加しました。この結果は、人工視覚モデルの霊長類の背側ストリームに類似した反復回路の実装を裏付けています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: MS-NAS: Multi-Scale Neural Architecture Search for Medical Image
  Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_15.html">
      <font color="black">MS-NAS: Multi-Scale Neural Architecture Search for Medical Image
  Segmentation</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、ネットワークバックボーンからセル操作までのマルチスケールサーチスペースと、さまざまなサイズの機能を融合するマルチスケールフュージョン機能を備えたマルチスケールNAS（MS-NAS）フレームワークについて説明します。より大きなサーチスペースでは、部分チャネル接続スキームと2ステップの復号化方法を使用して、最適化の品質を維持しながら計算オーバーヘッドを削減します。 -アート手法により、0.6〜5.4％のmIOUおよび0.4〜3.5％のDSCの改善を実現し、計算リソースの消費を18.0〜24.9％削減します。 
[要約]これらの一部は、検索スペースのコストを削減する簡単な方法です。これらには、部分的なチャネル接続スキームと2段階の復号化方法が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Relative Entropy Regularised TDLAS Tomography for Robust Temperature
  Imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_16.html">
      <font color="black">Relative Entropy Regularised TDLAS Tomography for Robust Temperature
  Imaging</font>
    </a>
  </h2>
  <font color="black">確立された同時代数的再構成手法（SART）と比較して、RETROアルゴリズムは、断層温度画像の品質を大幅に改善し、TDLAS断層測定ノイズに対して優れた堅牢性を示します。この問題に対処するために、新しいアルゴリズムを開発しました。 TDLASトモグラフィーでは、相対エントロピートモグラフィーRecOnstruction（RETRO）と呼びます。チューナブルダイオードレーザー吸収分光法（TDLAS）トモグラフィーは、その場での燃焼診断に広く使用されており、種の濃度と温度の両方の画像が得られます。 
[ABSTRACT]アルゴリズムディスペンストモグラフィー再構成（レトロ）はデータの反転に基づいています。tdlasトモグラフィーの産業分野のアプリケーションに大きな可能性をもたらします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: OpenStreetMap: Challenges and Opportunities in Machine Learning and
  Remote Sensing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_17.html">
      <font color="black">OpenStreetMap: Challenges and Opportunities in Machine Learning and
  Remote Sensing</font>
    </a>
  </h2>
  <font color="black">OSM（およびオープンランドマップの他のソース）は、リモートセンシングデータの解釈方法を変える可能性があり、機械学習との相乗効果により、参加型マップの作成とその品質を、グローバルで最新のサービスに必要なレベルにスケーリングできると考えていますこのような方法は、1）通常はGISとリモートセンシング技術を使用してOSMレイヤーのカバレッジと品質を改善すること、または2）既存のOSMレイヤーを使用して画像データに基づいてモデルをトレーニングし、ナビゲーションまたは{土地利用}分類。それは主に、さまざまなマッピングスキルを持つボランティアによって編集されるため、注釈の完全性と品質は、地理的な場所によって異なります。 osmによれば、
[ABSTRACT] osmはデータの解釈方法を変える可能性があると、osm.osmはデータの改善と使用に使用されていると述べています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: SIMBA: Specific Identity Markers for Bone Age Assessment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_18.html">
      <font color="black">SIMBA: Specific Identity Markers for Bone Age Assessment</font>
    </a>
  </h2>
  <font color="black">SIMBAは、患者に関して利用可能なすべてのデータを組み込んだ、コンピュータ支援診断法の新しい波のトレンドを設定します。次に、このロバストな表現を使用して、患者の相対的な骨年齢を推定します：年代と骨年齢の差..この目的のために、私たちは最新のモデルを基にして、アイデンティティマーカーに存在する情報を、元の手のレントゲン写真から作成された視覚的特徴と融合させます。 
[ABSTRACT]放射線科医はさまざまな年齢のマーカーを考慮に入れます。simbaは放射線学的手のポーズデータセットに基づいています。simbaは以前の状態より優れています-最先端の方法</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-10">
        <br><font color="black">2020-07-10</font>
      </time>
    </span>
</section>
<!-- paper0: Exploiting Deep Generative Prior for Versatile Image Restoration and
  Manipulation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_19.html">
      <font color="black">Exploiting Deep Generative Prior for Versatile Image Restoration and
  Manipulation</font>
    </a>
  </h2>
  <font color="black">また、ランダムジッタリング、画像モーフィング、カテゴリ転送などの多様な画像操作を可能にします。これらの実装が簡単で実用的な変更により、再構成を保存して自然画像の多様性を維持できるため、より正確で実際の画像の忠実な再構成..この作品は、大規模な自然画像でトレーニングされた生成的敵対的ネットワーク（GAN）によって事前にキャプチャされた画像を活用する効果的な方法を示しています。 
[ABSTRACT]以前の深い画像（dip）は、欠けている正気を復元するための説得力のある結果を提供します。これには、色、空間的一貫性、テクスチャ、および高レベルの概念が含まれます。これらは、既存のガン反転法の仮定を緩和することで可能になり、発電機を修理する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-30">
        <br><font color="black">2020-03-30</font>
      </time>
    </span>
</section>
<!-- paper0: Symmetric Dilated Convolution for Surgical Gesture Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.IV/paper_20.html">
      <font color="black">Symmetric Dilated Convolution for Surgical Gesture Recognition</font>
    </a>
  </h2>
  <font color="black">JIGSAWSデータセットからの基本的なロボット縫合タスクに対するアプローチの有効性を検証します。長期的な時間パターンをエンコードおよびデコードし、フレームを確立するために、自己注意モジュールによってブリッジされた対称拡張構造を使用して方法を考案します。これらの課題に取り組むために、RGBビデオのみを使用して、対応する境界を持つ外科ジェスチャーを自動的に検出およびセグメント化する新しい時間的畳み込みアーキテクチャを提案します。 
[ABSTRACT]この実験は、長期的なフレームの依存関係をキャプチャする方法の能力を示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Automated detection of corrosion in used nuclear fuel dry storage
  canisters using residual neural networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_0.html">
      <font color="black">Automated detection of corrosion in used nuclear fuel dry storage
  canisters using residual neural networks</font>
    </a>
  </h2>
  <font color="black">この論文は、使用済み核燃料を収容する乾燥貯蔵ステンレス鋼キャニスターにおける酸化鉄の変色、孔食および応力腐食割れを含む腐食のリアルタイム検出のために残余ニューラルネットワーク（ResNets）を使用することを提案します。タイル、これらのタイルでResNetをトレーニングし、ResNetによって腐食すると予測されたタイルの画像ごとの数を使用して、画像を腐食または無傷として分類します。結果は、このようなディープラーニングアプローチにより、タイルと同時に、腐食したキャニスターからの画像かどうかを高精度で推測します。 
[要約]提案されたアプローチは、核キャニスターの画像をトリミングし、これらのタイルでresnetをトレーニングし、腐食したものとして画像を分類します。結果は、注目度の高い故障やハイテク機器などの方法の精度を向上させるために使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-06">
        <br><font color="black">2020-03-06</font>
      </time>
    </span>
</section>
<!-- paper0: Universal-to-Specific Framework for Complex Action Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_1.html">
      <font color="black">Universal-to-Specific Framework for Complex Action Recognition</font>
    </a>
  </h2>
  <font color="black">次に、マスクネットワークは、ユニバーサルネットワークの出力に基づくカテゴリの正規化によって、クラスを混乱させるためのアテンションマスクを生成します。最近、ビデオベースのアクション認識がコンピュータービジョンの分野で大きな注目を集めています。ユニバーサルネットワークは最初にユニバーサル機能表現を学習します。 
[要旨] u2sフレームワークは3つのサブネットワークで構成されています：ユニバーサルネットワーク、カテゴリ固有のネットワーク、およびマスクnetwork.universalネットワークは、カテゴリの正規化によってクラスを混乱させるためのアテンションマスクを生成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: AI Playground: Unreal Engine-based Data Ablation Tool for Deep Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_2.html">
      <font color="black">AI Playground: Unreal Engine-based Data Ablation Tool for Deep Learning</font>
    </a>
  </h2>
  <font color="black">AIPは簡単に拡張可能で、コードの有無にかかわらず使用できます。また、さまざまなグラウンドトゥルース（深度や表面の法線値など）で使用できます。このホワイトペーパーでは、オープンソースのUnreal Engine-であるAI Playground（AIP）を紹介します。仮想画像データを生成してラベルを付けるためのベースのツール。 
[ABSTRACT]ディープニューラルネットワークをトレーニングして、（1）深度設定、（2）表面法線、または（3）オブジェクトラベルを予測しました。他の研究では、その忠実度を発見しましたが、照明に対しても同様に敏感であることがわかりました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Pick-Object-Attack: Type-Specific Adversarial Attack for Object
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_3.html">
      <font color="black">Pick-Object-Attack: Type-Specific Adversarial Attack for Object
  Detection</font>
    </a>
  </h2>
  <font color="black">Pick-Object-Attackは、ターゲットオブジェクトの境界ボックスにのみ摂動を追加し、画像内で検出された他のオブジェクトのラベルを保持します。特に、特定のオブジェクトの予測ラベルを変更することで、広く使用されているFaster R-CNNを攻撃することを目的としています。画像内：以前の作業が特定のオブジェクト（一時停止標識）を対象としている場合、任意のオブジェクトに一般化します。重要な課題は、そのオブジェクトタイプのすべてのインスタンスのすべての境界ボックスのラベルを変更する必要があることです。さらに、初めて、敵対攻撃がオブジェクト検出に及ぼす影響を、下流のタスクである画像キャプションの観点から調べます。すべてのオブジェクトタイプを変更できるメソッドがキャプションの非常に明白な変化をもたらす場合、制約された攻撃からの変化はそれほど明白ではないことを示します。 
[要約]このペーパーでは、オブジェクト検出の敵対的な例を生成します。これには、画像内に存在する複数のオブジェクトの周囲の境界ボックスを検出することが含まれます。これにより、画像分類よりも困難な作業になります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-05">
        <br><font color="black">2020-06-05</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Learn Parameterized Classification Networks for Scalable
  Input Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_4.html">
      <font color="black">Learning to Learn Parameterized Classification Networks for Scalable
  Input Images</font>
    </a>
  </h2>
  <font color="black">学習したメタネットワークは、メインネットワークを動的にパラメーター化して、個別にトレーニングされたモデルと比較して、一貫して優れた精度で任意のサイズの入力画像に作用することができます。たたみ込みニューラルネットワーク（CNN）には、入力解像度の変更に関して予測可能な認識動作がありません。 ImageNetでの広範囲な実験により、適応推論プロセスにおいて、本手法により精度と効率のトレードオフが改善されることが示されています。 
[ABSTRACT]ネットワークは、さまざまな入力解像度に基づいて、モデルの予測をオンザフライでナレッジ蒸留します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: REMIND Your Neural Network to Prevent Catastrophic Forgetting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_5.html">
      <font color="black">REMIND Your Neural Network to Prevent Catastrophic Forgetting</font>
    </a>
  </h2>
  <font color="black">一般的な治療法はリプレイです。これは、脳がメモリを統合する方法に触発されます。人々は生涯を通じて学習します。脳が圧縮されたメモリをリプレイするという神経科学的証拠はありますが、畳み込みネットワークの既存の方法は生の画像をリプレイします。 
[ABSTRACT] forgetは、脳に基づくアプローチであり、圧縮された表現で効率的に再生できます。マンチェスター大学で学ぶための脳にインスパイアされたアプローチです。これは、記憶喪失や記憶喪失などの例に基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-06">
        <br><font color="black">2019-10-06</font>
      </time>
    </span>
</section>
<!-- paper0: Stereo Visual Inertial Pose Estimation Based on Feedforward-Feedback
  Loops -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_6.html">
      <font color="black">Stereo Visual Inertial Pose Estimation Based on Feedforward-Feedback
  Loops</font>
    </a>
  </h2>
  <font color="black">設計されたフィードバックまたはフィードフォワードループが導入され、システムの安定した制御を実現します。これには、勾配減少フィードバックループ、ロールピッチフィードフォワードループ、バイアス推定フィードバックループが含まれます。広く使用されているフィルターベースまたは最適化と比較して-ベースのアプローチでは、姿勢推定プロセスは制御システムとしてモデル化されます。FLVISは、他の最先端の視覚的SLAMアプローチに比べて高い精度と堅牢性を実現します。 
[ABSTRACT] flvisは人気のあるeuroc mavデータセットで評価されます。flvisは制御システムの例です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-05">
        <br><font color="black">2020-07-05</font>
      </time>
    </span>
</section>
<!-- paper0: Location-Aware Box Reasoning for Anchor-Based Single-Shot Object
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_7.html">
      <font color="black">Location-Aware Box Reasoning for Anchor-Based Single-Shot Object
  Detection</font>
    </a>
  </h2>
  <font color="black">LAARは、境界ボックスの品質評価のために、位置と分類の両方の信頼度を考慮に入れます。オブジェクト検出フレームワークの大部分では、インスタンス分類の信頼度が、予測される境界ボックスの品質基準として使用されます。非最大抑制（NMS）。領域提案ネットワーク（RPN）ベースの検出器と比較して、シングルショットオブジェクト検出器は、ボックス提案の事前選択がないため、ボックスの品質が低下します。 
[要約]このホワイトペーパーでは、単発の物体検出器を目指しており、境界ボックスに関する情報の質の欠如を提案しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Reinforced Attention Learning for Quality-Aware Visual Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_8.html">
      <font color="black">Deep Reinforced Attention Learning for Quality-Aware Visual Recognition</font>
    </a>
  </h2>
  <font color="black">私たちの設計された報酬の離散性のため、提案された学習方法は強化学習設定に配置され、注意アクターと繰り返し批評家は交互に最適化され、一時的な注意表現に対する即時の批評と修正を提供します。したがって、ディープ強化注意学習と呼ばれます。 （DREAL）..このペーパーでは、畳み込みニューラルネットワークの中間注意マップの弱く監視された生成メカニズムに基づいて構築し、注意モジュールの有効性をより簡単に開示して、その潜在能力を完全に活用します。任意の注意モジュールでは、メインネットワークの注意マップの品質を評価するためのメタ評論家ネットワークを導入します。 
[ABSTRACT]これは、さまざまなタイプの注意構築を伴う批評ネットワークアーキテクチャに適用できます。また、それらの表現能力を促進するために使用することもできます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Not only Look, but also Listen: Learning Multimodal Violence Detection
  under Weak Supervision -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_9.html">
      <font color="black">Not only Look, but also Listen: Learning Multimodal Violence Detection
  under Weak Supervision</font>
    </a>
  </h2>
  <font color="black">さらに、広範な実験結果は、マルチモーダル（オーディオビジュアル）入力とモデリング関係のプラスの効果も示しています。この問題に対処するために、この作業では、最初にXD-Violenceという名前の大規模でマルチシーンのデータセットを合計でリリースしますオーディオ信号とウィークラベル付きの4754のトリミングされていないビデオを含む217時間の持続時間。次に、ビデオスニペット間の異なる関係をキャプチャし、機能を統合する3つの並列ブランチを含むニューラルネットワークを提案します。ローカライズされたブランチは、事前の近接度を使用してローカルの位置関係をキャプチャし、スコアブランチは予測スコアの近さを動的にキャプチャします。 
[要約]コードとデータセットはhttps：http：roc-ngでリリースされます。 io / xd-暴力</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-09">
        <br><font color="black">2020-07-09</font>
      </time>
    </span>
</section>
<!-- paper0: CheXphoto: 10,000+ Smartphone Photos and Synthetic Photographic
  Transformations of Chest X-rays for Benchmarking Deep Learning Robustness -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_10.html">
      <font color="black">CheXphoto: 10,000+ Smartphone Photos and Synthetic Photographic
  Transformations of Chest X-rays for Benchmarking Deep Learning Robustness</font>
    </a>
  </h2>
  <font color="black">ただし、胸部X線アルゴリズムに胸部X線の写真を適用するには、機械学習モデルのトレーニングに使用されるデジタルX線では通常見られない画面のまぶしさや視野角の悪さなど、スマートフォンの写真アーチファクトが存在する場合に信頼できる分類が必要です。 。スマートフォンの写真のデータセットであるCheXphotoとCheXpertデータセットからサンプリングされた胸部X線の合成写真変換を紹介します。胸部X線解釈のための深層学習アルゴリズムの臨床展開には、幅広い臨床スペクトルに統合できるソリューションが必要です。世界中のワークフロー。 
[ABSTRACT]世界のいくつかの地域では、臨床医や放射線科医が胸部X線の写真をキャプチャして、他の専門家や臨床医と共有しています。新しいテクノロジーは、スマートフォンの既存のユビキタスを使用することです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Prototype Rectification for Few-Shot Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_11.html">
      <font color="black">Prototype Rectification for Few-Shot Learning</font>
    </a>
  </h2>
  <font color="black">また、理論的分析を行って、その合理性とパフォーマンスの下限を導き出します。特に、このアプローチは、miniImageNetの両方で最先端のパフォーマンスを達成します（1ショットで70.31％、5ショットで81.89％）。 ）とtieredImageNet（1ショットで78.74％、5ショットで86.92％）。3つの数ショットのベンチマークで有効性が示されています。 
[ABSTRACT]次に、トランスダクティブ設定でのプロトタイプ修正に対するシンプルで効果的なアプローチを提案します。コンセプトコンセプトコンセプトは、プロトタイプ再線化のコンセプトです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-25">
        <br><font color="black">2019-11-25</font>
      </time>
    </span>
</section>
<!-- paper0: Towards High-Fidelity 3D Face Reconstruction from In-the-Wild Images
  Using Graph Convolutional Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_12.html">
      <font color="black">Towards High-Fidelity 3D Face Reconstruction from In-the-Wild Images
  Using Graph Convolutional Networks</font>
    </a>
  </h2>
  <font color="black">実験は、私たちの方法が質的および量的比較の両方で高品質の結果を生成し、最先端の方法よりも優れていることを示しています。最近の研究は、高品質の大規模データベースから訓練された生成ネットワークによる高品質の顔のテクスチャの回復を実証しています準備が難しく、公開されていない顔テクスチャの高解像度UVマップ。このため、UVマップを再構築する代わりに、グラフの畳み込みネットワークを使用してメッシュ頂点の詳細な色を再構築することを提案します。 
[要約]新しい方法は、グラフの畳み込みネットワークを使用して3D 3D面を再構築します。結果は、芸術に欠けている顔の形状の分析に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-12">
        <br><font color="black">2020-03-12</font>
      </time>
    </span>
</section>
<!-- paper0: ReActNet: Towards Precise Binary Neural Network with Generalized
  Activation Functions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_13.html">
      <font color="black">ReActNet: Towards Precise Binary Neural Network with Generalized
  Activation Functions</font>
    </a>
  </h2>
  <font color="black">この重要な観察に基づいて、それぞれの一般化された関数に対してRSignおよびRPReLUとして示される従来のSignおよびPReLU関数を一般化して、ゼロに近い追加コストで分布の再形成およびシフトの明示的な学習を可能にすることを提案します。これらすべてのアイデアを組み込んだ、提案されたReActNetは、すべての最先端技術を大幅に上回っています。最後に、分布損失を採用して、バイナリネットワークをさらに強化し、実数値ネットワークと同様の出力分布を学習させます。 。 
[要旨]まず、コンパクトな実数値ネットワークを変更して2値化することにより、ベースラインネットワークを構築します。これは、実数値からバイナリネットおよびmeliusnet29よりも、それぞれ4％、3％優れています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-07">
        <br><font color="black">2020-03-07</font>
      </time>
    </span>
</section>
<!-- paper0: Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for
  Annotation-efficient Cardiac Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_14.html">
      <font color="black">Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for
  Annotation-efficient Cardiac Segmentation</font>
    </a>
  </h2>
  <font color="black">この論文では、ラベル付けされていない豊富なデータと確立されたクロスモダリティデータを同時に活用して、アノテーション効率の高い医療画像セグメンテーションを行うことの実現可能性を調査することを目的としています。具体的には、生徒モデルはドメイン内の教師からラベル付けされていないターゲットデータの知識を学習します予測の一貫性を奨励することにより、知識抽出を介してドメイン間教師からのラベル付きソースデータに埋め込まれた形状の事前分布を奨励します。このために、新しい半教師付きドメイン適応アプローチ、つまり生徒モデルが存在するDual-Teacherを提案します。は、ラベル付きターゲットデータ（CTなど）から学習するだけでなく、2つの教師モデルによって、ラベルなしターゲットデータとラベル付きソースデータ（MRなど）も探索します。 
[要約]追加情報を効率的に利用するために複数のアプローチが開発されました。これらには、半教師あり学習と教師なしドメイン適応が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Compositional Video Synthesis with Action Graphs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_15.html">
      <font color="black">Compositional Video Synthesis with Action Graphs</font>
    </a>
  </h2>
  <font color="black">AG2VidはCATERおよびSomething-Somethingデータセットで評価され、他のベースラインよりも優れています。最後に、目に見えないアクションの新規構成を生成するためにアクショングラフを使用する方法を示します。この目的に向けて、アクショングラフ。オブジェクト間のアクションのダイナミクスを経時的に表す自然で便利な構造。 
[ABSTRACT] ag2vidモデルは外観と位置の特徴をほどき、より正確なモデルを可能にします。アクショングラフは、目に見えないアクションの新しい構成を生成するために使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-27">
        <br><font color="black">2020-06-27</font>
      </time>
    </span>
</section>
<!-- paper0: Embedded Deep Bilinear Interactive Information and Selective Fusion for
  Multi-view Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_16.html">
      <font color="black">Embedded Deep Bilinear Interactive Information and Selective Fusion for
  Multi-view Learning</font>
    </a>
  </h2>
  <font color="black">特に、さまざまなディープニューラルネットワークをトレーニングしてさまざまなビュー内表現を学習し、ビュー間の双線形関数を介してさまざまな双線形類似性から多次元双線形インタラクティブ情報を動的に学習します。つまり、さまざまなビュー内情報をシームレスに埋め込みます。 、クロスビュー多次元双一次インタラクティブ情報、および最適化を介して決定を下すための統一されたフレームワークへの新しいビューアンサンブルメカニズム。6つの公開データセットでの広範な実験により、提案された方法の有効性が実証されています。 
[ABSTRACT]さまざまなビュー内情報、クロスディメンションバイリニアインタラクティブ情報、新しいビューアンサンブルメカニズムを統合されたフレームワークにシームレスに埋め込み、意思決定を行います</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Thanks for Nothing: Predicting Zero-Valued Activations with Lightweight
  Convolutional Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_17.html">
      <font color="black">Thanks for Nothing: Predicting Zero-Valued Activations with Lightweight
  Convolutional Neural Networks</font>
    </a>
  </h2>
  <font color="black">ゼロアクティベーションプレディクター（ZAP）を軽量のCNNで実装します。これにより、オーバーヘッドが無視でき、既存のモデルに簡単に展開できます。さらに、再トレーニングすることなく、各ZAPをMAC削減のための異なる動作点のトレーディング精度に調整できます。たたみ込みニューラルネットワーク（CNN）は、さまざまなタスクに最先端の結果をもたらし、高い計算負荷を伴います。 
[ABSTRACT]ザップアクティベーションがゼロ値であるか、隣接するアクティベーション値に従っていない。ザップモデルは、非表示レイヤーouputsを模倣することによってトレーニングされます。ザップは、ザップを使用してザップ接続を排除するようにトレーニングされます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-17">
        <br><font color="black">2019-09-17</font>
      </time>
    </span>
</section>
<!-- paper0: Screen Tracking for Clinical Translation of Live Ultrasound Image
  Analysis Methods -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_18.html">
      <font color="black">Screen Tracking for Clinical Translation of Live Ultrasound Image
  Analysis Methods</font>
    </a>
  </h2>
  <font color="black">提案された方法は、超音波検査技師の視点に固定されたカメラで画面を追跡することにより米国の画像をキャプチャし、キャプチャした画像を平均アスペクト比87.66±3.73msで正しいアスペクト比に再フォーマットします。これにより、入力が可能になると仮定されています。この検索された画像を画像処理パイプラインに取り込み、検査の改善に役立つ情報を抽出します。この情報は、最終的には、例えば拡張現実（AR）ヘッドセットを使用してリアルタイムで検査者の視野に投影されます。 
[ABSTRACT]私たちは、米国の画像を抽出し、分析タスクの結果を重ね合わせる一般的なフレームワークを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_19.html">
      <font color="black">Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、重要な自然現象にヒントを得た新しいタイプのバックドア攻撃を紹介します。反射です。既存のバックドア攻撃は効果的ですが、ステルスではありません。物理反射モデルの数学的モデリングを使用して、反射バックドア（Refool）を提案します。被害者モデルへのバックドアとして反射を植えること。 
[ABSTRACT]新しいタイプのバックドア攻撃は、コンピューターデータを分析することによって作成されました。これは、トレーニングデータのごく一部にバックドアパターンを挿入することで、被害者モデルを攻撃するために使用できます。既存のバックドア攻撃が効果的である場合、ステルス性はありません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-05">
        <br><font color="black">2020-07-05</font>
      </time>
    </span>
</section>
<!-- paper0: Low to High Dimensional Modality Hallucination using Aggregated Fields
  of View -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_20.html">
      <font color="black">Low to High Dimensional Modality Hallucination using Aggregated Fields
  of View</font>
    </a>
  </h2>
  <font color="black">また、UWRGBDおよびNYUDデータセットで広範な分類とセグメンテーションの実験を行い、幻覚がモダリティの損失による悪影響を和らげることを実証します。失われた情報を回復するために、ローカル近隣の複数の視野から情報を集約する新しい幻覚アーキテクチャを提示します。現存のモダリティから。RGBなどの豊富な情報を備えたモダリティからデータを幻覚化することは広範囲にわたって研究されてきましたが、ロボット工学と自律システムにおける興味深いユースケースで、より挑戦的な低から高のモダリティ幻覚を調査します。 
[要約]これらのシステムのパフォーマンスは、1つ以上のモダリティにアクセスできなくなると劇的に低下する可能性があります。ロボット工学と自律システムで興味深いユースケースを使用して、より挑戦的な低から高のモダリティ幻覚を調査します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Temporal Self-Ensembling Teacher for Semi-Supervised Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_21.html">
      <font color="black">Temporal Self-Ensembling Teacher for Semi-Supervised Object Detection</font>
    </a>
  </h2>
  <font color="black">上記の自己強調戦略は、協調して、ラベルのない画像に対してより良い教師予測をもたらします。しかし、SSODにKDフレームワークを直接適用すると、次のような障害があります。（1）教師と生徒の予測は非常に近いため、生徒、および（2）オブジェクト検出からの密な予測によって引き起こされるデータの不均衡のジレンマは、教師と生徒の間の効率的な一貫性の正則化を妨げます。 
[要約]知識蒸留-kd）教師モデルと生徒モデルで構成されるフレームワークは、ラベルなしの画像をうまく活用するために広く使用されています。準教師付き学習方法は、ベースラインssod方法よりも1.44％優れています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Fine-Grained Crowd Counting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_22.html">
      <font color="black">Fine-Grained Crowd Counting</font>
    </a>
  </h2>
  <font color="black">立っている/座っているまたは暴力的な行動）次に、各カテゴリの人数をカウントします。次に、2つのブランチ間で情報を共有するための補足的な注意モデルを提案します。多くの実用的なアプリケーションでは、画像内の合計人数は各サブカテゴリの人数ほど有用ではありません。 
[ABSTRACT]これらは、きめの細かい群集カウントの単純な例です。これらには、歩道での移動レベル、立っているか座っているか、並んで待っているか、暴力的な行動を示しているかなどが含まれます。また、4つの単純な単純な単純な新しいデータセットを構築しますタスクには、歩道の進行方向、立って座っている、立っている、座っている、並んでいるなどが含まれます。この方法は、サブカテゴリの人数が金額ほど良くないという事実に基づいています人の</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Data-Efficient Ranking Distillation for Image Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_23.html">
      <font color="black">Data-Efficient Ranking Distillation for Image Retrieval</font>
    </a>
  </h2>
  <font color="black">トレーニングサンプルの設定が低い場合、私たちのアプローチは、教師の監督が可能な限り少ない2つの挑戦的な画像検索データセット、ROxford5kとRParis6k \ cite {Roxf}で完全に監督されたアプローチよりも優れています。教師モデルへの限定クエリ、ii）最終出力表現にアクセスできるブラックボックス教師モデル、およびiii）グラウンドトゥルースラベルのない元のトレーニングデータのほんの一部。さらに、蒸留法では、生徒と教師は必要ありません。同じ次元を持っています。 
[要約]このペーパーでは、計量学習の問題について、知識の抽出に取り組みました。さらに、抽出方法では、生徒と教師が同じ次元である必要はありません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-10">
        <br><font color="black">2020-07-10</font>
      </time>
    </span>
</section>
<!-- paper0: Bridging Knowledge Graphs to Generate Scene Graphs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_24.html">
      <font color="black">Bridging Knowledge Graphs to Generate Scene Graphs</font>
    </a>
  </h2>
  <font color="black">この目的のために、2つのグラフ間および各グラフ内で情報を反復的に伝播しながら、各反復で徐々にブリッジを洗練する、新しいグラフベースのニューラルネットワークを提案します。シーングラフは、画像を解析してそれらに変換する強力な表現です抽象的意味要素、つまりオブジェクトとその相互作用。視覚的な理解と説明可能な推論を促進します。広範な実験を通じて、最新の方法と比較してGB-Netの優れた精度を示し、新しい技術をもたらします。 
[ABSTRACT]常識知識グラフは、世界がどのように構造化され、一般的な概念がどのように相互作用するかをエンコードする豊富なリポジトリです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-07">
        <br><font color="black">2020-01-07</font>
      </time>
    </span>
</section>
<!-- paper0: Data from Model: Extracting Data from Non-robust and Robust Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_25.html">
      <font color="black">Data from Model: Extracting Data from Non-robust and Robust Models</font>
    </a>
  </h2>
  <font color="black">同じデータを使用すると、DtMプロセスが異なると、モデルが異なる機能を持つことになります。特に、異なるネットワークアーキテクチャファミリーでは、同等のパフォーマンスを達成していますが。この循環変換の成功は、データに存在する共有機能マッピングに起因する可能性があります。深層学習の本質は、データを活用して深層ニューラルネットワーク（DNN）モデルをトレーニングすることです。 
[要旨]この実験は、非ロバストモデルとロバストオリジンモデルの両方に対して実行されます。この循環変換の結果は、データとモデルに存在する共有フィーチャマッピングに起因する可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Validation of image-guided cochlear implant programming techniques -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_26.html">
      <font color="black">Validation of image-guided cochlear implant programming techniques</font>
    </a>
  </h2>
  <font color="black">キーワード：人工内耳、グラウンドトゥルース、セグメンテーション、検証。私たちのグループは、画像分析手法を使用して、移植前または移植後のCT画像で内耳構造をセグメント化し、移植後のCT画像でCI電極をローカライズする画像ガイドCIプログラミング（IGCIP）技術を開発しました。これらの2つのステップの精度を厳密に特徴付け、これらのステップの不正確さが全体的な結果にどのように影響するかを評価するために、35の側頭骨標本の従来のCTおよびマイクロCT画像を使用してグラウンドトゥルースデータセットを作成します。 
[ABSTRACT]聴覚の結果は、蝸牛内の解剖学と電極の配置と相関しています。これにより、連絡先のどれを無効にするかを提案することで、聴覚専門家がciプログラミングを支援できるようになります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-23">
        <br><font color="black">2019-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Multiple Expert Brainstorming for Domain Adaptive Person
  Re-identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_27.html">
      <font color="black">Multiple Expert Brainstorming for Domain Adaptive Person
  Re-identification</font>
    </a>
  </h2>
  <font color="black">MEB-Netは、さまざまなアーキテクチャで学習した専門家の異質性に対応し、専門家の権限に関する正則化スキームを導入することにより、適応されたre-IDモデルの識別機能を強化します。大規模データセット（Market-1501およびDukeMTMC-reID）に関する広範な実験）最先端のMEB-Netの優れたパフォーマンスを実証します。MEB-Netは相互学習戦略を採用しており、特定の機能を備えたエキスパートモデルとして、異なるアーキテクチャの複数のネットワークがソースドメイン内で事前トレーニングされていますそして、適応は、エキスパートモデル間のブレーンストーミング（相互学習）を通じて達成されます。 
[要旨]ドメイン適応型個人リード用の複数のエキスパートブレーンストーミングネットワーク（meb-net）は、監視されていない条件下でのモデルアンサンブル問題に関する有望な方向性を開きます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-03">
        <br><font color="black">2020-07-03</font>
      </time>
    </span>
</section>
<!-- paper0: Rethinking Depthwise Separable Convolutions: How Intra-Kernel
  Correlations Lead to Improved MobileNets -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_28.html">
      <font color="black">Rethinking Depthwise Separable Convolutions: How Intra-Kernel
  Correlations Lead to Improved MobileNets</font>
    </a>
  </h2>
  <font color="black">最終的に、MobileNetsなどのDSCベースのアーキテクチャはカーネル間の相関に暗黙的に依存しているのに対し、BSConvの公式はカーネル内の相関に基づいているため、通常の畳み込みをより効率的に分離できることがわかります。詳細なデータセットの場合、最大13.7パーセントポイントの改善を達成しました。大規模で細粒度の分類データセットに対する広範な実験により、BSConvsはMobileNetおよびその他のDSCベースのアーキテクチャをさらに複雑にすることなく、明確かつ一貫して改善することがわかりました。 
[要約]私たちは、深度軸に沿った相関の優位性を示すトレーニング済みモデルからのカーネルプロパティの定量分析に動機付けられています。これらは、一連のコンク（モバイルネットおよびその他のdsc）ベースのアーキテクチャに基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-30">
        <br><font color="black">2020-03-30</font>
      </time>
    </span>
</section>
<!-- paper0: Extracting the fundamental diagram from aerial footage -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_29.html">
      <font color="black">Extracting the fundamental diagram from aerial footage</font>
    </a>
  </h2>
  <font color="black">導出された方法論は、車両検出、車両追跡、および交通状態推定の3つのフェーズで構成されます。全体として、このシステムの動作は、道路セグメント、地域、またはネットワークの基本図によって特徴付けられます。システム全体の動作に影響を与える特性、需要、ネットワーク密度。 
[ABSTRACT]トラフィックの混雑は、システムの全体的な動作に影響を与える需要とネットワーク密度と強く相関しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-07">
        <br><font color="black">2020-07-07</font>
      </time>
    </span>
</section>
<!-- paper0: A Novel Attention-based Aggregation Function to Combine Vision and
  Language -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_30.html">
      <font color="black">A Novel Attention-based Aggregation Function to Combine Vision and
  Language</font>
    </a>
  </h2>
  <font color="black">私たちは、COCOとVQA 2.0の両方のデータセットで、画像とテキストのマッチングと視覚的な質問応答に関するアプローチをテストし、他の削減の選択肢との公正な比較を構築します。最近、ビジョンと言語の共同理解がコンピュータービジョンと自然言語処理の両方のコミュニティで多くの注目を集めており、画像のキャプション、画像とテキストのマッチング、視覚的な質問への回答などのタスクが登場しています。 
[ABSTRACT]ツールが言語を理解できるのは初めてです。これは、各言語の各要素のスコアのセットを分析することで機能します。これには、両方の分類に使用できる学習可能なクロスモーダル削減が含まれますとランキング</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-27">
        <br><font color="black">2020-04-27</font>
      </time>
    </span>
</section>
<!-- paper0: Gender Classification and Bias Mitigation in Facial Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_31.html">
      <font color="black">Gender Classification and Bias Mitigation in Facial Images</font>
    </a>
  </h2>
  <font color="black">これは、2つの新しい顔画像データベースを組み立てることで実現しました。1）人種的バランスのとれたLGBTQ母集団のサブセットを含む包括的データベース2）非バイナリジェンダーの人々で構成される包括的性別データベース。私たちの集団モデルは全体的な精度を達成しましたスコアは90.39％で、これはAdienceでトレーニングされたベースラインのバイナリジェンダー分類子から38.72％増加しています。現在のバイナリジェンダー分類子を拡張して、非バイナリジェンダークラスを含めるようにしました。 
[ABSTRACT]これは性別分類の偏りを緩和するための最初の試みですが、より包括的なデータベースを構築するには、さらに多くの作業が必要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Reducing Language Biases in Visual Question Answering with
  Visually-Grounded Question Encoder -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_32.html">
      <font color="black">Reducing Language Biases in Visual Question Answering with
  Visually-Grounded Question Encoder</font>
    </a>
  </h2>
  <font color="black">したがって、質問の表現自体が十分な視覚的根拠を得て、モデルの言語の事前依存度を低減します。最新の3つのVQAモデルに対するVGQEの効果を示し、バイアスに関する最先端の結果を達成しますVQAv2データセットの機密分割。 VQA-CPv2 ..さらに、既存のバイアス低減手法とは異なり、標準のVQAv2ベンチマークでは、私たちのアプローチは精度を落としません。代わりに、パフォーマンスが向上します。 
[ABSTRACT]最近の調査によると、vqaにはこの効果を低減する新しいモデルがあります。モデルは標準のvqav2ベンチマークと同じモデルを使用しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Hierarchical Dynamic Filtering Network for RGB-D Salient Object
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_33.html">
      <font color="black">Hierarchical Dynamic Filtering Network for RGB-D Salient Object
  Detection</font>
    </a>
  </h2>
  <font color="black">多数の実験により、提案されたモジュールと損失関数の有効性が検証されます。6つのメトリクスに関して、提案された方法は、8つの挑戦的なベンチマークデータセットで既存の12の方法よりも優れています。密に接続された構造を通して異なるモダリティの機能を統合し、それらの混合機能を使用して、さまざまなサイズの受容野を持つ動的フィルターを生成します。 
[要約]提案された方法は、8つの挑戦的なベンチマークデータセットで既存の12の方法よりも優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Patch SVDD: Patch-level SVDD for Anomaly Detection and Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_34.html">
      <font color="black">Patch SVDD: Patch-level SVDD for Anomaly Detection and Segmentation</font>
    </a>
  </h2>
  <font color="black">その結果、MVTec ADデータセットのAUROCで測定された異常検出とセグメンテーションのパフォーマンスは、以前の最先端の方法と比較して、それぞれ9.8％と7.0％増加しました。提案された方法の詳細な分析は、異常検出は、入力画像に異常が含まれているかどうかに関するバイナリの決定を行い、異常セグメンテーションはピクセルレベルで異常を特定することを目的としています。 
[要旨]たとえば、ページのセクションに異常が含まれています。これは、ピクセルレベルで異常を検出するために使用されます。この革新により、検出パフォーマンスも向上します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-29">
        <br><font color="black">2020-06-29</font>
      </time>
    </span>
</section>
<!-- paper0: Understanding Adversarial Examples from the Mutual Influence of Images
  and Perturbations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_35.html">
      <font color="black">Understanding Adversarial Examples from the Mutual Influence of Images
  and Perturbations</font>
    </a>
  </h2>
  <font color="black">このベクトル表現を利用して、クリーンな画像と敵対的な摂動のもつれを解くことによって敵対的な例を理解し、お互いへの影響を分析します。元のトレーニングデータを利用せずに、標的を定めた普遍的な攻撃の挑戦的なタスクを達成した最初の例です。画像と普遍的な摂動の関係に新しい視点を提案する：普遍的な摂動には支配的な特徴が含まれ、画像はノイズのように振る舞います。 
[ABSTRACT]私たちの結果は、画像と普遍的な摂動の間の関係に関する新しい視点を示唆しています。普遍的な摂動は支配的な特徴を含み、画像はそれらに対してノイズのように振る舞います</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: DA4AD: End-to-End Deep Attention-based Visual Localization for
  Autonomous Driving -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_36.html">
      <font color="black">DA4AD: End-to-End Deep Attention-based Visual Localization for
  Autonomous Driving</font>
    </a>
  </h2>
  <font color="black">さらに、私たちの学習した特徴記述子は、ロバストな一致を確立する能力があり、したがって高精度で最適なカメラポーズを正常に推定できることが実証されています。自動運転のための潜在的な低コストのローカリゼーションソリューションにつながる状況。高品質のグラウンドトゥルーストラジェクトリとセンサー間のハードウェア同期を備えた、新しく収集されたデータセットを使用して、メソッドの有効性を包括的に検証します。 
[ABSTRACT]視覚的なローカリゼーションの問題への従来のアプローチは、道路への手作りの機能または人工のオブジェクトに依存しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-06">
        <br><font color="black">2020-03-06</font>
      </time>
    </span>
</section>
<!-- paper0: RATT: Recurrent Attention to Transient Tasks for Continual Image
  Captioning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_37.html">
      <font color="black">RATT: Recurrent Attention to Transient Tasks for Continual Image
  Captioning</font>
    </a>
  </h2>
  <font color="black">このタスクボキャブラリはばらばらではありません。このホワイトペーパーでは、画像キャプションのためのLSTMベースのモデルの継続的な学習を系統的に見ていきます。反復的な継続的な学習問題への重みの偏り化と知識の蒸留に基づくアプローチ。 
[ABSTRACT]今まで驚くほどほとんど注意が継続的な学習に集中されていませんでした。一時的なタスク（ratt）に私たちのメソッドを繰り返し注意と呼びます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: A Convolutional Approach to Vertebrae Detection and Labelling in Whole
  Spine MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_38.html">
      <font color="black">A Convolutional Approach to Vertebrae Detection and Labelling in Whole
  Spine MRI</font>
    </a>
  </h2>
  <font color="black">最後に、腰椎と脊椎全体のMRスキャンで脊柱側弯症の自動検出に使用するこの方法の臨床的適用性を示します。結果として得られるシステムは、脊椎全体のスキャンの挑戦的な臨床データセットで98.1％の検出率と96.5％の識別率を実現します。腰部のみのスキャンでの以前のシステムのパフォーマンスと同等またはそれ以上です。この方法は、さまざまなMRシーケンスの範囲にわたる腰部、頸部、および胸部のみのスキャンに変更なしで適用できます。 
[要約]システムを使用して、椎体の椎骨の角を特定できます。次に、システムを使用して、自己整合的な方法で椎骨のレベルを特定できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-06">
        <br><font color="black">2020-07-06</font>
      </time>
    </span>
</section>
<!-- paper0: Embedding Propagation: Smoother Manifold for Few-Shot Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_39.html">
      <font color="black">Embedding Propagation: Smoother Manifold for Few-Shot Classification</font>
    </a>
  </h2>
  <font color="black">この作業では、数ショット分類でのマニホールド平滑化の教師なしノンパラメトリック正規化子として埋め込み伝播を使用することを提案します。 16 \％ポイントまで。伝搬を埋め込むと、より滑らかな埋め込み多様体が得られることが経験的に示されています。 
[ABSTRACT]この分布のシフトは、一般化が不十分になることが多いため、多様体の滑らかさは半教師あり学習の重要な要素です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-09">
        <br><font color="black">2020-03-09</font>
      </time>
    </span>
</section>
<!-- paper0: Expert Training: Task Hardness Aware Meta-Learning for Few-Shot
  Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_40.html">
      <font color="black">Expert Training: Task Hardness Aware Meta-Learning for Few-Shot
  Classification</font>
    </a>
  </h2>
  <font color="black">このアイデアから発想を得て、トレーニングタスクを適切に配置するために、ハードエキスパートのメタトレーニング戦略を提案します。第1フェーズでは簡単なタスクが優先され、第2フェーズではハードタスクが強調されます。 miniImageNetとtieredImageNetSketchデータセットは、メタ学習者が私たちの専門的なトレーニング戦略でより良い結果を得ることができることを示しています。基準。 
[ABSTRACT]ディープニューラルネットワークは、メタ-学習方法でますます人気が高まっています。メタ-学習者に大量の追加タスクをトレーニングして、少数を指示する知識を習得する-ショット分類</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage
  Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_41.html">
      <font color="black">BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage
  Models</font>
    </a>
  </h2>
  <font color="black">追加の再トレーニングまたは後処理手順なしで、ImageNetで共有ウェイトの単一セットをトレーニングし、これらのウェイトを使用して、サイズが200から1000 MFLOPの範囲の子モデルを取得できます。これらの手順により、計算要件と複雑さが大幅に増加します。アーキテクチャの検索とモデルの展開。これを補うために、既存の方法では、検索の完了後に重みを再トレーニング、微調整、またはその他の後処理する必要があると想定しています。 
[ABSTRACT] bignasは、単一の重みの従来の知識に挑戦するアプローチであり、優れた予測精度を得るために必要です。当社の発見されたモデルファミリーであるbignasmodelsは、76.5％から80.9％の範囲のトップ1の精度を達成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-24">
        <br><font color="black">2020-03-24</font>
      </time>
    </span>
</section>
<!-- paper0: A Branching and Merging Convolutional Network with Homogeneous Filter
  Capsules -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_42.html">
      <font color="black">A Branching and Merging Convolutional Network with Homogeneous Filter
  Capsules</font>
    </a>
  </h2>
  <font color="black">この設計は、ランダムに適用された拡張技術のドメイン固有のセットと組み合わせて、これらのモデルのアンサンブルに対して99.84％の精度でMNISTデータセットの新しい最先端技術を確立し、さらに単一モデルのアート（99.79％正確）。すべてのトレーニングはAdamオプティマイザを使用して実行され、オーバーフィッティングは発生しませんでした。これらの精度は、パラメータの数とトレーニングのエポック数の両方を75％削減して達成されました。以前はMNISTで最高のパフォーマンスを発揮したカプセルネットワーク。 
[要約]ネットワークの統合は84％の単一モデルで達成されました。これらのモデルは、パラメーターの数とトレーニングのエポックの量を75％削減することで達成されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-24">
        <br><font color="black">2020-01-24</font>
      </time>
    </span>
</section>
<!-- paper0: MS-NAS: Multi-Scale Neural Architecture Search for Medical Image
  Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_43.html">
      <font color="black">MS-NAS: Multi-Scale Neural Architecture Search for Medical Image
  Segmentation</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、ネットワークバックボーンからセル操作までのマルチスケール検索スペースと、さまざまなサイズの機能を融合するマルチスケールフュージョン機能を備えたマルチスケールNAS（MS-NAS）フレームワークを紹介します。ニューラルアーキテクチャの最近のブレークスルー検索（NAS）は、医用画像のセグメンテーションにおけるさまざまなアプリケーションを動機付けています。検索スペースの拡大による計算オーバーヘッドを軽減するために、部分チャネル接続方式と2段階の復号化方法を使用して、最適化品質を維持しながら計算オーバーヘッドを削減します。 
[要約]これらの一部は、検索スペースのコストを削減する簡単な方法です。これらには、部分的なチャネル接続スキームと2段階の復号化方法が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: PSConv: Squeezing Feature Pyramid into One Compact Poly-Scale
  Convolutional Layer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_44.html">
      <font color="black">PSConv: Squeezing Feature Pyramid into One Compact Poly-Scale
  Convolutional Layer</font>
    </a>
  </h2>
  <font color="black">より細かい粒度でマルチスケール機能を活用することにより、この後悔を埋めます。PSConvは、多くの一般的なCNNバックボーンにおけるバニラ畳み込みのドロップイン置換であり、追加のパラメーターや計算の複雑さを導入することなく、より良い表現学習を可能にします。操作は、Poly-Scale Convolution（PSConv）という名前で、拡張レートのスペクトルを混合し、単一の畳み込み層に関する各フィルターの個々の畳み込みカーネルに巧みに割り当てます。 
[ABSTRACT] poly-スケールコンボリューション（psconv）は、膨張率を組み合わせ、各フィルターの個々のコンボリューションカーネルに巧みに割り当てます。提案されたコンボリューション操作は、ドロップになる可能性があります-多くの一般的なcnnバックボーンのバニラコンボリューションの代わりに</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: On Saliency Maps and Adversarial Robustness -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_45.html">
      <font color="black">On Saliency Maps and Adversarial Robustness</font>
    </a>
  </h2>
  <font color="black">顕著性マップ..この作業では、このカップリングに別の視点を提供し、顕著性マップを使用してモデルの敵対的ロバスト性を改善する方法である顕著性ベースの敵対トレーニング（SAT）を提供します。より強い顕著性マップは、より堅牢なモデルにつながり、SATを既存の敵対的なトレーニング方法とどのように統合すると、これらの既存の方法のパフォーマンスがさらに向上します。 
[ABSTRACT]これらの作品は、敵対的に訓練されたモデルが、非ロバストなモデルよりも解釈可能な顕著性マップを示すことを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-14">
        <br><font color="black">2020-06-14</font>
      </time>
    </span>
</section>
<!-- paper0: Bridging Maximum Likelihood and Adversarial Learning via
  $α$-Divergence -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_46.html">
      <font color="black">Bridging Maximum Likelihood and Adversarial Learning via
  $α$-Divergence</font>
    </a>
  </h2>
  <font color="black">敵対学習は、モードドロップや微妙なトレーニングなどの実際的な課題にもかかわらず、非常にリアルな自然画像を合成することで知られています。MLと敵対学習の利点を統合する$ \ alpha $ -Bridgeを提案し、1から$ \ alpha $ -divergenceを介してその他。$ \ alpha $ -Bridgeの一般化は、敵対的な学習を正則化するために最近開発されたアプローチと密接に関連し、その以前の研究への洞察を提供し、$ \ alpha $ -Bridgeは実際にはうまく機能します。 
[ABSTRACT] mm学習はすべてのデータモードのキャプチャを促進し、通常は安定したトレーニングを特徴とします。mm学習は、モードの削除や繊細なトレーニングなどの実際的な課題にもかかわらず、非常にリアルな自然画像を合成することで知られています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: OpenStreetMap: Challenges and Opportunities in Machine Learning and
  Remote Sensing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_47.html">
      <font color="black">OpenStreetMap: Challenges and Opportunities in Machine Learning and
  Remote Sensing</font>
    </a>
  </h2>
  <font color="black">このような方法は、1）通常はGISとリモートセンシングテクノロジーを使用して、OSMレイヤーのカバレッジと品質を改善すること、または2）ナビゲーションや{土地利用}などのアプリケーションにサービスを提供する画像データに基づいてモデルをトレーニングするために既存のOSMレイヤーを使用することを目的としています。分類..それにもかかわらず、OSMは{Geosciences}、地球観測、環境科学のいくつかのアプリケーションで広く使用されています。この作業では、OSMデータを改善して使用するための機械学習に基づく最近の方法のレビューを示します。 osmによれば、
[ABSTRACT] osmはデータの解釈方法を変える可能性があると、osm.osmはデータの改善と使用に使用されていると述べています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Knowledge Distillation Meets Self-Supervision -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_48.html">
      <font color="black">Knowledge Distillation Meets Self-Supervision</font>
    </a>
  </h2>
  <font color="black">教師ネットワークから「暗い知識」を抽出して学生ネットワークの学習を導くことを含む知識抽出は、モデル圧縮と転移学習の重要な手法として浮上しています。一見異なる自己監督タスクが役立つことができることを示しますシンプルかつ強力なソリューションとして。これらの自己監視信号間の類似性を補助タスクとして活用することにより、教師から生徒に効果的に隠された情報を転送できます。 
[ABSTRACT]新しい論文で、事前トレーニング済みの教師モデルから「より豊かな暗黒の知識」を抽出する方法について説明します。これは、蒸留に使用できる自己知識蒸留の作業です。この記事では、ノイズの多い信号を利用する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-12">
        <br><font color="black">2020-06-12</font>
      </time>
    </span>
</section>
<!-- paper0: SIMBA: Specific Identity Markers for Bone Age Assessment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_49.html">
      <font color="black">SIMBA: Specific Identity Markers for Bone Age Assessment</font>
    </a>
  </h2>
  <font color="black">この目的のために、私たちは最新のモデルに基づいて構築し、アイデンティティマーカーに存在する情報を、元の手のレントゲン写真から作成された視覚的特徴と融合します。SIMBAは、コンピューター支援診断の新しい波のトレンドを打ち立てます。患者に関して利用可能なすべてのデータを組み込む方法。手作業によるアプローチでは、放射線医は骨年齢、すなわち年代順の年齢と性別を計算するときに、異なるアイデンティティーマーカーを考慮に入れます。 
[ABSTRACT]放射線科医はさまざまな年齢のマーカーを考慮に入れます。simbaは放射線学的手のポーズデータセットに基づいています。simbaは以前の状態より優れています-最先端の方法</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-10">
        <br><font color="black">2020-07-10</font>
      </time>
    </span>
</section>
<!-- paper0: Exploiting Deep Generative Prior for Versatile Image Restoration and
  Manipulation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_50.html">
      <font color="black">Exploiting Deep Generative Prior for Versatile Image Restoration and
  Manipulation</font>
    </a>
  </h2>
  <font color="black">また、ランダムジッタリング、画像モーフィング、カテゴリ転送などの多様な画像操作を可能にします。このような非常に柔軟な復元と操作は、ジェネレータを修正する傾向がある既存のGAN反転方法の仮定を緩和することで可能になります。 .1、深生成的事前（DGP）は、さまざまな劣化画像の欠落したセマンティクス（色、パッチ、解像度など）を復元するための強力な結果を提供します。 
[ABSTRACT]以前の深い画像（dip）は、欠けている正気を復元するための説得力のある結果を提供します。これには、色、空間的一貫性、テクスチャ、および高レベルの概念が含まれます。これらは、既存のガン反転法の仮定を緩和することで可能になり、発電機を修理する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-30">
        <br><font color="black">2020-03-30</font>
      </time>
    </span>
</section>
<!-- paper0: Point-Set Anchors for Object Detection, Instance Segmentation and Pose
  Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CV/paper_51.html">
      <font color="black">Point-Set Anchors for Object Detection, Instance Segmentation and Pose
  Estimation</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、この汎用的なアプローチが、これらの各タスクの最先端の方法と競争力のあるパフォーマンスを達成できることを示しています。このポイントセットは、トレーニングのモードなど、特定のタスクの適切な初期化を反映するように調整されています中心点よりもグラウンドトゥルースに近い位置にあるポーズ推定のデータ。回帰の情報提供機能を提供します。推論を容易にするために、代わりに、より有利な位置に配置された一連の点から回帰を実行することを提案します。 
[要約]ジェネレーターの中心はシンプルで効率的ですが、中心点で抽出された画像の特徴には、離れたキーポイントを予測するための限られた情報が含まれていると主張します。姿勢推定のトレーニングデータ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-06">
        <br><font color="black">2020-07-06</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Transformer with Depth-Wise LSTM -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CL/paper_0.html">
      <font color="black">Transformer with Depth-Wise LSTM</font>
    </a>
  </h2>
  <font color="black">モデルの深さを増やすと、ニューラルモデルが複雑な関数をモデル化できるようになりますが、最適化の問題が発生する可能性もあります。6層トランスフォーマーを使った実験では、このアプローチにより、WMT 14英語-ドイツ語と英語-フランス語の両方でBLEUが大幅に改善されることを示していますタスクと、ディープトランスフォーマーの実験は、ディープトランスフォーマーの収束に対する深度方向のLSTMの効果を示しています。私たちの分析結果は、残留接続よりも深度方向のLSTMで層ごとの非線形性のより効率的な使用をサポートします。 
[ABSTRACT]トランスフォーマー変換モデルは、深さを活用する方法を示しています-残差接続のようなlstmです。これは、レイヤーの英語以外-残差がパフォーマンスに与える影響を測定することをお勧めします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Open-Domain Conversational Agents: Current Progress, Open Problems, and
  Future Directions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CL/paper_1.html">
      <font color="black">Open-Domain Conversational Agents: Current Progress, Open Problems, and
  Future Directions</font>
    </a>
  </h2>
  <font color="black">特に、継続的な学習、魅力的なコンテンツの提供、適切な行動の特性、およびそれらの提供の成功を測定する方法について詳しく説明します。最後に、私たちの経験と学習、およびコミュニティ..私たちは、魅力的なオープンドメインの会話エージェントを構築するために必要なものについての見解を提示します。そのようなエージェントの品質、これまでに構築されたパズルのピース、そしてまだ埋めていないギャップホールをカバーします。 。 
[ABSTRACT]私たちは偏見を示し、私たち自身のグループが行った作業に焦点を当てます。最後に、経験と学習について話し合い、それらに従事します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-22">
        <br><font color="black">2020-06-22</font>
      </time>
    </span>
</section>
<!-- paper0: What BERT is not: Lessons from a new suite of psycholinguistic
  diagnostics for language models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CL/paper_2.html">
      <font color="black">What BERT is not: Lessons from a new suite of psycholinguistic
  diagnostics for language models</font>
    </a>
  </h2>
  <font color="black">言語モデリングによる事前トレーニングは、NLPタスクへの人気のある成功したアプローチになっていますが、これらの事前トレーニングプロセスがモデルに与える言語能力はまだ正確に理解していません。ケーススタディとして、これらの診断を一般的なBERTに適用しますモデルは、人間よりも感度は低いものの、一般的には共有カテゴリまたは役割の反転を含む不適切な完了から良いものを区別でき、名詞の上位語を確実に取得しますが、困難な推論と役割ベースのイベント予測に苦労しています-特に、それは否定の文脈上の影響に明らかに鈍感であることを示しています。この論文では、人間の言語実験から引き出された一連の診断を紹介します。 
[ABSTRACT]言語テストは、言語テストから学ぶ方法の一部です。人間の言語実験からの一連の診断も導入します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-07-31">
        <br><font color="black">2019-07-31</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Scholarly Knowledge Representation: Evaluating BERT-based
  Models for Scientific Relation Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CL/paper_3.html">
      <font color="black">Improving Scholarly Knowledge Representation: Evaluating BERT-based
  Models for Scientific Relation Classification</font>
    </a>
  </h2>
  <font color="black">私たちの研究は、強化された学術情報組織のための知識グラフベースのシステムを構築するための適切な手法を選択するためのデジタルライブラリの利害関係者に推奨事項を提供することを目的としています。3つのコーパスでの実験は、ドメイン固有の事前トレーニングコーパスがバートベースのコーパスに利益をもたらすことを示しています科学的関係のタイプを識別する分類モデル。このようなグラフベースのパイプライン内では、関連する科学的概念間の関係タイプを推測することが重要なステップです。 
[ABSTRACT] 8つのbertベースの分類モデルの迅速な評価。3つのシナリオが必要です。8つのデジタルモデル出版物に関する研究。これは、自動関係分類の長期システムの最初のステップです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-13">
        <br><font color="black">2020-04-13</font>
      </time>
    </span>
</section>
<!-- paper0: ProtTrans: Towards Cracking the Language of Life's Code Through
  Self-Supervised Deep Learning and High Performance Computing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CL/paper_4.html">
      <font color="black">ProtTrans: Towards Cracking the Language of Life's Code Through
  Self-Supervised Deep Learning and High Performance Computing</font>
    </a>
  </h2>
  <font color="black">動機：NLPは、自動回帰および自動エンコード言語モデルによって大幅に改善し続けます。学習した情報は、いわゆる埋め込みによって下流の予測タスクに転送されます。ここで、2つの質問に取り組みました。（1）HPCはどの程度規模のデータベースおよびより大きなモデルへのタンパク質LMの大規模なスケール？ 
[ABSTRACT]これらのlmは、巨大なラベルなしテキストコーパスからの自己監視または非監視の学習のために高価なコンピューティングリソースを必要とします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Do You Have the Right Scissors? Tailoring Pre-trained Language Models
  via Monte-Carlo Methods -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CL/paper_5.html">
      <font color="black">Do You Have the Right Scissors? Tailoring Pre-trained Language Models
  via Monte-Carlo Methods</font>
    </a>
  </h2>
  <font color="black">さまざまなテキスト生成データセットの実験により、MC-Tailorが一貫して大幅に微調整アプローチよりも優れていることが示されています。このホワイトペーパーでは、MC-Tailorを提案します。MC-Tailorは、切り捨てて転送することにより、テキスト生成タスクで上記の問題を軽減する新しい方法です。過大評価された領域から過小評価された領域への確率質量。コードはこのURLで入手できます。 
[ABSTRACT] mc-tailorは、微調整されたアプローチを一貫して大幅に上回っています。さまざまなテキスト生成データセットの実験から、mc-tailorが示されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: RNA-2QCFA: Evolving Two-way Quantum Finite Automata with Classical
  States for RNA Secondary Structures -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CL/paper_6.html">
      <font color="black">RNA-2QCFA: Evolving Two-way Quantum Finite Automata with Classical
  States for RNA Secondary Structures</font>
    </a>
  </h2>
  <font color="black">オートマトン理論を使用したDNAおよびRNAの二次構造のモデリングは、コンピューターサイエンスの分野に大きな影響を与えました。量子計算モデルを使用してRNAの二次生体分子構造をモデル化することは自然な目標です。古典的な状態の双方向量子有限オートマトン言語認識では、双方向の確率的有限オートマトンよりも支配的です。 
[要約]量子コンピューティングモデルは、デオキシリボ核酸（dna）およびリボ核酸（rna）構造のモデリングに大きな可能性を秘めています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: CIRCE at SemEval-2020 Task 1: Ensembling Context-Free and
  Context-Dependent Word Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CL/paper_7.html">
      <font color="black">CIRCE at SemEval-2020 Task 1: Ensembling Context-Free and
  Context-Dependent Word Representations</font>
    </a>
  </h2>
  <font color="black">主な調査結果は、（1）文脈自由な単語表現は強力で堅牢なベースラインであり、（2）文分類の目的は、有用な文脈依存の単語表現を取得するために使用でき、（3）これらの表現を組み合わせると、場合によってはパフォーマンスを改善し、両方に固有の関連情報が含まれていることを示唆します。このペーパーでは、SemEval-2020タスク1への優勝した貢献について説明します：チームUG Student Internから渡された教師なし語彙意味変化検出（サブタスク2）。文脈自由および文脈依存の単語表現に基づく予測。 
[ABSTRACT]文脈に基づいて予測を行うアンサンブルモデルを提示します-無料および文脈依存の単語表現</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-30">
        <br><font color="black">2020-04-30</font>
      </time>
    </span>
</section>
<!-- paper0: PLATO-2: Towards Building an Open-Domain Chatbot via Curriculum Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CL/paper_8.html">
      <font color="black">PLATO-2: Towards Building an Open-Domain Chatbot via Curriculum Learning</font>
    </a>
  </h2>
  <font color="black">PLATO-2は中国語と英語の両方のデータでトレーニングされ、その有効性と優位性は包括的な評価を通じて検証され、新しい最先端の結果が得られます。学習プロセスには2つの段階があります。2番目の段階では、きめの細かい生成モデルと評価モデルは、多様な応答生成と応答コヒーレンス推定をそれぞれ学習するようにさらにトレーニングされます。 
[要約]学習プロセスには2つの段階があります。1つの段階は、きめの細かい生成モデルと評価モデルです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-30">
        <br><font color="black">2020-06-30</font>
      </time>
    </span>
</section>
<!-- paper0: CoSDA-ML: Multi-Lingual Code-Switching Data Augmentation for Zero-Shot
  Cross-Lingual NLP -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CL/paper_9.html">
      <font color="black">CoSDA-ML: Multi-Lingual Code-Switching Data Augmentation for Zero-Shot
  Cross-Lingual NLP</font>
    </a>
  </h2>
  <font color="black">既存の作業と比較して、私たちの方法はトレーニングにバイリンガルの文に依存せず、複数のターゲット言語に対して1つのトレーニングプロセスのみを必要とします。19言語の5つのタスクでの実験結果は、この方法によりすべてのタスクのパフォーマンスが大幅に向上することを示しています。 mBERTと比較して。多言語BERT（mBERT）などの多言語のコンテキスト化された埋め込みは、さまざまなゼロショットのクロス言語タスクで成功を収めています。 
[要約]私たちは、多言語コードを生成するデータ拡張フレームワークを提案します-データを切り替えてmbertを微調整します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-11">
        <br><font color="black">2020-06-11</font>
      </time>
    </span>
</section>
<!-- paper0: A theory of interaction semantics -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CL/paper_10.html">
      <font color="black">A theory of interaction semantics</font>
    </a>
  </h2>
  <font color="black">したがって、プロトコルにその文字セットを割り当てると、プロトコルの一貫性が維持され、履行関係が作成されます。提案されている相互作用セマンティクスの理論は、情報の転送と処理のモデルと一貫しており、正式なセマンティクスのモデルと明確な関係があります。文字の意味は名前の変更に対して不変であり、相互作用の技術的な説明で意味の概念を特定するという事実を説明します。この記事の目的は、相互作用のセマンティクスの理論を描写し、それによって「インタラクション内で交換された文字の意味」。 
[要約]そのようなプロトコルの一貫性は、その文字セットの特定の選択に依存します。文字の解釈は、その意味になります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Topic Modeling on User Stories using Word Mover's Distance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CL/paper_11.html">
      <font color="black">Topic Modeling on User Stories using Word Mover's Distance</font>
    </a>
  </h2>
  <font color="black">クラウドワーカーによって記述および分類された2,966のユーザーストーリーの公的に利用可能なセットでのアプローチを評価します。要件の抽出は最近、クラウドベースの手法で補完され、さまざまな方法でフィードバックを表現するユーザーの大規模な異種グループが継続的に関与します。メディア..アプローチで使用する埋め込みに応じて、ユーザーストーリーを2つの方法でクラスター化します。1つは元の分類に近い方法、もう1つはデータセットへの新しい洞察を可能にする方法、たとえば
[ABSTRACT]群集-ベース顕在化は、早い段階でユーザーと交流する大きな可能性を秘めていますが、生の構造化されていないフィードバックの大きなセットももたらします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-10">
        <br><font color="black">2020-07-10</font>
      </time>
    </span>
</section>
<!-- paper0: Fine-Grained Analysis of Cross-Linguistic Syntactic Divergences -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CL/paper_12.html">
      <font color="black">Fine-Grained Analysis of Cross-Linguistic Syntactic Divergences</font>
    </a>
  </h2>
  <font color="black">異なる言語の構文が収束および分岐するパターンは、クロスリンガル転送に関する作業を通知するためによく使用されます。さらに、5つの言語で、Parallel UDコーパスの手動で単語に揃えられたサブセットである新しいデータセットを提示し、それを使用します。詳細なコーパススタディを実行します。クロスリンガルパーサーのパフォーマンスパターンの説明に役立つことを示すことにより、結果の分析の有用性を示します。 
[ABSTRACT]言語ペア全体のさまざまなコーパスの相違の有病率を定量化するための研究はほとんど行われていません。我々のフレームワークは、言語間の相違の詳細な図を提供し、以前のアプローチを一般化し、完全な自動化に役立っていることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-07">
        <br><font color="black">2020-05-07</font>
      </time>
    </span>
</section>
<!-- paper0: A Novel Attention-based Aggregation Function to Combine Vision and
  Language -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CL/paper_13.html">
      <font color="black">A Novel Attention-based Aggregation Function to Combine Vision and
  Language</font>
    </a>
  </h2>
  <font color="black">実験的に、私たちのアプローチが両方のタスクでパフォーマンスの向上につながることを実証します。さらに、アプローチの各コンポーネントの役割を検証するためにアブレーション研究を実施します。画像テキストのマッチングと視覚的な質問応答、構築に関するアプローチをテストしますCOCOとVQA 2.0の両方のデータセットについて、他の削減の選択肢との公平な比較。 
[ABSTRACT]ツールが言語を理解できるのは初めてです。これは、各言語の各要素のスコアのセットを分析することで機能します。これには、両方の分類に使用できる学習可能なクロスモーダル削減が含まれますとランキング</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-27">
        <br><font color="black">2020-04-27</font>
      </time>
    </span>
</section>
<!-- paper0: Reducing Language Biases in Visual Question Answering with
  Visually-Grounded Question Encoder -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CL/paper_14.html">
      <font color="black">Reducing Language Biases in Visual Question Answering with
  Visually-Grounded Question Encoder</font>
    </a>
  </h2>
  <font color="black">最近の3つのVQAモデルに対するVGQEの効果を示し、バイアスに敏感なVQAv2データセットの分割で最先端の結果を達成します。 VQA-CPv2 ..したがって、質問の表現自体が十分な視覚的根拠を得て、モデルの言語優先度への依存を低減します。さらに、既存のバイアス低減手法とは異なり、標準のVQAv2ベンチマークでは、このアプローチは精度を落とす;代わりに、パフォーマンスが向上します。 
[ABSTRACT]最近の調査によると、vqaにはこの効果を低減する新しいモデルがあります。モデルは標準のvqav2ベンチマークと同じモデルを使用しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Sentiment Analysis on Social Media Content -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CL/paper_15.html">
      <font color="black">Sentiment Analysis on Social Media Content</font>
    </a>
  </h2>
  <font color="black">ただし、私たちが提案するモデルは、教師ありと教師なしの機械学習アルゴリズムの使用を組み合わせているため、この分野の以前の研究とは異なります。抽出された各ツイートは、ポジティブ、ネガティブ、またはニュートラルであるかどうかの感情に基づいて分類されています。これらの結果モデルは、相互検証やfスコアなどのさまざまなテスト指標を使用してテストされました。たとえば、twitterは、ユーザーがツイートと呼ばれる投稿を送受信し、さまざまなコミュニティと対話するプラットフォームです。抽出された各ツイートは、肯定的、否定的、または中立であるかどうかの意見に基づいて分類されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-04">
        <br><font color="black">2020-07-04</font>
      </time>
    </span>
</section>
<!-- paper0: Generating Fluent Adversarial Examples for Natural Languages -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CL/paper_16.html">
      <font color="black">Generating Fluent Adversarial Examples for Natural Languages</font>
    </a>
  </h2>
  <font color="black">IMDBとSNLIの実験は、提案されたMHAが攻撃機能のベースラインモデルよりも優れていることを示しています。MAHを使用した敵対的なトレーニングも、堅牢性とパフォーマンスの向上につながります。次に、生成された例の流暢さを保証できません。 
[ABSTRACT] mhaは、メトロポリスを実行することによって両方の問題に対処するように設計されています-ヘイスティングスサンプリング、その提案は勾配のガイダンスで設計されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating
  Cross-lingual Generalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/cs.CL/paper_17.html">
      <font color="black">XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating
  Cross-lingual Generalization</font>
    </a>
  </h2>
  <font color="black">このために、多言語エンコーダのクロスリンガルトランスファー評価XTREMEベンチマークを紹介します。XTREMEベンチマークは、40の言語と9つのタスクにわたる多言語表現のクロスリンガル一般化機能を評価するためのマルチタスクベンチマークです。機械学習のアプリケーションにおける最近の進歩NLPへのモデルは、さまざまなタスクにわたってモデルを評価するベンチマークによって推進されてきました。英語でテストされたモデルは多くのタスクで人間のパフォーマンスに到達しますが、言語を越えて転送されたモデルのパフォーマンスにはかなりのギャップがあることを示しています。特に構文および文検索タスクについて。 
[ABSTRACT]これは、多言語モデルへの関心が高まっているにもかかわらずです。ベンチマークにより、さまざまな言語とタスクでそのようなメソッドを包括的に評価できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-24">
        <br><font color="black">2020-03-24</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: Vector-Quantized Timbre Representation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.AS/paper_0.html">
      <font color="black">Vector-Quantized Timbre Representation</font>
    </a>
  </h2>
  <font color="black">与えられた音色分布の量子化された表現を学習するために、ラウドネスから分離された離散潜在空間を備えたオートエンコーダを導入します。可変長入力信号を量子化された潜在特徴にエンコードすることにより、音色転送を実行できます。学習された音色に従ってデコードされます。いくつかの作品では、さまざまな楽器の特徴の関係を分析することによって高レベルの音色合成を研究しましたが、音響特性は絡み合ったままで、生成は個々の音に結びついています。 
[ABSTRACT] timbreはtimbreとして知られており、通信方法として特定されています。これは、timbreの周波数を使用するために使用されますが、代替として使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Generative Modelling for Controllable Audio Synthesis of Expressive
  Piano Performance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.AS/paper_1.html">
      <font color="black">Generative Modelling for Controllable Audio Synthesis of Expressive
  Piano Performance</font>
    </a>
  </h2>
  <font color="black">想定される使用例の1つは、既存のピアノ曲の創造的で真新しい解釈を刺激することです。これは、前のサンプルからサンプリングしたり、他の曲から推測したりできる潜在変数である条件に基づいています。制御可能なニューラルGaussian Mixture Variational Autoencoders（GM-VAE）に基づくオーディオシンセサイザ。アーティキュレーションとダイナミクスという2つの重要なスタイルのピアノ演奏の時間的条件に厳密に従う、オーディオドメインでリアルなピアノ演奏を生成できます。 
[ABSTRACT]モデルは、オーディオを合成する過程で細粒度のモーフィングを適用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-16">
        <br><font color="black">2020-06-16</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic Lyrics Transcription using Dilated Convolutional Neural
  Networks with Self-Attention -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.AS/paper_2.html">
      <font color="black">Automatic Lyrics Transcription using Dilated Convolutional Neural
  Networks with Self-Attention</font>
    </a>
  </h2>
  <font color="black">セルフアテンションメカニズムの詳細な分析は、コンテキストの幅とアテンションヘッドの数を調整しながら行われます。この調査で使用したデータセットは、DAMP-Sing！です。この論文は、このタスクのための完全なパイプラインを提案します。このパイプラインは、一般に自動歌詞転記（ALT）と呼ばれます。 
[要約]この論文は、このタスクのための完全なパイプラインを提案します。これは、一般に自動歌詞認識と呼ばれることがあります。英語の多くのポップソングでトレーニングされています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Drum Beats and Where To Find Them: Sampling Drum Patterns from a Latent
  Space -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-07-14/eess.AS/paper_3.html">
      <font color="black">Drum Beats and Where To Find Them: Sampling Drum Patterns from a Latent
  Space</font>
    </a>
  </h2>
  <font color="black">私たちの知る限り、これはドラムパターン生成へのACAIの最初のアプリケーションです。敵対的に制約されたオートエンコーダ補間（ACAI）は、標準の変分オートエンコーダと比較してより良い結果を示します。いくつかの認識可能なジャンル領域を持つ潜在的な探索可能な空間を生成する人工ニューラルネットワークのアーキテクチャ。 
[ABSTRACT]オートエンコーダネットワークは、標準の変分オートエンコーダと比較してより良い結果を示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
