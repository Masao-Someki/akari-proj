<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-02-18の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Addressing the confounds of accompaniments in singer identification -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/cs.SD/paper_0.html">
      Addressing the confounds of accompaniments in singer identification
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      また、パフォーマンスを向上させるために、ボーカルメロディの輪郭から学習したメロディーの機能も取り入れています。したがって、歌手が目に見えないコンテキストで歌う場合、モデルはうまく一般化できません。具体的には、オープンアンミックス、ソース分離におけるアートパフォーマンス。音楽のボーカルトラックとインストゥルメンタルトラックを分離します。 
[要旨]歌手が特定の音楽コンテキストでのみ歌う場合、歌手識別モデルは歌のインストゥルメンタル部分から非ボーカル関連の特徴を抽出することを学習する場合があります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-17">
        <br>2020-02-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Using VAEs and Normalizing Flows for One-shot Text-To-Speech Synthesis
  of Expressive Speech -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/cs.SD/paper_1.html">
      Using VAEs and Normalizing Flows for One-shot Text-To-Speech Synthesis
  of Expressive Speech
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      知覚MUSHRAの評価は、標準のニューラルテキスト読み上げよりも9％の相対的な自然さの向上を伴う音声を作成できることを示しています。 22％KL-ダイバージェンスの削減と最先端の知覚指標を共同で改善。合成時に、表現力豊かなスタイルの一例を、希望するスタイルのテキストを生成するためのエンコーダへの参照入力として使用します。 
[概要]変分オートエンコーダー（vae）と世帯主フローを使用して、システムのもつれ解除機能を強化します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-28">
        <br>2019-11-28
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Lifter Training and Sub-band Modeling for Computationally Efficient and
  High-Quality Voice Conversion Using Spectral Differentials -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/cs.SD/paper_2.html">
      Lifter Training and Sub-band Modeling for Computationally Efficient and
  High-Quality Voice Conversion Using Spectral Differentials
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この方法では、トレーニングでフィルターの切り捨てが考慮されるため、変換精度を維持しながらフィルターのタップ長を短くすることができます。他の方法は、従来の方法を狭帯域（16 kHz）から完全に拡張するサブバンド処理です。帯域（48 kHz）VC。これは、フルバンド波形をより高い変換音声品質で変換できます。実験結果は、1）提案された狭帯域VCのリフタートレーニング方法により、タップ長を1/16に短縮できることを示しています。変換された音声品質を低下させること、および2）フルバンドVCに対して提案されたサブバンド処理方法は、従来の方法よりも変換された音声品質を改善することができます。 
[概要]サブフェーズフィルターを使用した従来の方法では、フルバンドの波形をより高い変換音声品質で変換できます。また、フィルター処理で給油することなく音声品質の量を削減するために使用することもできます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-17">
        <br>2020-02-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Interactive Text-to-Speech via Semi-supervised Style Transfer Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/cs.SD/paper_3.html">
      Interactive Text-to-Speech via Semi-supervised Style Transfer Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      その結果、入力クエリから抽出するか、手動で割り当てることができる、指定されたターゲットスタイルの埋め込みに一致する応答を持つ制御可能なマルチスタイルTTSシステムを開発しました。スタイル埋め込みとしてスタイル分類子からsoftmaxレイヤーを取得し、適用しますこのスタイル埋め込み抽出モデルは、ラベルのない内部TTSデータセットのソフトスタイルラベルを生成します。この半教師ありアプローチにより、信頼性の高いスタイル埋め込みが抽出され、マルチスタイルTTSシステムをトレーニングします。 
[概要]最初に、音響および文学的応答を使用してマルチモーダルスタイル分類モデルをトレーニングします。これは、音声埋め込みの分析に基づいてソフトスタイルラベルを作成します。この方法は、特定の種類の音声から応答を抽出するために使用できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-17">
        <br>2020-02-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Content Based Singing Voice Extraction From a Musical Mixture -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/cs.SD/paper_4.html">
      Content Based Singing Voice Extraction From a Musical Mixture
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      モデルのエンコーダー部分は、教師ネットワークを使用して知識の蒸留を介して学習し、コンテンツの埋め込みを学習します。コンテンツの埋め込みは、対応するボコーダー機能を生成するためにデコードされます。従来の客観的評価指標と矛盾するため、リスニングテストによる主観的評価を使用して、方法論を最新の深層学習ベースのソース分離アルゴリズムと比較します。 
[概要]このモデルは、音楽の混合物のエンコーダモデルに基づいています。ボーカルとの混合物のスペクトログラムの振幅成分を取ります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Feedback Recurrent AutoEncoder -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/cs.SD/paper_5.html">
      Feedback Recurrent AutoEncoder
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      具体的には、FRAEを強力なニューラルボコーダーと組み合わせて、低固定ビットレートで高品質の音声波形を生成できることを示します。さらに、潜在空間の学習済み事前分布を追加し、エントロピーコーダーを使用することにより、さらに低い可変ビットレートを実現できます。音声スペクトログラム圧縮におけるその有効性を実証します。 
[要約] fraeの反復構造は、時間次元に沿って冗長性を効率的に抽出するように設計されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-11">
        <br>2019-11-11
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Using VAEs and Normalizing Flows for One-shot Text-To-Speech Synthesis
  of Expressive Speech -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/cs.CL/paper_0.html">
      Using VAEs and Normalizing Flows for One-shot Text-To-Speech Synthesis
  of Expressive Speech
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      知覚MUSHRAの評価は、標準のニューラルテキスト読み上げよりも9％の相対的な自然さの向上を伴う音声を作成できることを示しています。 22％KL-ダイバージェンスの削減と最先端の知覚指標を共同で改善。合成時に、表現力豊かなスタイルの一例を、希望するスタイルのテキストを生成するためのエンコーダへの参照入力として使用します。 
[概要]変分オートエンコーダー（vae）と世帯主フローを使用して、システムのもつれ解除機能を強化します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-28">
        <br>2019-11-28
      </time>
    </span>
  </h3>
</article>
<!-- paper0: GameWikiSum: a Novel Large Multi-Document Summarization Dataset -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/cs.CL/paper_1.html">
      GameWikiSum: a Novel Large Multi-Document Summarization Dataset
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      参照サマリーの取得にはコストがかかるため、既存のデータセットには最大で数百個のサンプルしか含まれていないため、手作りの機能に大きく依存するか、手動で注釈を付けた追加データが必要になります。本稿では、新しいドメイン固有のGameWikiSumを提案しますマルチドキュメント要約のデータセットは、一般的に使用されるデータセットよりも100倍大きく、ニュースよりも別のドメインにあります。マルチドキュメント要約の分野における今日の研究の進展は、利用可能なデータセットが少ないため妨げられています。 
[ABSTRACT]既存のデータセットにはせいぜい数百個のサンプルしか含まれていないため、手作りの機能に大きく依存しています。ビデオゲームドメインには、同等のデータセットはありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-17">
        <br>2020-02-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: ESPnet-TTS: Unified, Reproducible, and Integratable Open Source
  End-to-End Text-to-Speech Toolkit -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/cs.CL/paper_2.html">
      ESPnet-TTS: Unified, Reproducible, and Integratable Open Source
  End-to-End Text-to-Speech Toolkit
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、統一された設計により、ASR機能とTTSの統合が可能になります。たとえば、ASRベースの客観的評価やASRモデルとTTSモデルの両方による半教師あり学習。ツールキットは、すべてのレシピの事前トレーニングモデルとサンプルも提供します。実験結果は、モデルが他の最新のツールキットに匹敵する最先端のパフォーマンスを達成し、LJSpeechデータセットで4.25の平均オピニオンスコア（MOS）をもたらすことを示しています。 
[概要]ツールキットは、最新のe2e-ttsモデルをサポートしています。ツールキットは、kaldi自動音声認識ツールキットに触発されたレシピも提供します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-24">
        <br>2019-10-24
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Incorporating BERT into Neural Machine Translation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/cs.CL/paper_3.html">
      Incorporating BERT into Neural Machine Translation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      BERT融合モデルという名前の新しいアルゴリズムを提案します。このアルゴリズムでは、最初にBERTを使用して入力シーケンスの表現を抽出し、次にその表現をアテンションメカニズムを通じてNMTモデルのエンコーダーとデコーダーの各レイヤーと融合します。 \ url {https://github.com/bert-nmt/bert-nmt}で入手できます。監視あり（文レベルおよび文書レベルの翻訳を含む）、半監視あり、監視なしの機械翻訳、および7つのベンチマークデータセットで最先端の結果を達成します。 
[ABSTRACT]これは、コンテキストに対してbertをより活用する方法を考える動機になります。教師あり、半教師あり、教師なしの機械翻訳で実験を行い、7つのベンチマークデータセットで最新の結果を達成します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-17">
        <br>2020-02-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Compositional Languages Emerge in a Neural Iterated Learning Model -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/cs.CL/paper_4.html">
      Compositional Languages Emerge in a Neural Iterated Learning Model
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      NILの確率モデルと合成言語の利点が存在する理由の説明を提供します。我々の実験は分析を確認し、また、出現した言語が神経エージェントコミュニケーションの一般化力を大幅に改善することを実証します。相互作用する神経エージェントに適用すると、より構造化されたタイプの言語の出現を促進する効果的な神経反復学習（NIL）アルゴリズムを提案します。 
[ABSTRACT] composedalは言語の自然な性質であり、言語ゲームの神経エージェントによって作成される通信プロトコルに現れることが期待される場合があります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-04">
        <br>2020-02-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Combined Stochastic and Physical Framework for Modeling Indoor 5G
  Millimeter Wave Propagation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/cs.CL/paper_5.html">
      A Combined Stochastic and Physical Framework for Modeling Indoor 5G
  Millimeter Wave Propagation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      その実装は、適応リンクバジェットを策定し、新しいメモリ最適化アルゴリズムを設計することによって解決する多くの計算上の課題を引き起こします。このアプローチは、屋内から屋内への5G mmWave伝播を調査するために特に適合します。素材、周波数、またはユースケースを使用して、屋内環境パラメーターがmmWave伝搬特性、特にカバレッジとパス損失に与える影響を統計的に理解することを目的としています。 
[ABSTRACT] igeostatは、確率的屋内環境モデリングと高度な物理通信シミュレーションを組み合わせたもので、電磁波と材料特性との相互作用に基づいて無線通信をシミュレートします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Generating Synthetic Audio Data for Attention-Based Speech Recognition
  Systems -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/cs.CL/paper_6.html">
      Generating Synthetic Audio Data for Attention-Based Speech Recognition
  Systems
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ASRシステムとTTSシステムは別々に構築され、テキストのみのデータを使用して、パラメーターやアーキテクチャを変更することなく既存のエンドツーエンドASRシステムを強化できることを示します。低リソース環境（LibriSpeech-100h）でのデータ拡張を使用した強力なベースラインに対する-rate（WER）。同等のOracle実験とのギャップを50 \％以上縮小します。 ASRコーパス自体でのみトレーニングされたTTSシステムによって生成された合成音声を使用した注意ベースの自動音声認識（ASR）システム。 
[ABSTRACT] librispeechの状態-960hをasrシステムに拡張します。librispeechの最新のasrベースラインに対して最大5％の相対werの改善も示しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-19">
        <br>2019-12-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Computing rank-revealing factorizations of matrices stored out-of-core -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/cs.CL/paper_7.html">
      Computing rank-revealing factorizations of matrices stored out-of-core
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      正確には、$ n \ times n $行列を完全に因数分解するための計算時間は$ cn ^ {3} $としてスケーリングします。スケーリング定数$ c $は、行列がコア外に格納されている場合にわずかに大きくなります。この論文では、2つの異なる方法について説明します。数値実験により、新しいアルゴリズムは、ハードドライブに格納されたデータを処理する場合、従来のアルゴリズムがメインメモリに格納されたデータとほぼ同じ速度であることを示しています。 
[ABSTRACT]列ピボットqr分解など、因子を明らかにするランクを計算するための従来のアルゴリズムは、非常に通信集約的です。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-17">
        <br>2020-02-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: HotelRec: a Novel Very Large-Scale Hotel Recommendation Dataset -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/cs.CL/paper_8.html">
      HotelRec: a Novel Very Large-Scale Hotel Recommendation Dataset
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、ホテルドメインは、従来の推奨データセットよりも高いデータスパース性に悩まされているため、従来の協調フィルタリングアプローチをそのようなデータに適用することはできません。 50M対0.9M）、さらに、単一ドメイン内でテキストレビューを含む最大の推奨データセット（50M対22M）..この論文では、HotelRecを提案します。 100万件のレビュー。 
[概要] tripadvisorに基づく最大のホテルレビューデータセットには、5,000万件のレビューが含まれています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-17">
        <br>2020-02-17
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Addressing the confounds of accompaniments in singer identification -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/eess.AS/paper_0.html">
      Addressing the confounds of accompaniments in singer identification
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      また、パフォーマンスを向上させるために、ボーカルメロディの輪郭から学習したメロディーの機能も取り入れています。具体的には、音楽のボーカルトラックとインストゥルメンタルトラックを分離するために、ソース分離で最先端のパフォーマンスを備えたオープンソースツールであるopen-unmixを採用しています.. artist20と呼ばれるベンチマークデータセットの評価結果は、このデータ増強方法が歌手識別の精度を大幅に改善することを示しています。 
[要旨]歌手が特定の音楽コンテキストでのみ歌う場合、歌手識別モデルは歌のインストゥルメンタル部分から非ボーカル関連の特徴を抽出することを学習する場合があります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-17">
        <br>2020-02-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Using VAEs and Normalizing Flows for One-shot Text-To-Speech Synthesis
  of Expressive Speech -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/eess.AS/paper_1.html">
      Using VAEs and Normalizing Flows for One-shot Text-To-Speech Synthesis
  of Expressive Speech
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      具体的には、Variable AutoEncoder（VAE）とHouseholder Flowを使用して、最先端のシーケンス間ベースのシステムのディスエンタングルメント機能を強化します。知覚的なMUSHRA評価により、9％の音声を作成できることが示されています。標準的なニューラルテキスト読み上げに対する相対的な自然性の向上、知覚される感情の強さの改善（ニュートラルスピーチの55と比較して59）。提案されたシステムは、状態に対する知覚メトリックを共同で改善しながら、最先端の。 
[概要]変分オートエンコーダー（vae）と世帯主フローを使用して、システムのもつれ解除機能を強化します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-28">
        <br>2019-11-28
      </time>
    </span>
  </h3>
</article>
<!-- paper0: ESPnet-TTS: Unified, Reproducible, and Integratable Open Source
  End-to-End Text-to-Speech Toolkit -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/eess.AS/paper_2.html">
      ESPnet-TTS: Unified, Reproducible, and Integratable Open Source
  End-to-End Text-to-Speech Toolkit
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果は、モデルが他の最新のツールキットに匹敵する最先端のパフォーマンスを達成できることを示しています。その結果、LJSpeechデータセットの平均オピニオンスコア（MOS）は4.25になります。さらに、統一された設計により、ASR機能のTTSとの統合が可能になります。たとえば、ASRベースの客観的評価やASRモデルとTTSモデルの両方での半教師あり学習です。 
[概要]ツールキットは、最新のe2e-ttsモデルをサポートしています。ツールキットは、kaldi自動音声認識ツールキットに触発されたレシピも提供します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-24">
        <br>2019-10-24
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Lifter Training and Sub-band Modeling for Computationally Efficient and
  High-Quality Voice Conversion Using Spectral Differentials -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/eess.AS/paper_3.html">
      Lifter Training and Sub-band Modeling for Computationally Efficient and
  High-Quality Voice Conversion Using Spectral Differentials
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この方法では、トレーニングでフィルターの切り捨てが考慮されるため、変換精度を維持しながらフィルターのタップ長を短くすることができます。他の方法は、従来の方法を狭帯域（16 kHz）から完全に拡張するサブバンド処理です。帯域（48 kHz）VC。これは、フルバンド波形をより高い変換音声品質で変換できます。実験結果は、1）提案された狭帯域VCのリフタートレーニング方法により、タップ長を1/16に短縮できることを示しています。変換された音声品質を低下させること、および2）フルバンドVCに対して提案されたサブバンド処理方法は、従来の方法よりも変換された音声品質を改善することができます。 
[概要]サブフェーズフィルターを使用した従来の方法では、フルバンドの波形をより高い変換音声品質で変換できます。また、フィルター処理で給油することなく音声品質の量を削減するために使用することもできます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-17">
        <br>2020-02-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Interactive Text-to-Speech via Semi-supervised Style Transfer Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/eess.AS/paper_4.html">
      Interactive Text-to-Speech via Semi-supervised Style Transfer Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      スタイル分類子からsoftmaxレイヤーをスタイル埋め込みとして取得し、このスタイル埋め込み抽出モデルを適用して、ラベルのない内部TTSデータセットのソフトスタイルラベルを生成します。ラベル付きデータの量が限られているため、感情認識データセットを組み合わせました。スタイルモデルトレーニング用の内部TTSデータセットの小さなラベル付きサブセットを使用したインタラクティブな感情的ダイアディックモーションキャプチャデータベース（IEMOCAP）。具体的には、音声発話の音響およびテキスト機能を使用してマルチモーダルスタイル分類モデルを最初にトレーニングします。 
[概要]最初に、音響および文学的応答を使用してマルチモーダルスタイル分類モデルをトレーニングします。これは、音声埋め込みの分析に基づいてソフトスタイルラベルを作成します。この方法は、特定の種類の音声から応答を抽出するために使用できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-17">
        <br>2020-02-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Meta-learning Extractors for Music Source Separation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/eess.AS/paper_5.html">
      Meta-learning Extractors for Music Source Separation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      後者と比較して、抽出プログラムのパラメーターが少なく、実行時のパフォーマンスが高速です。Meta-TasNetは、独立してトレーニングされたモデルまたはマルチタスク設定でトレーニングされたモデルよりも効果的であり、state-of -最先端の方法..重要なアーキテクチャ上の考慮事項について説明し、このアプローチのコストと利点を調査します。 
[要約]これにより、wi-in wi-wirelesslyエクストラクターがターゲットをテストできます。これは、「エクストラクター」として知られるツールにリンクされています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-17">
        <br>2020-02-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Content Based Singing Voice Extraction From a Musical Mixture -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/eess.AS/paper_6.html">
      Content Based Singing Voice Extraction From a Musical Mixture
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      モデルのエンコーダー部分は、教師ネットワークを使用して知識の蒸留によりトレーニングされ、コンテンツの埋め込みを学習します。コンテンツの埋め込みはデコードされ、対応するボコーダー機能を生成します。従来の客観的評価指標と矛盾するため、リスニングテストによる主観的評価を使用して、方法論を最新の深層学習ベースのソース分離アルゴリズムと比較します。 
[概要]このモデルは、音楽の混合物のエンコーダモデルに基づいています。ボーカルとの混合物のスペクトログラムの振幅成分を取ります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-12">
        <br>2020-02-12
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Generating Synthetic Audio Data for Attention-Based Speech Recognition
  Systems -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/eess.AS/paper_7.html">
      Generating Synthetic Audio Data for Attention-Based Speech Recognition
  Systems
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ASRシステムとTTSシステムは別々に構築され、テキストのみのデータを使用して、パラメーターやアーキテクチャを変更することなく既存のエンドツーエンドASRシステムを強化できることを示しています。 LibriSpeech-960hの最新のASRベースライン。同じテキストデータの言語モデル統合およびSpecAugmentのような単純なデータ拡張メソッドとこのメソッドを比較し、パフォーマンスの改善がほとんど独立していることを示します。 
[ABSTRACT] librispeechの状態-960hをasrシステムに拡張します。librispeechの最新のasrベースラインに対して最大5％の相対werの改善も示しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-19">
        <br>2019-12-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Feedback Recurrent AutoEncoder -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-18/eess.AS/paper_8.html">
      Feedback Recurrent AutoEncoder
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、学習した事前空間を潜在空間に追加し、エントロピーコーダーを使用することで、さらに低い可変ビットレートを実現できることを示します。低、固定ビットレートでの波形。FRAEの再帰構造は、時間次元に沿って冗長性を効率的に抽出するように設計されており、学習するデータのコンパクトな離散表現を可能にします。 
[要約] fraeの反復構造は、時間次元に沿って冗長性を効率的に抽出するように設計されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-11">
        <br>2019-11-11
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="">
      
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本日更新された論文はありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="">
        <br>
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
