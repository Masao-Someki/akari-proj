<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-03-11の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: A Multi-Phase Gammatone Filterbank for Speech Separation via TasNet -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/cs.SD/paper_0.html">
      A Multi-Phase Gammatone Filterbank for Speech Separation via TasNet
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      一般的なガンマトーンフィルターバンクとは対照的に、低レイテンシー処理を可能にするため、フィルターの長さは2ミリ秒に制限されています。学習したエンコーダーを提案されたマルチフェーズガンマトーンフィルターバンク（MP-GTF）に置き換えると、対数不変のソース対ノイズ比（SI-SNR）の0.7 dBの改善。Conv-TasNetによって学習されたエンコーダーに触発され、対数間隔のフィルターに加えて、提案されたフィルターバンクは同じ中心周波数で複数のガンマトーンフィルターを保持します。さまざまな位相シフト。 
[概要]提案されたフィルターバンクは確定的ガンマトーンフィルターバンクです。conv-tasnetによって学習された学習エンコーダーに基づいています。提案されたエンコーダーは同じ中心周波数で複数のガンマトーンフィルターを保持します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-25">
        <br>2019-10-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Neural Network Based Framework for Archetypical Sound Synthesis -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/cs.SD/paper_1.html">
      A Neural Network Based Framework for Archetypical Sound Synthesis
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      特に、人間が知覚するカオス/秩序レベルを音で予測し、この特徴の所望の量を示す新しい音色を合成するアプローチを提案します。その内部の素因をモデル化するために、ニューラルネットワークベースの方法を採用しました知覚的および抽象的な機能..このペーパーでは、音を分類するために人間が採用した典型的な構造をアルゴリズムで再現するための予備的なアプローチについて説明します。 
[要約]私たちは、人間が知覚するカオス/音のレベルを予測し、この特徴の望ましい量を示す新しい音色を合成するアプローチを提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-06">
        <br>2020-03-06
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Unsupervised Interpretable Representation Learning for Singing Voice
  Separation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/cs.SD/paper_2.html">
      Unsupervised Interpretable Representation Learning for Singing Voice
  Separation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの方法の利点を示すために、得られた表現をバイナリマスキングによる情報に基づいた歌声分離のタスクに採用し、スケール不変の信号対歪み比によって得られた分離品質を測定します。この作業では、解釈可能な音楽信号表現を波形信号から直接学習する方法を提示します。この機能は、単純な正弦波モデルをデコード関数として使用して、歌声を再構築します。 
[要約]教師なし目的を使用してメソッドをトレーニングできます。これは、単純な正弦波モデルを使用して歌声を再構築するノイズ除去自動エンコーダモデルに依存しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-03">
        <br>2020-03-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: The Airbus Air Traffic Control speech recognition 2018 challenge:
  towards ATC automatic transcription and call sign detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/cs.SD/paper_3.html">
      The Airbus Air Traffic Control speech recognition 2018 challenge:
  towards ATC automatic transcription and call sign detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      パイロットのスピーチの転写は、コントローラーのスピーチの2倍難しいことがわかりました。ATCスピーチ処理は、いくつかの理由で困難です。高いスピーチ率、非常に多様なアクセントのある外国語スピーチ、ノイズの多いコミュニケーションチャネル。 40時間のスピーチと手動の書き起こしが提供されました。 
[概要]課題は、英語の航空管制（atc）スピーチに適用される2つのタスクで構成されます。最高ランクのチームは7. 62％の単語エラー率と82. 41％csdの勝利を達成しました。また議論した
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-10-30">
        <br>2018-10-30
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: On the Variance of the Adaptive Learning Rate and Beyond -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/cs.CL/paper_0.html">
      On the Variance of the Adaptive Learning Rate and Beyond
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      画像分類、言語モデリング、ニューラル機械翻訳に関する広範な実験結果により、直感が検証され、提案された方法の有効性と堅牢性が実証されます。すべての実装は、https：//github.com/LiyuanLucasLiu/RAdam。適応学習率の分散を修正する用語を導入して、Adamの新しいバリアントであるRAdamを提案します。 
[要約]適応学習率が初めて提案されました。この方法は、アダムの新しい変種であるラダムと呼ばれます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-08">
        <br>2019-08-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: ReZero is All You Need: Fast Convergence at Large Depth -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/cs.CL/paper_1.html">
      ReZero is All You Need: Fast Convergence at Large Depth
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      12層のトランスフォーマーに適用すると、ReZeroはenwiki8で56％高速に収束します。深い信号伝搬を促進するため、ReZeroを提案します。 .. ReZeroはトランスフォーマーを超えて他の残留ネットワークに適用され、CIFAR 10でトレーニングされたResNet-56で、完全に接続された深いネットワークで1,500％高速な収束、32％高速な収束を可能にします。変圧器では、複数の層を訓練するのは困難です。マルチヘッドの自己注意がこの劣悪な信号通信の主な原因です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-10">
        <br>2020-03-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual
  Lexical Semantic Similarity -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/cs.CL/paper_2.html">
      Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual
  Lexical Semantic Similarity
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      単一言語およびクロスリンガルベンチマークでは、静的およびコンテキスト化された単語の埋め込み（fastText、M-BERT、XLMなど）を含む、最新の最先端の単一言語およびクロスリンガル表現モデルの幅広い配列を評価および分析します、外部からの情報に基づいた字句表現、および完全に教師なしおよび（わずかに）監督されたクロスリンガルワード埋め込み。これらの貢献を行います。Multi-SimLexデータセットの公開リリース、作成プロトコル、強力なベースライン結果、および詳細Multi-Simlexをさらに多くの言語にさらに拡張するためのコミュニティの取り組みを促進するWebサイトから入手できる、多言語の語彙セマンティクスと表現学習の将来の開発を支援するのに役立つ分析。追加の言語用に一貫したマルチシンプレックススタイルのリソースを作成するためのデータセット作成プロトコル。 
[ABSTRACT]各言語データセットには、意味的類似性の字句関係について注釈が付けられています。これには、1,888個の意味的に整列した概念ペアが含まれ、単語クラスの代表的なカバレッジを提供します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-10">
        <br>2020-03-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Variational Self-attention Model for Sentence Representation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/cs.CL/paper_3.html">
      Variational Self-attention Model for Sentence Representation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      自己注意メカニズムは、ソース情報を重み付き和による注意ベクトルとして要約します。重みは学習された確率分布です。さらに、潜在変数を無視することにより、VSAMは過剰適合に対してより堅牢になります。私たちの方法の優位性。 
[概要] vsamに組み込まれた確率単位により、マルチモーダルな注意分布が可能になります。確率分布を課すことにより、情報をランダム変数としてモデル化します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-12-30">
        <br>2018-12-30
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Efficient Intent Detection with Dual Sentence Encoders -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/cs.CL/paper_4.html">
      Efficient Intent Detection with Dual Sentence Encoders
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらの要件に基づいて、USEやConveRTなどの事前トレーニングされたデュアルセンテンスエンコーダーに裏付けられたインテント検出方法を紹介します。提案されたインテント検出器の有用性と幅広い適用性を実証します。1）微調整に基づいてインテント検出器よりも優れている完全なBERT大モデル、または3つの多様な意図検出データセットで固定ブラックボックスエンコーダーとしてBERTを使用。 2）ゲインは、特にショット数が少ないセットアップで顕著です（つまり、インテントごとに注釈付きの例が10または30のみ）。 3）意図検出器は、単一のCPUで数分でトレーニングできます。 4）さまざまなハイパーパラメータ設定で安定しています。新しいドメインに会話型システムを構築し、機能を追加すると、低データ領域で動作するリソース効率の高いモデルが必要になります（つまり、少数ショット設定）。 
[ABSTRACT]意図検出方法は、事前学習済みのデュアルセンテンスエンコーダーによってサポートされています。新しい方法は、潜在的な検出数を減らすように設計されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-10">
        <br>2020-03-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Framework for Evaluation of Machine Reading Comprehension Gold
  Standards -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/cs.CL/paper_5.html">
      A Framework for Evaluation of Machine Reading Comprehension Gold
  Standards
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このデータに存在する課題についての理解は限られていますが、比較を導き、信頼性の高い仮説を立てるのが難しくなります。最初の定性的な注釈スキーマと、後者の近似メトリックのセットを提案します。問題の軽減に向けて、本書では、現在の言語的特徴、必要な推論と背景知識、事実上の正確さを体系的に調査し、他方の理解の要件の下限としての語彙キューの存在を統合するフレームワークを提案します手。 
[要旨]新しい論文は、現在の言語的特徴を体系的に調査するための統一フレームワークを提案している。語彙のあいまいさおよび語彙キューの存在に寄与する特徴の欠如は、読解の複雑さと評価データの品質を低下させる可能性がある。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-10">
        <br>2020-03-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Video Caption Dataset for Describing Human Actions in Japanese -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/cs.CL/paper_6.html">
      Video Caption Dataset for Describing Human Actions in Japanese
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      人間の行動を説明するには、人、場所、行動の詳細を特定することが重要です。実際、人間の行動を説明するときは、通常、シーン、人、および行動に言及します。かなりの注目を集めています。 
[概要]日本語のビデオキャプションデータセットは、79、822ビデオと399、233キャプションで構成されています。実験では、ベンチマーク結果を得るために2つのキャプション生成方法を評価しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-10">
        <br>2020-03-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Learning to Respond with Stickers: A Framework of Unifying
  Multi-Modality in Multi-Turn Dialog -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/cs.CL/paper_7.html">
      Learning to Respond with Stickers: A Framework of Unifying
  Multi-Modality in Multi-Turn Dialog
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらの課題に取り組むために、ステッカー応答セレクター（SRS）モデルを提案します。1つは、対応するテキストラベルのないステッカーの意味を学習することです。もう1つの課題は、候補ステッカーをマルチターンダイアログコンテキストでモデル化することです。 
[ABSTRACT]ステッカーは、テキストラベルよりもステッカーのテキストラベルを必要とする傾向があります。これは、紙とステッカーが大量にあるためです。ステッカー選択フィールドでは、340kのマルチターンダイアログとステッカーのペアのデータセットをリリースします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-10">
        <br>2020-03-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: On the coexistence of competing languages -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/cs.CL/paper_8.html">
      On the coexistence of competing languages
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      また、さまざまなモデルパラメータの関数として、生き残っている言語の数の予測を取得します。競合する言語の進化を調査します。これは、多くの以前の文献が、結果が常に他のすべての言語の支配であると示唆しているテーマです。この出現は対称性の破れに関連していることを発見し、2つの特定のシナリオを調査します.1つは単一の地理的地域における言語話者の人口動態の不均衡に関連し、2つ目は言語の好みが存在する空間的不均一性に関連するものです異なる地理的地域に固有。 
[要旨]ここでは、言語競争の問題を再検討し、共存が生じる可能性のある方法を明らかにすることに重点を置いています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-10">
        <br>2020-03-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Ecological Semantics: Programming Environments for Situated Language
  Understanding -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/cs.CL/paper_9.html">
      Ecological Semantics: Programming Environments for Situated Language
  Understanding
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      広範な現代の認知科学に続いて、私たちは、環境を意味表現で「一流の市民」として扱い、それ自体で研究開発に値することを提案します。さまざまなタスクに柔軟に適用でき、最小限の構造的仮定を採用します。両方の世界の最高の部分をどのように享受できますか。 
[概要]これらは優れた一般化、グラウンディング、説明可能性であり、モデルは環境内の単なるアクターではなく、環境の作成と構成のパートナーであるべきだとアダム・ソベルは述べています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-10">
        <br>2020-03-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Pilot Study on Multiple Choice Machine Reading Comprehension for
  Vietnamese Texts -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/cs.CL/paper_10.html">
      A Pilot Study on Multiple Choice Machine Reading Comprehension for
  Vietnamese Texts
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      Machine Reading Comprehension（MRC）は、自然言語処理のタスクであり、非構造化テキストを読んで理解し、質問に対する正しい答えを見つける能力を研究します。ベトナム語の機械読解に関するさらなる研究。回答は、対応する読解テキストの単一または複数の文の内容から抽出される場合があります。 
[概要]このコーパスには、2、783の複数選択の質問と回答が含まれています。コーパスは、研究目的で当社のWebサイトで無料です。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-16">
        <br>2020-01-16
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Document Sub-structure in Neural Machine Translation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/cs.CL/paper_11.html">
      Document Sub-structure in Neural Machine Translation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験を実行するデータを作成してリリースします-ウィキペディアの伝記から3つの言語ペア（中国語-英語、フランス語-英語、ブルガリア語-英語）の並列コーパス、自動的に抽出し、記事内のセクションの境界を保持します。 。各文が含まれるセクションのトピックに関する情報を含める2つの異なる方法を比較します。1つはサイド制約を使用し、もう1つはキャッシュベースのモデルを使用します。これを使用するLouis and Webber（2014）統計的MTを改善し、その提案をニューラルMTのフレームワークに転送するための情報。 
[概要]データは、ウィキペディアのbiographies.theyからの3つの言語ペアに基づいています。自動的に抽出され、記事内のセクションの境界を保持します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-13">
        <br>2019-12-13
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: A Multi-Phase Gammatone Filterbank for Speech Separation via TasNet -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/eess.AS/paper_0.html">
      A Multi-Phase Gammatone Filterbank for Speech Separation via TasNet
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      Conv-TasNetの訓練されたエンコーダーと聴覚フィルターバンクの類似性を動機として、確定的なガンマトーンフィルターバンクを採用することを提案します。一般的なガンマトーンフィルターバンクとは対照的に、フィルターは低遅延処理を可能にするために2 msの長さに制限されています。 Conv-TasNetによって学習されたエンコーダーに触発され、提案されたフィルターバンクは、対数間隔フィルターに加えて、位相シフトが変化する同じ中心周波数で複数のガンマトーンフィルターを保持します。 
[概要]提案されたフィルターバンクは確定的ガンマトーンフィルターバンクです。conv-tasnetによって学習された学習エンコーダーに基づいています。提案されたエンコーダーは同じ中心周波数で複数のガンマトーンフィルターを保持します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-25">
        <br>2019-10-25
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Neural Network Based Framework for Archetypical Sound Synthesis -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/eess.AS/paper_1.html">
      A Neural Network Based Framework for Archetypical Sound Synthesis
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      内的傾向を利用して知覚的および抽象的な特徴をモデル化するために、ニューラルネットワークに基づく方法を採用しました。特に、人間が知覚するカオス/秩序レベルを音で予測し、望ましい音色を合成するアプローチを提案しますこの機能の量。このペーパーでは、音を分類するために人間が採用した典型的な構造をアルゴリズムで再現するための予備的なアプローチについて説明します。 
[要約]私たちは、人間が知覚するカオス/音のレベルを予測し、この特徴の望ましい量を示す新しい音色を合成するアプローチを提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-06">
        <br>2020-03-06
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Vowels and Prosody Contribution in Neural Network Based Voice Conversion
  Algorithm with Noisy Training Data -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/eess.AS/paper_2.html">
      Vowels and Prosody Contribution in Neural Network Based Voice Conversion
  Algorithm with Noisy Training Data
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      音声のない音は、ノイズの多いトレーニングデータの影響を最も大きく受けることがわかりました。ノイズフロアを40 dB上回る平均ノイズレベルは、有声音に比べて音声変換の成功を55.14％低下させることがわかりました。性別を超えた音声変換では、女性がターゲットスピーカーであるシナリオでは、韻律変換がより重要です。 
[概要]ノイズレベルの40 dB上のノイズレベルは、音声変換の成功を55.14％低下させました。結果は、音声と韻律がシステムの最も重要なコンポーネントであることを示しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-10">
        <br>2020-03-10
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Unsupervised Interpretable Representation Learning for Singing Voice
  Separation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/eess.AS/paper_3.html">
      Unsupervised Interpretable Representation Learning for Singing Voice
  Separation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの発見は、私たちの方法が歌の声の分離のための意味のある表現を学習できることを示唆している一方で、音声や音声で望まれる非負性、滑らかさ、時間周波数マスキングの対象となる再構成などの短時間フーリエ変換の利便性を維持します音楽ソースの分離。私たちの方法は、教師なしの目的を使用して訓練することができ、歌声を再構成するデコード関数として単純な正弦波モデルを使用するノイズ除去オートエンコーダモデルに依存します。波形信号から直接信号表現。 
[要約]教師なし目的を使用してメソッドをトレーニングできます。これは、単純な正弦波モデルを使用して歌声を再構築するノイズ除去自動エンコーダモデルに依存しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-03">
        <br>2020-03-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: The Airbus Air Traffic Control speech recognition 2018 challenge:
  towards ATC automatic transcription and call sign detection -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/eess.AS/paper_4.html">
      The Airbus Air Traffic Control speech recognition 2018 challenge:
  towards ATC automatic transcription and call sign detection
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      登録された参加者には、手動の書き起こしとともに40時間のスピーチが提供されました。最高ランクのチームは、7.62％の単語エラー率と82.41％のCSD F1スコアを達成しました。 &#39;スピーチ。 
[概要]課題は、英語の航空管制（atc）スピーチに適用される2つのタスクで構成されます。最高ランクのチームは7. 62％の単語エラー率と82. 41％csdの勝利を達成しました。また議論した
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-10-30">
        <br>2018-10-30
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multitask Emotion Recognition with Incomplete Labels -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-03-11/eess.AS/paper_5.html">
      Multitask Emotion Recognition with Incomplete Labels
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最初に、3つのタスクすべてを実行する教師モデルをトレーニングします。各インスタンスは、対応するタスクのグラウンドトゥルースラベルによって訓練されます。このアルゴリズムには2つのステップがあります。次に、教師モデルの出力をソフトラベルと呼びます。 。 
[ABSTRACT]ほとんどの既存のデータセットには3つのタスクすべてのラベルが含まれていません。2番目の課題に取り組むため、マルチタスクモデルが欠落している（不完全な）ラベルから学習するアルゴリズムを提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-10">
        <br>2020-02-10
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="">
      
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本日更新された論文はありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="">
        <br>
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
