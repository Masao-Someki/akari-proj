<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-06-09の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: DeepSonar: Towards Effective and Robust Detection of AI-Synthesized Fake
  Voices -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-09/cs.SD/paper_0.html">
      DeepSonar: Towards Effective and Robust Detection of AI-Synthesized Fake
  Voices
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この作業では、レイヤーごとのニューロン活性化パターンの力を活用して、実際の音声とAI合成された偽の音声の微妙な違いをキャプチャし、生の入力よりも明確な信号を分類子に提供できると推測します。最近の進歩により、 WaveNetのような音声合成、AIで合成された偽の音声は人間の耳と区別がつかず、誰にとっても本当の脅威である現実的で自然なDeepFakeの生成に広く適用されています。しかし、合成された偽の音声のための効果的で堅牢な検出器はまだ初期段階にあり、この新たな脅威に完全に取り組む準備ができていません。 
[ABSTRACT]カリフォルニア大学の研究者は新しいシステムに取り組んでいます。彼らはこの新たな脅威に完全に取り組む準備ができていないと言います。彼らは英語と中国語の言語を含む3つのデータセットで作成されています。彼らは高い検出率と低い誤りを裏付けていますこだわりの偽声におけるディープソナーの警報率
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-28">
        <br>2020-05-28
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Demographics Should Not Be the Reason of Toxicity: Mitigating
  Discrimination in Text Classifications with Instance Weighting -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-09/cs.CL/paper_0.html">
      Demographics Should Not Be the Reason of Toxicity: Mitigating
  Discrimination in Text Classifications with Instance Weighting
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      その結果、これらのデータセットでトレーニングされたモデルは、「同性愛者」という言葉だけが原因で、「彼女は私を同性愛者であることを幸せにさせます」のような文を乱用と見なす場合があります。最近のテキスト分類の使用の急増に伴い、研究者は、テキスト分類データセットに特定の意図しないバイアスがあることを発見しました。この形式化に基づいて、インスタンスを使用して非差別分布を回復することにより、モデルにとらわれない偏見トレーニングフレームワークをさらに提案します重み付け。事前に定義された人口統計学的識別用語のセット以外に、追加のリソースや注釈は必要ありません。 
[ABSTRACT]新しいデータセットは、この方法がテキスト分類データセットの意図しないバイアスの影響を軽減できることを示しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-29">
        <br>2020-04-29
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Transfer Learning for Information Extraction with Limited Data -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-09/cs.CL/paper_1.html">
      Transfer Learning for Information Extraction with Limited Data
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの提案の主なアイデアは、ディープニューラルネットワークの事前トレーニング済みモデルを再利用する転移学習の概念を、共通の統計的分類子の組み合わせと組み合わせて、抽出された各用語のクラスを決定することです。アプローチを検証するには、私たちはモデルをドキュメント処理の実際のケースに適用しました。これは日本の政府プロジェクトの競争入札のプロセスです。それを行うには、まずBERTを活用して実際のシナリオでのトレーニングデータの制限に対処し、次にBERTをスタックします。畳み込みニューラルネットワークを使用して、分類の隠れた表現を学習します。 
[ABSTRACT]このモデルにより、対象となるビジネスプロセスに特化した詳細レベルの情報の正確さを備えた、きめ細かい名前付きエンティティを抽出できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-06">
        <br>2020-03-06
      </time>
    </span>
  </h3>
</article>
<!-- paper0: On the Limitations of Cross-lingual Encoders as Exposed by
  Reference-Free Machine Translation Evaluation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-09/cs.CL/paper_2.html">
      On the Limitations of Cross-lingual Encoders as Exposed by
  Reference-Free Machine Translation Evaluation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      参照なしのMT評価のセマンティックエンコーダーとしてのパフォーマンスは低く、2つの主要な制限、つまり（a）相互翻訳の表現間の意味の不一致、そしてより顕著に、（b）「translationese」を罰することができないことがわかります。つまり、低品質のリテラル翻訳です。セグメントレベルのMT評価では、最良のメトリックは参照ベースのBLEUを5.7相関ポイント上回っています。参照なしの評価は、MTシステムのWebスケールの比較を可能にします。 
[ABSTRACT] study：ソーステキストを（時には低品質の）システム翻訳と比較します。これは、いずれかの言語の意味論的転送（m --suz）に基づいています。これらには、次のような対策が含まれます。指標の再特定と再開
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-03">
        <br>2020-05-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Exploration Based Language Learning for Text-Based Games -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-09/cs.CL/paper_3.html">
      Exploration Based Language Learning for Text-Based Games
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この作品は、テキストベースのコンピューターゲームをプレイする際に最先端のパフォーマンスを発揮できる探索および模倣学習ベースのエージェントを提示します。これらのゲームを学習エージェントにとって特に挑戦的なものにしている1つの側面は、組み合わせの大きなアクションスペースです。私たちの実験は、このアプローチがテキストベースのゲームを解く上で既存のソリューションよりも優れており、環境との相互作用の数の点でより効率的なサンプルであることを示しています。 
[ABSTRACT]テキストベースのコンピューターゲームはプレーヤーに自分の世界を説明します。プレーヤーはテキストを使用してゲームと対話することを期待します。これらのスキルは環境との対話を通じて取得できます。これにより、固定コーパスを使用するのではなく、学習環境が提供されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-24">
        <br>2020-01-24
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Massive Choice, Ample Tasks (MaChAmp):A Toolkit for Multi-task Learning
  in NLP -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-09/cs.CL/paper_4.html">
      Massive Choice, Ample Tasks (MaChAmp):A Toolkit for Multi-task Learning
  in NLP
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      MaChAmpの利点は、その柔軟な構成オプションと、テキスト分類からシーケンスのラベル付け、依存関係の解析まで、統一されたツールキットでのさまざまなNLPタスクのサポートです。転移学習、特にマルチタスク学習と事前トレーニング済みのアプローチを組み合わせるアプローチコンテキスト化された埋め込みと微調整により、自然言語処理の分野が近年飛躍的に進歩しました。この論文では、マルチタスク設定でBERTのような微調整モデルを簡単に使用するためのツールキットであるMaChAmpを紹介します。 
[要約]ツールキットは、さまざまな病気の高度な学習のためのツールキットです。マニュアルには、学習に使用できるツールキットであるmachampが含まれます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-29">
        <br>2020-05-29
      </time>
    </span>
  </h3>
</article>
<!-- paper0: ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework
  for Natural Language Generation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-09/cs.CL/paper_5.html">
      ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework
  for Natural Language Generation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      既存の事前トレーニング方法とは異なり、ERNIE-GENは多粒度ターゲットサンプリングを組み込んで事前トレーニングデータを構築します。これにより、エンコーダーとデコーダー間の相関が強化されます。生成を人間の書き込みパターンに近づけるために、このフレームワークはスパンバイ単語ごとに予測するのではなく、意味的に完全なスパンを連続的に予測するようにモデルをトレーニングするスパン生成フロー。実験結果は、ERNIE-GENがはるかに少ない量の事前トレーニングデータとパラメーターで最先端の結果を達成することを示しています。抽象的要約（GigawordおよびCNN / DailyMail）、質問生成（SQuAD）、対話生成（Persona-Chat）、生成的質問応答（CoQA）など、さまざまな言語生成タスク。 
[ABSTRACT] ernie-genは、事前トレーニングと微調整されたタスクをシーケンスするための拡張されたマルチフローシーケンスです。システムは、充填と生成のメカニズムとノイズを意識した生成方法を使用して、トレーニングと結論の不一致を埋めます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-26">
        <br>2020-01-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Character-level Japanese Text Generation with Attention Mechanism for
  Chest Radiography Diagnosis -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-09/cs.CL/paper_6.html">
      Character-level Japanese Text Generation with Attention Mechanism for
  Chest Radiography Diagnosis
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最初の課題は、日本語の単語の境界がはっきりしないため、単語の分割が難しいことです。しかし、胸部X線所見の生成に関する研究は主に英語に焦点を当てており、私たちの知る限り、日本語を研究した研究はありませんこの主題に関するデータ。胸部X線撮影は、患者の状態を診断し、重要な情報を特定するための一般的な方法です。そのため、緊急医療や健康診断など、さまざまな場面での日常診療で広く利用されています。 
[要約]日本語での調査結果の生成には2つの課題があります。1つの理論は、複数の正書法の変形があるということです。これは、多くの正書法の変形があるためです
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-06">
        <br>2020-04-06
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: FastSpeech 2: Fast and High-Quality End-to-End Text-to-Speech -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-09/eess.AS/paper_0.html">
      FastSpeech 2: Fast and High-Quality End-to-End Text-to-Speech
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、FastSpeech 2sを設計します。これは、テキストから音声波形を並列で直接生成する最初の試みであり、完全なエンドツーエンドのトレーニングとFastSpeechよりもさらに高速な推論の利点を享受しています。実験結果は、1）FastSpeech 2および2s大幅に簡素化されたトレーニングパイプラインと短縮されたトレーニング時間により、音声品質でFastSpeechより優れています。 2）FastSpeech 2および2sは、推論速度を大幅に向上させながら、自己回帰モデルの音声品質と一致させることができます。 1）教師からの簡略化された出力の代わりにグラウンドトゥルースターゲットを使用してモデルを直接トレーニングし、2）条件付き入力として音声のより多くの変動情報（ピッチ、エネルギー、より正確な継続時間など）を導入する。 
[概要] autoregressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressigressi
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-08">
        <br>2020-06-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Zero resource speech synthesis using transcripts derived from perceptual
  acoustic units -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-09/eess.AS/paper_1.html">
      Zero resource speech synthesis using transcripts derived from perceptual
  acoustic units
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      クラスター化されたCVCセグメントは、開始（CV）と減衰（VC）が過渡現象に対応し、韻が定常状態に対応するように初期化されます。このようにして得られたAUシーケンスは、合成モデルのトレーニングに使用されます。これらのCVCセグメントは、接続されたコンポーネントベースのグラフクラスタリング手法。 
[ABSTRACT]これらの信号は、テスト中に合成に使用できます。これらの信号は、接続されたタスクシステムを使用して収集されます。これらは、連続音声で再編成できます。提案されたアプローチのパフォーマンスは、zerospeech 2019チャレンジで評価されますデータベース
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-08">
        <br>2020-06-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: WaveNODE: A Continuous Normalizing Flow for Speech Synthesis -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-09/eess.AS/paper_2.html">
      WaveNODE: A Continuous Normalizing Flow for Speech Synthesis
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この論文では、音声合成のための連続正規化フローを利用するWaveNODEと呼ばれる新しい生成モデルを提案します。さらに、WaveNODEは教師ネットワークや補助損失項を必要とせずに尤度を最大化するように最適化できます。近年、さまざまなフローベースの生成モデルは、リアルタイムで高忠実度の波形を生成するために提案されています。 
[ABSTRACT] wavenodeを使用すると、より柔軟で複雑な関数を作成できます。これらのモデルには、十分にトレーニングされた教育済みネットワークまたは多数のフローステップが必要です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-08">
        <br>2020-06-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A non-causal FFTNet architecture for speech enhancement -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-09/eess.AS/paper_3.html">
      A non-causal FFTNet architecture for speech enhancement
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最後に、主観的および客観的なメトリックに基づいて、SE-FFTNetは信号品質の向上の点でWaveNetよりも優れていますが、SEGANと同等の優れたパフォーマンスを提供します。浅いネットワークを提案し、特定の制限内で非因果性を適用することにより、音声用に提案されたFFTNet強化（SE-FFTNet）は、WaveNetやSEGANなどの音声強化のための他のニューラルネットワークベースのアプローチと比較して、使用するパラメーターがはるかに少ないです。このホワイトペーパーでは、FFTNetに基づく音声強化のための新しい並列非因果的で浅い波形ドメインアーキテクチャを提案します。 、高品質のオーディオ波形を生成するためのニューラルネットワーク。 
[ABSTRACT] fftnetは、広い拡張パターンなどの初期の広い拡張パターンを使用します。これは、非因果的なfftnetアーキテクチャに基づいています。これは、ネットワークがモデルパラメーターを大幅に削減していることを示唆しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-08">
        <br>2020-06-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: DeepSonar: Towards Effective and Robust Detection of AI-Synthesized Fake
  Voices -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-09/eess.AS/paper_4.html">
      DeepSonar: Towards Effective and Robust Detection of AI-Synthesized Fake
  Voices
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ただし、合成された偽の音声のための効果的で堅牢な検出器はまだ初期段階にあり、この新たな脅威に完全に取り組む準備ができていません。この作業では、レイヤーごとのニューロン活性化パターンの力を活用して、本物とAIで合成された偽の声の微妙な違いがあり、生の入力よりも明確な信号を分類器に提供します。このホワイトペーパーでは、話者認識（SR）システムのニューロンの動作の監視に基づく、DeepSonarという新しいアプローチを考案しました。ニューラルネットワーク（DNN）。AIで合成された偽の音声を識別します。 
[ABSTRACT]カリフォルニア大学の研究者は新しいシステムに取り組んでいます。彼らはこの新たな脅威に完全に取り組む準備ができていないと言います。彼らは英語と中国語の言語を含む3つのデータセットで作成されています。彼らは高い検出率と低い誤りを裏付けていますこだわりの偽声におけるディープソナーの警報率
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-28">
        <br>2020-05-28
      </time>
    </span>
  </h3>
</article>
<!-- paper0: MultiSpeech: Multi-Speaker Text to Speech with Transformer -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-09/eess.AS/paper_5.html">
      MultiSpeech: Multi-Speaker Text to Speech with Transformer
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このホワイトペーパーでは、MultiSpeechと呼ばれる堅牢で高品質なマルチスピーカートランスフォーマーTTSシステムを開発し、テキストから音声へのアラインメントを改善するためにいくつかの特別に設計されたコンポーネント/手法を使用します。1）エンコーダーデコーダーの重み行列に対する対角制約トレーニングと推論の両方で注意; 2）位置情報をより適切に保存するためにエンコーダーに埋め込んだ音素のレイヤー正規化。 3）連続する音声フレーム間のコピーを防ぐためのデコーダープレネットのボトルネック.. VCTKおよびLibriTTSマルチスピーカーデータセットでの実験は、マルチスピーチの有効性を実証します。1）ナイーブトランスフォーマーベースよりも堅牢で高品質のマルチスピーカー音声を合成します。 TTS; 2）教師としてMutiSpeechモデルを使用すると、非常に高速な推論速度を楽しみながら、品質劣化がほとんどない強力なマルチスピーカーFastSpeechモデルが得られます。トランスフォーマーベースのテキスト音声変換（TTS）モデル（たとえば、トランスフォーマーTTS〜\ cite） {li2019neural}、FastSpeech〜\ cite {ren2019fastspeech}）は、トレーニングや推論における並列計算により、RNNベースのモデル（例：Tacotron〜\ cite {shen2018natural}）よりもトレーニングと推論の効率が優れていることを示しています。 
[ABSTRACT]変圧器ベースのモデルを使用して、multi-spnatchのモデルを改善できます。これは、「multi-spnatural。」と呼ばれるモデルの開発に使用されていますが、このモデルは、使用を防ぐのに役立つモデルにリンクされていますスピーチの
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-08">
        <br>2020-06-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Semi-Supervised Contrastive Learning with Generalized Contrastive Loss
  and Its Application to Speaker Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-09/eess.AS/paper_6.html">
      Semi-Supervised Contrastive Learning with Generalized Contrastive Loss
  and Its Application to Speaker Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      GCLは、損失関数の定義を変更せずに、教師あり学習、半教師あり学習、教師なし学習の3つの方法で話者の埋め込みの学習を可能にすることを示します。GCLは、2つの異なる学習フレームワーク、教師あり計量学習からの損失を統合します。教師なし対比学習、したがって、半教師あり学習の損失を自然に決定します。提案されたフレームワークは、一般化対比損失（GCL）を採用しています。 
[要約]提案されたフレームワークは「gcl」の定義を使用しています。テクノロジーから学ぶことは可能です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-08">
        <br>2020-06-08
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="">
      
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本日更新された論文はありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="">
        <br>
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
