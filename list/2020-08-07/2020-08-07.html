<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-08-07の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: MIRNet: Learning multiple identities representations in overlapped
  speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_0.html">
      <font color="black">MIRNet: Learning multiple identities representations in overlapped
  speech</font>
    </a>
  </h2>
  <font color="black">この論文では、オーバーラップした音声から複数のスピーカーのアイデンティティを確実に抽出できる新しいディープスピーカー表現戦略を提案します。トレーニングに参照音響機能を必要とする従来のアプローチとは異なり、提案されたアルゴリズムはオーバーラップした音声セグメントのスピーカーアイデンティティラベルのみを必要とします。 ..与えられた混合物から各話者のアイデンティティに関する情報を含む高レベルの埋め込みを抽出できるネットワークを設計します。 
[要約]特定の混合物から各話者のアイデンティティに関する情報を含む高レベルの埋め込みを抽出できるネットワーク。学習するには、話者の確認と対象話者に条件付けされた音声分離システムが必要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-04">
        <br><font color="black">2020-08-04</font>
      </time>
    </span>
</section>
<!-- paper0: PPSpeech: Phrase based Parallel End-to-End TTS System -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_1.html">
      <font color="black">PPSpeech: Phrase based Parallel End-to-End TTS System</font>
    </a>
  </h2>
  <font color="black">さらに、エンコーダの条件として音響埋め込みとテキストコンテキスト埋め込みを提案し、連続性を維持し、突然のスタイルや音色の変化を防ぎます。Tacotron2）は、合成音声の品質に関して従来の並列アプローチよりも優れています。実験により、 PPSpeechの合成速度は、文に5つ以上の句がある場合、文レベルの自己回帰Tacotron 2よりもはるかに高速です。 
[要約] tacotregressiveシステム（ppspeech）は、ソーススピーチの品質に関して従来のパラレルアプローチよりも優れています。ただし、提案されたシステムを使用して、さまざまなタイプのスピーチを作成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: A fully recurrent feature extraction for single channel speech
  enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_2.html">
      <font color="black">A fully recurrent feature extraction for single channel speech
  enhancement</font>
    </a>
  </h2>
  <font color="black">この目的のために、CNNレイヤーを抽出する機能に反復係数を追加して、シングルチャネル音声拡張のための堅牢なコンテキスト認識機能抽出戦略を紹介します。ニューラルモデル。バニラCNNモジュールを使用してエンハンスメントモデルに対して評価すると、目に見えないノイズ条件で、特徴抽出レイヤーに反復性がある推奨モデルにより、最大1.5 dBのセグメントSNR（SSNR）ゲインが生成され、パラメーターが最適化されます。 25％削減されます。 
[ABSTRACT] cnnモジュールの特徴抽出能力は、ネットワークのノイズコンテキストを適切にモデル化できませんでした。新しいモデルは、非常に騒々しい状況でも、音声キューの区別に非常に効果的です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-09">
        <br><font color="black">2020-06-09</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Learning a Universal Non-Semantic Representation of Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_3.html">
      <font color="black">Towards Learning a Universal Non-Semantic Representation of Speech</font>
    </a>
  </h2>
  <font color="black">埋め込みは公開されているデータセットでトレーニングされ、パーソナライゼーションタスクや医療ドメインなど、さまざまな低リソースのダウンストリームタスクでテストされます。ベンチマーク、モデル、評価コードは公開されています。提案されている表現は他のパフォーマンスよりも優れていますベンチマークでの表現、および多くの転移学習タスクでの最先端のパフォーマンスを超えています。 
[ABSTRACT]ベンチマーク、モデル、および評価コードが公開されています。提案された表現は他の表現よりも優れており、多くの転移学習タスクでの国家的パフォーマンスを超えています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br><font color="black">2020-02-25</font>
      </time>
    </span>
</section>
<!-- paper0: Improving on-device speaker verification using federated learning with
  privacy -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_4.html">
      <font color="black">Improving on-device speaker verification using federated learning with
  privacy</font>
    </a>
  </h2>
  <font color="black">補助モデルの知識は、マルチタスク学習を使用して話者検証システムに抽出されます。この補助モデルによって予測されるサイド情報ラベルは追加のタスクです。さらに、多数の話者の学習を可能にし、優れたモデルをトレーニングする際の話者特性のカバレッジ。これらのアプローチにより、ユーザーのデータをデバイスに残したまま、ユーザーのプライバシーを保護しながら中央モデルをトレーニングできます。 
[要約]モデルは、サイド情報として有用と見なされる話者特性ラベルを予測します。これらの機能は、話者の多数の母集団に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Mixing-Specific Data Augmentation Techniques for Improved Blind
  Violin/Piano Source Separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_5.html">
      <font color="black">Mixing-Specific Data Augmentation Techniques for Improved Blind
  Violin/Piano Source Separation</font>
    </a>
  </h2>
  <font color="black">このライトに続いて、このホワイトペーパーでは、現代の音楽制作ルーチンで採用されているより洗練されたミキシング設定、結合されるトラック間の関係、および沈黙の要因を考慮する拡張データ拡張方法についてさらに詳しく調べますこれらの新しいデータ拡張方法では、トレーニングデータの量の影響も調査します。ブラインドミュージックソースの分離は、音楽情報の検索と信号処理の両方のコミュニティで人気のある活発な研究テーマです。 
[要約]提案された混合-特定のデータ拡張方法は、深層学習のパフォーマンスを向上させるのに役立ちます-ソース分離のためのベースのモデル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Aalto's End-to-End DNN systems for the INTERSPEECH 2020 Computational
  Paralinguistics Challenge -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_6.html">
      <font color="black">Aalto's End-to-End DNN systems for the INTERSPEECH 2020 Computational
  Paralinguistics Challenge</font>
    </a>
  </h2>
  <font color="black">これらの各タスクで、アンサンブルは単一のE2Eモデルよりも優れています。高齢者のサブチャレンジでは、価数と覚醒レベルを予測することで、マルチタスクトレーニングを調査し、クラスの不均衡を処理するデータサンプリング戦略を実装するよう求められます。サブチャレンジでは、タスクのパフォーマンスに対するマルチロス戦略の影響を調査します。 
[要約]以前の以前の作業で単一のe2eモデルまたは同じe2eモデルを適用しました。タスクでは、アンサンブルが単一のe2eモデルよりも優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Few-Shot Drum Transcription in Polyphonic Music -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_7.html">
      <font color="black">Few-Shot Drum Transcription in Polyphonic Music</font>
    </a>
  </h2>
  <font color="black">同時に、モデルはトレーニング中に見られなかった、より細かい、または拡張された語彙に正常に一般化できることを示します。監視付きアプローチはまったく機能しません。合成データセットでプロトタイプネットワークをトレーニングし、複数のポリフォニックな伴奏を含む実際のADTデータセット。実験結果の詳細な分析を提供します。これには、サウンドクラス別およびポリフォニー別のパフォーマンスの内訳が含まれます。 
[ABSTRACT]私たちのモデルは、より細かく一般化することができます-トレーニング中に見えない語彙または拡張語彙</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: HooliGAN: Robust, High Quality Neural Vocoding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_8.html">
      <font color="black">HooliGAN: Robust, High Quality Neural Vocoding</font>
    </a>
  </h2>
  <font color="black">また、HooliGANとのシームレスな統合を可能にするTacotronベースのモデルへの簡単な変更を示します。次のデモページでサンプルを提供しています：https://resemble-ai.github.io/hooligan_demo/。生成モデルの最近の発展により、ディープラーニングと従来のデジタル信号処理（DSP）技術を組み合わせることで、説得力のあるバイオリンサンプルが正常に生成され
[1]、ソース励起とWaveNetを組み合わせることで、高品質のボコーダーが生成され
[2、3]、生成敵対的ネットワーク（GAN）トレーニングは自然さを向上させることができます
[4、5]。 
[ABSTRACT]新しいモデルには、最新の結果が得られ、より小さなデータセットにうまく調整できる堅牢なボコーダーhooliganが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Simultaneous measurement of time-invariant linear and nonlinear, and
  random and extra responses using frequency domain variant of velvet noise -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_9.html">
      <font color="black">Simultaneous measurement of time-invariant linear and nonlinear, and
  random and extra responses using frequency domain variant of velvet noise</font>
    </a>
  </h2>
  <font color="black">オープンソースリポジトリで利用できるようにしました。2つおよび4つの直交シーケンスを使用する2つの便利なケースを紹介し、シミュレーションと音響測定の例を使用してその使用方法を示します。線形時不変量を測定できる新しい音響測定方法を紹介します。応答、非線形時不変応答、およびランダム応答と時変応答が同時に発生します。 
[ABSTRACT]提案された方法は、プログラムされた応答応答応答分析のセットを使用します。これは、余分な機器なしでそれを可能にする高度な設計自由度です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Shouted Speech Compensation for Speaker Verification Robust to Vocal
  Effort Conditions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_10.html">
      <font color="black">Shouted Speech Compensation for Speaker Verification Robust to Vocal
  Effort Conditions</font>
    </a>
  </h2>
  <font color="black">補正の前に、叫び声の状態はロジスティック回帰によって自動的に検出されます。実験結果は、提案されたアプローチを声の努力の不一致がある場合に適用すると、叫び声のない音声を適用しないシステムに対して、最大13.8％の同等のエラー率の相対改善が得られる検出も補償も。この論文では、ガウシアン混合モデルを利用して、埋め込みの線形補償のためのさまざまな方法の研究を提示し、叫んだドメインと通常の音声ドメインをクラスタリングします。 
[ABSTRACT]これらの補正技術は、自動音声認識の堅牢性の領域から借用されています。この作業では、話者検証での叫び声と通常の状態の間の不一致を補正するためにそれらを適用します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Self-Supervised Contrastive Learning for Unsupervised Phoneme
  Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_11.html">
      <font color="black">Self-Supervised Contrastive Learning for Unsupervised Phoneme
  Segmentation</font>
    </a>
  </h2>
  <font color="black">結果のモデルを、トレーニングフェーズで見られなかった分布と言語（英語、ヘブライ語、ドイツ語）で評価し、追加の未転記データを使用することがモデルのパフォーマンスに有益であることを示しました。このように、提案されたモデルは完全に教師なしターゲット境界や音声表記の形での手動注釈のない方法。TIMITとBuckeyeコーパスの両方を使用して、提案されたアプローチをいくつかの教師なしベースラインと比較します。 
[ABSTRACT]モデルは、生の波形を直接操作する畳み込みニューラルネットワークです。これは、モデルの出力に適用され、最終的な境界を生成するピーク検出アルゴリズムです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-27">
        <br><font color="black">2020-07-27</font>
      </time>
    </span>
</section>
<!-- paper0: Attentive Fusion Enhanced Audio-Visual Encoding for Transformer Based
  Robust Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_12.html">
      <font color="black">Attentive Fusion Enhanced Audio-Visual Encoding for Transformer Based
  Robust Speech Recognition</font>
    </a>
  </h2>
  <font color="black">LRS3-TEDデータセットの実験は、提案された方法が、最新の状態と比較して、クリーンな状態、目に見える状態、見えない状態のノイズ条件下で、平均でそれぞれ0.55％、4.51％、4.61％認識率を向上できることを示していますアプローチ..各モダリティをエンコードした後に視聴覚融合が実行される以前のエンドツーエンドアプローチとは異なり、このホワイトペーパーでは、注意深い融合ブロックをエンコードプロセスに統合することを提案します。アーキテクチャでは、一方向または双方向の相互作用を持つマルチヘッド注意ベースの視聴覚融合を使用して、埋め込み融合ブロックを実装します。 
[要約]エンコーダモジュールで提案された方法は、オーディオを豊かにする-視覚的表現</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Robust Estimation of Hypernasality in Dysarthria with Acoustic Model
  Likelihood Features -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_13.html">
      <font color="black">Robust Estimation of Hypernasality in Dysarthria with Acoustic Model
  Likelihood Features</font>
    </a>
  </h2>
  <font color="black">これらの音響モデルから派生した機能が高鼻音に固有であることを示すために、さまざまな構音障害全体でそれらを評価します。高鼻音は、多くの運動音声障害に共通の特徴的な症状です。設計された機能は、関連する複雑な音響パターンのキャプチャに失敗することが多い一方、機械学習に基づくメトリックは、それらが訓練されている小さな疾患固有の音声データセットに過剰適合しがちです。 
[ABSTRACT] hypernasalityは、低周波数に追加の共鳴を導入します。hypernasalに基づいて、鼻腔から空気が漏れるため、調音の精度が低下します。これらの機能は、1つの疾患からの高鼻腔の高鼻声のトレーニングでも一般化します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-26">
        <br><font color="black">2019-11-26</font>
      </time>
    </span>
</section>
<!-- paper0: Memory Controlled Sequential Self Attention for Sound Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_14.html">
      <font color="black">Memory Controlled Sequential Self Attention for Sound Recognition</font>
    </a>
  </h2>
  <font color="black">メモリー制御シーケンシャルセルフアテンションの提案された使用は、サウンドイベントトークンのフレーム間の関係を誘導する方法を提供します。メモリー制御セルフアテンションモデルは、URBAN-SEDデータセットで33.92％のイベントベースのFスコアを達成し、優れたパフォーマンスを発揮することを示します。セルフアテンションなしのモデルによって報告された20.10％のFスコア。各アテンションヘッドが明示的なアテンション幅の値で埋め込まれたオーディオを処理するマルチヘッドセルフアテンションメカニズムを使用して、提案されたアイデアを拡張します。 
[要約]ポリフォニックサウンドイベント検出（sed）のために、畳み込みリカレントニューラルネットワーク（crnn）モデルに加えて、メモリ制御の自己注意メカニズムを使用することを提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-13">
        <br><font color="black">2020-05-13</font>
      </time>
    </span>
</section>
<!-- paper0: Data balancing for boosting performance of low-frequency classes in
  Spoken Language Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_15.html">
      <font color="black">Data balancing for boosting performance of low-frequency classes in
  Spoken Language Understanding</font>
    </a>
  </h2>
  <font color="black">特に、SLUの既存のデータバランシングテクニックの適用について説明し、インテント分類とスロット充填のためのマルチタスクSLUモデルを提案します。実際のデータセットの結果は、i）提案されたモデルが低パフォーマンスを向上できることを示しています。ヘッドインテントの潜在的なパフォーマンス低下を回避しながら、頻度インテントを大幅に増加、ii）現実的なデータが利用できない場合に合成データは新しいインテントをブートストラップするのに有益ですが、iii）一定の量の現実的なデータが利用可能になると、補助データで合成データを使用しますタスクは、プライマリタスクトレーニングデータに追加するよりもパフォーマンスが向上するだけです。iv）ジョイントトレーニングシナリオでは、インテントの分散を個別に分散することで、インテントの分類だけでなくスロット充填のパフォーマンスも向上します。データの不均衡がますます高まっているにもかかわらず、実際の音声言語理解（SLU）アプリケーションではより一般的ですが、 文学。 
[ABSTRACT]私たちの意図に加えて、slu.itのデータの不均衡の処理に関する新しい研究は、合成データの使用に関する最初の体系的な研究です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: FastLR: Non-Autoregressive Lipreading Model with Integrate-and-Fire -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_16.html">
      <font color="black">FastLR: Non-Autoregressive Lipreading Model with Integrate-and-Fire</font>
    </a>
  </h2>
  <font color="black">私たちの実験は、FastLRが10.97 $ \ times $までのスピードアップを達成し、GRIDおよびLRS2リピーディングデータセットでWER絶対増加がわずか1.5 \％および5.5 \％である最新のリピーディングモデルと比較して、提案された方法の有効性..読唇術は印象的な手法であり、近年、確度の明確な改善がありました。3）課題4を克服するために、I \＆Fの新しいノイズのある並列デコード（NPD）を提案し、バイトをもたらします-ペアエンコーディング（BPE）から読み上げ。 
[要約]読唇術の新しいメソッドは、主に自己回帰（ar）モデルに基づいて構築され、ターゲットトークンを1つずつ生成します。ar言語モデルの削除は、読唇術の固有のあいまいさの問題を悪化させます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Quantification of Transducer Misalignment in Ultrasound Tongue Imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_17.html">
      <font color="black">Quantification of Transducer Misalignment in Ultrasound Tongue Imaging</font>
    </a>
  </h2>
  <font color="black">ハンガリー語とスコットランド語の英語の子データセットに対して広範な実験が行われます。これらの測定値は発話のタイムスタンプの関数として視覚化されます。結果は、平均二乗誤差（MSE）の大きな値と構造類似性インデックス（ SSIM）およびComplex Wavelet SSIMは、データ記録中の破損または問題を示します。これは、トランスデューサーの調整不良またはゲルの欠如によって引き起こされる可能性があります。 
[アブストラクト]超音波は手頃な価格で非侵襲的な画像モダリティです。このホワイトペーパーでは、シンプルでありながら効果的なミスアライメントの定量化アプローチを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Sound field reconstruction in rooms: inpainting meets super-resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_18.html">
      <font color="black">Sound field reconstruction in rooms: inpainting meets super-resolution</font>
    </a>
  </h2>
  <font color="black">実際のリスニングルームでの実験的検証とともにシミュレーションデータを使用した実験が示されています。この方法は、グリーン関数の数値シミュレーションから構築された、シミュレーションデータのみでトレーニングされた部分畳み込みを持つUネットのようなニューラルネットワークに基づいています。何千もの一般的な長方形の部屋にまたがります。3次元および異なる部屋の形状に拡張可能ですが、この方法は、3次元の音場の測定から長方形の部屋の2次元平面を再構築することに焦点を当てています。 
[ABSTRACT]このメソッドは、任意に配置された非常に少数のマイクを使用します。部屋全体の音圧を再構築することが可能です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-30">
        <br><font color="black">2020-01-30</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Multi-Scale Aggregation Using Feature Pyramid Module for
  Robust Speaker Verification of Variable-Duration Utterances -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_19.html">
      <font color="black">Improving Multi-Scale Aggregation Using Feature Pyramid Module for
  Robust Speaker Verification of Variable-Duration Utterances</font>
    </a>
  </h2>
  <font color="black">さまざまな時間スケールの豊富なスピーカー情報を含む拡張機能を使用して、スピーカーの埋め込みを抽出します。このモジュールは、トップダウンの経路と横方向の接続を介して、複数のレイヤーからの機能のスピーカー識別情報を拡張します。また、状態よりも優れたパフォーマンスを実現します。短い発話と長い発話の両方に対する最新のアプローチ。 
[ABSTRACT]たとえば、話者の特徴抽出の最後の層から話者の埋め込みを抽出します。これらには、msaの数から抽出されたマルチスケールの特徴が含まれています。任意の継続時間の発話を処理するロバスト性を高めるために、このペーパーが導入されました機能ピラミッドモジュールを使用してmsaを改善するには</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-07">
        <br><font color="black">2020-04-07</font>
      </time>
    </span>
</section>
<!-- paper0: Spectral-change enhancement with prior SNR for the hearing impaired -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.SD/paper_20.html">
      <font color="black">Spectral-change enhancement with prior SNR for the hearing impaired</font>
    </a>
  </h2>
  <font color="black">第2に、実際の混合物から得られた推定SNRが使用され、別のSCE-eSNRが得られました。SCE-eSNRアルゴリズムは、高SMRでのSSNマスカーと低SMRでのSTSマスカーの改善されたSI、およびより優れた自然性と音声品質を示しました。 STSマスカーの場合。12人のHIリスナーに対して、定常スピーチスペクトルノイズ（SSN）と6トークスピーチ（STS）マスカーで、それぞれ音声明瞭度（SI）と明瞭度の好みを測定しました。 
[要約]結果は、sce-hi hiアルゴリズムが両方のmaskers.siでsiを大幅に改善したことを示し、自然性と音声品質に関する主観的評価が7 hi被験者でテストされました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Deep-Learning Based Adaptive Ultrasound Imaging from Sub-Nyquist Channel
  Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.IV/paper_0.html">
      <font color="black">Deep-Learning Based Adaptive Ultrasound Imaging from Sub-Nyquist Channel
  Data</font>
    </a>
  </h2>
  <font color="black">まず、サブナイキストサンプリングチャネルデータを検討し、周波数領域で時間を揃え、追加の回復ステップなしで時間領域に変換し直します。このアプローチでは、以前に提案された再構成アプローチよりも高い解像度で、高品質のBモード画像が得られます。 （NESTA）圧縮データと、完全にサンプリングされたデータの遅延加算ビームフォーミング（DAS）から。データは、スパース配列からの取得をエミュレートするために、空間的にさらにサンプリングされます。 
[ABSTRACT]これはデータの標準標準規格と比較されます。次に、エンコーダーへの入力として与えられます-デコーダー畳み込みニューラルネットワーク</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning Based Defect Detection for Solder Joints on Industrial
  X-Ray Circuit Board Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.IV/paper_1.html">
      <font color="black">Deep Learning Based Defect Detection for Solder Joints on Industrial
  X-Ray Circuit Board Images</font>
    </a>
  </h2>
  <font color="black">自動化されたX線検査では、X線画像上の関心のある関節が関心領域（ROI）によって特定され、いくつかのアルゴリズムによって検査されます。このホワイトペーパーでは、PCB中のX線イメージングベースの品質管理にディープラーニングが組み込まれています品質検査..専門家はフォローアップチェックを行う必要があります。 
[ABSTRACT]電子回路基板（pcb）の組み立て中にはんだ欠陥の可能性が高まっています。多くの高度なアルゴリズムにより、デジタル画像に基づいて生産品質を制御することが期待されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: MED-TEX: Transferring and Explaining Knowledge with Less Data from
  Pretrained Medical Imaging Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.IV/paper_2.html">
      <font color="black">MED-TEX: Transferring and Explaining Knowledge with Less Data from
  Pretrained Medical Imaging Models</font>
    </a>
  </h2>
  <font color="black">さらに、ジョイントフレームワークは、情報理論の観点から導き出された原理的な方法でトレーニングされます。教師モデルを解釈し、生徒の学習を支援するために、説明医モジュールが導入され、入力医療画像の領域を強調表示します。教師モデルの予測には重要です。私たちのフレームワークのパフォーマンスは、眼底疾患データセットの最新の方法と比較した知識の抽出とモデルの解釈タスクに関する包括的な実験によって実証されています。 
[要約]小さな医療画像分類のための知識抽出とモデル解釈フレームワークの新しいシステムが開発されています。教師モデルを解釈するだけでなく、生徒の学習を支援するために、入力医療画像の領域を強調表示する説明モジュールが導入されています教師モデルの予測にとって重要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Multimodal Spatial Attention Module for Targeting Multimodal PET-CT Lung
  Tumor Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.IV/paper_3.html">
      <font color="black">Multimodal Spatial Attention Module for Targeting Multimodal PET-CT Lung
  Tumor Segmentation</font>
    </a>
  </h2>
  <font color="black">腫瘍に関連する領域（空間領域）を強調し、生理学的高摂取で正常な領域を抑制することを自動的に学習するマルチモーダル空間注意モジュール（MSAM）を紹介します。ただし、これらの方法は、ガイドとなる可能性のある高いPET腫瘍感度を完全には活用していません。セグメンテーション..従来のU-Netバックボーンを備えたMSAMは、ダイス類似度係数（DSC）で7.6％のマージンで、最先端の肺腫瘍セグメンテーションアプローチを上回っています。 
[要約]私たちは、腫瘍に関連する領域（空間領域）を強調し、生理学的高摂取で正常な領域を抑制することを自動的に学習するマルチモーダル空間注意モジュール（msam）を導入します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-29">
        <br><font color="black">2020-07-29</font>
      </time>
    </span>
</section>
<!-- paper0: Zero-Shot Multi-View Indoor Localization via Graph Location Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.IV/paper_4.html">
      <font color="black">Zero-Shot Multi-View Indoor Localization via Graph Location Networks</font>
    </a>
  </h2>
  <font color="black">GLNは、メッセージパッシングネットワークを介して画像から抽出されたロバストな位置表現に基づいて位置予測を行います。屋内の位置特定は、位置ベースのアプリケーションの基本的な問題です。大規模な展開。 
[ABSTRACT]提案されたglnは、新しいニューラルネットワークベースのアーキテクチャグラフロケーションネットワーク（gln）です。ベースのロケーションネットワークは、インフラストラクチャを実行できます-マルチビュー画像ベースの屋内ローカリゼーション。新しいゼロショットの屋内ローカリゼーション設定もあります。新しいメカニズムmap2vecを利用して場所をトレーニングします-埋め込みを認識し、目に見えない新しい場所を予測します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Subjective Quality Study and Database of Compressed Point Clouds with
  6DoF Head-mounted Display -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.IV/paper_5.html">
      <font color="black">Subjective Quality Study and Database of Compressed Point Clouds with
  6DoF Head-mounted Display</font>
    </a>
  </h2>
  <font color="black">次に、現在の客観的PCQA方法で主観的データベースを評価し、オブザーバーの認識と予測されたビューの重要性の間の一貫性を改善するために、客観的加重投影ベースの方法を提案します。主観的データセットと品質スコアは、パブリックリポジトリで利用できます。 ..主観的なデータベースと調査結果は、特にバーチャルリアリティアプリケーションの場合、知覚ベースの点群の処理、送信、コーディングに使用できます。 
[ABSTRACT]小点の点群の品質データベース（siat-pcqd）は、6自由度のヘッドマウントディスプレイ（hmd）に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Scratch that! An Evolution-based Adversarial Attack against Neural
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.IV/paper_6.html">
      <font color="black">Scratch that! An Evolution-based Adversarial Attack against Neural
  Networks</font>
    </a>
  </h2>
  <font color="black">MicrosoftのCognitive Services Image Captioning APIに対する攻撃を成功裏に開始し、さまざまな軽減戦略を提案します。スクラッチが単一の色である極端な状態では、CIFAR-10で目標攻撃成功率$ 66 \％$が得られます。クエリは、同等の攻撃よりも桁違いに少ない。私たちのスクラッチは、単色または複数の色の直線または放物線のベジエ曲線などの多様な形状の下で効果的であることを示している。 
[要旨]敵が、5ドル未満をカバーするローカライズされた「敵のスクラッチ」を生成する可能性があります。画像のピクセルの％$。画像の場合、目標とする攻撃の成功率は66ドルのみです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br><font color="black">2019-12-05</font>
      </time>
    </span>
</section>
<!-- paper0: FISTA-Net: Learning A Fast Iterative Shrinkage Thresholding Network for
  Inverse Problems in Imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.IV/paper_7.html">
      <font color="black">FISTA-Net: Learning A Fast Iterative Shrinkage Thresholding Network for
  Inverse Problems in Imaging</font>
    </a>
  </h2>
  <font color="black">FISTA-Netの重要な部分は、エンドツーエンドのトレーニングを通じて効果的に学習できる非線形しきい値処理の近位オペレーターネットワークを開発することです。さらに、モデルベースのパラメーターに正の単調な制約を課して、それらが正しく収束するようにします。 。電磁トモグラフィー（EMT）およびX線計算機トモグラフィー（X線CT）。 
[ABSTRACT] fista-netは、fistaをディープネットワークにキャストすることによって設計されました。すべての側面には、さまざまなイメージングタスクのさまざまなパラメーターを最適化できるユーザーモデルが含まれます。emtとsparse-両方のビューctで、優れた結果が得られます状態-最先端のモデル-ベースのディープラーニング手法</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: A coarse-to-fine framework for unsupervised multi-contrast MR image
  deformable registration with dual consistency constraint -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.IV/paper_8.html">
      <font color="black">A coarse-to-fine framework for unsupervised multi-contrast MR image
  deformable registration with dual consistency constraint</font>
    </a>
  </h2>
  <font color="black">登録速度に関しては、同じCPUでテストすると、SyNの最も競争力のある方法よりも約17倍高速になります。さらに、登録を強化するために、二重整合性制約と新しい事前知識ベースの損失関数が開発されています。パフォーマンス..提案された方法は、ボクセルモーフ、SyN、LDDMMを含む一般的に利用される登録方法と比較して、脳卒中病変の識別において0.826のダイススコアで最高の登録パフォーマンスを達成します。 
[要約]提案された方法は、555ケースからなる臨床データセットで評価されています。これは、マルチステップ反復プロセスと複雑な画像前処理操作を取り除くように設計されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-05">
        <br><font color="black">2020-08-05</font>
      </time>
    </span>
</section>
<!-- paper0: PCSGAN: Perceptual Cyclic-Synthesized Generative Adversarial Networks
  for Thermal and NIR to Visible Image Transformation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.IV/paper_9.html">
      <font color="black">PCSGAN: Perceptual Cyclic-Synthesized Generative Adversarial Networks
  for Thermal and NIR to Visible Image Transformation</font>
    </a>
  </h2>
  <font color="black">NIRおよびTHM画像には限られた詳細しか含まれていません。したがって、変換された画像の品質、細部、現実性を向上させるには、より優れた目的関数が必要です。GANベースの利用可能な方法のほとんどは、敵対的なピクセルと訓練の目的関数としての賢い損失（$ L_1 $や$ L_2 $など）。 
[要約] thm / nirからvisへの画像変換の新しいモデルが導入されました。知覚循環ソースvisと呼ばれ、コードはwwwで入手できます。 github。 com</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-13">
        <br><font color="black">2020-02-13</font>
      </time>
    </span>
</section>
<!-- paper0: From IC Layout to Die Photo: A CNN-Based Data-Driven Approach -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.IV/paper_10.html">
      <font color="black">From IC Layout to Die Photo: A CNN-Based Data-Driven Approach</font>
    </a>
  </h2>
  <font color="black">さらに、LithoNetはウェーハ製造パラメーターを潜在ベクトルとして取り、SEM画像で検査できるパラメトリック製品のばらつきをモデル化できます。2つの畳み込みニューラルネットワークで構成されるディープラーニングベースのデータ駆動型フレームワークを提案します。 IC製造による回路の形状変形を予測します。ii）このような形状変形を補正するためにICレイアウトの修正を提案するOPCNet。さらに、リソグラフィフォトマスクの修正を提案するために使用される従来の光近接効果補正（OPC）高価な。 
[ABSTRACT]提案されたリソネット-opcnetフレームワークは、レイアウト形状を使用して、作成されたICの形状を予測できます。提案された設計ツールは、予測された形状と指定されたレイアウトとの一貫性に従ってレイアウト修正も予測できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-11">
        <br><font color="black">2020-02-11</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Pansharpening Based on Self-Attention Mechanism -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.IV/paper_11.html">
      <font color="black">Unsupervised Pansharpening Based on Self-Attention Mechanism</font>
    </a>
  </h2>
  <font color="black">広範な実験結果は、提案されたアプローチが、最先端のものと比較してより詳細でスペクトル歪みが少ない、さまざまなタイプのよりシャープなMSIを再構築できることを示しています。この論文の貢献は3倍です。 、詳細抽出および注入機能は、注意の表現に基づいて空間的に変化するため、再構成の精度が大幅に向上します。 
[ABSTRACT]ピクセルは、監視なしのパンシャープニング（アップ）メソッドです。アップサムと呼ばれる自己注意メカニズム（sam）に基づく課題に対処するために提案されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-16">
        <br><font color="black">2020-06-16</font>
      </time>
    </span>
</section>
<!-- paper0: Efficient Non-Line-of-Sight Imaging from Transient Sinograms -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.IV/paper_12.html">
      <font color="black">Efficient Non-Line-of-Sight Imaging from Transient Sinograms</font>
    </a>
  </h2>
  <font color="black">私たちは、（1）これらのC2NLOS測定値が、正弦波の重ね合わせで構成されていることを観察しました。これは、過渡シノグラムと呼ばれます。（2）これらの正弦波測定値を、非表示の散乱体の3D位置または非表示のNLOS画像に変換する計算効率の高い再構成手順が存在します。これらのC2NLOSスキャンは、以前のアプローチよりもはるかに少ない測定で動作しているにもかかわらず、これらのさまざまなNLOSイメージングタスクを解決するために非表示のシーンに関する十分な情報を提供します。リレーウォール全体をとおして、収集時間と計算要件の両方を削減するNLOSスキャンのより効率的な形式を探索します。シミュレーションと実際のC2NLOSスキャンの両方の結果を示します。 
[要約] 1つのアプローチは、レーザーを使用して多重散乱光の移動時間を測定することを含みます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Optical Flow and Mode Selection for Learning-based Video Coding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.IV/paper_13.html">
      <font color="black">Optical Flow and Mode Selection for Learning-based Video Coding</font>
    </a>
  </h2>
  <font color="black">提案されたコーディングスキームは、Learned Image Compression 2020（CLIC20）Pフレームコーディング条件のチャレンジの下で評価され、最先端のビデオコーデックITU / MPEG HEVCと同等のパフォーマンスを発揮することが示されています。は、2つの補完的なオートエンコーダ（MOFNetとCodecNet）に基づくフレーム間コーディングの新しい方法を紹介しています。オプティカルフローは、コーディングするフレームの予測を実行するために使用されます。 
[ABSTRACT] mofnetはオプティカルフローとピクセル単位のコーディングモード選択を伝えることを目的としています。64歳のコーデックネットはそれを伝えることを目的としたプロジェクトです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-end learning for semiquantitative rating of COVID-19 severity on
  Chest X-rays -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.IV/paper_14.html">
      <font color="black">End-to-end learning for semiquantitative rating of COVID-19 severity on
  Chest X-rays</font>
    </a>
  </h2>
  <font color="black">このような困難な視覚的タスクを解決するために、異なるデータセットを含む「部分から全体へ」の手順でトレーニングされたさまざまなタスク（セグメンテーション、空間配置、スコア推定）を処理するように構造化された弱く監視された学習戦略を採用します。特に、同じ病院で収集された約5,000のCXR注釈付き画像の臨床データセット。CXRデータセット、ソースコード、トレーニング済みモデルは、研究目的で公開されています。 
[要約]このシステムは、イタリアで高いパンデミックピークを経験した病院の1つで患者の連続監視に適用されます。この方法は、同じ病院で収集されたほぼxr画像のデータセットを活用しています。このデータセットは、ほぼすべてのデータセットに基づいています病院で撮影した5,00枚のcxr画像</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-08">
        <br><font color="black">2020-06-08</font>
      </time>
    </span>
</section>
<!-- paper0: Study of 3D Virtual Reality Picture Quality -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.IV/paper_15.html">
      <font color="black">Study of 3D Virtual Reality Picture Quality</font>
    </a>
  </h2>
  <font color="black">また、新しいデータベースで公開されているいくつかのIQAモデルを評価し、比較したIQAモデルのパフォーマンスの統計的評価も報告します。これらの課題に対処するためには、発生し、影響を与える歪みを理解できることが重要です。表示されたVRコンテンツの知覚品質。これらの課題に対処するには、VR品質モデルを開発でき、VR品質予測アルゴリズムのベンチマークに使用できる、大規模で代表的な主観的VR品質データベースの形式の基本ツールが必要です。 
[ABSTRACT]没入型3D主観的画質評価研究が新しいデータベースによって実施されました。新しいデータベースは、視線方向と知覚される品質の関係がよりよく理解されることを期待して開発されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-07">
        <br><font color="black">2019-10-07</font>
      </time>
    </span>
</section>
<!-- paper0: GL-GAN: Adaptive Global and Local Bilevel Optimization model of Image
  Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.IV/paper_16.html">
      <font color="black">GL-GAN: Adaptive Global and Local Bilevel Optimization model of Image
  Generation</font>
    </a>
  </h2>
  <font color="black">シンプルなネットワーク構造で、GL-GANはローカルの2値最適化によって不均衡の性質を効果的に回避できます。これは、最初に低品質の領域を特定してから最適化することによって達成されます。現在のGANメソッドと比較して、モデルはCelebA、CelebA-HQおよびLSUNデータセットでの印象的なパフォーマンス。このモデルは、グローバルな最適化が画像全体を最適化することであり、ローカルは低品質領域のみを最適化することである、補完的かつ促進的な方法で高解像度画像の生成を実現します。 。 
[要約]一部のモデルの結果は、生成された画像の品質の不均衡を表示します。一部のモデルは、他の領域と比較して表示されます。モデルは、グローバルな不確実性が存在する別の方法で高解像度画像を生成します画像全体を最適化する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Pairwise Relation Learning for Semi-supervised Gland Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.IV/paper_17.html">
      <font color="black">Pairwise Relation Learning for Semi-supervised Gland Segmentation</font>
    </a>
  </h2>
  <font color="black">S-Netは、セグメンテーション用のラベル付きデータでトレーニングされ、PR-Netは、教師なしの方法でラベル付きデータとラベルなしデータの両方でトレーニングされ、特徴空間内の画像の各ペア間の意味の一貫性を活用して画像表現能力を高めます。また、オブジェクトレベルのサイコロ損失を設計して、腺に触れることによって引き起こされる問題に対処し、S-Netの他の2つの損失関数と組み合わせます。両方のネットワークがエンコーダーを共有しているため、PR-Netで学習した画像表現機能を転送できます。セグメンテーションのパフォーマンスを向上させるためにS-Netに。 
[ABSTRACT]ディープラーニング画像は、組織学画像の注釈に関連する多大な労力と関連する専門家の費用のために入手が困難です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: F2GAN: Fusing-and-Filling GAN for Few-shot Image Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_0.html">
      <font color="black">F2GAN: Fusing-and-Filling GAN for Few-shot Image Generation</font>
    </a>
  </h2>
  <font color="black">特定のカテゴリの画像を生成するために、既存のディープジェネレーティブモデルは通常、豊富なトレーニング画像に依存しています。さらに、識別器は、モードシーク損失と補間回帰損失によって生成画像の多様性を保証できます。少数ショット画像の生成、新しいカテゴリのいくつかの画像のみから画像を生成することを目的として、いくつかの研究関心が集まっています。 
[ABSTRACT]私たちのf2versでは、フュージョンジェネレーターは条件付き画像の高レベルの機能を融合し、非ローカルアテンションモジュールを使用して、対応する低レベルの詳細を入力して新しい画像を生成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-05">
        <br><font color="black">2020-08-05</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning Based Defect Detection for Solder Joints on Industrial
  X-Ray Circuit Board Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_1.html">
      <font color="black">Deep Learning Based Defect Detection for Solder Joints on Industrial
  X-Ray Circuit Board Images</font>
    </a>
  </h2>
  <font color="black">フォローアップチェックを行うにはスペシャリストが必要です。このホワイトペーパーでは、PCB品質検査中のX線イメージングベースの品質管理にディープラーニングが組み込まれています。ノイズのあるROIの問題とさまざまなサイズのイメージング寸法の問題に対処します。 
[ABSTRACT]電子回路基板（pcb）の組み立て中にはんだ欠陥の可能性が高まっています。多くの高度なアルゴリズムにより、デジタル画像に基づいて生産品質を制御することが期待されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: MED-TEX: Transferring and Explaining Knowledge with Less Data from
  Pretrained Medical Imaging Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_2.html">
      <font color="black">MED-TEX: Transferring and Explaining Knowledge with Less Data from
  Pretrained Medical Imaging Models</font>
    </a>
  </h2>
  <font color="black">具体的には、データを大量に消費する問題に対処するために、面倒な事前トレーニング済みの教師モデルからのみ知識を抽出することにより、少ないデータで小さな学生モデルを学習することを提案します。さらに、共同フレームワークは、情報理論から導き出された原理的な方法でトレーニングされます視点..私たちのフレームワークのパフォーマンスは、眼底疾患データセットの最先端の方法と比較した知識の抽出とモデルの解釈タスクに関する包括的な実験によって実証されています。 
[要約]小さな医療画像分類のための知識抽出とモデル解釈フレームワークの新しいシステムが開発されています。教師モデルを解釈するだけでなく、生徒の学習を支援するために、入力医療画像の領域を強調表示する説明モジュールが導入されています教師モデルの予測にとって重要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: VoxelPose: Towards Multi-Camera 3D Human Pose Estimation in Wild
  Environment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_3.html">
      <font color="black">VoxelPose: Towards Multi-Camera 3D Human Pose Estimation in Wild
  Environment</font>
    </a>
  </h2>
  <font color="black">ベルとホイッスルがないと、パブリックデータセットの最先端のパフォーマンスを上回ります。この目標を達成するために、すべてのカメラビューの機能は、共通の3Dスペースにワープおよび集約され、Cuboid Proposal Network（CPN ）すべての人を大まかにローカライズする..ノイズの多い不完全な2Dポーズ推定に基づいてクロスビュー対応を確立する必要がある以前の取り組みとは対照的に、$ 3 $ D空間で直接動作するエンドツーエンドのソリューションを提示します。したがって、2D空間での誤った決定を回避できます。 
[要旨]は、$ 3 $ d空間で直接動作するエンドビューソリューションを提示します。したがって、2d空間での誤った決定を回避します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-13">
        <br><font color="black">2020-04-13</font>
      </time>
    </span>
</section>
<!-- paper0: Gibbs Sampling with People -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_4.html">
      <font color="black">Gibbs Sampling with People</font>
    </a>
  </h2>
  <font color="black">ユーティリティ理論の観点から両方の方法を定式化し、新しい方法が「Gibbs Sampling with People」（GSP）として解釈できることを示します。ただし、MCMCPのバイナリ選択パラダイムは、試行ごとに比較的少ない情報とそのローカル提案関数を生成します。パラメータ空間を探索して分布のモードを見つけるのが遅くなります。これらの結果は、大規模な知覚評価実験を通じて検証されます。 
[要旨] mcmcpは、そのような表現を研究するための優れた方法です。参加者には、マルコフ連鎖モンテカルロ受け入れ規則に従うように構成された2項選択試験が提示されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Multimodal Spatial Attention Module for Targeting Multimodal PET-CT Lung
  Tumor Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_5.html">
      <font color="black">Multimodal Spatial Attention Module for Targeting Multimodal PET-CT Lung
  Tumor Segmentation</font>
    </a>
  </h2>
  <font color="black">以前の自動セグメンテーション方法は、PETおよびCTモダリティとは別に抽出された融合情報に主に焦点を当てていましたが、各モダリティには補足情報が含まれているという根本的な前提があります。 ..セグメンテーションは、さまざまなイメージングエキスパートによって手動で行われる傾向があり、労働集約的であり、エラーや不整合が発生しやすくなります。 
[要約]私たちは、腫瘍に関連する領域（空間領域）を強調し、生理学的高摂取で正常な領域を抑制することを自動的に学習するマルチモーダル空間注意モジュール（msam）を導入します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-29">
        <br><font color="black">2020-07-29</font>
      </time>
    </span>
</section>
<!-- paper0: Fashion Captioning: Towards Generating Accurate Descriptions with
  Semantic Rewards -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_6.html">
      <font color="black">Fashion Captioning: Towards Generating Accurate Descriptions with
  Semantic Rewards</font>
    </a>
  </h2>
  <font color="black">最初に属性を特定してアイテムの説明をシードし、テキストレベルの説明の品質を向上させるための指標として、属性レベルのセマンティック（ALS）報酬と文レベルのセマンティック（SLS）報酬を導入します。最尤推定（MLE）、属性埋め込み、および強化学習（RL）を使用したモデル。この作業の目標は、正確で表現力のあるファッションキャプションのための新しい学習フレームワークを開発することです。 
[ABSTRACT]ファッションアイテムの豊富な属性を識別および説明する機能は識別が困難です。ファサードスタイルの例は、顧客の関心をより引き付けます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Zero-Shot Multi-View Indoor Localization via Graph Location Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_7.html">
      <font color="black">Zero-Shot Multi-View Indoor Localization via Graph Location Networks</font>
    </a>
  </h2>
  <font color="black">GLNは、メッセージパッシングネットワークを介して画像から抽出されたロバストな位置表現に基づいて位置予測を行います。この問題への現在のアプローチは、通常、サポートするインフラストラクチャだけでなく、信号を測定および調整する人間の努力を必要とする無線周波数技術に依存しています。実験は、提案されたアプローチが標準設定で最先端の方法よりも優れており、場所の半分のデータが利用できないゼロショット設定でも約束された精度を達成することを示しています。 
[ABSTRACT]提案されたglnは、新しいニューラルネットワークベースのアーキテクチャグラフロケーションネットワーク（gln）です。ベースのロケーションネットワークは、インフラストラクチャを実行できます-マルチビュー画像ベースの屋内ローカリゼーション。新しいゼロショットの屋内ローカリゼーション設定もあります。新しいメカニズムmap2vecを利用して場所をトレーニングします-埋め込みを認識し、目に見えない新しい場所を予測します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Image Generation for Efficient Neural Network Training in Autonomous
  Drone Racing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_8.html">
      <font color="black">Image Generation for Efficient Neural Network Training in Autonomous
  Drone Racing</font>
    </a>
  </h2>
  <font color="black">このデータの収集は、ドローンを手動で飛ばす必要があり、収集されたデータがセンサーの故障の影響を受ける可能性があるため、面倒なプロセスです。この作業では、実際の背景画像とランダム化された画像の組み合わせを使用した半合成データセット生成方法を提案します。ゲートの3Dレンダリング。これらの欠点のない無制限のトレーニングサンプルを提供します。背景オブジェクトやさまざまな照明条件などの課題により、色やジオメトリに基づく従来のオブジェクト検出アルゴリズムは失敗する傾向があります。 
[ABSTRACT]畳み込みニューラルネットワークは、コンピュータビジョンに大きな進歩をもたらしますが、学習するには膨大な量のデータが必要</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Assessing the (Un)Trustworthiness of Saliency Maps for Localizing
  Abnormalities in Medical Imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_9.html">
      <font color="black">Assessing the (Un)Trustworthiness of Saliency Maps for Localizing
  Abnormalities in Medical Imaging</font>
    </a>
  </h2>
  <font color="black">2つの大規模な公共放射線データセットで利用可能なローカリゼーション情報を使用して、精度リコール曲線（AUPRC）と構造類似性インデックス（SSIM）の下の面積を使用して、上記の基準に対して一般的に使用される8つの顕著性マップアプローチのパフォーマンスを定量化し、さまざまなベースライン指標..フレームワークを使用して顕著性マップの信頼性を定量化すると、8つの顕著性マップ手法すべてが基準の少なくとも1つに合格せず、ほとんどの場合、ベースラインと比較すると信頼性が低いことがわかります。調査結果の再現性を促進するために、このリンクでこの作業で実行されたすべてのテストに使用したコードを提供します：https://github.com/QTIM-Lab/Assessing-Saliency-Maps。 
[要旨]顕著性マップは、ニューラルネットワークが行う決定について臨床的にもっともらしい説明を提供するために医療画像で使用されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Flow Models for Arbitrary Conditional Likelihoods -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_10.html">
      <font color="black">Flow Models for Arbitrary Conditional Likelihoods</font>
    </a>
  </h2>
  <font color="black">従来の条件付きアプローチは、観測された共変量の別の固定セットで条件付けされた共変量の固定セットのモデルを提供します。フロー生成モデル、任意の条件付けフローモデル（AC-Flow）の新しい拡張を提案します。以前は実行不可能であった、観測された共変量の任意のサブセットを条件とします。コードはhttps://github.com/lupalab/ACFlowで入手できます。 
[ABSTRACT]任意のモデリングアプローチの大半は、共同分布$ p（x）$のみに基づいています。代わりに、この作業では、すべての条件付き賞品を生成できるモデルを開発します。これらには、共変量のすべての条件付きバンドが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-13">
        <br><font color="black">2019-09-13</font>
      </time>
    </span>
</section>
<!-- paper0: Geometric Attention for Prediction of Differential Properties in 3D
  Point Clouds -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_11.html">
      <font color="black">Geometric Attention for Prediction of Differential Properties in 3D
  Point Clouds</font>
    </a>
  </h2>
  <font color="black">具体的には、生の点群から法線と鋭い特徴線を推定することで、メッシュの品質が向上し、より正確な表面再構成手法を使用できるようになります。ラインこのような問題への学習可能なアプローチを設計するときの主な困難は、点群の近傍を選択し、点間の幾何学的関係を組み込むことです。 
[要約]合成的に現実的な幾何学的情報を点群で使用できます。この研究の結果は生の点群で開発されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-06">
        <br><font color="black">2020-07-06</font>
      </time>
    </span>
</section>
<!-- paper0: Compact Graph Architecture for Speech Emotion Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_12.html">
      <font color="black">Compact Graph Architecture for Speech Emotion Recognition</font>
    </a>
  </h2>
  <font color="black">人気のIEMOCAPデータベースで音声感情認識のモデルのパフォーマンスを評価しました。既存の音声感情認識方法と比較すると、このモデルは最先端のパフォーマンス（4クラス、$ 65.29 \％$）を大幅に達成しています。学習可能なパラメーターが少なくなります。このモデルは、標準のGCNや、アプローチの有効性を示す他の関連するディープグラフアーキテクチャよりも優れています。 
[ABSTRACT] Googleのモデルは、標準のGCNおよびその他の関連するディープグラフアーキテクチャよりも優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-05">
        <br><font color="black">2020-08-05</font>
      </time>
    </span>
</section>
<!-- paper0: Noisy Student Training using Body Language Dataset Improves Facial
  Expression Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_13.html">
      <font color="black">Noisy Student Training using Body Language Dataset Improves Facial
  Expression Recognition</font>
    </a>
  </h2>
  <font color="black">実験分析は、ノイズの多い学生のネットワークを繰り返しトレーニングすることで、大幅に優れた結果を得るのに役立つことを示しています。さらに、このモデルは、顔のさまざまな領域を分離し、マルチレベルの注意メカニズムを使用してそれらを個別に処理します。ラベル付けされたデータセットとラベル付けされていないデータセットの組み合わせを利用する自己学習法を使用します（Body Language Dataset-BoLD）。 
[ABSTRACT]大規模なdnnアーキテクチャとアンサンブルメソッドによりパフォーマンスが向上しましたが、データが不十分なため、ある時点ですぐに飽和に達します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: The VISIONE Video Search System: Exploiting Off-the-Shelf Text Search
  Engines for Large-Scale Video Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_14.html">
      <font color="black">The VISIONE Video Search System: Exploiting Off-the-Shelf Text Search
  Engines for Large-Scale Video Retrieval</font>
    </a>
  </h2>
  <font color="black">私たちのアプローチの特徴は、単一のテキスト検索エンジンでインデックス化された便利なテキストエンコーディングを使用して、キーフレームから抽出されたすべての情報（視覚的な深い特徴、タグ、色、オブジェクトの場所など）をエンコードすることです。ビデオブラウザーショーダウン（VBS）2019の競技中に生成されたクエリログを使用して、システムの取得パフォーマンス。これにより、テストしたものの中から最適なパラメーターと戦略を選択することにより、システムを微調整することができました。 
[ABSTRACT]これらのモダリティを組み合わせて複雑なクエリを表現し、ユーザーのニーズを満たすことができます。これにより、テスト中に最適なパラメーターと戦略を選択してシステムを微調整できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Dual-modality seq2seq network for audio-visual event localization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_15.html">
      <font color="black">Dual-modality seq2seq network for audio-visual event localization</font>
    </a>
  </h2>
  <font color="black">経験的結果は、提案された方法が両方の設定で最近のディープラーニングアプローチに対して有利に機能することを確認します。入力として各時間セグメントでオーディオとビジュアルの両方の特徴を共同で取得することにより、提案されたモデルは、グローバルイベントとローカルイベントの情報を順番に学習します。完全に監視されているか、監視が弱い設定のいずれか。.オーディオビジュアルイベントのローカリゼーションでは、ビデオで（フレームレベルまたはビデオレベルで）可視と可聴の両方であるイベントを識別する必要があります。 
[要約]このタスクに対処するために、私たちは、オーディオ-ビジュアルシーケンス、提案されたデュアルネットワーク（avsdn）という名前のディープニューラルネットワークを提案します。提案された方法は、最近の電子メールで有利に機能します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-02-20">
        <br><font color="black">2019-02-20</font>
      </time>
    </span>
</section>
<!-- paper0: Gender and Ethnicity Classification based on Palmprint and Palmar Hand
  Images from Uncontrolled Environment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_16.html">
      <font color="black">Gender and Ethnicity Classification based on Palmprint and Palmar Hand
  Images from Uncontrolled Environment</font>
    </a>
  </h2>
  <font color="black">実験結果は、制御されていない環境での性別および民族性の分類では、手のひらの画像よりも完全で分割された手の画像の方が適していることを示しています。 、管理されていない環境での性別および民族性の分類が考慮されます。 
[ABSTRACT]手は生体認証のために広く研究されてきましたが、手からの柔らかい生体認証には比較的注意が払われていません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: PLG-IN: Pluggable Geometric Consistency Loss with Wasserstein Distance
  in Monocular Depth Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_17.html">
      <font color="black">PLG-IN: Pluggable Geometric Consistency Loss with Wasserstein Distance
  in Monocular Depth Estimation</font>
    </a>
  </h2>
  <font color="black">提案手法はKITTIデータセットを使用して評価されます。単眼カメラ画像の深度とポーズの推定パフォーマンスを向上させるために、幾何学的不整合にペナルティを課すための新しい目的を提案します。この目的は、画像から推定される2つの点群間のWasserstein距離を使用して設計されています。さまざまなカメラポーズ。 
[ABSTRACT]私たちの目的は、最先端の手法と同じ量を使用して設計されています。他の最先端の手法と同じレベルに基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-03">
        <br><font color="black">2020-06-03</font>
      </time>
    </span>
</section>
<!-- paper0: Few-shot Classification via Adaptive Attention -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_18.html">
      <font color="black">Few-shot Classification via Adaptive Attention</font>
    </a>
  </h2>
  <font color="black">実験的に示されているように、提案されたモデルは、さまざまなベンチマークの少数ショット分類と細粒度認識データセットで最先端の分類結果を実現します。このような適応注意モデルは、分類モデルが探しているものを説明することもできます。この作業では、ごく少数の参照サンプルに基づいてクエリサンプルの表現を最適化し、高速に適応させることにより、新規の数ショット学習法を提案します。 
[ABSTRACT]最近のいくつかのショット学習方法は、主に2つの側面からのさまざまなメタ学習戦略の開発に焦点を当てています。これらには、初期モデルの最適化または距離メトリックに関する距離メトリックの学習が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: CaSPR: Learning Canonical Spatiotemporal Point Cloud Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_19.html">
      <font color="black">CaSPR: Learning Canonical Spatiotemporal Point Cloud Representations</font>
    </a>
  </h2>
  <font color="black">前の作業とは異なり、CaSPRは時空の連続性をサポートする表現を学習し、可変で不規則な時空間でサンプリングされた点群にロバストであり、見えないオブジェクトインスタンスに一般化します。最初に、入力点群シーケンスを時空間的にマッピングすることにより、時間を明示的にエンコードします。正規化されたオブジェクト空間..私たちのアプローチは、問題を2つのサブタスクに分割します。 
[ABSTRACT]私たちの目標は、時間の経過に伴う情報共有と、過去または過去の任意の時空間近傍におけるオブジェクトの状態の調査を可能にすることです。これらには、分析される任意の時空間近傍のオブジェクトの観測が含まれるかどうか</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Handwritten Character Recognition from Wearable Passive RFID -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_20.html">
      <font color="black">Handwritten Character Recognition from Wearable Passive RFID</font>
    </a>
  </h2>
  <font color="black">また、たたみ込みニューラルネットワークアーキテクチャも提案します。この新しいアップサンプリング構造により、入力サイズが10x10ピクセルと小さいにもかかわらず、従来のImageNet事前トレーニング済みネットワークをうまく利用できます。この論文では、新しいウェアラブルによってキャプチャされたデータからの手書き文字の認識について検討します。電気テキスタイルセンサーパネル。データは、合計7500文字を含む10人の被験者から収集されます。 
[ABSTRACT]データは、合計750個の文字を含む10人の被験者から収集されます。提案されたモデルにより、テストを適切と見なすことができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: A Practical Privacy-preserving Method in Federated Deep Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_21.html">
      <font color="black">A Practical Privacy-preserving Method in Federated Deep Learning</font>
    </a>
  </h2>
  <font color="black">詳細なセキュリティ分析と広範な実験は、提案された方法がモデルの正確さを犠牲にせず、余計なコストをかけずにプライバシー保護を実現できることを示しています。 、または元のトレーニングと同じモデルの精度を維持できない、または手頃なコストがかかる。特に、この方法では、暗号化されたドメインで非線形アクティベーション機能を適切にサポートできるため、セミトラストクライアントがディープニューラルネットワークを効率的にトレーニングできます。ローカルで暗号化されたモデルを反復処理します（つまり、サーバー側のモデルのプライバシーを保護します）。 
[ABSTRACT]これをシークレット共有方法と組み合わせて、半信頼サーバーが各クライアントのローカルグラディエントを確実に取得できるようにすることができます。また、シークレット共有手法と組み合わせてテストすることもできます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-23">
        <br><font color="black">2020-02-23</font>
      </time>
    </span>
</section>
<!-- paper0: Salvage Reusable Samples from Noisy Data for Robust Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_22.html">
      <font color="black">Salvage Reusable Samples from Noisy Data for Robust Learning</font>
    </a>
  </h2>
  <font color="black">この目的のために、Webイメージを使用してディープFGモデルをトレーニングする際のラベルノイズに対処するために、CRSSCと呼ばれる確実性に基づく再利用可能なサンプル選択および修正アプローチを提案します。理論的および実験的観点の両方から、提案されたアプローチの優位性を示します。 ..ただし、両方ともFGモデルの堅牢性を高めることができる「ハード」でラベルの誤った例も削除されます。 
[ABSTRACT]損失補正方法は、ノイズ遷移行列を推定しようとします。しかし、不可避の誤った補正は、深刻な累積エラーを引き起こします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Dual Gaussian-based Variational Subspace Disentanglement for
  Visible-Infrared Person Re-Identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_23.html">
      <font color="black">Dual Gaussian-based Variational Subspace Disentanglement for
  Visible-Infrared Person Re-Identification</font>
    </a>
  </h2>
  <font color="black">従来のVAEのような効率的な最適化を実現するために、監視された設定の下で事前にMoGの2つの変分推論項を導出します。これにより、モデルが明示的にクロスモダリティのアイデンティティ内分散を処理するように、アイデンティティー識別可能な部分空間が制限されるだけでなく、 MoG分布が事後崩壊を回避することを可能にします。モダリティ間で識別可能な識別機能を解きほぐし、VI-ReIDの検索をより堅牢にします。広範な実験により、2つのVI-ReIDデータセットで最新のメソッドよりも優れた方法が示されています。 
[要約] va-rgbとrgb-どちらも余分な相互モダリティ変動の影響を受けます。これは、ハイパーエンパーパーパーパーによる解きほぐしプロセスが原因です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: On the Accuracy of CRNNs for Line-Based OCR: A Multi-Parameter
  Evaluation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_24.html">
      <font color="black">On the Accuracy of CRNNs for Line-Based OCR: A Multi-Parameter
  Evaluation</font>
    </a>
  </h2>
  <font color="black">オープンソースフレームワークCalamariに依存するトレーニングパイプラインのすべてのコンポーネントのアブレーションを表示します。2値化、入力ラインの高さ、ネットワーク幅、ネットワーク深度、およびドロップアウトなどの他のネットワークトレーニングパラメーターなどの要因の影響について説明します。広範なグリッド検索を通じて、ニューラルネットワークアーキテクチャと最適なデータ拡張設定のセットを取得します。 
[ABSTRACT]ニューラルネットワークアーキテクチャと一連の最適なデータ拡張設定を取得します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: To Ensemble or Not Ensemble: When does End-To-End Training Fail? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_25.html">
      <font color="black">To Ensemble or Not Ensemble: When does End-To-End Training Fail?</font>
    </a>
  </h2>
  <font color="black">この作品はまた、ドロップアウトへのリンクを明らかにし、アンサンブルの多様性と多分岐ネットワークの性質について疑問を投げかけます。驚くべき結果は、最適なものがアンサンブルシステムでもE2Eシステムでもない場合があるということです。過大なパラメータ化されたモデルをE2Eでトレーニングできない場合の明確な失敗例。 
[ABSTRACT] e2eトレーニングとe2eトレーニングのトレンドは継続します。今週、トレンドが継続するかどうかを確認します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-02-12">
        <br><font color="black">2019-02-12</font>
      </time>
    </span>
</section>
<!-- paper0: Fine-grained Iterative Attention Network for TemporalLanguage
  Localization in Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_26.html">
      <font color="black">Fine-grained Iterative Attention Network for TemporalLanguage
  Localization in Videos</font>
    </a>
  </h2>
  <font color="black">さらに、ターゲットセグメントをより正確に予測するために、最近のアンカーベースのローカリゼーションを適用する代わりに、コンテンツ指向のローカリゼーション戦略を提案します。具体的には、反復アテンションモジュールでは、クエリ内の各単語は、最初に細かい注意を払ってビデオを再生し、次に統合クエリに繰り返し参加します。ただし、この分野でのこれまでのほとんどの試みは、ビデオからクエリへの一方向の対話にのみ焦点を当て、どの単語を聞くかを強調し、バニラソフトを介して文章情報に注意を向けます注意が必要ですが、どこを見るかを示唆するビデオによるクエリの対話からの手掛かりは考慮されません。 
[要約]このホワイトペーパーでは、きめの細かい反復的なビデオインタラクションを提案します。反復的注意ネットワークは、視覚的モダリティと文学的モダリティの両方から基礎情報を抽出することを目的としています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Disentangling Human Error from the Ground Truth in Segmentation of
  Medical Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_27.html">
      <font color="black">Disentangling Human Error from the Ground Truth in Segmentation of
  Medical Images</font>
    </a>
  </h2>
  <font color="black">次に、3つのパブリックメディカルイメージングセグメンテーションデータセットに対して、シミュレーション（必要な場合）および実際の多様なアノテーションを使用して、メソッドの有用性を示します。1）MSLSC（多発性硬化症病変）。 2）BraTS（脳腫瘍）; 3）LIDC-IDRI（肺異常）。この作業では、2つの結合されたCNNを使用して、純粋なノイズのある観測のみから、個々のアノテーターの信頼性と真のセグメンテーションラベル分布を共同で学習する方法を示します。これらのノイズの処理グラウンドトゥルースが自動セグメンテーションアルゴリズムが達成できるパフォーマンスを制限するため、盲目的にラベルを付けます。 
[ABSTRACT]アルゴリズムはアノテーターからのデータに基づいており、コストは2,000ドルです。これらのアルゴリズムはさまざまな人間の専門家に基づいています。これらのアルゴリズムの結果はラベルの品質に依存すると述べています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-31">
        <br><font color="black">2020-07-31</font>
      </time>
    </span>
</section>
<!-- paper0: Modeling Data Reuse in Deep Neural Networks by Taking Data-Types into
  Cognizance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_28.html">
      <font color="black">Modeling Data Reuse in Deep Neural Networks by Taking Data-Types into
  Cognizance</font>
    </a>
  </h2>
  <font color="black">したがって、設計時にそれを推定することは困難です。DNNのさまざまなデータ型の等しくない重要性を説明する「データ型認識重み付き算術強度」（$ DI $）と呼ばれる新しいモデルを提案します。前者は、設計時に推定。ただし、後者は複雑なデータ再利用パターンと基盤となるハードウェアアーキテクチャに依存します。 
[ABSTRACT] duseのエネルギー消費は、mac操作の数と各mac操作のエネルギー効率に依存します。これは、dnnsでのデータ再利用の量と比較されます。これは、すべてのデータタイプに同等の重要性を与えます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Factorize and Relight a City -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_29.html">
      <font color="black">Learning to Factorize and Relight a City</font>
    </a>
  </h2>
  <font color="black">学習した絡み合っていない要素を使用して、照明効果やシーンジオメトリの変更など、リアルな方法で新しい画像を操作できることを示します。トレーニングを容易にするために、同じ場所が時間の経過とともに繰り返しキャプチャされます。このデータは、前例のない時空間的な屋外画像のスケールを表しています。 
[ABSTRACT]私たちの学習信号は2つの洞察に基づいて構築されています：絡み合っていない要因を組み合わせると、元の画像が再構築されます。データは、絡み合っていない画像の前例のないスケールを表します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: A coarse-to-fine framework for unsupervised multi-contrast MR image
  deformable registration with dual consistency constraint -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_30.html">
      <font color="black">A coarse-to-fine framework for unsupervised multi-contrast MR image
  deformable registration with dual consistency constraint</font>
    </a>
  </h2>
  <font color="black">ボクセルモーフ、SyN、LDDMMなどの一般的に利用されている登録方法と比較して、提案された方法は、脳卒中病変の識別において0.826のDiceスコアで最高の登録パフォーマンスを実現します。登録速度に関しては、この方法は約17倍高速です。同じCPUでテストする場合、SyNの最も競争力のある方法よりも優れています。このホワイトペーパーでは、教師なしの新しい学習ベースのフレームワークを提案し、正確で効率的なマルチコントラストMR画像登録を実現します。 
[要約]提案された方法は、555ケースからなる臨床データセットで評価されています。これは、マルチステップ反復プロセスと複雑な画像前処理操作を取り除くように設計されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-05">
        <br><font color="black">2020-08-05</font>
      </time>
    </span>
</section>
<!-- paper0: Approach for document detection by contours and contrasts -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_31.html">
      <font color="black">Approach for document detection by contours and contrasts</font>
    </a>
  </h2>
  <font color="black">実行された実験では、このような変更により、代替の順序付けエラーが40％減少し、検出エラーの総数が10％減少しました。オープンなMIDV-500データセットの最先端のパフォーマンスを更新し、競争力のある結果を示しましたSmartDocデータセットの最新技術を使用してください。古典的な輪郭ベースのアプローチは、オクルージョン、複雑な背景、またはぼかしのあるケースを誤って処理することがよくあります。 
[ABSTRACT]古典的な等高線ベースのアプローチは、オクルージョン、複雑な背景、またはぼかしのケースを誤って処理することがよくあります。カウンターベースの方法に基づいて、競合他社は、境界の内側と外側の領域間のコントラストに従ってランク付けされます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Joint Self-Attention and Scale-Aggregation for Self-Calibrated Deraining
  Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_32.html">
      <font color="black">Joint Self-Attention and Scale-Aggregation for Self-Calibrated Deraining
  Network</font>
    </a>
  </h2>
  <font color="black">さらに、畳み込みニューラルネットワーク（CNN）の基本的な畳み込み機能の変換プロセスを改善するために、セルフキャリブレーションされた畳み込みを適用して、各畳み込みの視野を明示的に拡大する各空間位置の周囲に、長距離の空間依存性とチャネル間依存性を構築します内部通信を介して層を形成し、出力機能を強化します。最新の方法と比較して、この方法の優位性を実証するために広範な実験が行われます。自己調整されたスケール集約モジュールと自己注意モジュールを設計することによりたたみ込みを巧みに行うことにより、提案されたモデルは、実際のデータセットと合成データセットの両方でより優れた結果を排出します。 
[ABSTRACT] cnnは、たたみ込みと呼ばれる効果的なアルゴリズムを提案して、単一の画像排出問題を解決し、アプリケーションのセグメンテーションと検出タスクを実行します。同時に、自己情報モジュールが導入されて、それらのたたみ込みの対応物と一致または性能が向上します。各チャンネルに適応</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: A Sensitivity Analysis Approach for Evaluating a Radar Simulation for
  Virtual Testing of Autonomous Driving Functions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_33.html">
      <font color="black">A Sensitivity Analysis Approach for Evaluating a Radar Simulation for
  Virtual Testing of Autonomous Driving Functions</font>
    </a>
  </h2>
  <font color="black">シミュレーションベースのテストは、自動運転機能の検証作業を大幅に削減するための有望なアプローチです。このホワイトペーパーでは、レーダーシミュレーションを開発および評価するための感度分析アプローチを紹介します。特にレーダーは、伝統的にモデル化するのが最も難しいセンサーの1つでした。 
[ABSTRACT]レーダーは、モデル化するのが最も難しいセンサーの1つです。カメラやレーダーなどのセンサーは、このテスト作業で重要な役割を果たします。システムは現在、日本でテストされています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: LiSHT: Non-Parametric Linearly Scaled Hyperbolic Tangent Activation
  Function for Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_34.html">
      <font color="black">LiSHT: Non-Parametric Linearly Scaled Hyperbolic Tangent Activation
  Function for Neural Networks</font>
    </a>
  </h2>
  <font color="black">非常に有望なパフォーマンスの改善が、多層パーセプトロン（MLP）、畳み込みニューラルネットワーク（CNN）、および長期短期記憶（LSTM）などのリカレントニューラルネットワークを含む3種類のニューラルネットワークで観察されています。線形関数によって非線形の双曲線正接（Tanh）関数をスケーリングし、死にかけている勾配の問題に取り組む試み。ただし、ゼロハード整流のため、ReLUやSwishなどの既存のアクティブ化関数の一部は、大きな負の入力値であり、瀕死の勾配の問題に苦しむ可能性があります。 
[ABSTRACT]大きな負の入力値を使用するためのreluやswishミスなどの既存のアクティベーション関数の一部であり、瀕死の勾配の問題が発生する可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-01-01">
        <br><font color="black">2019-01-01</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Accurate Pixel-wise Object Tracking by Attention Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_35.html">
      <font color="black">Towards Accurate Pixel-wise Object Tracking by Attention Retrieval</font>
    </a>
  </h2>
  <font color="black">最初に、開始フレームにグラウンドトゥルースマスクを使用してルックアップテーブル（LUT）を作成し、次にLUTを取得して空間制約のアテンションマップを取得します。さらに、マルチ解像度マルチステージセグメンテーションを導入します。ネットワーク（MMS）は、バックボーンフィーチャーをフィルターするために予測マスクを再利用することにより、バックグラウンドクラッターの影響をさらに弱めます。効率的ですが、バックグラウンドクラッターの悪影響を考慮せずにバックボーンフィーチャーを直接融合すると、偽陰性の予測が導入され、セグメンテーションの精度が遅れます。 。 
[要旨]バックボーンフィーチャに対してソフトな空間制約を実行するための注意検索ネットワーク（arn）を提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: IV-SLAM: Introspective Vision for Simultaneous Localization and Mapping -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_36.html">
      <font color="black">IV-SLAM: Introspective Vision for Simultaneous Localization and Mapping</font>
    </a>
  </h2>
  <font color="black">IV-SLAMは、視覚機能からの再投影エラーのノイズプロセスを明示的にモデル化し、コンテキストに依存するようにします。この学習されたノイズモデルを使用して、IV-SLAMは特徴抽出をガイドし、画像の一部からより低いノイズをもたらす可能性が高い特徴をより多く選択し、さらに学習されたノイズモデルを共同最尤推定に組み込んで、前述のタイプのエラー..このような障害に対処するために、以前の作業は、より堅牢なビジュアルフロントエンドの構築に焦点を当て、困難な機能を除外しました。 
[ABSTRACT] v-検出された画像に鏡面反射、レンズフレア、または動的オブジェクトの影などの困難な条件が含まれている場合、スラムアルゴリズムは致命的な追跡障害を起こしやすい</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Injecting Prior Knowledge into Image Caption Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_37.html">
      <font color="black">Injecting Prior Knowledge into Image Caption Generation</font>
    </a>
  </h2>
  <font color="black">画像から自然言語の説明を自動的に生成することは、視覚信号とテキスト信号、およびそれらの間の相関関係を十分に理解する必要がある人工知能の挑戦的な問題です。このホワイトペーパーでは、最新のパフォーマンスを向上させることを提案します。 -事前知識の2つのソースを組み込むことによるアートイメージキャプションモデル：（i）条件付き潜在トピックアテンション。潜在変数（トピック）のセットをアンカーとして使用して、可能性の高い単語を生成し、（ii）正規化手法を利用します。キャプションの構文的および意味的構造における誘導バイアスと画像キャプションモデルの一般化を向上させます。特にデータが限られている場合、画像キャプションの最先端の方法は人間レベルのパフォーマンスに近づくのに苦労しています。 
[ABSTRACT]最先端の画像キャプションの手法は、特にデータが限られている場合に、人間レベルのパフォーマンスに近づくのに苦労しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-22">
        <br><font color="black">2019-11-22</font>
      </time>
    </span>
</section>
<!-- paper0: Exploring Relations in Untrimmed Videos for Self-Supervised Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_38.html">
      <font color="black">Exploring Relations in Untrimmed Videos for Self-Supervised Learning</font>
    </a>
  </h2>
  <font color="black">実験結果は、ERUVがより豊かな表現を学習でき、大幅なマージンを備えた最先端の自己監視手法よりもパフォーマンスが優れていることを示しています。戦略は、自己監視信号として保存されます。ショットチェンジ検出。 
[ABSTRACT]トリミングされたデータセットは、トリミングされていない動画から手動で注釈が付けられます。この方法は、トリミングされていない動画（実際のラベルなし）に簡単に適用して、時空間機能を学習できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Learnable Graph Inception Network for Emotion Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_39.html">
      <font color="black">Learnable Graph Inception Network for Emotion Recognition</font>
    </a>
  </h2>
  <font color="black">提案されたアーキテクチャを、3つの異なるモダリティ（ビデオ、オーディオ、モーションキャプチャ）にまたがる4つのベンチマーク感情認識データベースで評価します。各データベースは、顔の表情、スピーチ、体のジェスチャーなどの感情的な手がかりの1つをキャプチャします。このために、 \ emph {学習可能なグラフ開始ネットワーク}（L-GrIN）は、感情を認識し、データの根本的なグラフ構造を特定することを共同で学習します。すべてのデータベースで最先端のパフォーマンスを達成し、いくつかの競争力のあるベースラインと既存の関連するパフォーマンスを達成しますメソッド。 
[ABSTRACT]視覚的な手がかりは、オーディオ、ビデオ、モーション-キャプチャ（mocap）または他のモダリティを使用してキャプチャできます。これを結合グラフ学習および分類タスクとしてキャストします。すべてのデータベースで最先端のパフォーマンスを達成し、いくつかの競合製品より優れていますベースラインと関連する既存の方法</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Cross-Model Image Annotation Platform with Active Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_40.html">
      <font color="black">Cross-Model Image Annotation Platform with Active Learning</font>
    </a>
  </h2>
  <font color="black">私たちは、支援画像注釈（注釈支援）、能動学習、モデルのトレーニングと評価をシームレスに組み込んだモジュール式画像注釈プラットフォームを開発しました。3番目に、複数の画像モデル間で相互運用可能であり、ツールはオブジェクトモデルのトレーニングと一連の事前トレーニング済みモデル全体の評価。達成された最高の精度は74％です。 
[要約]機械学習の中心的な考え方は、優れたデータから学習するアルゴリズムの学習にあります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: PCSGAN: Perceptual Cyclic-Synthesized Generative Adversarial Networks
  for Thermal and NIR to Visible Image Transformation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_41.html">
      <font color="black">PCSGAN: Perceptual Cyclic-Synthesized Generative Adversarial Networks
  for Thermal and NIR to Visible Image Transformation</font>
    </a>
  </h2>
  <font color="black">多くの現実世界のシナリオでは、照明条件が悪いため、可視光スペクトル（VIS）で画像をキャプチャすることは困難です。したがって、変換された画像の品質、細部、リアルさを向上させるには、より優れた目的関数が必要です。提案されたPCSGANは、SSIM、MSE、PSNR、LPIPSの評価指標の点で、Pix2pix、DualGAN、CycleGAN、PS2GAN、PANを含む最新の画像変換モデルよりも優れています。 
[要約] thm / nirからvisへの画像変換の新しいモデルが導入されました。知覚循環ソースvisと呼ばれ、コードはwwwで入手できます。 github。 com</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-13">
        <br><font color="black">2020-02-13</font>
      </time>
    </span>
</section>
<!-- paper0: Regularized Pooling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_42.html">
      <font color="black">Regularized Pooling</font>
    </a>
  </h2>
  <font color="black">手書きの文字画像とテクスチャ画像の実験結果は、正規化されたプーリングが認識精度を向上させるだけでなく、従来のプーリング操作と比較して学習の収束を加速することを示しました。この論文では、実際の変形のみを補償するために、隣接するカーネル間でプーリング操作の値選択方向を空間的に滑らかにすることができる、正規化されたプーリングを提案します。 
[ABSTRACT]一般的に、最大プーリングは、OSごとに個別に実行されます。つまり、最大プーリングは柔軟性がありすぎて、実際の負傷を補償できない</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-06">
        <br><font color="black">2020-05-06</font>
      </time>
    </span>
</section>
<!-- paper0: Fast Approximate Modelling of the Next Combination Result for Stopping
  the Text Recognition in a Video -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_43.html">
      <font color="black">Fast Approximate Modelling of the Next Combination Result for Stopping
  the Text Recognition in a Video</font>
    </a>
  </h2>
  <font color="black">これらの方法は、ビデオのドキュメントテキストフィールド認識と任意のテキスト認識のタスクについて評価されました。最初に、次の結合結果のモデリングに基づいて、そのようなプロセスを最適に停止する既存の方法について説明します。実験的な比較により、導入された近似は、達成された組み合わせ結果の精度の観点から停止方法の品質を低下させず、停止の決定に必要な時間を劇的に削減します。 
[ABSTRACT]ビデオストリーム認識の停止の問題は未調査のトピックですが、高性能のビデオ認識システムを構築するために重要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Generative Adversarial Networks for Image and Video Synthesis:
  Algorithms and Applications -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_44.html">
      <font color="black">Generative Adversarial Networks for Image and Video Synthesis:
  Algorithms and Applications</font>
    </a>
  </h2>
  <font color="black">また、コンテンツ作成における多くの新しいアプリケーションの作成にもつながりました。生成的敵対的ネットワーク（GAN）フレームワークは、さまざまな画像およびビデオ合成タスクのための強力なツールとして登場し、無条件または入力でのビジュアルコンテンツの合成を可能にしました。条件付きの方法。画像の翻訳、画像処理、ビデオ合成、神経レンダリングへの応用についても説明します。 
[ABSTRACT]高解像度のフォトリアリスティックな画像やビデオの生成を可能にしました。これは、従来の方法では困難または不可能だったタスクです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Object-based Illumination Estimation with Rendering-aware Neural
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_45.html">
      <font color="black">Object-based Illumination Estimation with Rendering-aware Neural
  Networks</font>
    </a>
  </h2>
  <font color="black">これらの問題に対処するために、逆レンダリングの物理的原理を利用してソリューションを制約するアプローチを提案します。また、ニューラルネットワークを利用して処理の計算コストの高い部分を効率化し、ノイズの多い入力データの堅牢性を向上させるとともに、時間的および空間的安定性を改善します。推定された照明により、仮想オブジェクトを実際のシーンと一致するシェーディングを使用してARシナリオでレンダリングでき、リアリズムが向上します。RGBDの外観からの高速環境光推定のスキームを提示します。個々のオブジェクトとそのローカル画像領域。 
[ABSTRACT]従来の逆レンダリングは非常に複雑で、リアルタイムアプリケーションに必要ではありません。代わりに、学習ベースの手法のパフォーマンスは、個々のオブジェクトから利用できるわずかな入力データによって制限される場合があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Graph Convolutional Networks for Hyperspectral Image Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_46.html">
      <font color="black">Graph Convolutional Networks for Hyperspectral Image Classification</font>
    </a>
  </h2>
  <font color="black">さらに重要なことに、私たちのminiGCNは、ネットワークを再トレーニングせずに分類外のパフォーマンスを改善することなく、サンプル外のデータを推論することができます。それにもかかわらず、サンプル間の関係をモデル化する能力は制限されたままです。 GCN）が最近提案され、不規則な（または非グリッドの）データ表現と分析にうまく適用されています。 
[ABSTRACT] cnnsとgcnsは、さまざまなタイプのhs機能を抽出できます。この方法は、それらを融合して、単一モデルのパフォーマンスのバブルネックを解消します。新しいgcnを使用すると、大規模なgcnをミニバッチ方式でトレーニングできます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: From IC Layout to Die Photo: A CNN-Based Data-Driven Approach -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_47.html">
      <font color="black">From IC Layout to Die Photo: A CNN-Based Data-Driven Approach</font>
    </a>
  </h2>
  <font color="black">さらに、LithoNetはウェーハ製造パラメーターを潜在ベクトルとして取り、SEM画像で検査できるパラメトリック製品のばらつきをモデル化できます。2つの畳み込みニューラルネットワークで構成されるディープラーニングベースのデータ駆動型フレームワークを提案します。は、IC製造による回路の形状変形を予測します。ii）このような形状変形を補正するためのICレイアウト修正を提案するOPCNet。製造された回路の形状は、元のレイアウト設計と最適に一致します。 
[ABSTRACT]提案されたリソネット-opcnetフレームワークは、レイアウト形状を使用して、作成されたICの形状を予測できます。提案された設計ツールは、予測された形状と指定されたレイアウトとの一貫性に従ってレイアウト修正も予測できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-11">
        <br><font color="black">2020-02-11</font>
      </time>
    </span>
</section>
<!-- paper0: IIIT-AR-13K: A New Dataset for Graphical Object Detection in Documents -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_48.html">
      <font color="black">IIIT-AR-13K: A New Dataset for Graphical Object Detection in Documents</font>
    </a>
  </h2>
  <font color="black">私たちのデータセットが、ビジネスドキュメント内のさまざまなタイプのグラフィカルオブジェクトを検出するための研究を進める上で役立つことを願っています。FasterR-CNN 
[20]とMaskを使用した2つの最先端のグラフィカルオブジェクト検出手法でIIIT-AR-13KデータセットをベンチマークR-CNN 
[11]とさらなる研究のための高いベースラインを確立します。このデータセットIIIT-AR-13kは、公開されている年次報告書でグラフィカルオブジェクトまたはページオブジェクトの境界ボックスに手動で注釈を付けることによって作成されます。 
[ABSTRACT]このデータセットiiit-ar-13kは、公開されている年次報告書のグラフィックオブジェクトの境界ボックスに手動で注釈を付けることによって作成されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Self-supervised Video Representation Learning Using Inter-intra
  Contrastive Framework -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_49.html">
      <font color="black">Self-supervised Video Representation Learning Using Inter-intra
  Contrastive Framework</font>
    </a>
  </h2>
  <font color="black">提案された内部コントラストフレームワークを使用すると、ビデオ表現を学習するために時空間たたみ込みネットワークをトレーニングできます。提案された方法は、16.7％や9.5％ポイントの改善など、現在の最先端の結果を大幅に上回っています。ビデオ検索用のUCF101およびHMDB51データセットのトップ1の精度。それぞれ、提案されたフレームワークには多くの柔軟なオプションがあり、いくつかの異なる構成を使用して実験を行っています。 
[ABSTRACT]従来の自己学習法の標準的なアプローチでは、ポジティブ-ネガティブ-ネガティブデータのペアを使用して、対照的な学習戦略でトレーニングします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Iterative Distance-Aware Similarity Matrix Convolution with
  Mutual-Supervised Point Elimination for Efficient Point Cloud Registration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_50.html">
      <font color="black">Iterative Distance-Aware Similarity Matrix Convolution with
  Mutual-Supervised Point Elimination for Efficient Point Cloud Registration</font>
    </a>
  </h2>
  <font color="black">キーポイントの追加アノテーションなしでモデルをトレーニングするために、新しい相互監視損失が提案されています。コードはhttps://github.com/jiahaowork/idamで公開されています。パイプラインは、従来の両方と簡単に統合できます（たとえば、
[ABSTRACT ]提案されたモデルには、反復距離を考慮した類似性マトリックスが含まれています。また、2段階の学習可能な点消去法も含まれています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-23">
        <br><font color="black">2019-10-23</font>
      </time>
    </span>
</section>
<!-- paper0: SymmetryNet: Learning to Predict Reflectional and Rotational Symmetries
  of 3D Shapes from Single-View RGB-D Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_51.html">
      <font color="black">SymmetryNet: Learning to Predict Reflectional and Rotational Symmetries
  of 3D Shapes from Single-View RGB-D Images</font>
    </a>
  </h2>
  <font color="black">さらに、私たちのネットワークは、さまざまなタイプの複数の対称性を特定の形状で検出することができます。ただし、対称性予測のためにディープモデルを直接トレーニングすると、過剰適合の問題がすぐに発生する可能性があります。マルチタスク学習アプローチを採用します。 
[要約]ディープニューラルネットワークは、入力rgb-d画像に存在する3dオブジェクトの反射対称性と回転対称性の両方を予測できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-02">
        <br><font color="black">2020-08-02</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Pansharpening Based on Self-Attention Mechanism -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_52.html">
      <font color="black">Unsupervised Pansharpening Based on Self-Attention Mechanism</font>
    </a>
  </h2>
  <font color="black">第3に、詳細抽出および注入関数は、注意の表現に基づいて空間的に変化するため、再構成の精度が大幅に向上します。広範な実験結果は、提案されたアプローチが、詳細とスペクトル歪みの少ない、さまざまなタイプのよりシャープなMSIを再構成できることを示しています各ピクセルが複数の構成要素をカバーする傾向がある衛星画像には混合ピクセルが広く存在するため、サブピクセルレベルでのパンシャープンが不可欠になります。 
[ABSTRACT]ピクセルは、監視なしのパンシャープニング（アップ）メソッドです。アップサムと呼ばれる自己注意メカニズム（sam）に基づく課題に対処するために提案されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-16">
        <br><font color="black">2020-06-16</font>
      </time>
    </span>
</section>
<!-- paper0: Distortion Robust Image Classification using Deep Convolutional Neural
  Network with Discrete Cosine Transform -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_53.html">
      <font color="black">Distortion Robust Image Classification using Deep Convolutional Neural
  Network with Discrete Cosine Transform</font>
    </a>
  </h2>
  <font color="black">DCT-Netは「盲目的に」一度だけ訓練され、さらなる再訓練なしで一般的な状況で適用されます。実験結果は、訓練されると、DCT-Netはさまざまな目に見えない画像の歪みにうまく一般化するだけでなく、文献の他の方法よりも優れていることを示しています..文献の他の作品とは異なり、DCT-Netは、トレーニング中とテスト中の両方で、画像の歪みのタイプとレベルに「盲目」です。 
[ABSTRACT]提案されたdct-vgg16の上に構築されたネットワーク内のネットワークは、画質の低下に対して脆弱であることがわかります。これは、これらの作業が初めて適用された可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-11-14">
        <br><font color="black">2018-11-14</font>
      </time>
    </span>
</section>
<!-- paper0: Efficient Non-Line-of-Sight Imaging from Transient Sinograms -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_54.html">
      <font color="black">Efficient Non-Line-of-Sight Imaging from Transient Sinograms</font>
    </a>
  </h2>
  <font color="black">私たちは、（1）これらのC2NLOS測定値が、正弦波の重ね合わせで構成されていることを観察しました。これは、過渡シノグラムと呼ばれます。（2）これらの正弦波測定値を、非表示の散乱体の3D位置または非表示のNLOS画像に変換する計算効率の高い再構成手順が存在します。オブジェクト、および（3）以前のアプローチよりも測定値が1桁少ないにもかかわらず、これらのC2NLOSスキャンは、これらの異なるNLOSイメージングタスクを解決するために隠されたシーンに関する十分な情報を提供します。共通点を照らして画像化し、この点を壁に沿って円形の経路で走査する視力（C2NLOS）スキャン。シミュレーションと実際の両方のC2NLOSスキャンの結果を示します。 
[要約] 1つのアプローチは、レーザーを使用して多重散乱光の移動時間を測定することを含みます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Learning for Identifying Events in Active Target
  Experiments -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_55.html">
      <font color="black">Unsupervised Learning for Identifying Events in Active Target
  Experiments</font>
    </a>
  </h2>
  <font color="black">また、イベント分離のためのオートエンコーダニューラルネットワークの潜在空間のクラスタリングのアプリケーションについても検討します。VGG16の潜在空間のシミュレーションデータに適用された$ K $ -meansアルゴリズムは、ほぼ完璧なクラスターを形成します。さらに、VGG16 + $ K $平均法は、実際の実験データに対して、プロトンイベントの高純度クラスターを検出します。 
[ABSTRACT]包括的な目標は、データ分析の初期段階で同様のイベントをグループ化することです。オートエンコーダニューラルネットワークと事前トレーニングvgg16畳み込みニューラルネットワークのパフォーマンスを調査します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Data-driven Meta-set Based Fine-Grained Visual Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_56.html">
      <font color="black">Data-driven Meta-set Based Fine-Grained Visual Classification</font>
    </a>
  </h2>
  <font color="black">具体的には、少量のクリーンなメタセットによって導かれ、メタ学習の方法で選択ネットをトレーニングして、分布内と分布外のノイズのある画像を区別します。モデルの堅牢性をさらに高めるために、ネットにラベルを付けて、分布内のノイズの多いデータのラベルを修正します。このように、提案された方法は、分布外ノイズによって引き起こされる有害な影響を軽減し、分布内のノイズの多いサンプルをトレーニングに適切に利用できます。 
[ABSTRACT]ウェブ画像からの学習は、きめの細かい視覚認識の代替方法になります。この方法には、ノイズの多いデータではなく、アウトの知識を修正するためのラベリングネットの学習が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Optical Flow and Mode Selection for Learning-based Video Coding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_57.html">
      <font color="black">Optical Flow and Mode Selection for Learning-based Video Coding</font>
    </a>
  </h2>
  <font color="black">提案されたコーディングスキームは、Learned Image Compression 2020（CLIC20）Pフレームコーディング条件のチャレンジの下で評価され、最先端のビデオコーデックITU / MPEG HEVCと同等に機能することが示されています。フローは、コード化するフレームの予測を実行するために使用されます。このペーパーでは、2つの補完的なオートエンコーダー、MOFNetとCodecNetに基づくフレーム間コーディングの新しい方法を紹介します。 
[ABSTRACT] mofnetはオプティカルフローとピクセル単位のコーディングモード選択を伝えることを目的としています。64歳のコーデックネットはそれを伝えることを目的としたプロジェクトです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: FastLR: Non-Autoregressive Lipreading Model with Integrate-and-Fire -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_58.html">
      <font color="black">FastLR: Non-Autoregressive Lipreading Model with Integrate-and-Fire</font>
    </a>
  </h2>
  <font color="black">私たちの実験は、FastLRが10.97 $ \ times $までのスピードアップを達成し、GRIDおよびLRS2リピーディングデータセットでWER絶対増加がわずか1.5 \％および5.5 \％である最新のリピーディングモデルと比較して、提案手法の有効性..読唇術は印象的な手法であり、近年、確実に精度が向上しています。しかし、既存の読唇術の手法は、主に自己回帰（AR）モデルに基づいており、ターゲットトークンを1つずつ生成し、高い推論レイテンシから。 
[要約]読唇術の新しいメソッドは、主に自己回帰（ar）モデルに基づいて構築され、ターゲットトークンを1つずつ生成します。ar言語モデルの削除は、読唇術の固有のあいまいさの問題を悪化させます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Group Activity Prediction with Sequential Relational Anticipation Model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_59.html">
      <font color="black">Group Activity Prediction with Sequential Relational Anticipation Model</font>
    </a>
  </h2>
  <font color="black">2つの一般的に使用されているデータセットの実験結果は、私たちのアプローチが最先端の活動予測方法を大幅に上回っていることを示しています。このモデルは、2つのグラフオートエンコーダーによって活動の特徴と位置の両方を明示的に予測し、グループ活動予測..既存の行動予測アプローチは、部分観察の表現力を強化することを学びます。 
[ABSTRACT]既存の行動予測アプローチは、見かけの部分的な観察の表現力を強化することを学びます。我々は、豊富な識別情報で人々のグループの位置を要約するシステムを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Structured Convolutions for Efficient Neural Network Design -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_60.html">
      <font color="black">Structured Convolutions for Efficient Neural Network Design</font>
    </a>
  </h2>
  <font color="black">この手法を幅広いCNNアーキテクチャに適用することで、最大2 $ \ times $小さいResNetの「構造化」バージョンと、精度の低下を1％以内に抑えながらより効率的な新しいStructured-MobileNetV2を実証しますImageNetおよびCIFAR-10データセットで。さらに、トレーニング後に、ニューラルネットワークレイヤーがこの望ましい構造を活用できるようにする構造的正則化損失を提示します。これにより、トレーニング後、無視できるほどのパフォーマンス損失で分解できます。既存のテンソル分解やチャネルプルーニング手法と比較して、複雑さの軽減という点で優れています。 
[ABSTRACT]これに加えて、resnetシステムの「ネットワーク」を示しました。これらは、この分解を2dおよび3dカーネルに適用する方法を示しています。このモデルは、異なるレイヤーのネットワークを構築することで使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Self-Paced Deep Regression Forests with Consideration on
  Underrepresented Examples -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_61.html">
      <font color="black">Self-Paced Deep Regression Forests with Consideration on
  Underrepresented Examples</font>
    </a>
  </h2>
  <font color="black">2つのコンピュータービジョンタスク、つまり顔の年齢の推定と頭のポーズの推定に関する広範な実験は、最先端のパフォーマンスが達成されるSPUDRFの有効性を実証します。次に、自然な疑問が生じます。より強力で偏りの少ないソリューションを実現するための深い識別モデル。これは、SPLの基本的なランク付けと選択の問題に新しい視点から取り組みます。公平性です。 
[要約]ディープクォータフォレストは、問題を解決するために最近成功を収めています。この方法は基本的なものであり、さまざまな深い判別モデルと簡単に組み合わせることができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-03">
        <br><font color="black">2020-04-03</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-end learning for semiquantitative rating of COVID-19 severity on
  Chest X-rays -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_62.html">
      <font color="black">End-to-end learning for semiquantitative rating of COVID-19 severity on
  Chest X-rays</font>
    </a>
  </h2>
  <font color="black">当社のBS-Netは、すべての処理段階で自己注意行動と高度な精度を実証します。CXRデータセットは、ソースコードおよびトレーニング済みモデルとともに、研究目的で公開されます。このような困難な視覚的タスクを解決するために、さまざまなデータセットを含む「一部から全体へ」の手順でトレーニングされたさまざまなタスク（セグメンテーション、空間配置、およびスコア推定）を処理するように構造化された弱く監視された学習戦略を採用します。 
[要約]このシステムは、イタリアで高いパンデミックピークを経験した病院の1つで患者の連続監視に適用されます。この方法は、同じ病院で収集されたほぼxr画像のデータセットを活用しています。このデータセットは、ほぼすべてのデータセットに基づいています病院で撮影した5,00枚のcxr画像</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-08">
        <br><font color="black">2020-06-08</font>
      </time>
    </span>
</section>
<!-- paper0: Do We Really Need to Access the Source Data? Source Hypothesis Transfer
  for Unsupervised Domain Adaptation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_63.html">
      <font color="black">Do We Really Need to Access the Source Data? Source Hypothesis Transfer
  for Unsupervised Domain Adaptation</font>
    </a>
  </h2>
  <font color="black">SHOTは、ソースモデルの分類子モジュール（仮説）をフリーズし、情報の最大化と自己教師付き疑似ラベリングの両方を利用してターゲット固有の特徴抽出モジュールを学習し、表現をターゲットドメインからソース仮説に暗黙的に整列させます。この作業は、訓練されたソースモデルのみが利用可能であり、ソースデータなしでそのようなモデルをどのように効果的に使用してUDA問題を解決できるかを調査する実用的な設定です。その多様性を検証するために、クローズドセット、部分的なものを含むさまざまな適応ケースでSHOTを評価します-set、およびオープンセットドメイン適応。 
[要旨]以前のudaメソッドは、通常、モデルの適応を学習するときにソースデータにアクセスする必要があります。これにより、分散型プライベートデータに対してリスクが高くなり、非効率になります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br><font color="black">2020-02-20</font>
      </time>
    </span>
</section>
<!-- paper0: Scribble-based Domain Adaptation via Co-segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_64.html">
      <font color="black">Scribble-based Domain Adaptation via Co-segmentation</font>
    </a>
  </h2>
  <font color="black">詳細だが時間のかかる注釈を要求する代わりに、ターゲットドメインの落書きを使用して、ドメインの適応を実行します。この作業では、新しい弱く監視された方法を提案します。1つのイメージングモダリティ）から別のドメインの適応を実行する必要があります。 。 
[ABSTRACT]この作業では、新しい「不安定」な監視ありメソッドを提案します。代わりに、実際にはオプションではない可能性がある追加のデータに完全に注釈を付ける必要があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-07">
        <br><font color="black">2020-07-07</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Perspective, Simultaneous Embedding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_65.html">
      <font color="black">Multi-Perspective, Simultaneous Embedding</font>
    </a>
  </h2>
  <font color="black">可変投影を使用するMPSEは、入力としてペアワイズ距離行列のセットを取り、ポイントを3Dに埋め込みながら、ペアワイズ距離を維持する適切な投影を計算します。マルチの適応的で確率的な一般化に基づいたMPSEの機能プロトタイプを提供します複数の距離と複数の変数投影への3次元スケーリング。さまざまなサイズのデータセットとさまざまな数の投影を使用した広範な定量的評価と、結果のソリューションの品質を示すいくつかの例を提供します。 
[ABSTRACT] mpseは、指定された距離の石のそれぞれを保存する2D投影（平面）を使用して、データのさまざまなビューを提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-13">
        <br><font color="black">2019-09-13</font>
      </time>
    </span>
</section>
<!-- paper0: StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated
  Images using Conditional Continuous Normalizing Flows -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_66.html">
      <font color="black">StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated
  Images using Conditional Continuous Normalizing Flows</font>
    </a>
  </h2>
  <font color="black">最後に、広範な定性的および定量的比較を通じて、他の同時作業に対するStyleFlowの優位性を示します。たとえば、顔の場合、カメラのポーズ、照明のバリエーション、表情、顔の毛、性別、年齢を変化させます。StyleFlowは属性フィーチャで条件付けされたGAN潜在空間内の条件付き連続正規化フローのインスタンスとして条件付き探索を定式化することにより、両方のサブ問題に対するシンプルで効果的で堅牢なソリューション。 
[要約]新しい論文で、属性の2つの問題-条件付きサンプリングと関連付け-制御された編集を調査します。これらには、実際の写真とstyleganで生成された画像の両方のさまざまな属性に沿ったきめの細かいもつれのない編集が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: GL-GAN: Adaptive Global and Local Bilevel Optimization model of Image
  Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_67.html">
      <font color="black">GL-GAN: Adaptive Global and Local Bilevel Optimization model of Image
  Generation</font>
    </a>
  </h2>
  <font color="black">シンプルなネットワーク構造で、GL-GANはローカルの2値最適化によって不均衡の性質を効果的に回避できます。これは、最初に低品質の領域を特定してから最適化することによって達成されます。現在のGANメソッドと比較して、モデルはCelebA、CelebA-HQ、およびLSUNデータセットでの印象的なパフォーマンス。さらに、弁別子出力からの特徴マップキューを使用して、特定の実装のための適応型ローカルおよびグローバル最適化手法（Ada-OP）を提案し、収束速度を向上させることを確認します。 
[要約]一部のモデルの結果は、生成された画像の品質の不均衡を表示します。一部のモデルは、他の領域と比較して表示されます。モデルは、グローバルな不確実性が存在する別の方法で高解像度画像を生成します画像全体を最適化する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning based HEp-2 Image Classification: A Comprehensive Review -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_68.html">
      <font color="black">Deep Learning based HEp-2 Image Classification: A Comprehensive Review</font>
    </a>
  </h2>
  <font color="black">この論文は、この分野における新たな機会と将来の研究の方向性についての議論で終わります。各方法のコアアイデア、注目すべき成果、および主要な長所と短所が批判的に分析されます。HEp-2細胞パターンの分類は、人体の自己免疫疾患を同定するための間接免疫蛍光検査。 
[ABSTRACT]多くの自動hep-2セル分類方法が近年提案されています。これらの方法は、2つのレベルでhep -2画像分類を実行します。これらは、深いネットワーク使用に基づく分類で構成されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-20">
        <br><font color="black">2019-11-20</font>
      </time>
    </span>
</section>
<!-- paper0: Pairwise Relation Learning for Semi-supervised Gland Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_69.html">
      <font color="black">Pairwise Relation Learning for Semi-supervised Gland Segmentation</font>
    </a>
  </h2>
  <font color="black">また、オブジェクトレベルのサイコロ損失を設計して、腺に触れることによって引き起こされる問題に対処し、S-Netの他の2つの損失関数と組み合わせます。S-Netは、セグメンテーション用のラベル付きデータでトレーニングされ、PR-Netは、ラベル付きデータとラベルなしデータの両方を教師なしの方法で使用して、特徴空間内の画像の各ペア間の意味の一貫性を活用することにより、画像表現能力を強化します。この結果は、提案されたPR-Netとオブジェクトレベルのダイス損失の有効性を示すだけではありませんだけでなく、PRS ^ 2モデルが両方のベンチマークで最先端の腺セグメンテーションパフォーマンスを達成していることも示しています。 
[ABSTRACT]ディープラーニング画像は、組織学画像の注釈に関連する多大な労力と関連する専門家の費用のために入手が困難です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Rethinking Generative Zero-Shot Learning: An Ensemble Learning
  Perspective for Recognising Visual Patches -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_70.html">
      <font color="black">Rethinking Generative Zero-Shot Learning: An Ensemble Learning
  Perspective for Recognising Visual Patches</font>
    </a>
  </h2>
  <font color="black">プロセスは、複数のスペシャリスト生成モデルを使用して、事前定義されたローカルパッチのセットのノイズの多いテキスト記述から差別的な視覚的特徴を生成することから始まります。これらの問題に対処するために、ローカルパッチを合成するマルチパッチ生成敵対ネット（MPGAN）と呼ばれる新しいフレームワークを提案します特徴とラベル付けされた新しいクラスに重み付けされた投票戦略を使用してラベルを付けます。投票戦略は分類子から出力される確率分布を平均化し、一部のパッチが他よりも識別力がある場合、識別ベースの注意メカニズムは各パッチに応じて重み付けするのに役立ちます。 
[ABSTRACT]マルチパッチ「mpgan」は、ローカルパッチ機能を合成し、未確認のクラスを新しい投票戦略でラベル付けする新しい概念です。次に、未確認クラスの各パッチから合成された機能を使用して、さまざまな教師付き分類子の集合を構築します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-27">
        <br><font color="black">2020-07-27</font>
      </time>
    </span>
</section>
<!-- paper0: Shonan Rotation Averaging: Global Optimality by Surfing $SO(p)^n$ -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_71.html">
      <font color="black">Shonan Rotation Averaging: Global Optimality by Surfing $SO(p)^n$</font>
    </a>
  </h2>
  <font color="black">以前の作業とは対照的に、既存の高性能（ただしローカル）構造からモーションパイプラインを再利用して、（わずかに）高次元回転多様体で多様体最小化を使用してこれらの緩和の大規模なインスタンスを解決する方法を示します。 。したがって、私たちの方法は、グローバルに最適なソリューションを回復しながら、現在のSFMメソッドの速度とスケーラビリティを維持します。 。 
[ABSTRACT]私たちの方法は、回転の平均化問題から回復するために半明確な緩和を採用しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Learning To Pay Attention To Mistakes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CV/paper_72.html">
      <font color="black">Learning To Pay Attention To Mistakes</font>
    </a>
  </h2>
  <font color="black">提案されたメカニズムには、2つの補完的な実装があります。（a）前景領域のより大きな有効受容野に対応するためのモデルの「明示的な」ステアリング。 （b）背景領域の小さな有効受容野に注意を払うことにより、偽陽性に向けた「暗黙の」ステアリング。これは、高い偽陰性検出率につながります。このホワイトペーパーでは、このような高率に直接対処する新しい注意メカニズムを提案します。間違いに注意を払うと呼ばれる偽陰性率。 
[ABSTRACT]プロジェクトはマリに基づいていました。これは、フォレネルllllllllllが前に述べたものです。ネガティブを回避するためには、より効果的である必要があると主張しています。これらには、誤検出に対する既存のバイアスに対抗する、誤検出の識別が含まれています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-29">
        <br><font color="black">2020-07-29</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Encoding formulas as deep networks: Reinforcement learning for zero-shot
  execution of LTL formulas -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CL/paper_0.html">
      <font color="black">Encoding formulas as deep networks: Reinforcement learning for zero-shot
  execution of LTL formulas</font>
    </a>
  </h2>
  <font color="black">これは、RLエージェントのマルチタスク学習の新しい形式であり、エージェントは1つの多様なタスクのセットから学習し、新しい多様なタスクのセットに一般化します。それらを満足させるための一般化..入力としてLTL式を取り、満足のいくアクションを決定する合成リカレントニューラルネットワークを使用する強化学習エージェントを示します。 
[ABSTRACT]ネットワークはゼロショット一般化を実行してそれらを満たします。エージェントのアクションはこれまでに見られたことはありません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-01">
        <br><font color="black">2020-06-01</font>
      </time>
    </span>
</section>
<!-- paper0: Question and Answer Test-Train Overlap in Open-Domain Question Answering
  Datasets -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CL/paper_1.html">
      <font color="black">Question and Answer Test-Train Overlap in Open-Domain Question Answering
  Datasets</font>
    </a>
  </h2>
  <font color="black">また、テストセットの質問の30％は、対応するトレーニングセットでほぼ重複した言い換えを持っていることもわかります。トレーニングセットから記憶できない質問では、すべてのモデルのパフォーマンスが劇的に低下し、平均絶対パフォーマンス差は63です。これらの調査結果を使用して、さまざまな一般的なオープンドメインモデルを評価し、実際に一般化できる範囲、および全体的なパフォーマンスを促進する要因についての洞察をさらに深めます。 
[ABSTRACT]単一のテストセットのスコアは、モデルが実際に持っている機能の全体像を示していません。テストの60〜70％-時間の回答もトレーニングセットのどこかにあることがわかります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Injecting Prior Knowledge into Image Caption Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CL/paper_2.html">
      <font color="black">Injecting Prior Knowledge into Image Caption Generation</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、事前知識の2つのソースを組み込むことにより、最先端の画像キャプションモデルのパフォーマンスを改善することを提案します。（i）条件付き潜在トピックの注意。潜在変数（トピック）のセットを次のように使用します。非常に可能性の高い単語を生成するためのアンカー、および（ii）キャプションの構文的および意味的構造における誘導バイアスを活用し、画像キャプションモデルの一般化を改善する正則化手法。私たちの実験は、私たちの方法が人間の解釈可能なキャプションを生成し、さらに完全なデータ領域と低いデータ領域の両方でMSCOCOデータセットが大幅に改善されます。画像から自然言語の説明を自動的に生成することは、視覚信号とテキスト信号、およびそれらの間の相関関係を十分に理解する必要がある人工知能の課題です。 
[ABSTRACT]最先端の画像キャプションの手法は、特にデータが限られている場合に、人間レベルのパフォーマンスに近づくのに苦労しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-22">
        <br><font color="black">2019-11-22</font>
      </time>
    </span>
</section>
<!-- paper0: ConvBERT: Improving BERT with Span-based Dynamic Convolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CL/paper_3.html">
      <font color="black">ConvBERT: Improving BERT with Span-based Dynamic Convolution</font>
    </a>
  </h2>
  <font color="black">実験により、ConvBERTはさまざまなダウンストリームタスクでBERTとそのバリアントよりもはるかに優れており、トレーニングコストとモデルパラメーターが少なくなっています。コードと事前トレーニング済みモデルがリリースされます。BERTにこの混合注意設計を装備してConvBERTモデルを構築します。 
[要旨]私たちは自然なオランダオランダ語のオランダ人に混合注意設計を装備し、変換モデルを構築します。このモデルはグローバルな自己畳み込みブロックに基づいています。ただし、ニューヨークベースのモデルには86.4接着剤の合計容量があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Discovering and Categorising Language Biases in Reddit -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CL/paper_4.html">
      <font color="black">Discovering and Categorising Language Biases in Reddit</font>
    </a>
  </h2>
  <font color="black">Googleニュースのデータセットで見つかったバイアスを以前の文献で見つかったバイアスと比較することで、この方法の有効性を検証します。このペーパーでは、Redditのオンライン談話コミュニティの語彙にエンコードされた言語バイアスを自動的に発見するデータ駆動型アプローチを提案します。 。ディスカッションプラットフォームRedditでの言語バイアスを発見して分類するために、単語の埋め込みを使用したデータ駆動型のアプローチを紹介します。 
[ABSTRACT]人種差別、性差別、その他の差別の問題にますます関連している</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: DeText: A Deep Text Ranking Framework with BERT -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CL/paper_5.html">
      <font color="black">DeText: A Deep Text Ranking Framework with BERT</font>
    </a>
  </h2>
  <font color="black">3つの現実世界の検索システムでのDeTextのオフラインとオンラインの実験により、最先端のアプローチよりも大幅に改善されます。ただし、これは通常、各クエリワードと各ドキュメントワードを徹底的に相互作用させることによって行われます。これは、オンライン検索サービスシステムでは非効率的です。 .. BERTは、コンテキストエンベディングを学習する最も成功したモデルの1つであり、検索ランキングのために複雑なクエリとドキュメントの関係をキャプチャするために適用されています。 
[ABSTRACT]効果的なランキングシステムには、text.bertの深い理解が必要です。コンテキストの埋め込みを学習する最も成功したモデルの1つです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: CTC-synchronous Training for Monotonic Attention Model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CL/paper_6.html">
      <font color="black">CTC-synchronous Training for Monotonic Attention Model</font>
    </a>
  </h2>
  <font color="black">参照CTCアラインメントは、デコーダーと同じエンコーダーを共有するCTCブランチから抽出されます。TEDLIUMrelease-2およびLibrispeech corporaの実験的評価により、提案された方法は、特に長い発話の認識を大幅に改善することが示されています。また、CTC- STは、MoChAに対するSpecAugmentの潜在能力を最大限に引き出すことができます。 
[ABSTRACT] ctc-同期トレーニング（ctc-st）は、ctcアライメントを使用して最適なモノクロを学習します。モカからの予想される境界がアライメントと同期するように、モデル全体が共同で最適化されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-10">
        <br><font color="black">2020-05-10</font>
      </time>
    </span>
</section>
<!-- paper0: Listen Attentively, and Spell Once: Whole Sentence Generation via a
  Non-Autoregressive Architecture for Low-Latency Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CL/paper_7.html">
      <font color="black">Listen Attentively, and Spell Once: Whole Sentence Generation via a
  Non-Autoregressive Architecture for Low-Latency Speech Recognition</font>
    </a>
  </h2>
  <font color="black">この問題に対処するために、LASOと呼ばれる非自己回帰のエンドツーエンドの音声認識システムを提案します（注意深く聞き、一度だけスペルを入力します）。LASOは6.4％の文字エラー率を達成し、最新のパフォーマンスよりも優れています。アート自己回帰変圧器モデル（6.7％）。モデルは注意ベースのフィードフォワード構造に基づいているため、効率的に並列に計算を実装できます。 
[ABSTRACT] laso（注意深く聞いて、スペルを1回）と呼ばれるシステム。自動回帰でない音声認識システムを予測します。自己回帰音声認識を使用しない場合、ワンパス通信によりブラウザの時間が大幅に短縮されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-11">
        <br><font color="black">2020-05-11</font>
      </time>
    </span>
</section>
<!-- paper0: Data balancing for boosting performance of low-frequency classes in
  Spoken Language Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CL/paper_8.html">
      <font color="black">Data balancing for boosting performance of low-frequency classes in
  Spoken Language Understanding</font>
    </a>
  </h2>
  <font color="black">特に、SLUの既存のデータ分散手法の適用について説明し、インテント分類とスロット充填のためのマルチタスクSLUモデルを提案します。過剰適合を回避するために、モデルではデータ分散の方法を補助的に間接的に活用しています。クラスバランスのバッチジェネレーターと（場合によっては）合成データを使用するタスク。実際のデータセットでの結果は、i）提案されたモデルが低周波数インテントのパフォーマンスを大幅に向上させ、同時にパフォーマンスの低下を回避できることを示しています。ヘッドインテント、ii）合成データは、現実的なデータが利用できない場合に新しいインテントをブートストラップするのに役立ちますが、iii）一定量の現実的なデータが利用可能になると、補助タスクで合成データを使用すると、プライマリに追加するよりもパフォーマンスが向上しますタスクトレーニングデータ、およびiv）共同トレーニングシナリオで、インテントの分散を個別に分散することで、インテントの分類だけでなくalsも改善しますoスロット充填性能。 
[ABSTRACT]私たちの意図に加えて、slu.itのデータの不均衡の処理に関する新しい研究は、合成データの使用に関する最初の体系的な研究です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: FastLR: Non-Autoregressive Lipreading Model with Integrate-and-Fire -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CL/paper_9.html">
      <font color="black">FastLR: Non-Autoregressive Lipreading Model with Integrate-and-Fire</font>
    </a>
  </h2>
  <font color="black">したがって、このホワイトペーパーでは、FastLRとARモデル間のギャップを減らす3つの方法を紹介します。1）課題1と2に対処するために、統合射撃（I \＆F）モジュールを利用して、ソースビデオフレーム間の対応をモデル化します。と出力テキストシーケンス。私たちの実験は、FastLRがGRIDとLRS2のリピーディングで1.5 \％と5.5 \％のわずかなWER絶対増加を伴う最新のリピーディングモデルと比較して、最大10.97 $ \ times $のスピードアップを達成することを示しています。提案された方法の有効性をそれぞれ示すデータセット。NARリプリーディングは、多くの困難を伴う挑戦的なタスクです。1）ソースとターゲットの間のシーケンス長の不一致により、出力シーケンスの長さを推定することが困難になります。 2）NAR生成の条件付きで独立した動作には、時間の相関関係が欠けているため、ターゲット分布の近似が不十分です。 3）エンコーダーの機能表現能力は、効果的な位置合わせメカニズムがないために弱い場合があります。 4）AR言語モデルの削除は、読唇の固有のあいまいさの問題を悪化させます。 
[要約]読唇術の新しいメソッドは、主に自己回帰（ar）モデルに基づいて構築され、ターゲットトークンを1つずつ生成します。ar言語モデルの削除は、読唇術の固有のあいまいさの問題を悪化させます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Multi-Scale Aggregation Using Feature Pyramid Module for
  Robust Speaker Verification of Variable-Duration Utterances -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CL/paper_10.html">
      <font color="black">Improving Multi-Scale Aggregation Using Feature Pyramid Module for
  Robust Speaker Verification of Variable-Duration Utterances</font>
    </a>
  </h2>
  <font color="black">さまざまな時間スケールの豊富な話者情報を含む拡張機能を使用して、話者の埋め込みを抽出します。現在、話者検証に最も広く使用されているアプローチは、深層話者の埋め込み学習です。このモジュールは、複数の層からの機能の話者識別情報を強化します。トップダウン経路と横方向の接続。 
[ABSTRACT]たとえば、話者の特徴抽出の最後の層から話者の埋め込みを抽出します。これらには、msaの数から抽出されたマルチスケールの特徴が含まれています。任意の継続時間の発話を処理するロバスト性を高めるために、このペーパーが導入されました機能ピラミッドモジュールを使用してmsaを改善するには</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-07">
        <br><font color="black">2020-04-07</font>
      </time>
    </span>
</section>
<!-- paper0: Compositional Networks Enable Systematic Generalization for Grounded
  Language Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/cs.CL/paper_11.html">
      <font color="black">Compositional Networks Enable Systematic Generalization for Grounded
  Language Understanding</font>
    </a>
  </h2>
  <font color="black">人間は、これまでに出会ったことのない概念の組み合わせを含む新しい文章を理解するときに非常に柔軟です。私たちが採用する主要な原則は構成性です。ネットワークの構成構造は、それらが対処する問題ドメインの構成構造を反映し、他のすべての弱い監督でエンドツーエンドで学習するパラメーターとプロパティ。劇的な失敗や隅の原因がない堅牢な言語理解は、安全で公正なロボットを構築するために重要です。私たちは、その目標を達成する上で構成性が果たすことができる重要な役割を示しています。 
[ABSTRACT]私たちのベースネットワークは、先行技術と同じ最先端のパフォーマンス、97％の実行精度を備えていますが、同時に、先行研究がそうでない場合に知識を一般化しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: MIRNet: Learning multiple identities representations in overlapped
  speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_0.html">
      <font color="black">MIRNet: Learning multiple identities representations in overlapped
  speech</font>
    </a>
  </h2>
  <font color="black">トレーニングに参照音響機能を必要とする従来のアプローチとは異なり、提案されたアルゴリズムは重複する音声セグメントの話者識別ラベルのみを必要とします。この論文では、重複する音声から複数の話者識別を確実に抽出できる新しい深層スピーカー表現戦略を提案します..多くのアプローチは、音響パラメータの一貫した特性を認識することを学ぶことにより、スピーチから単一の話者のアイデンティティに関する情報を引き出すことができます。 
[要約]特定の混合物から各話者のアイデンティティに関する情報を含む高レベルの埋め込みを抽出できるネットワーク。学習するには、話者の確認と対象話者に条件付けされた音声分離システムが必要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-04">
        <br><font color="black">2020-08-04</font>
      </time>
    </span>
</section>
<!-- paper0: PPSpeech: Phrase based Parallel End-to-End TTS System -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_1.html">
      <font color="black">PPSpeech: Phrase based Parallel End-to-End TTS System</font>
    </a>
  </h2>
  <font color="black">ただし、これらは同時に新しい問題を引き起こします。PPSpeechは、フレーズ内で自己回帰アプローチを使用し、異なるフレーズに対して並列戦略を実行します。突然のスタイルや音色の変化。 
[要約] tacotregressiveシステム（ppspeech）は、ソーススピーチの品質に関して従来のパラレルアプローチよりも優れています。ただし、提案されたシステムを使用して、さまざまなタイプのスピーチを作成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: A fully recurrent feature extraction for single channel speech
  enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_2.html">
      <font color="black">A fully recurrent feature extraction for single channel speech
  enhancement</font>
    </a>
  </h2>
  <font color="black">この目的のために、CNNレイヤーを抽出する機能に反復係数を追加して、単一チャネルの音声強調のための堅牢なコンテキスト認識機能抽出戦略を導入します。抽出された機能でノイズ属性のローカル統計をキャプチャするのに堅牢であるため、提案されたモデルは、非常に騒々しい条件でも、音声キューの区別に非常に効果的です。目に見えないノイズ条件でバニラCNNモジュールを使用して強化モデルに対して評価すると、特徴抽出レイヤーに反復性がある推奨モデルにより、セグメントSNR（SSNR）ゲインが生成されます。最大1.5 dBで、最適化されるパラメータは25％削減されます。 
[ABSTRACT] cnnモジュールの特徴抽出能力は、ネットワークのノイズコンテキストを適切にモデル化できませんでした。新しいモデルは、非常に騒々しい状況でも、音声キューの区別に非常に効果的です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-09">
        <br><font color="black">2020-06-09</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Learning a Universal Non-Semantic Representation of Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_3.html">
      <font color="black">Towards Learning a Universal Non-Semantic Representation of Speech</font>
    </a>
  </h2>
  <font color="black">埋め込みは、公開されているデータセットでトレーニングされ、パーソナライゼーションタスクや医療ドメインなど、さまざまな低リソースのダウンストリームタスクでテストされます。提案された表現は、ベンチマークの他の表現よりも優れており、最先端の-いくつかの転移学習タスクのアートパフォーマンス。ベンチマーク、モデル、および評価コードが公開されています。 
[ABSTRACT]ベンチマーク、モデル、および評価コードが公開されています。提案された表現は他の表現よりも優れており、多くの転移学習タスクでの国家的パフォーマンスを超えています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br><font color="black">2020-02-25</font>
      </time>
    </span>
</section>
<!-- paper0: Improving on-device speaker verification using federated learning with
  privacy -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_4.html">
      <font color="black">Improving on-device speaker verification using federated learning with
  privacy</font>
    </a>
  </h2>
  <font color="black">補助モデルの知識は、マルチタスク学習を使用して話者検証システムに抽出され、この補助モデルによって予測されるサイド情報ラベルが追加タスクになります。ここで説明する補助モデルは、話者検証をトリガーするフレーズから抽出された機能を使用します。システムは、これらの機能から、サイド情報として有用と見なされる話者特性ラベルを予測します。 
[要約]モデルは、サイド情報として有用と見なされる話者特性ラベルを予測します。これらの機能は、話者の多数の母集団に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Compact Graph Architecture for Speech Emotion Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_5.html">
      <font color="black">Compact Graph Architecture for Speech Emotion Recognition</font>
    </a>
  </h2>
  <font color="black">人気のIEMOCAPデータベースで音声感情認識のモデルのパフォーマンスを評価しました。このモデルは、標準のGCNや、アプローチの有効性を示す他の関連するディープグラフアーキテクチャよりも優れています。既存の音声感情認識方法と比較すると、モデルは状態を達成します。最先端のパフォーマンス（4クラス、$ 65.29 \％$）で、学習可能なパラメーターが大幅に少なくなっています。 
[ABSTRACT] Googleのモデルは、標準のGCNおよびその他の関連するディープグラフアーキテクチャよりも優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-05">
        <br><font color="black">2020-08-05</font>
      </time>
    </span>
</section>
<!-- paper0: Mixing-Specific Data Augmentation Techniques for Improved Blind
  Violin/Piano Source Separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_6.html">
      <font color="black">Mixing-Specific Data Augmentation Techniques for Improved Blind
  Violin/Piano Source Separation</font>
    </a>
  </h2>
  <font color="black">このライトに続いて、このホワイトペーパーでは、現代の音楽制作ルーチンで採用されているより洗練されたミキシング設定、結合されるトラック間の関係、および沈黙の要因を考慮する拡張データ拡張方法についてさらに検討します。これらの新しいデータ拡張方法では、トレーニングデータの量の影響も調査します。私たちの評価は、提案された混合固有のデータ拡張方法が、特に、音源分離のための深層学習ベースのモデルのパフォーマンスを改善できることを示しています。小さなトレーニングデータの場合。 
[要約]提案された混合-特定のデータ拡張方法は、深層学習のパフォーマンスを向上させるのに役立ちます-ソース分離のためのベースのモデル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Aalto's End-to-End DNN systems for the INTERSPEECH 2020 Computational
  Paralinguistics Challenge -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_7.html">
      <font color="black">Aalto's End-to-End DNN systems for the INTERSPEECH 2020 Computational
  Paralinguistics Challenge</font>
    </a>
  </h2>
  <font color="black">これらの各タスクで、アンサンブルは単一のE2Eモデルよりも優れています。マスクサブチャレンジでは、フィーチャーエンジニアリングのないE2Eシステムを使用すると、フィーチャーエンジニアリングのベースラインと競合し、フィーチャーエンジニアリングのベースラインと組み合わせると大幅な向上が得られます。 -to-endニューラルネットワークモデル（E2E）は、さまざまなINTERSPEECH ComParEタスクでパフォーマンスの大幅な向上を示しています。 
[要約]以前の以前の作業で単一のe2eモデルまたは同じe2eモデルを適用しました。タスクでは、アンサンブルが単一のe2eモデルよりも優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Few-Shot Drum Transcription in Polyphonic Music -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_8.html">
      <font color="black">Few-Shot Drum Transcription in Polyphonic Music</font>
    </a>
  </h2>
  <font color="black">この作業では、タスクに少数ショット学習を導入することにより、オープンボキャブラリーADTに対処します。同時に、私たちのモデルは、トレーニング中に見られない、より細かいまたは拡張ボキャブラリーに正常に一般化できることを示します。推論時に選択されたほんの一握りの例を考えると、固定されたボキャブラリー設定の下で、最先端の監視付きADTアプローチに匹敵し、場合によってはそれを上回ることができることを示しています。 
[ABSTRACT]私たちのモデルは、より細かく一般化することができます-トレーニング中に見えない語彙または拡張語彙</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: HooliGAN: Robust, High Quality Neural Vocoding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_9.html">
      <font color="black">HooliGAN: Robust, High Quality Neural Vocoding</font>
    </a>
  </h2>
  <font color="black">次のデモページでサンプルを提供しています：https://resemble-ai.github.io/hooligan_demo/。また、HooliGANとのシームレスな統合を可能にするTacotronベースのモデルへの簡単な変更も示します。リスニングテストの結果は、提案されたモデルが大小さまざまなデータセットで高品質のオーディオを一貫して出力できることを示しています。 
[ABSTRACT]新しいモデルには、最新の結果が得られ、より小さなデータセットにうまく調整できる堅牢なボコーダーhooliganが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Simultaneous measurement of time-invariant linear and nonlinear, and
  random and extra responses using frequency domain variant of velvet noise -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_10.html">
      <font color="black">Simultaneous measurement of time-invariant linear and nonlinear, and
  random and extra responses using frequency domain variant of velvet noise</font>
    </a>
  </h2>
  <font color="black">オープンソースリポジトリで利用できるようにしました。2つおよび4つの直交シーケンスを使用する2つの有用なケースを紹介し、シミュレーションと音響測定の例を使用してそれらの使用法を示します。提案された応答分析方法は一般的で、次のような他の分野に適用されます。サウンドレコーディングとコーディングの聴覚フィードバック調査と評価。 
[ABSTRACT]提案された方法は、プログラムされた応答応答応答分析のセットを使用します。これは、余分な機器なしでそれを可能にする高度な設計自由度です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Shouted Speech Compensation for Speaker Verification Robust to Vocal
  Effort Conditions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_11.html">
      <font color="black">Shouted Speech Compensation for Speaker Verification Robust to Vocal
  Effort Conditions</font>
    </a>
  </h2>
  <font color="black">補正の前に、叫び声の状態はロジスティック回帰によって自動的に検出されます。実験結果は、音声努力の不一致がある場合に提案されたアプローチを適用すると、叫び声のない音声を適用するシステムに対して、最大13.8％のエラー率の相対的な改善が得られることを示しています。検出も補償も..プロセスは計算上軽く、x-ベクトルシステムのバックエンドで実行されます。 
[ABSTRACT]これらの補正技術は、自動音声認識の堅牢性の領域から借用されています。この作業では、話者検証での叫び声と通常の状態の間の不一致を補正するためにそれらを適用します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Self-Supervised Contrastive Learning for Unsupervised Phoneme
  Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_12.html">
      <font color="black">Self-Supervised Contrastive Learning for Unsupervised Phoneme
  Segmentation</font>
    </a>
  </h2>
  <font color="black">このように、提案されたモデルは完全に監視されていない方法でトレーニングされ、ターゲットの境界や音声の書き起こしの形での手動による注釈はありません。結果は、私たちのアプローチがベースラインモデルを上回り、両方のデータで最先端のパフォーマンスに到達することを示唆しています。セット..トレーニングフェーズでは見られなかった分布と言語（英語、ヘブライ語、ドイツ語）で得られたモデルを評価し、追加の未転写データを利用することがモデルのパフォーマンスに有益であることを示しました。 
[ABSTRACT]モデルは、生の波形を直接操作する畳み込みニューラルネットワークです。これは、モデルの出力に適用され、最終的な境界を生成するピーク検出アルゴリズムです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-27">
        <br><font color="black">2020-07-27</font>
      </time>
    </span>
</section>
<!-- paper0: Attentive Fusion Enhanced Audio-Visual Encoding for Transformer Based
  Robust Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_13.html">
      <font color="black">Attentive Fusion Enhanced Audio-Visual Encoding for Transformer Based
  Robust Speech Recognition</font>
    </a>
  </h2>
  <font color="black">各モダリティのエンコード後にオーディオビジュアルフュージョンが実行される以前のエンドツーエンドアプローチとは異なり、このホワイトペーパーでは、注意深いフュージョンブロックをエンコードプロセスに統合することを提案します。トランスベースのアーキテクチャに沿って、一方向または双方向のインタラクションを備えたマルチヘッド注意ベースのオーディオビジュアルフュージョンを使用して、埋め込みフュージョンブロックを実装します。提案された方法は、2つのストリームを十分に組み合わせ、オーディオモダリティへの過度の依存を弱めることができます。 
[要約]エンコーダモジュールで提案された方法は、オーディオを豊かにする-視覚的表現</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Robust Beam Search for Encoder-Decoder Attention Based Speech
  Recognition without Length Bias -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_14.html">
      <font color="black">Robust Beam Search for Encoder-Decoder Attention Based Speech
  Recognition without Length Bias</font>
    </a>
  </h2>
  <font color="black">ビームサイズが小さい場合でも非常に大きい場合でも、堅牢な意思決定と一貫して優れたパフォーマンスを提供します。LibriSpeechコーパスの実験的検証では、提案されたアプローチがヒューリスティックスや追加の調整作業なしで長さバイアスの問題を解決することを示しています。ビームプルーニングでは、得られた最終的な確率によりロバストなモデル変更が行われ、異なる長さの出力シーケンス間の信頼できる比較が可能になります。 
[要約]ヒューリスティックベースの検索が問題を緩和するために適用されました。これらのほとんどはヒューリスティックベースであり、かなりの調整が必要です。提案されたアプローチは、ヒューリスティックなしで長さバイアスの問題を解決します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br><font color="black">2020-05-19</font>
      </time>
    </span>
</section>
<!-- paper0: Robust Estimation of Hypernasality in Dysarthria with Acoustic Model
  Likelihood Features -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_15.html">
      <font color="black">Robust Estimation of Hypernasality in Dysarthria with Acoustic Model
  Likelihood Features</font>
    </a>
  </h2>
  <font color="black">これらの音響モデルから派生した機能が高鼻腔音声に固有であることを示すために、異なる構音障害コーパス全体でそれらを評価します。機能は、健康な音声の大規模コーパスでトレーニングされた2つの音響モデルに基づいています。機械学習に基づくメトリックは、それらが訓練されている小さな疾患固有の音声データセットに過剰適合しがちですが、超鼻音に関連する複雑な音響パターン。 
[ABSTRACT] hypernasalityは、低周波数に追加の共鳴を導入します。hypernasalに基づいて、鼻腔から空気が漏れるため、調音の精度が低下します。これらの機能は、1つの疾患からの高鼻腔の高鼻声のトレーニングでも一般化します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-26">
        <br><font color="black">2019-11-26</font>
      </time>
    </span>
</section>
<!-- paper0: Memory Controlled Sequential Self Attention for Sound Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_16.html">
      <font color="black">Memory Controlled Sequential Self Attention for Sound Recognition</font>
    </a>
  </h2>
  <font color="black">提案されたアイデアをマルチヘッドセルフアテンションメカニズムで拡張します。各アテンションヘッドは、明示的なアテンション幅の値を使用してオーディオの埋め込みを処理します。メモリ制御セルフアテンションモデルは、URBANでイベントベースのFスコア33.92％を達成することを示します。 -SEDデータセット、自己注意なしでモデルによって報告されたFスコア20.10％を上回る。メモリ制御順次自己注意の提案された使用は、サウンドイベントトークンのフレーム間の関係を誘導する方法を提供します。 
[要約]ポリフォニックサウンドイベント検出（sed）のために、畳み込みリカレントニューラルネットワーク（crnn）モデルに加えて、メモリ制御の自己注意メカニズムを使用することを提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-13">
        <br><font color="black">2020-05-13</font>
      </time>
    </span>
</section>
<!-- paper0: Listen Attentively, and Spell Once: Whole Sentence Generation via a
  Non-Autoregressive Architecture for Low-Latency Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_17.html">
      <font color="black">Listen Attentively, and Spell Once: Whole Sentence Generation via a
  Non-Autoregressive Architecture for Low-Latency Speech Recognition</font>
    </a>
  </h2>
  <font color="black">LASOは、6.4％の文字エラー率を達成します。これは、最先端の自己回帰トランスフォーマーモデル（6.7％）よりも優れています。注意ベースのエンドツーエンドモデルは、音声認識で有望なパフォーマンスを達成していますが、マルチパスビーム検索での前方計算は、推論の時間コストを増加させ、実際のアプリケーションを制限します。非自己回帰特性のため、LASOは、他のトークンに依存することなく、シーケンス内のテキストトークンを予測します。 
[ABSTRACT] laso（注意深く聞いて、スペルを1回）と呼ばれるシステム。自動回帰でない音声認識システムを予測します。自己回帰音声認識を使用しない場合、ワンパス通信によりブラウザの時間が大幅に短縮されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-11">
        <br><font color="black">2020-05-11</font>
      </time>
    </span>
</section>
<!-- paper0: Data balancing for boosting performance of low-frequency classes in
  Spoken Language Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_18.html">
      <font color="black">Data balancing for boosting performance of low-frequency classes in
  Spoken Language Understanding</font>
    </a>
  </h2>
  <font color="black">現実世界のデータセットの結果は、i）提案されたモデルが低頻度インテントのパフォーマンスを大幅に向上させ、ヘッドインテントの潜在的なパフォーマンス低下を回避できることを示しています。ii）合成データは、現実的なデータが利用できない場合に新しいインテントをブートストラップするのに役立ちます。 、ただしiii）一定量の現実的なデータが利用可能になると、補助タスクで合成データを使用しても、プライマリタスクのトレーニングデータに追加するよりもパフォーマンスが向上します。iv）共同トレーニングシナリオでは、インテントの分散のバランスが個別に改善されます意図的分類だけでなく、スロット充填パフォーマンスも含まれます。特に、SLUの既存のデータバランシングテクニックの適用について説明し、意図的分類およびスロット充填のためのマルチタスクSLUモデルを提案します。過剰適合を避けるために、データバランシングのモデルメソッドは、クラスバランスのバッチジェネレーターを使用する補助タスクを介して間接的に活用され、 （おそらく）合成データ。 
[ABSTRACT]私たちの意図に加えて、slu.itのデータの不均衡の処理に関する新しい研究は、合成データの使用に関する最初の体系的な研究です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: FastLR: Non-Autoregressive Lipreading Model with Integrate-and-Fire -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_19.html">
      <font color="black">FastLR: Non-Autoregressive Lipreading Model with Integrate-and-Fire</font>
    </a>
  </h2>
  <font color="black">私たちの実験は、FastLRが10.97 $ \ times $までのスピードアップを達成し、GRIDおよびLRS2リピーディングデータセットでWER絶対増加が1.5 \％および5.5 \％とわずかに増加していることを示しています。したがって、このホワイトペーパーでは、FastLRとARモデル間のギャップを減らすための3つの方法を紹介します。1）課題1と2に対処するために、統合射撃（I \＆F）モジュールを活用ソースビデオフレームと出力テキストシーケンス間の対応をモデル化します。この制約を突破するために、すべてのターゲットトークンを同時に生成する非自己回帰（NAR）リピーディングモデルであるFastLRを提案します。 
[要約]読唇術の新しいメソッドは、主に自己回帰（ar）モデルに基づいて構築され、ターゲットトークンを1つずつ生成します。ar言語モデルの削除は、読唇術の固有のあいまいさの問題を悪化させます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Quantification of Transducer Misalignment in Ultrasound Tongue Imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_20.html">
      <font color="black">Quantification of Transducer Misalignment in Ultrasound Tongue Imaging</font>
    </a>
  </h2>
  <font color="black">ただし、超音波舌イメージングの長期にわたる問題は、より長いデータ記録セッション中のトランスデューサーの位置ずれです。ハンガリー語とスコットランド語の英語の子データセットで広範な実験が行われます。分析では、MSE距離と2つの類似性測定基準を使用して、相対顎とトランスデューサーの間の変位。 
[アブストラクト]超音波は手頃な価格で非侵襲的な画像モダリティです。このホワイトペーパーでは、シンプルでありながら効果的なミスアライメントの定量化アプローチを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Sound field reconstruction in rooms: inpainting meets super-resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_21.html">
      <font color="black">Sound field reconstruction in rooms: inpainting meets super-resolution</font>
    </a>
  </h2>
  <font color="black">実際のリスニングルームでの実験的検証とともにシミュレーションデータを使用した実験が示されています。この方法は、グリーン関数の数値シミュレーションから構築された、シミュレーションデータのみでトレーニングされた部分畳み込みを持つUネットのようなニューラルネットワークに基づいています。何千もの一般的な長方形の部屋にまたがる。この論文では、音場再構成のための深層学習ベースの方法が提案されている。 
[ABSTRACT]このメソッドは、任意に配置された非常に少数のマイクを使用します。部屋全体の音圧を再構築することが可能です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-30">
        <br><font color="black">2020-01-30</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Multi-Scale Aggregation Using Feature Pyramid Module for
  Robust Speaker Verification of Variable-Duration Utterances -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_22.html">
      <font color="black">Improving Multi-Scale Aggregation Using Feature Pyramid Module for
  Robust Speaker Verification of Variable-Duration Utterances</font>
    </a>
  </h2>
  <font color="black">さまざまな時間スケールの豊富な話者情報を含む拡張機能を使用して、話者の埋め込みを抽出します。モジュールは、トップダウンの経路と横方向の接続を介して複数のレイヤーからの機能の話者識別情報を拡張します。VoxCelebデータセットの実験では、提案されたモジュールは、より少ない数のパラメーターで以前のMSAメソッドを改善します。 
[ABSTRACT]たとえば、話者の特徴抽出の最後の層から話者の埋め込みを抽出します。これらには、msaの数から抽出されたマルチスケールの特徴が含まれています。任意の継続時間の発話を処理するロバスト性を高めるために、このペーパーが導入されました機能ピラミッドモジュールを使用してmsaを改善するには</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-07">
        <br><font color="black">2020-04-07</font>
      </time>
    </span>
</section>
<!-- paper0: Spectral-change enhancement with prior SNR for the hearing impaired -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-07/eess.AS/paper_23.html">
      <font color="black">Spectral-change enhancement with prior SNR for the hearing impaired</font>
    </a>
  </h2>
  <font color="black">SCE-eSNRアルゴリズムは、高SMRでのSSNマスカーと低SMRでのSTSマスカーの改善されたSI、およびSTSマスカーの優れた自然性と音声品質を示しました。次に、実際の混合から得られた推定SNRが使用され、 SCE-eSNR ..結果は、SCE-iSNRアルゴリズムが、高い信号対マスカー比（SMR）のマスカーと低いSMRのSTSマスカーの両方でSIを大幅に改善する一方で、音声品質への処理効果は小さかったことを示しています。 
[要約]結果は、sce-hi hiアルゴリズムが両方のmaskers.siでsiを大幅に改善したことを示し、自然性と音声品質に関する主観的評価が7 hi被験者でテストされました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
