<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-06-24の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Unsupervised Sound Separation Using Mixtures of Mixtures -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/cs.SD/paper_0.html">
      Unsupervised Sound Separation Using Mixtures of Mixtures
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      特に、残響混合を組み込むことで残響スピーチ分離性能を大幅に改善し、ノイズの多い混合から音声強調システムをトレーニングし、大量の野生データを組み込むことでユニバーサルサウンド分離を改善します。この合成トレーニングデータへの依存優れたパフォーマンスは、特に音響条件と音源の分布に関して、トレーニングデータと実際のオーディオとの一致の度合いに依存するため、問題があります。このホワイトペーパーでは、完全に教師なしの方法である混合不変トレーニング（MixIT ）、それは単一チャンネルの音響混合のみを必要とします。 
[ABSTRACT]合成混合は、孤立したグラウンド-トゥルースソースを加算することによって作成されます。合成混合は、正確にシミュレートすることが困難な場合があり、サウンドタイプの分布を再現するのが難しい場合があります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-23">
        <br>2020-06-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: CLC: Complex Linear Coding for the DNS 2020 Challenge -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/cs.SD/paper_1.html">
      CLC: Complex Linear Coding for the DNS 2020 Challenge
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この作業では、CLCをDeep Noise Suppression（DNS）チャレンジに適用し、従来のマスクベースの処理の代替としてCLCを提案します。提供されたテストセットと、ベースラインで使用される実際の定常および非定常ノイズを含む追加の検証セットを使用して、モデルを評価しました。 
[要約]ノイズ低減プロセスは、時間-周波数マスクに基づいています。通常、ノイズの多いスペクトログラムに適用される代替マスクに基づいています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-23">
        <br>2020-06-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Further Study of Unsupervised Pre-training for Transformer Based
  Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/cs.SD/paper_2.html">
      A Further Study of Unsupervised Pre-training for Transformer Based
  Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらの方法の中で、マスク予測コーディングは、BERTのようなマスク再構成損失とトランスフォーマーバックボーンを備えたさまざまな音声認識データセットで大幅な改善を実現しました。優れた音声認識システムを構築するには、通常、大量の文字起こしデータが必要であり、収集に費用がかかります。論文では、MPCについてさらに調査を行い、3つの重要な側面に焦点を当てています。事前トレーニングデータスピーキングスタイルの効果、ストリーミングモデルへの拡張、および事前トレーニングステージからダウンストリームタスクに学習した知識をより適切に転送する方法。 
[ABSTRACT]多くの教師なし事前トレーニング方法が提案されています。これらには、これらのタイプの音声ベースの音声認識と音声認識が含まれます。ただし、mpcの一部の側面は完全には調査されていません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-20">
        <br>2020-05-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multi-channel U-Net for Music Source Separation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/cs.SD/paper_3.html">
      Multi-channel U-Net for Music Source Separation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      マルチタスク損失の2つの重み付け戦略を調査します。1）動的加重平均（DWA）、および2）エネルギーベースの重み付け（EBW）です。DWAは、トレーニング中に各タスクの損失の変化率を追跡することによって重みを決定します。 。EBWは、混合された各ソースのエネルギーレベルの違いから生じるトレーニングバイアスの影響を中和することを目的としています。 
[要約] c-uの代替として、加重マルチタスク損失を使用してトレーニングされたマルチチャネルu-netを提案します。 based.two-foldの利点：条件付けなしのエポックごとの有効な単一モデルモデルの機能強化が少なく、厳密にトレーニング可能なネットワークが少ない
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-23">
        <br>2020-03-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Real Time Speech Enhancement in the Waveform Domain -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/cs.SD/paper_4.html">
      Real Time Speech Enhancement in the Waveform Domain
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      提案されたモデルは、生の波形を直接処理しながら、因果的および非因果的方法の両方の最先端のパフォーマンスと一致します。その一般化能力..私たちは、客観的な指標と人間の判断の両方を使用して、いくつかの標準的なベンチマークの評価を行います。 
[ABSTRACT]提案されたモデルは、skip-connectionsを備えたエンコーダー/デコーダーアーキテクチャに基づいています。定常および非定常ノイズを含むさまざまな種類のバックグラウンドノイズ、およびルームリバーブを削除できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-23">
        <br>2020-06-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Lightweight Online Noise Reduction on Embedded Devices using
  Hierarchical Recurrent Neural Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/cs.SD/paper_5.html">
      Lightweight Online Noise Reduction on Embedded Devices using
  Hierarchical Recurrent Neural Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      階層型リカレントニューラルネットワークを使用すると、階層接続を介して時間的コンテキストを含めながら、レイヤーあたりのニューロン数を大幅に減らすことができます。これらは、多くのパラメーターと計算能力を必要とするため、最新のCPUを使用した場合にのみ実行できます。これにより、前の作業と比較してノイズ低減の品質を維持しながら、パラメーターと浮動小数点演算（FLOP）の最小数に向けてモデルを最適化する。 
[ABSTRACT]これは現在、最新の方法では使用できません。これらには多くのレイテンシと最新のプロセッサが必要です。これらは、オンライン処理には適していません。このため、フィルタバンクとアルゴリズム自体による低レイテンシなどの制約が必要です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-23">
        <br>2020-06-23
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: NLPContributions: An Annotation Scheme for Machine Reading of Scholarly
  Contributions in Natural Language Processing Literature -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/cs.CL/paper_0.html">
      NLPContributions: An Annotation Scheme for Machine Reading of Scholarly
  Contributions in Natural Language Processing Literature
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらの情報単位に基づいて開発したアノテーションスキームは、NLPContributionsと呼ばれます。この演習を通じて、アノテーション手法を取得しました。そして、NLP-ML学術研究の貢献を反映する8つのコア情報ユニットを発見しました。自然言語処理（NLP）記事、特に機械学習（ML）を論じる記事における学術貢献をキャプチャするための注釈イニシアチブについて説明しますさまざまな情報抽出タスクのアプローチ。 
[要約]パイロットアノテーションタスクは、50 nlp-mlの文学記事に対するパイロットアノテーション演習に基づいています。目的は、意味構造化のための主語-述語-オブジェクトステートメントの体系的なパターンセットを見つけることです。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-23">
        <br>2020-06-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Combining Neural Language Models for WordSense Induction -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/cs.CL/paper_1.html">
      Combining Neural Language Models for WordSense Induction
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちのアプローチは、新しい最先端のレベルを確立し、2つのRUSSE 2018データセットでロシア語のWSIの現在の最良の結果を大幅に改善しました。次に、あいまいな単語すべてに対してクラスターの数を固定する代わりに、手法を提案します。各単語のクラスターの数を個別に選択するため。この作業では、このアプローチをロシア語に適用し、2つの方法で改善します。 
[ABSTRACT]このタスクへの新しいアプローチが提案されました。これは、特定のコンテキストで曖昧な単語の可能な代替を生成します。代わりに、左と右のコンテキストを組み合わせる方法を提案し、より良い代替案を生成します。単語は2つのロシア語に基づいています2018データセット
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-23">
        <br>2020-06-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Distill, Adapt, Distill: Training Small, In-Domain Models for Neural
  Machine Translation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/cs.CL/paper_2.html">
      Distill, Adapt, Distill: Training Small, In-Domain Models for Neural
  Machine Translation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      機械翻訳の大規模な経験的結果（それぞれ3つのドメインを持つ3つの言語ペア）では、最適なパフォーマンスを得るために2回蒸留することをお勧めします。ドメイン適応設定でシーケンスレベルの知識抽出を使用して、メモリ効率の高い小さな機械翻訳モデルをトレーニングします。ドメイン適応と知識抽出の両方が広く使用されていますが、それらの相互作用はほとんど理解されていません。 
[ABSTRACT]収益化の手法と知識の抽出はよく知られていますが、データの理解はまだほとんどありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-05">
        <br>2020-03-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Further Study of Unsupervised Pre-training for Transformer Based
  Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/cs.CL/paper_3.html">
      A Further Study of Unsupervised Pre-training for Transformer Based
  Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このホワイトペーパーでは、MPCについてさらに調査し、3つの重要な側面に焦点を当てます。事前トレーニングデータスピーキングスタイルの効果、ストリーミングモデルへの拡張、および事前トレーニングステージからダウンストリームタスクに学習した知識をより適切に転送する方法。 。優れた音声認識システムを構築するには、通常、大量の書き起こされたデータが必要であり、収集にはコストがかかります。また、ターゲットデータの適応と層ごとの識別トレーニングの組み合わせにより、MPCの知識の伝達が促進され、3.99％の相対エラーが削減されました。強いベースラインを超えるAISHELL。 
[ABSTRACT]多くの教師なし事前トレーニング方法が提案されています。これらには、これらのタイプの音声ベースの音声認識と音声認識が含まれます。ただし、mpcの一部の側面は完全には調査されていません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-20">
        <br>2020-05-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Inductive Unsupervised Domain Adaptation for Few-Shot Classification via
  Clustering -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/cs.CL/paper_4.html">
      Inductive Unsupervised Domain Adaptation for Few-Shot Classification via
  Clustering
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      高品質の疑似ラベルを導出するために、類似性エントロピー最小化と敵対的分布調整を介してターゲットドメインのより優れた機能を学習するためのクラスタリングプロモーションメカニズムを提案します。ターゲットドメインからラベルなしデータの機能を導き出し（テストデータは不要）、それらをクラスターマイナーでグループ化します。実験はFewRel 2.0データセットで実行されます。 
[要約]このホワイトペーパーでは、帰納的フレームワークdafecを導入してこの問題に取り組み、クラスタリングによる少数ショット分類のパフォーマンスを向上させました。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-23">
        <br>2020-06-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Automatic Validation of Textual Attribute Values in E-commerce Catalog
  by Learning with Limited Labeled Data -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/cs.CL/paper_5.html">
      Automatic Validation of Textual Attribute Values in E-commerce Catalog
  by Learning with Limited Labeled Data
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      通常、個々の小売業者はこれらの重要な値を自己報告するため、カタログ情報にノイズのある事実が不可避に含まれます。既存のディープニューラルネットワークモデルは、2つのテキスト間のクロスチェックの実行に成功を示していますが、その成功は大きなこの検証タスクでは取得が困難な品質ラベル付きデータのセット：製品はさまざまなカテゴリにまたがっています。前述の課題に対処するため、MetaBridgeと呼ばれる新しいメタ学習潜在変数アプローチを提案します。ラベル付きデータが限られているカテゴリのサブセットであり、ラベルなしのデータがある未確認のカテゴリの不確実性をキャプチャします。 
[ABSTRACT]これらのタイプの短いテキストの概念は、いくつかのモデルに統合する必要があります。これらには、Tシャツ、Tシャツ、Tシャツなどが含まれます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-15">
        <br>2020-06-15
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Recent Advances and Challenges in Task-oriented Dialog System -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/cs.CL/paper_6.html">
      Recent Advances and Challenges in Task-oriented Dialog System
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      また、タスク指向のダイアログシステムの3つの重要なトピックについても説明します。（1）低リソース設定でのダイアログモデリングを容易にするためのデータ効率の向上、（2）ダイアログポリシー学習のマルチターンダイナミクスのモデリングによるタスク完了パフォーマンスの向上、および（3）ドメインオントロジーの知識を対話モデルに統合する。人間とコンピュータの相互作用と自然言語処理における重要性と価値により、タスク指向の対話システムは、学術界と産業界の両方でますます注目を集めています。この調査は、不完全ではありますが、タスク指向の対話システムの将来の研究に光を当てることができます。 
[ABSTRACT]私たちは、タスクの最近の進歩と課題を調査します-ダイアログシステムを促進します。また、ダイアログのパフォーマンスの最近の進捗状況も確認します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-17">
        <br>2020-03-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Keyframe Segmentation and Positional Encoding for Video-guided Machine
  Translation Challenge 2020 -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/cs.CL/paper_7.html">
      Keyframe Segmentation and Positional Encoding for Video-guided Machine
  Translation Challenge 2020
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このシステムは、キーフレームベースのビデオ特徴抽出とビデオ特徴位置エンコーディングを採用しています。ビデオとテキストの両方を具体的に使用することにより高品質のテキスト翻訳を生成することを目的としたマルチモーダルニューラル機械翻訳タスクの1つとして、ビデオ誘導機械翻訳。仕事では、ビデオガイド機械翻訳チャレンジ2020への取り組みにおけるビデオガイド機械翻訳システムを紹介しました。
[要約]ビデオタン機械翻訳チャレンジ2020への取り組みにおけるビデオガイド機械翻訳システムを紹介しました。このシステムは36を採用しています。 60コーパス-レベルブルー-4、ビデオで1位を達成
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-23">
        <br>2020-06-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: SEEK: Segmented Embedding of Knowledge Graphs -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/cs.CL/paper_8.html">
      SEEK: Segmented Embedding of Knowledge Graphs
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ソースコードとデータは\ url {https://github.com/Wentao-Xu/SEEK}にあります。さらに、パブリックベンチマークでの大規模な実験により、フレームワークの効率と効果が実証されています。スコアリング関数の一般的でエレガントなデザインである私たちのフレームワークは、多くの有名な既存のメソッドを特別なケースとして組み込むことができます。 
[ABSTRACT]ナレッジグラフの埋め込みのための既存の方法では、モデルの複雑さとモデルの表現力の間でgitmoのトレードオフを作成できません。新しいフレームワークはスコアリング関数の設計に焦点を当て、2つの重要な特性を強調します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-02">
        <br>2020-05-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Domain Adaptation for Semantic Parsing -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/cs.CL/paper_9.html">
      Domain Adaptation for Semantic Parsing
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      細かい段階では、モデルはドメイン関連の詳細に集中するようにガイドされます。ベンチマークデータセットでの実験は、この方法がいくつかの一般的なドメイン適応戦略よりも常に優れていることを示しています。さらに、モデルが限られたターゲットデータをうまく活用して、ターゲットドメインのトレーニングインスタンスがはるかに少ない場合でも、ソースドメインとターゲットドメインの違い。 
[ABSTRACT]私たちのセマンティックパーサーは、2段階の緩やかに微調整されたフレームワークから恩恵を受けます。細かい段階では、モデルはドメイン関連の詳細に集中するようにガイドされます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-23">
        <br>2020-06-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Can you tell? SSNet -- a Sagittal Stratum-inspired Neural Network
  Framework for Sentiment Analysis -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/cs.CL/paper_10.html">
      Can you tell? SSNet -- a Sagittal Stratum-inspired Neural Network
  Framework for Sentiment Analysis
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      代表的なベンチマークデータセットの実験結果と他の方法との比較1は、新しいネットワークアーキテクチャの利点を示しています。この生物学的形成を、同じテキストの異なるモデルの予測を組み合わせて堅牢で正確なニューラルネットワークアーキテクチャを設計するためのインスピレーションとして使用します。感情分析のための計算効率の高い分類子。人々がニュアンスのある言語を理解しようとするとき、彼らは通常、複数の入力センサーモダリティを処理してこの認知タスクを完了します。 
[要約]人間の脳には、矢状層と呼ばれる特殊なニューロン形成さえあり、皮肉を理解するのに役立ちます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-23">
        <br>2020-06-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving Query Safety at Pinterest -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/cs.CL/paper_11.html">
      Improving Query Safety at Pinterest
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験は、薬物関連クエリのドメインで、PinSetsが20個のシードクエリを99 \％を超える精度で15,670のポジティブトレーニングの例に拡張することを示しています。シンプルでありながら強力なメカニズムを適用するクエリセット拡張のシステムであるPinSetsを提示します。ユーザーセッションを検索し、小さなシードセットを数千の関連クエリにほぼ完全な精度で拡張し、最後まで理解しやすくします。ただし、解釈が容易な説明があります。しかし、これらを特定することは簡単ではありません。大きな語彙、社会グループ固有の俗語やタイプミス、そして用語の不適切さは文脈に依存するため。 
[ABSTRACT]これは、ユーザーが特定の特定のクエリにアクセスできないためです。これらは、不適切な検索候補からユーザーを保護するために必要です。ピンセットのシードベースの拡張は、解釈の組み合わせに基づいています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-20">
        <br>2020-06-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Unsupervised Evaluation of Interactive Dialog with DialoGPT -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/cs.CL/paper_12.html">
      Unsupervised Evaluation of Interactive Dialog with DialoGPT
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      FEDメトリックは、（1）グラウンドトゥルースレスポンスに依存しない、（2）トレーニングデータを必要としない、（3）ターンとダイアログ全体の両方のレベルできめ細かいダイアログ品質を測定します。このペーパーでは、FEDメトリックを紹介します。 （ダイアログのきめの細かい評価）、DialoGPTを使用する自動評価指標。微調整や監視は必要ありません。オープンドメインダイアログの研究では、意味のある解釈可能な自動評価指標を定義することが重要です。 
[ABSTRACT]標準言語生成メトリックは効果がないことが示されています。メトリックはダイアログでは効果的ではありませんが、ダイアログは簡単ではありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-23">
        <br>2020-06-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Train Large, Then Compress: Rethinking Model Size for Efficient Training
  and Inference of Transformers -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/cs.CL/paper_13.html">
      Train Large, Then Compress: Rethinking Model Size for Efficient Training
  and Inference of Transformers
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ただし、大きいモデルは、小さいモデルよりも量子化やプルーニングなどの圧縮手法に対してロバストであることを示しています。したがって、最も計算効率の高いトレーニング戦略は、非常に大きいモデルを直感的にトレーニングし、少数の反復後に停止することです。これはは、大きなTransformerモデルのトレーニング効率と小さなTransformerモデルの推論効率の間の明らかなトレードオフにつながります。 
[ABSTRACT]この設定でのモデルサイズの影響を調査し、計算によって制限されるnlpタスクのトランスフォーマーモデルに焦点を当てます。自己監視型事前トレーニングと高リソース機械翻訳。この加速は、通常、より大きなものを使用する追加の計算を上回りますモデル
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-26">
        <br>2020-02-26
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Unsupervised Sound Separation Using Mixtures of Mixtures -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/eess.AS/paper_0.html">
      Unsupervised Sound Separation Using Mixtures of Mixtures
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      特に、残響混合を組み込むことで残響スピーチ分離性能を大幅に改善し、ノイズの多い混合物から音声強調システムをトレーニングし、大量の野生データを組み込むことでユニバーサルサウンド分離を改善します。MixITでは、トレーニング例が構築されています。既存の混合物を混合することにより、モデルはそれらを可変数の潜在的なソースに分離し、分離されたソースを再混合して元の混合物に近づけることができます。この論文では、完全に教師なしの方法である混合不変トレーニング（MixIT ）、それは単一チャンネルの音響混合のみを必要とします。 
[ABSTRACT]合成混合は、孤立したグラウンド-トゥルースソースを加算することによって作成されます。合成混合は、正確にシミュレートすることが困難な場合があり、サウンドタイプの分布を再現するのが難しい場合があります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-23">
        <br>2020-06-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: CLC: Complex Linear Coding for the DNS 2020 Challenge -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/eess.AS/paper_1.html">
      CLC: Complex Linear Coding for the DNS 2020 Challenge
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この作業では、CLCをDeep Noise Suppression（DNS）チャレンジに適用し、従来のマスクベースの処理の代替としてCLCを提案します。ベースラインによって使用されます。実際、線形結合は、周波数帯域内のスペクトルのような準定常特性をモデル化することを可能にします。 
[要約]ノイズ低減プロセスは、時間-周波数マスクに基づいています。通常、ノイズの多いスペクトログラムに適用される代替マスクに基づいています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-23">
        <br>2020-06-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Further Study of Unsupervised Pre-training for Transformer Based
  Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/eess.AS/paper_2.html">
      A Further Study of Unsupervised Pre-training for Transformer Based
  Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このホワイトペーパーでは、MPCについてさらに調査し、3つの重要な側面に焦点を当てます。事前トレーニングデータの発話スタイルの影響、ストリーミングモデルへの拡張、および事前トレーニングステージからダウンストリームタスクに学習した知識をより適切に転送する方法です。 。ただし、MPCの多くの側面は完全には調査されていません。これらの方法の中で、マスク予測予測コーディングは、BERTのようなマスク再構築損失とトランスバックボーンを備えたさまざまな音声認識データセットで大幅な改善を実現しました。 
[ABSTRACT]多くの教師なし事前トレーニング方法が提案されています。これらには、これらのタイプの音声ベースの音声認識と音声認識が含まれます。ただし、mpcの一部の側面は完全には調査されていません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-20">
        <br>2020-05-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Real Time Speech Enhancement in the Waveform Domain -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/eess.AS/paper_3.html">
      Real Time Speech Enhancement in the Waveform Domain
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      経験的証拠は、定常および非定常ノイズを含むさまざまな種類のバックグラウンドノイズ、および室内の残響を除去できることを示しています。複数の損失関数を使用して、時間ドメインと周波数ドメインの両方で最適化されています。提案されたモデルは、スキップ接続のあるエンコーダー/デコーダーアーキテクチャ。 
[ABSTRACT]提案されたモデルは、skip-connectionsを備えたエンコーダー/デコーダーアーキテクチャに基づいています。定常および非定常ノイズを含むさまざまな種類のバックグラウンドノイズ、およびルームリバーブを削除できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-23">
        <br>2020-06-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Lightweight Online Noise Reduction on Embedded Devices using
  Hierarchical Recurrent Neural Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/eess.AS/paper_4.html">
      Lightweight Online Noise Reduction on Embedded Devices using
  Hierarchical Recurrent Neural Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      EUROMと実際のノイズデータベースを組み合わせてモデルを評価し、目に見えないノイズに関する客観的な指標を報告します。これにより、ノイズを維持しながら、モデルを最小数のパラメーターと浮動小数点演算（FLOP）に向けて最適化できます。以前の作業と比較して品質が低下します。階層型リカレントニューラルネットワークを使用すると、階層接続を介した時間的コンテキストを含めながら、レイヤーあたりのニューロン数を大幅に削減できます。 
[ABSTRACT]これは現在、最新の方法では使用できません。これらには多くのレイテンシと最新のプロセッサが必要です。これらは、オンライン処理には適していません。このため、フィルタバンクとアルゴリズム自体による低レイテンシなどの制約が必要です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-23">
        <br>2020-06-23
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: EXERCISE-INDUCED BENEFITS ON GLUCOSE HANDLING IN A MODEL OF DIET-INDUCED OBESITY ARE REDUCED BY CONCURRENT NICOTINAMIDE MONONUCLEOTIDE -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-24/biorxiv.physiology/paper_0.html">
      EXERCISE-INDUCED BENEFITS ON GLUCOSE HANDLING IN A MODEL OF DIET-INDUCED OBESITY ARE REDUCED BY CONCURRENT NICOTINAMIDE MONONUCLEOTIDE
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      方法：5週齢の雌C57BL / 6Jマウスを対照食またはHFDに曝露しました。HFD誘発性肥満の代謝異常に対するNMNとトレッドミル運動の併用効果を調べました。HFDを与えられたマウスは、飲酒中にNMNで処理されました水（400mg / kg; HNMN）、トレッドミル運動（HEx）またはNMNと運動の組み合わせ（HNEx）。 
[要約]運動は肥満への有益な介入であり、一部はミトコンドリア活性の増加が原因であり、ニコチンアミドアデニンジヌクレオの同時増加の潜在的な役割を伴います。hfdを与えられたマウスは、飲料水中の分子で処理されました（400mg / kg）、トレッドミル運動（hnmn）と運動（hnex）
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-23">
        <br>2020-06-23
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
