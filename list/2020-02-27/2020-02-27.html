<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-02-27の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: A Density Ratio Approach to Language Model Fusion in End-To-End
  Automatic Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/cs.SD/paper_0.html">
      A Density Ratio Approach to Language Model Fusion in End-To-End
  Automatic Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      密度比法は、LMおよびエンドツーエンドのASR統合であるShallow Fusionへの支配的なアプローチよりも常に優れていることがわかりました。提案されたアプローチは、クロスドメインおよび限定データシナリオで評価されます。このシナリオでは、LMトレーニングにかなりの量のターゲットドメインテキストデータが使用されますが、{audio、transcript}トレーニングは限られています（またはありません）データペアはRNN-Tのトレーニングに使用されます。 
[概要]提案された方法は、ベイズのルールを使用して、ターゲットドメインのrnn-t事後を定義します。これは、ディープニューラルネットワーク（dnns）またはlstmsに基づくasrのタイプに基づいています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-26">
        <br>2020-02-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: La Furca: Iterative Context-Aware End-to-End Monaural Speech Separation
  Based on Dual-Path Deep Parallel Inter-Intra Bi-LSTM with Attention -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/cs.SD/paper_1.html">
      La Furca: Iterative Context-Aware End-to-End Monaural Speech Separation
  Based on Dual-Path Deep Parallel Inter-Intra Bi-LSTM with Attention
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      デュアルパス双方向ロングショートタームメモリ（BiLSTM）ブロックを備えたディープニューラルネットワークは、シーケンスモデリング、特に音声分離などで非常に効果的であることが証明されています。 https://github.com/ShiZiqiang/dual-path-RNNs-DPRNNs-based-speech-separationでDPRNN-TasNetの再実装をオープンソース化し、これに基づいて「La Furca」を実現していますDPRNN-TasNetの実装により、この論文の結果はスムーズに再現できると考えられます。パブリックWSJ0-2mixデータコーパスでの実験では、SDRが19.86dB、PESQが3.63、ESTOIが94.2％でした。提案されているネットワークが話者分離タスクのパフォーマンス向上につながる可能性があることを示しています。 
[概要] bitme-ledネットワークはbitm、bitmと呼ばれ、bitm.networkは順列直観的トレーニング（pit）スタイルで音声対歪み比（sdr）を使用します。レベル信号-歪み比
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-23">
        <br>2020-01-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Deep Ad-hoc Beamforming -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/cs.SD/paper_2.html">
      Deep Ad-hoc Beamforming
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      3つの新しいコンポーネントが含まれています。提案されたフレームワークの多くの実装を開発し、音声ソースの位置が遠距離、ランダム、およびマイクにブラインドのシナリオで広範な実験を実施しました。ディープニューラルネットワークに基づく監視されたチャネル選択フレームワークによる、ローカルマイクアレイへの音声ソースの周り。 
[概要]アドホックマイクアレイと深層学習ベースのマルチチャンネル音声エンハンスメントを組み合わせ、遠距離場の音響環境の発生確率を大幅に低減します。また、異なる時間を持つチャンネルを同期するためのシンプルな時間同期フレームワークを開発します。ディレイ
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-11-03">
        <br>2018-11-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: An Open-set Recognition and Few-Shot Learning Dataset for Audio Event
  Classification in Domestic Environments -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/cs.SD/paper_3.html">
      An Open-set Recognition and Few-Shot Learning Dataset for Audio Event
  Classification in Domestic Environments
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      オーディオ領域では、音楽詐欺や話者認識はFSL方式から明らかに恩恵を受けることができます。しかし、多くのアプリケーションでは、このような多数のサンプルを取得することはできません。多種多様なサウンドクラスが行われます。 
[概要]ディープラーニング（dl）アルゴリズムは、大きなデータセットでトレーニングすると非常に良好なパフォーマンスを示します。画像ドメインでは、fslアプリケーションは顔認識に関連するアプリケーションです。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-26">
        <br>2020-02-26
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: MaSS: A Large and Clean Multilingual Corpus of Sentence-aligned Spoken
  Utterances Extracted from the Bible -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/cs.CL/paper_0.html">
      MaSS: A Large and Clean Multilingual Corpus of Sentence-aligned Spoken
  Utterances Extracted from the Bible
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      対象言語（バスク語、英語、フィンランド語、フランス語、ハンガリー語、ルーマニア語、ロシア語、スペイン語）を使用すると、音声と音声のアライメント、および類型学的に異なる言語ペアの翻訳に関する研究が可能になります。そのため、この記事では、異なる言語の音声セグメント間に多言語リンクを追加することを提案し、8言語（56言語）で8,130の並列音声発話の大規模でクリーンなデータセットを共有しますペア）.. CMU Wilderness Multilingual Speech Dataset（Black、2019）は、新約聖書の記録された読みに基づいて新しく公開された多言語音声データセットです。 
[要旨]自動音声認識（asr）とテキストを構築するためのデータを提供します-新しいモデルの場合。多言語言語には、単語などの単語が含まれます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-07-30">
        <br>2019-07-30
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Transformer-CNN: Fast and Reliable tool for QSAR -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/cs.CL/paper_1.html">
      Transformer-CNN: Fast and Reliable tool for QSAR
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このような有効性の理由を議論し、メソッド開発の将来の方向性をドラフトします。リポジトリには、個々の原子の寄与を計算し、モデルの結果を解釈するQSAR予測用のスタンドアロンプログラムもあります。 SMILESをSeq2Seq問題として正規化するようにトレーニングされたTransformer 
[1]モデルの内部エンコーダー状態。 
[概要]埋め込みにcharnn（2）アーキテクチャを使用すると、多様なベンチマークデータセットに対してより高品質で解釈可能なqsar / qsprモデルが得られます。この方法は、小さなデータセットに対して良好な結果を提供できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-21">
        <br>2019-10-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Recent Advances in Natural Language Inference: A Survey of Benchmarks,
  Resources, and Approaches -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/cs.CL/paper_2.html">
      Recent Advances in Natural Language Inference: A Survey of Benchmarks,
  Resources, and Approaches
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらのベンチマークが手段となり、NLP研究コミュニティの原動力となるため、本書は、最新のベンチマークの概要、関連する知識リソース、最新の学習および推論アプローチを提供して、この成長分野..このような自然言語推論能力の開発と評価をサポートするために、多くのベンチマークタスクとデータセットが作成されました。NLPコミュニティでは、近年、マシンが深い言語を実行する能力に取り組む研究活動が急増しています。むしろ、世界の推論と知識に頼って、テキストで明示的に述べられていることを超える理解。 
[ABSTRACT]このような逸話的能力の開発と評価をサポートするために、多くのベンチマークが作成されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-02">
        <br>2019-04-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A Density Ratio Approach to Language Model Fusion in End-To-End
  Automatic Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/cs.CL/paper_3.html">
      A Density Ratio Approach to Language Model Fusion in End-To-End
  Automatic Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      提案されたアプローチは、クロスドメインおよび限定データシナリオで評価されます。このシナリオでは、LMトレーニングにかなりの量のターゲットドメインテキストデータが使用されますが、トレーニングには限定（またはなし）{audio、transcript}トレーニングデータペアのみが使用されます密度比法は、LMおよびエンドツーエンドのASR統合、Shallow Fusionへの支配的なアプローチよりも常に優れていることが判明しました。ANNモデルでトレーニングされたリカレントニューラルネットワークトランスデューサー（RNN-T）ASRモデルに適用与えられたドメイン、一致したドメイン内RNN-LM、およびターゲットドメインRNN-LMで、提案された方法は、ベイズのルールを使用して、ASRの古典的なハイブリッドモデルに直接類似した方法で、ターゲットドメインのRNN-T事後を定義しますHidden Markov Model（HMM）フレームワークのディープニューラルネットワーク（DNN）またはLSTMに基づいています（Bourlard＆Morgan、1994）。 
[概要]提案された方法は、ベイズのルールを使用して、ターゲットドメインのrnn-t事後を定義します。これは、ディープニューラルネットワーク（dnns）またはlstmsに基づくasrのタイプに基づいています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-26">
        <br>2020-02-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Guider l'attention dans les modeles de sequence a sequence pour la
  prediction des actes de dialogue -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/cs.CL/paper_4.html">
      Guider l'attention dans les modeles de sequence a sequence pour la
  prediction des actes de dialogue
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、提案されたアプローチは、SwDAで85％の比類のない精度スコア、MRDAで91.6％の最先端の精度スコアを達成します。この作業では、以下を使用してDA分類に合わせたseq2seqモデルを導入します。エンコーダ、斬新なガイド付き注意メカニズム、およびトレーニングと推論の両方に適用されるビーム検索。最新技術と比較して、このモデルは手作りの機能を必要とせず、エンドツーエンドでトレーニングされます。 
[ABSTRACT] seq2seqモデルは、複雑なグローバル依存関係を学習することが知られています。線形条件付きランダムフィールド（crf）のみを使用して現在提案されているアプローチは、ローカルタグ依存関係のみをモデル化します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-21">
        <br>2020-02-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Object Relational Graph with Teacher-Recommended Learning for Video
  Captioning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/cs.CL/paper_5.html">
      Object Relational Graph with Teacher-Recommended Learning for Video
  Captioning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      一方、教師が推奨する学習（TRL）メソッドを設計して、成功した外部言語モデル（ELM）を最大限に活用し、豊富な言語知識をキャプションモデルに統合します。広範なアブレーション研究と視覚化は、システムの有効性を示しています。 。本論文では、新しいモデルと効果的なトレーニング戦略の両方を含む完全なビデオキャプションシステムを提案します。 
[ABSTRACT] object.elmでの相互作用を無視するため、既存のモデルでは適切な視覚的表現が不足しているため、より意味的に類似した単語提案が生成されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-26">
        <br>2020-02-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Detecting Potential Topics In News Using BERT, CRF and Wikipedia -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/cs.CL/paper_6.html">
      Detecting Potential Topics In News Using BERT, CRF and Wikipedia
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      F1、特にリコールに関して、業界最高のFlair、Spacy、Stanford-caseless-NERと比較した場合、有望な結果を示しています。モデルは、Wikipediaのタイトルデータ、プライベート英語ニュースコーパス、BERT-Multilingual pretrained model、Bi- GRUとCRFのアーキテクチャ。たとえば、「私も動きすぎ」、「ビーフ禁止」、「alwar mob lynching」。 
[要約]また、named-entityの定義に必ずしも適合しないn-グラムを識別する必要がありますが、それらは重要です。f1に関して、spacyおよびstanford-caseless-nerと比較すると、有望な結果を示しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-26">
        <br>2020-02-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Studying the Effects of Cognitive Biases in Evaluation of Conversational
  Agents -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/cs.CL/paper_7.html">
      Studying the Effects of Cognitive Biases in Evaluation of Conversational
  Agents
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      2つの実験条件での評価の一貫性の向上は、固定バイアスの結果である可能性があります。この結果は、会話エージェントを評価する最適な方法に関する洞察を提供します。評価者間の一貫性。 
[ABSTRACT]コミュニケーションエージェントは、接続を使用して米国で開発されました。ただし、認知バイアスが人々の意思決定に影響を与えているかどうかは不明です。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-18">
        <br>2020-02-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Guiding attention in Sequence-to-sequence models for Dialogue Act
  prediction -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/cs.CL/paper_8.html">
      Guiding attention in Sequence-to-sequence models for Dialogue Act
  prediction
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この作業では、以下を使用してDA分類用に調整されたseq2seqモデルを紹介します。階層エンコーダー、新しいガイド付き注意メカニズム、およびトレーニングと推論の両方に適用されるビーム検索。さらに、提案されたアプローチは、SwDAで85％の比類のない精度スコアを達成し、MRDAで91.6％の最先端の精度スコアを達成します。 
[ABSTRACT] seq2seqモデルは、複雑なグローバル依存関係を学習することが知られています。線形条件付きランダムフィールド（crf）のみを使用して現在提案されているアプローチは、ローカルタグ依存関係のみをモデル化します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Sparse Sinkhorn Attention -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/cs.CL/paper_9.html">
      Sparse Sinkhorn Attention
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この目的のために、Causal Sinkhorn BalancingおよびSortCutなどの新しいアルゴリズムの革新を提案します。これは、エンコードおよび/またはデコードの目的でSinkhorn Attentionを調整するための動的なシーケンス切り捨て方法です。具体的には、シーケンス..私たちの方法は、内部表現の微分可能なソートに基づいています。 
[ABSTRACT]私たちの方法は、内部表現の微分可能なソートに基づいています。その後、ローカルウィンドウのみで準グローバルな注意をシミュレートできます。これにより、注意モジュールのメモリ効率が向上します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-26">
        <br>2020-02-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Do We Need Word Order Information for Cross-lingual Sequence Labeling -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/cs.CL/paper_10.html">
      Do We Need Word Order Information for Cross-lingual Sequence Labeling
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      対話の自然言語理解、品詞タグ付け、および名前付きエンティティ認識タスクに関する実験結果は、語順情報を削除することで、ベースラインモデルよりも優れたゼロショットのクロスリンガルパフォーマンスを達成できることを示しています。言語間シーケンスラベリングタスクの順序に依存しないモデルを構築します。言語間適応の最近の研究のほとんどは、異なる言語での語順の分散を考慮していません。 
[概要]クロスリンガルモデルは、ソース言語の語順に適合します。当社のモデルは、入力シーケンスの語順情報をエンコードしません。各トークンの予測は、シーケンス全体の注意に基づいています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-30">
        <br>2020-01-30
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Marathi To English Neural Machine Translation With Near Perfect Corpus
  And Transformers -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/cs.CL/paper_11.html">
      Marathi To English Neural Machine Translation With Near Perfect Corpus
  And Transformers
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この演習では、さまざまなNeural Machine MarathiをBERT-tokenizerで学習した英語の翻訳者と、ハグフェイスおよびさまざまなTransformerベースのアーキテクチャを、制限されているがほぼ正しいコーパスを使用してFacebookのFairseqプラットフォームを使用してトレーニングし、比較して、TatoebaおよびWikimediaでGoogleよりも優れたBLEUスコアを達成しましたこれらの中で、Googleからの翻訳結果は、一般的な検査に基づいてより良いはずです。インド語のニューラル機械翻訳タスクの最先端アルゴリズムのパフォーマンスをベンチマークする試みはほとんどありませんでした。 
[ABSTRACT] google、binge、facebook、yandexは、インドの言語で多数の翻訳システムを構築した数少ない企業の一部です。googleのbingeプラットフォーム、googleのbingset、facebookは、独自の翻訳を構築した企業です。システム
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-26">
        <br>2020-02-26
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: A Density Ratio Approach to Language Model Fusion in End-To-End
  Automatic Speech Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/eess.AS/paper_0.html">
      A Density Ratio Approach to Language Model Fusion in End-To-End
  Automatic Speech Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この記事では、外部言語モデル（LM）を自動音声認識（ASR）のエンドツーエンドモデルに統合する密度比アプローチについて説明します。密度比法は、LMへの支配的なアプローチとエンドツーエンドモデルを一貫して上回ることがわかりました。 ASR統合の終了、Shallow Fusion ..提案されたアプローチは、クロスドメインおよび限定データシナリオで評価されます。このシナリオでは、LMトレーニングにかなりの量のターゲットドメインテキストデータが使用されますが、{音声、トランスクリプト}トレーニングデータペアは、RNN-Tのトレーニングに使用されます。 
[概要]提案された方法は、ベイズのルールを使用して、ターゲットドメインのrnn-t事後を定義します。これは、ディープニューラルネットワーク（dnns）またはlstmsに基づくasrのタイプに基づいています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-26">
        <br>2020-02-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Lightweight Online Separation of the Sound Source of Interest through
  BLSTM-Based Binary Masking -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/eess.AS/paper_1.html">
      Lightweight Online Separation of the Sound Source of Interest through
  BLSTM-Based Binary Masking
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ビームフォーマ出力の性質により、ラベル置換問題は最初から処理されます。このホワイトペーパーでは、2段階の手法を提案します。1）SOIの推定に加えて、位相ベースのビームフォーマ累積環境干渉の推定;および2）累積環境干渉からSOIを分離することを目的とするバイナリマスクを計算するBLSTMベースのTFバイナリマスキングステージ。 、より少ない計算リソースで済みます。 
[概要]実行する主なタイプの手法は、空間フィルタリング（またはビームフォーミング）です。関心のあるソース（soi）の位置（主に、到来方向）が既知であると想定されます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-26">
        <br>2020-02-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: La Furca: Iterative Context-Aware End-to-End Monaural Speech Separation
  Based on Dual-Path Deep Parallel Inter-Intra Bi-LSTM with Attention -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/eess.AS/paper_2.html">
      La Furca: Iterative Context-Aware End-to-End Monaural Speech Separation
  Based on Dual-Path Deep Parallel Inter-Intra Bi-LSTM with Attention
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      https://github.com/ShiZiqiang/dual-path-RNNs-DPRNNs-based-speech-separationでDPRNN-TasNetの再実装をオープンソース化し、これに基づいて「La Furca」を実現していますDPRNN-TasNetの実装により、この論文の結果はスムーズに再現できると考えられています。デュアルパス双方向ロングショートタームメモリ（BiLSTM）ブロックを備えたディープニューラルネットワークは、シーケンスモデリングで非常に効果的であることが証明されています。 、特に音声の分離など。 DPRNN-TasNet \ cite {luo2019dual}。 
[概要] bitme-ledネットワークはbitm、bitmと呼ばれ、bitm.networkは順列直観的トレーニング（pit）スタイルで音声対歪み比（sdr）を使用します。レベル信号-歪み比
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-23">
        <br>2020-01-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Deep Ad-hoc Beamforming -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/eess.AS/paper_3.html">
      Deep Ad-hoc Beamforming
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      第二に、ディープニューラルネットワークに基づく監視されたチャネル選択フレームワークによって、音声ソースの周りのマイクをローカルマイクアレイにグループ化します。提案されたフレームワークの多くの実装を開発し、音声の場所が存在するシナリオで広範な実験を実施しました音源は、遠距離音場、ランダム、およびマイクにブラインドです。3つの新しいコンポーネントが含まれています。 
[概要]アドホックマイクアレイと深層学習ベースのマルチチャンネル音声エンハンスメントを組み合わせ、遠距離場の音響環境の発生確率を大幅に低減します。また、異なる時間を持つチャンネルを同期するためのシンプルな時間同期フレームワークを開発します。ディレイ
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-11-03">
        <br>2018-11-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multitask Learning and Multistage Fusion for Dimensional Audiovisual
  Emotion Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/eess.AS/paper_4.html">
      Multitask Learning and Multistage Fusion for Dimensional Audiovisual
  Emotion Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      第二に、さまざまなモダリティの最終予測からの結果を結合するために、多段階融合が提案されています。私たちのアプローチでは、マルチモーダル学習を使用しました。後期融合アプローチで採用されたマルチステージ法は、覚醒、価、好み。 
[要旨]この論文では、マルチタスク学習と融合戦略を使用して、音声データと視覚データから感情属性を予測する2つの方法を提案します。まず、さまざまなモダリティの最終予測の結果を結合する多段階融合を提案します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-26">
        <br>2020-02-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: BUT System for the Second DIHARD Speech Diarization Challenge -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/eess.AS/paper_5.html">
      BUT System for the Second DIHARD Speech Diarization Challenge
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      5番目のCHiMEチャレンジからの録音を含むトラック3および4について、マルチチャネルダイアライゼーションを行うためのさまざまなアプローチを検討し、チャネルごとの確率線形判別分析スコアの融合にAHCを適用したときに最高のパフォーマンスが得られました。このステップでは、第2 DIHARD音声ダイアライゼーションチャレンジの4つのトラック用にBUTチームが開発した受賞システムについて説明します。 
[概要]トラック1と2のシステムは主にダイアライゼーションに基づいていました。トラック3と4のシステムはシステムを開発しました。チャネルごとの確率線形判別分析スコアの融合にahcを適用すると、最高のパフォーマンスが得られました。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-26">
        <br>2020-02-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: An Open-set Recognition and Few-Shot Learning Dataset for Audio Event
  Classification in Domestic Environments -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/eess.AS/paper_6.html">
      An Open-set Recognition and Few-Shot Learning Dataset for Audio Event
  Classification in Domestic Environments
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      オーディオ領域では、音楽詐欺や話者認識はFSL方式の恩恵を明確に受けることができます。これらの音は通常、さまざまな音声クラスに対応する多くのイベントが発生する家庭環境で発生します。限られた数のサンプルを使用して、ドアベルや火災警報器などのさまざまなタイプの音響警報によって与えられる特定の意図的な音響イベントの検出。 
[概要]ディープラーニング（dl）アルゴリズムは、大きなデータセットでトレーニングすると非常に良好なパフォーマンスを示します。画像ドメインでは、fslアプリケーションは顔認識に関連するアプリケーションです。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-26">
        <br>2020-02-26
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Physiological properties and tailored feeds to support aquaculture of marbled crayfish in closed systems -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/biorxiv.physiology/paper_0.html">
      Physiological properties and tailored feeds to support aquaculture of marbled crayfish in closed systems
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      マーブルザリガニ（Procambarus virginalis）は新しい淡水ザリガニ種であり、アポミティックな単為生殖により繁殖し、結果として単クローン性で全雌の個体群になります。キトサンおよびその他のバイオプラスチックの合成用。これらの飼料は、飼料試験で優れた性能を示し、顕著な飼料転換率は1.4でした。 
[要旨]動物は商業養殖のために考慮されています。大理石のザリガニのサイズと重量は、一般に養殖された淡水ザリガニに匹敵します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-26">
        <br>2020-02-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Stress and behavioral correlates in the head-fixed method -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-27/biorxiv.physiology/paper_1.html">
      Stress and behavioral correlates in the head-fixed method
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらのデータは、2時間の頭部固定馴化セッション中の歩行や慢性的な軽度のストレスの影響を受けることが知られているいくつかの古典的な行動測定を含む行動データと比較対照されました。25日間連続してマウスを記録用に準備しました実験、すなわち。 25日間を通して、血液サンプルを定期的に尾静脈から採取して、ストレス関連ホルモンであるコルチコステロンの変動を測定しました。 
[概要]ニューロタール、フィンランドの科学者はモバイルホームケージシステムを使用しました。動物は頭部-アルミニウムフレームに固定されていますが、それ以外は空気の上に浮かぶ超軽量カーボン容器で移動します-計量台
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-26">
        <br>2020-02-26
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
