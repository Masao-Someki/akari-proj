<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-09-24の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Hardware Aware Training for Efficient Keyword Spotting on General
  Purpose and Specialized Hardware -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.SD/paper_0.html">
      <font color="black">Hardware Aware Training for Efficient Keyword Spotting on General
  Purpose and Specialized Hardware</font>
    </a>
  </h2>
  <font color="black">また、8.79 $ \ mu $ WのSotA電力効率を実現するカスタム設計のアクセラレータハードウェアの電力要件を特徴づけ、汎用低電力ハードウェア（マイクロコントローラー）を24倍、特殊用途ASICを16倍にしています。最先端の（SotA）精度と少ないパラメーター数を実現するLegendre Memory Unit（LMU）に基づく新しいKWSニューラルネットワークを構築するための認識トレーニング（HAT）。キーワードスポッティング（KWS）は、電話、ウェアラブル、自動車など、多くのモバイルおよびエッジアプリケーション。 
[要旨] kwsシステムは通常、スマートフォンの「ユーザーw」です。kwsは、車で使用できるスマートフォンベースのデバイスです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: DCCRN: Deep Complex Convolution Recurrent Network for Phase-Aware Speech
  Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.SD/paper_1.html">
      <font color="black">DCCRN: Deep Complex Convolution Recurrent Network for Phase-Aware Speech
  Enhancement</font>
    </a>
  </h2>
  <font color="black">Interspeech 2020 Deep Noise Suppression（DNS）チャレンジに提出された当社のDCCRNモデルは、わずか3.7Mのパラメーターで、平均オピニオンスコア（MOS）に関して、リアルタイムトラックで1位、非リアルタイムトラックで2番目にランク付けされました。 。従来の時間周波数（TF）領域法は、単純な畳み込みニューラルネットワーク（CNN）または再帰型ニューラルネットワーク（RNN）を介して、TFマスクまたは音声スペクトルを予測することに焦点を当てています。音声拡張は、ディープラーニングの成功の恩恵を受けています。了解度と知覚品質の。 
[要旨]たたみ込み実ネットワークは、たたみ込みニューラルネットワーク（cnn）を使用します。これらのたたみ込み実用語には、たたみ込み部分と長い短期記憶（lstm）が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-01">
        <br><font color="black">2020-08-01</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-End Speech Recognition and Disfluency Removal -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.SD/paper_2.html">
      <font color="black">End-to-End Speech Recognition and Disfluency Removal</font>
    </a>
  </h2>
  <font color="black">対照的に、このペーパーは、エンドツーエンドの音声認識と流暢性の除去のタスクを調査することを目的としています。特に、個別の流暢さに依存せずに、流暢な発話を流暢な筆記録に直接マッピングするASRモデルをトレーニングできるかどうかを調査します。検出モデル。エンドツーエンドモデルは、流暢な筆記録を直接生成することを学習することを示します。ただし、それらのパフォーマンスは、ASRシステムとディスフルエンシー検出モデルで構成されるベースラインパイプラインアプローチよりもわずかに劣ります。 
[ABSTRACT]新しい論文は、流暢性の除去のためのモデルのタスクを調査することを目的としています。また、asrシステムと流暢さの検出を調査することを目的としています。調査結果は、さらなる研究のベンチマークとして役立ちます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-22">
        <br><font color="black">2020-09-22</font>
      </time>
    </span>
</section>
<!-- paper0: Narrow-band Deep Filtering for Multichannel Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.SD/paper_3.html">
      <font color="black">Narrow-band Deep Filtering for Multichannel Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">ネットワークの出力は、対応する単一チャネルのクリーンな音声のシーケンスです。提案された方法は、狭帯域ディープフィルタリングと呼ばれます。提案されたディープフィルタリングは、音声とノイズを時間的および空間的特性を利用して区別することができます。音声は非定常で空間的にコヒーレントですが、ノイズは比較的定常的で、チャネル間で相関が弱くなります。 
[要約]提案された方法は、狭帯域ディープフィルタリングと呼ばれます。これは、異なる周波数特性と空間特性を利用することにより、音声とノイズを区別できます。これらには、同じ周波数、複雑なstftエントリ、および（平滑化された）空間特性が含まれます。フィルタ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-25">
        <br><font color="black">2019-11-25</font>
      </time>
    </span>
</section>
<!-- paper0: The Freesound Loop Dataset and Annotation Tool -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.SD/paper_4.html">
      <font color="black">The Freesound Loop Dataset and Annotation Tool</font>
    </a>
  </h2>
  <font color="black">自動ループ特性評価からアルゴリズム構成までのアプリケーションで、コミュニティがさらに多くのデータの用途を見つけることを期待しています。注釈には、楽器、テンポ、メーター、キー、およびジャンルタグが含まれます。データ、およびデータ内のタグの分布とアノテーター間の合意について報告します。 
[ABSTRACT] freesoundはクリエイティブコモンズライセンスの下でリリースされたオーディオ録音のコミュニティデータベースです。fsldはfsldを使用してテンポとキーを推定し、音楽トラックを生成し、ループ分離アルゴリズムを評価します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Cough Against COVID: Evidence of COVID-19 Signature in Cough Sounds -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.SD/paper_5.html">
      <font color="black">Cough Against COVID: Evidence of COVID-19 Signature in Cough Sounds</font>
    </a>
  </h2>
  <font color="black">これらの問題は、農村部や開発途上地域でさらに深刻です。確認プロトコルの前に個人のリスク層別化を有効にすることにより、全体的なテストプロトコルのトリアージステップで使用すると、当社のツールはヘルスケアシステムのテスト能力を43％増加できます病気の有病率5％、追加の物資、訓練を受けた要員、または物理的なインフラストラクチャなし。 COVID-19のテスト能力は、十分な供給、訓練を受けた要員、およびサンプル処理機器の不足により、世界的に依然として課題となっています。 
[ABSTRACT]このツールは、追加の供給、訓練を受けた担当者、または物理インフラストラクチャなしで、ヘルスケアシステムのテスト能力を43％増やすことができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-17">
        <br><font color="black">2020-09-17</font>
      </time>
    </span>
</section>
<!-- paper0: Residual Shuffle-Exchange Networks for Fast Processing of Long Sequences -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.SD/paper_6.html">
      <font color="black">Residual Shuffle-Exchange Networks for Fast Processing of Long Sequences</font>
    </a>
  </h2>
  <font color="black">提案されたアーキテクチャは、より長いシーケンスにスケーリングするだけでなく、より速く収束し、より良い精度を提供します。しかし、モデルは非常に複雑で、Gated Recurrent Unitから派生した洗練されたゲーティングメカニズムを含みます。改良されたShuffle-を組み合わせる方法を示します。畳み込み層を備えた交換ネットワーク。これを、長いシーケンス処理アプリケーションでの有用なビルディングブロックとして確立します。 
[ABSTRACT]最近導入されたニューラルシャッフル-交換ネットワークは、geluとレイヤーの正規化を使用した残余ネットワークに基づいています。シャッフルを超えますが、それを超えますが、musicnetデータセットの複雑さを防ぎます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-06">
        <br><font color="black">2020-04-06</font>
      </time>
    </span>
</section>
<!-- paper0: Comparing Natural Language Processing Techniques for Alzheimer's
  Dementia Prediction in Spontaneous Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.SD/paper_7.html">
      <font color="black">Comparing Natural Language Processing Techniques for Alzheimer's
  Dementia Prediction in Spontaneous Speech</font>
    </a>
  </h2>
  <font color="black">提供された自然発話データセットのテキストトランスクリプトのみを分析し、ADとコントロールの分類およびメンタルミニステート試験スコアの予測のための多数のモデル全体でパフォーマンスを構築および比較します。サポートベクターマシン（SVM）を厳密にトレーニングおよび評価します。勾配ブースティングディシジョンツリー（GBDT）、およびディープラーニングトランスフォーマーベースのモデルと一緒に条件付きランダムフィールド（CRF）。最高のパフォーマンスを発揮するモデルは、SVMへの入力として単純な用語周波数逆ドキュメント周波数（TF-IDF）ベクトライザーであることがわかります。単純な線形モデルへの埋め込みレイヤーとして使用される場合、モデルと事前トレーニング済みのトランスフォーマーベースのモデル「DistilBERT」。 
[ABSTRACT]自然発話タスクによるアルツハイマー病の認識は、広告と関連する表現型の診断と予測のための前処理済みでバランスのとれたデータセットを提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-12">
        <br><font color="black">2020-06-12</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: GSR-Net: Graph Super-Resolution Network for Predicting High-Resolution
  from Low-Resolution Functional Brain Connectomes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.IV/paper_0.html">
      <font color="black">GSR-Net: Graph Super-Resolution Network for Predicting High-Resolution
  from Low-Resolution Functional Brain Connectomes</font>
    </a>
  </h2>
  <font color="black">提案されたGSR-Netフレームワークは、低解像度のコネクトームから高解像度の脳機能コネクトームを予測するためのバリアントよりも優れていました。元のノード機能がない場合、最初に各脳ROI（ノード）に識別機能ベクトルを割り当て、学習したノードの特徴表現を学習するためのローカル受容フィールド..最初に、非ユークリッドデータに固有のグラフのたたみ込み、プーリング、およびアンプーリング操作に基づくU-Netのようなアーキテクチャを採用します。 
[ABSTRACT]私たちのgsr-netは、グラフ構造化データで動作する最初の超解像フレームワークであり、耐性のない画像を生成します。グラフ超解像（gsr）レイヤーと2つのグラフ畳み込みネットワークレイヤーを使用して、hrグラフを予測しますlrログログログの特性を維持します。これにより、費用のかかるデータ収集や解剖学的脳領域の手動によるラベル付けの必要性が回避されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Improved gradient descent-based chroma subsampling method for color
  images in VVC -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.IV/paper_1.html">
      <font color="black">Improved gradient descent-based chroma subsampling method for color
  images in VVC</font>
    </a>
  </h2>
  <font color="black">コダックとIMAXのデータセットに基づく包括的な実験結果は、新しくリリースされた多用途ビデオコーディング（VVC）プラットフォームVTM-8.0で、上記の3種類のカラー画像に対して、クロマサブサンプリング法が既存のクロマサブサンプリング法より明らかに優れていることを示しています。 。最後に、効果的な反復法が開発され、最初にサブサンプリングされた$（U、V）$ペアを改善します。RGBフルカラー、Bayerカラーフィルターアレイ（CFA）、およびデジタル時間遅延積分用にカラー画像をエンコードする前に（ DTDI）CFA画像、変換されたクロマ画像でクロマサブサンプリングを実行することが必要かつ重要です。 
[ABSTRACT]上記の3種類のカラー画像の場合、当社のクロマサブサンプリング方法は既存のクロマサブサンプリング方法よりも明らかに優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Robustification of Segmentation Models Against Adversarial Perturbations
  In Medical Imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.IV/paper_2.html">
      <font color="black">Robustification of Segmentation Models Against Adversarial Perturbations
  In Medical Imaging</font>
    </a>
  </h2>
  <font color="black">私たちの提案した方法が既存の防御方法に比べて優れた性能を持っていることを経験的に示す実験があります。この論文は、医用画像における敵対的攻撃に対するセグメンテーションモデルのための新しい、しかし効率的な防御フレームワークを提示します。ターゲットの深層学習モデルを修正せずに深層学習モデルを使用できるほか、敵対的な攻撃から独立させることもできます。 
[要約]セグメンテーションモデルの防御方法はあまり検討されていません。リフォーマーはターゲットモデルをより正確に予測するのに役立ちます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Schizophrenia-mimicking layers outperform conventional neural network
  layers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.IV/paper_3.html">
      <font color="black">Schizophrenia-mimicking layers outperform conventional neural network
  layers</font>
    </a>
  </h2>
  <font color="black">得られた結果は、統合失調症の接続層が過剰適合に耐性があり、完全に接続された層よりも優れていることを明らかにしました。競争力のあるパフォーマンス..統合失調症症例の脳ネットワークのナノメートルスケールの3次元研究を報告し、それらの神経突起が健康なコントロールと比較して薄く曲がりくねっていることを発見しました。 
[要約]統合失調症-畳み込み層の模倣もテストされました。統合失調症のリンク障害は脳内のネットワークではありませんが、より優れた脳のパフォーマンスを達成するための生物学的役割があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Shape from Projections via Differentiable Forward Projector for Computed
  Tomography -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.IV/paper_4.html">
      <font color="black">Shape from Projections via Differentiable Forward Projector for Computed
  Tomography</font>
    </a>
  </h2>
  <font color="black">単一オブジェクトの問題の実験結果は、ノイズの多いシミュレーションデータに対して、この方法が従来のボクセルベースの方法よりも優れていることを示しています。フォワードプロジェクションはレンダリングプロセスと見なされ、微分可能なラスタライゼーションで最近の作業を拡張することで微分可能にします。トモグラフィーの問題については、 3Dメッシュは、データ取得をシミュレートするために主に研究されていますが、再構成のためではありません。これは、3Dメッシュの場合、投影から形状を推定する逆のプロセスを意味します。 
[要約]このホワイトペーパーでは、3Dメッシュの微分可能なフォワードモデルを提案します。提案されたフォワードモデルを使用して、投影から直接3D形状を再構築します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-29">
        <br><font color="black">2020-06-29</font>
      </time>
    </span>
</section>
<!-- paper0: Device for ECG prediction based on retinal vasculature analysis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.IV/paper_5.html">
      <font color="black">Device for ECG prediction based on retinal vasculature analysis</font>
    </a>
  </h2>
  <font color="black">このようなデバイスは、心電図と同期でき、さまざまな眼や全身の血管疾患の存在を評価するようにプログラムできます。ただし、専門家の可用性は常に保証されているわけではなく、専門家がいる場合でも評価が行われます手動で..デバイスを使用して、血管径の時間変動を推定できます。これをさらに使用して、ECGや他のさまざまな全身性血管疾患の存在を予測できます。 
[要約]心周期の脈動変化の分析は、眼および全身の血管疾患の存在に対する潜在的な非侵襲的評価である</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Perceptual Video Quality Prediction Emphasizing Chroma Distortions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.IV/paper_6.html">
      <font color="black">Perceptual Video Quality Prediction Emphasizing Chroma Distortions</font>
    </a>
  </h2>
  <font color="black">この問題の調査に向けて、発生する彩度の歪みの種類、輝度の歪みとの関係、およびそれらが知覚される品質にどのように影響するかを理解することが重要です。人間の観察者が見るデジタルビデオの品質の測定は、一般的な方法になっていますアダプティブビデオストリーミング、品質監視、その他のデジタルTVアプリケーションなど、多数のマルチメディアアプリケーションで使用されます。具体的には、新しい主観的なデータセットは、さまざまな量の彩度と混ざったさまざまなレベルの輝度量子化によって引き起こされる歪みによって影響を受けた合計$ 210 $ビデオで構成されています。量子化。 
[ABSTRACT]ビデオの知覚品質の測定は、圧縮によるルマとクロマの両方の歪みから生じました。新しい実験は、ビデオの知覚品質を測定するために開発されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Estimation of Motion Parameters for Ultrasound Images Using Motion Blur
  Invariants -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.IV/paper_7.html">
      <font color="black">Estimation of Motion Parameters for Ultrasound Images Using Motion Blur
  Invariants</font>
    </a>
  </h2>
  <font color="black">これらの結果は、超音波画像アプリケーションにおけるこの不変法の大きな可能性を示唆する可能性があります。超音波画像のぼけたたみ込みの不変特徴を分析するために、周波数領域とモーメント領域の両方で線形モーションブラーの新しいモデルを提案します。胎児の超音波画像の品質イメージングシステムが正確なデータをキャプチャするために低いモーション品質を必要とする一方で、モーションブラーの影響を大きく受けます。 
[ABSTRACT]モーションモーションをモデルに適用して、正確に正確なシステムを作成できます。モデルを使用して、動作の説明を説明することができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: ApproxNet: Content and Contention-Aware Video Analytics System for
  Embedded Clients -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.IV/paper_8.html">
      <font color="black">ApproxNet: Content and Contention-Aware Video Analytics System for
  Embedded Clients</font>
    </a>
  </h2>
  <font color="black">これは、モデルのアンサンブル（MCDNN 
[MobiSys-16]など）を作成して維持するのではなく、単一のDNNモデル内で2つの近似ノブを有効にすることでこれを実現します。このホワイトペーパーでは、組み込みまたはモバイルクライアント用のビデオ分析システムであるApproxNetを紹介します（これを総称して「センサーデバイス」と呼びます）。 
[ABSTRACT]このようなクライアント用のプロトタイプディープニューラルネットワーク（dnns）を作成する上で重要な作業がありました。これらのどれも、監視カメラやar / vrガジェットなどの変化するランタイム条件に適応できません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-28">
        <br><font color="black">2019-08-28</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning-Based Reconstruction of Interventional Tools from Four
  X-Ray Projections for Tomographic Interventional Guidance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.IV/paper_9.html">
      <font color="black">Deep Learning-Based Reconstruction of Interventional Tools from Four
  X-Ray Projections for Tomographic Interventional Guidance</font>
    </a>
  </h2>
  <font color="black">これらの予測は再構築され、2番目のCNNに送られます。これは、この非常にアンダーサンプリングされた再構築を介入ツールのセグメンテーションにマッピングします。この作業では、リアルタイムトモグラフィー（4次元）介入ガイダンス用の深層学習ベースのパイプラインを提案します当社のパイプラインは、非常に高い精度で事前に患者を必要とせずに、4つのX線投影のみから介入ツールを再構築することができます。 
[要約]データは、介入ツールの位置に関する限られた情報のみを提供します。介入ツールは、4つの円錐-ビームct投影から抽出され、深い畳み込みニューラルネットワークを使用します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Non-Unique Segmentation with Reward-Penalty Dice Loss -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.IV/paper_10.html">
      <font color="black">Learning Non-Unique Segmentation with Reward-Penalty Dice Loss</font>
    </a>
  </h2>
  <font color="black">異なる外科医が同じ患者に対してわずかに異なる方法で成功した手術を行う可能性があるため、非固有のセグメンテーションアノテーションがある場合があります。RPDLは、共通領域を強化し、外部の領域にペナルティを課すことにより、非固有のセグメンテーションを学習するのに役立ちます。独自のセグメンテーションタスクでは、ディープたたみ込みニューラルネットワーク（DCNN）の最適化の目的として、報酬ペナルティダイス損失（RPDL）関数を提案します。 
[ABSTRACT]セマンティックセグメンテーションのほとんどの研究とアプリケーションは、固有のパフォーマンス問題に対処することに焦点を当てており、すべての入力画像に対して1つのゴールドスタンダード結果しかありません。異なる外科医が同じ患者に対して成功した手術を行う可能性があるため、固有でないセグメンテーションアノテーションがある場合があります少し違う方法</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Attention with Multiple Sources Knowledges for COVID-19 from CT Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.IV/paper_11.html">
      <font color="black">Attention with Multiple Sources Knowledges for COVID-19 from CT Images</font>
    </a>
  </h2>
  <font color="black">広範な実験は、最近のベースラインと比較して、アプローチの優れたパフォーマンスを示しています。この手順は、システムをノイズに対してより堅牢にするだけでなく、ネットワークを局所病変領域に焦点を合わせて導きます。特に、学習したネットワークから抽出された感染領域とヒートマップは学習プロセス中に注意メカニズムを介してグローバルイメージと統合されます。 
[要約] covid-19の早期診断は時期尚早である可能性があります。これらのアプローチは主に、新しいアーキテクチャの導入、転移学習技術、または大規模データの構築に重点を置いています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Whole Slide Images based Cancer Survival Prediction using Attention
  Guided Deep Multiple Instance Learning Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.IV/paper_12.html">
      <font color="black">Whole Slide Images based Cancer Survival Prediction using Attention
  Guided Deep Multiple Instance Learning Networks</font>
    </a>
  </h2>
  <font color="black">全体のスライド画像（WSI）から派生したキーパッチまたはクラスターに制限する現在の画像ベースの生存モデルとは異なり、シャムMI-FCNと注意ベースのMILプーリングの両方を導入することにより、ディープアテンションマルチインスタンスサバイバルラーニング（DeepAttnMISL）を提案します。 WSIからイメージング機能を効率的に学習し、WSIレベルの情報を患者レベルに集約します。従来の画像ベースの生存予測モデルは、これらのメソッドを大規模なデータセットに拡張するためにスケーラブルでない判別パッチラベルに依存しています。注意ベースの集約は最近の生存モデルでは、集計手法よりも柔軟で適応性があります。 
[要約]複数インスタンス学習（mil）フレームワークは、患者のリスクを予測するために使用できます。個別化医療の提供を支援するためにも使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Anisotropic 3D Multi-Stream CNN for Accurate Prostate Segmentation from
  Multi-Planar MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.IV/paper_13.html">
      <font color="black">Anisotropic 3D Multi-Stream CNN for Accurate Prostate Segmentation from
  Multi-Planar MRI</font>
    </a>
  </h2>
  <font color="black">方法：追加のスキャン方向を処理して高解像度の等方性前立腺セグメンテーションを生成する異方性3DマルチストリームCNNアーキテクチャを提案します。したがって、提案されたモデルは、前立腺癌の診断と治療の結果を改善する可能性があります。公正な比較を実現するために、ハイパーパラメータ最適化戦略を採用して、個々のアプローチに最適な構成を選択します。 
[要約] cnnのセグメンテーションアプローチの大部分は、mrスキャンのみを考慮します。進行状況は特にベースで観察できると彼は言います</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: CA-Net: Comprehensive Attention Convolutional Neural Networks for
  Explainable Medical Image Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.IV/paper_14.html">
      <font color="black">CA-Net: Comprehensive Attention Convolutional Neural Networks for
  Explainable Medical Image Segmentation</font>
    </a>
  </h2>
  <font color="black">コードはhttps://github.com/HiLab-git/CA-Netで入手できます。特に、最初にネットワークを前景領域により集中させるための共同空間注意モジュールを提案します。 U-Netと比較した場合の平均セグメンテーションダイススコアは、皮膚病変で87.77％から92.08％、胎盤で84.79％から87.08％、胎児脳で93.20％から95.88％です。 
[ABSTRACT]畳み込みニューラルネットワーク（cnns）は、自動医用画像セグメンテーションの扱いが複雑です。より正確で説明可能な医療処置のために包括的な注意ベースのcnn（ca-net）を提案します。既存のネットワークよりもはるかに優れた説明可能性があると言います注意の重みマップを視覚化することにより</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-22">
        <br><font color="black">2020-09-22</font>
      </time>
    </span>
</section>
<!-- paper0: Accurate Retinal Vessel Segmentation via Octave Convolution Neural
  Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.IV/paper_15.html">
      <font color="black">Accurate Retinal Vessel Segmentation via Octave Convolution Neural
  Network</font>
    </a>
  </h2>
  <font color="black">オクターブ畳み込みとオクターブ転置畳み込みの両方を統合するオクターブUNetという名前の畳み込みニューラルネットワークの新しいアーキテクチャは、UNetのエンコーダー/デコーダーアーキテクチャーに基づいて提案されており、後処理ステップなしで単一の順送りで高解像度の血管セグメンテーションを生成できます。網膜血管セグメンテーションは、糖尿病、眼科疾患、心血管疾患などのさまざまな疾患の診断とスクリーニングにおける重要なステップです。特徴抽出に標準の畳み込みを利用する他の畳み込みネットワークと比較して、提案された方法は、学習にオクターブ畳み込みとオクターブ転置畳み込みを利用します。複数の空間周波数機能により、さまざまなサイズと形状の網膜血管系をより適切に捉えることができます。 
[要約]提案された方法は、エンコーダー-デコーダーベースのオクターブたたみ込みネットワークの使用を含みます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-28">
        <br><font color="black">2019-06-28</font>
      </time>
    </span>
</section>
<!-- paper0: Federated Learning for Computational Pathology on Gigapixel Whole Slide
  Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.IV/paper_16.html">
      <font color="black">Federated Learning for Computational Pathology on Gigapixel Whole Slide
  Images</font>
    </a>
  </h2>
  <font color="black">複数の機関にまたがる医療データのマルチセントリックで協調的な統合は、当然この課題を克服してモデルのパフォーマンスを向上させるのに役立ちますが、モデルが数十万の使用に向けて拡大するにつれて複雑なデータ共有プロセスで発生する可能性のある他の困難の中でプライバシーの懸念によって制限されますギガピクセルのスライド画像全体。さらに、スライド画像全体からの生存予測と患者の層別化のための弱く監視された学習フレームワークを提示し、フェデレーション環境での有効性を示しています。結果は、フェデレーション学習を使用すると、正確に弱く正確に開発できることを示しています。分散データサイロからのディープラーニングモデルを監視し、直接のデータ共有とそれに関連する複雑さを伴わずに、ランダム化されたノイズ生成を使用して差分プライバシーを保護します。 
[要約]ディープラーニングの開発-データセットに基づいて、多くの場合、収集と時間に依存します-コストのかかるキュレーション。これらのデータは、理想的には、そのようなデータセットに存在する不均一性に対応するために、さまざまなソースと患者集団から取得する必要があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Foreseeing Brain Graph Evolution Over Time Using Deep Adversarial
  Network Normalizer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.IV/paper_17.html">
      <font color="black">Foreseeing Brain Graph Evolution Over Time Using Deep Adversarial
  Network Normalizer</font>
    </a>
  </h2>
  <font color="black">ここでは、固定された中心の人口主導型接続テンプレートの変換として各脳ネットワークを表すための敵対的な脳ネットワークノーマライザを設計します。テストの進化の軌跡は、選択されたトレーニンググラフとそれに対応する進化の軌跡によって広がります。興味深いことに、脳グラフ進化モデルは、文献にはほとんどありません。 
[要約]脳グラフ進化モデルは文献にほとんど存在しません。脳ガンモデルを分析して、テスト脳法の進化を予測できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic Breast Lesion Classification by Joint Neural Analysis of
  Mammography and Ultrasound -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.IV/paper_18.html">
      <font color="black">Automatic Breast Lesion Classification by Joint Neural Analysis of
  Mammography and Ultrasound</font>
    </a>
  </h2>
  <font color="black">さらに、それは平均的な放射線科医と同様に機能し、読者の研究に参加している4人の放射線科医のうち2人を上回ります。最初に、異なるニューラルネットワークが各モダリティに対して個別にトレーニングされ、高レベルの機能を生成します。次に、各モダリティは、マルチモーダルネットワークをトレーニングして最終的な分類を行うために使用されます。 
[ABSTRACT]現在、乳房の既存のコンピューター支援診断システムは、一般に単一のモーダルに基づいています。提案されたアプローチは、0のaucを達成します。94。乳房放射線科医にとって有用な意思決定支援ツールになる可能性があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Cross-Domain Facial Expression Recognition: A Unified Evaluation
  Benchmark and Adversarial Graph Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_0.html">
      <font color="black">Cross-Domain Facial Expression Recognition: A Unified Evaluation
  Benchmark and Adversarial Graph Learning</font>
    </a>
  </h2>
  <font color="black">この作業では、最初にこれらの一貫性のない選択によって引き起こされるパフォーマンスの影響を分析し、次にいくつかの良好なパフォーマンスのCD-FERと最近公開されたドメイン適応アルゴリズムを再実装します。それぞれが優れたパフォーマンスを達成すると宣言していますが、ソース/ターゲットデータセットと特徴エクストラクタの一貫性のない選択。現在の主要なアルゴリズムのほとんどは敵対学習を使用して、全体的なドメイン不変の特徴を学習し、ドメインシフトを緩和していることがわかります。 
[要約]これらのアルゴリズムはすべて、公平なCD-FER評価のために同じソースデータセットと特徴エクストラクタを採用するようにします。これらのアルゴリズムは、ローカルデータセットを無視します。これは、異なるデータセット間でより多くのトランスファリアとなり、より詳細なコンテンツを運ぶ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Self-Supervised Localisation between Range Sensors and Overhead Imagery -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_1.html">
      <font color="black">Self-Supervised Localisation between Range Sensors and Overhead Imagery</font>
    </a>
  </h2>
  <font color="black">私たちは、ミリ波レーダーの使用に特に注意を払っています。ミリ波レーダーは、シーンとの複雑な相互作用と、天候や照明に対する耐性から、説得力があり価値のあるユースケースになります。公開されている衛星画像は、ユビキタスで安価です。 、および以前のセンサーマップが利用できない場合の車両の位置特定のための強力なツールです。モダリティの違いを処理するだけでなく、トレーニングが安価であり、計量的に正確なグラウンドトゥルースなしで自己管理された方法で学習する、学習された位置特定方法を示します。 
[要旨]衛星画像はモダリティが異なるため、直接比較できません。衛星画像は、異なる地上センサーの分析にのみ使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-03">
        <br><font color="black">2020-06-03</font>
      </time>
    </span>
</section>
<!-- paper0: GSR-Net: Graph Super-Resolution Network for Predicting High-Resolution
  from Low-Resolution Functional Brain Connectomes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_2.html">
      <font color="black">GSR-Net: Graph Super-Resolution Network for Predicting High-Resolution
  from Low-Resolution Functional Brain Connectomes</font>
    </a>
  </h2>
  <font color="black">私たちが提案したGSR-Netフレームワークは、低解像度のコネクトームから高解像度の脳機能コネクトームを予測するためのバリアントよりも優れていました。まず、非ユークリッドデータに固有のグラフのたたみ込み、プーリング、アンプーリング操作に基づいたU-Netのようなアーキテクチャを採用します。 。次に、スペクトル理論に触発されて、LRの特性を保持しながらHRグラフを予測するために、グラフ超解像（GSR）レイヤーと2つのグラフ畳み込みネットワークレイヤーを追加して、U-Netアーキテクチャーの対称性を壊します入力。 
[ABSTRACT]私たちのgsr-netは、グラフ構造化データで動作する最初の超解像フレームワークであり、耐性のない画像を生成します。グラフ超解像（gsr）レイヤーと2つのグラフ畳み込みネットワークレイヤーを使用して、hrグラフを予測しますlrログログログの特性を維持します。これにより、費用のかかるデータ収集や解剖学的脳領域の手動によるラベル付けの必要性が回避されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Robustification of Segmentation Models Against Adversarial Perturbations
  In Medical Imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_3.html">
      <font color="black">Robustification of Segmentation Models Against Adversarial Perturbations
  In Medical Imaging</font>
    </a>
  </h2>
  <font color="black">私たちの提案した方法が既存の防御方法に比べて優れた性能を持っていることを経験的に示す実験があります。この提案した方法は、ターゲットの深層学習モデルを修正せずにあらゆる深層学習モデルに使用でき、敵対的な攻撃から独立させることができます..周波数ドメインコンバーターは、検出器が画像のフレームドメインを使用して敵対的な例を検出するのに役立ちます。 
[要約]セグメンテーションモデルの防御方法はあまり検討されていません。リフォーマーはターゲットモデルをより正確に予測するのに役立ちます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Shape from Projections via Differentiable Forward Projector for Computed
  Tomography -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_4.html">
      <font color="black">Shape from Projections via Differentiable Forward Projector for Computed
  Tomography</font>
    </a>
  </h2>
  <font color="black">提案されたフォワードモデルを使用して、投影から直接3D形状を再構築します。単一オブジェクト問題の実験結果は、この方法が、ノイズの多いシミュレーションデータで従来のボクセルベースの方法よりも優れていることを示しています。フォワード投影をレンダリングプロセスと見なし、最近の作業を差別化可能なラスタライズに拡張することで差別化できます。 
[要約]このホワイトペーパーでは、3Dメッシュの微分可能なフォワードモデルを提案します。提案されたフォワードモデルを使用して、投影から直接3D形状を再構築します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-29">
        <br><font color="black">2020-06-29</font>
      </time>
    </span>
</section>
<!-- paper0: Exploring global diverse attention via pairwise temporal relation for
  video summarization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_5.html">
      <font color="black">Exploring global diverse attention via pairwise temporal relation for
  video summarization</font>
    </a>
  </h2>
  <font color="black">さらに、提案されたモデルは、大幅に少ない計算コストで並行して実行できるため、非常に要求の厳しいアプリケーションでの展開に役立ちます。SumMe、TVSum、およびVTWの3つのデータセットでの広範な実験により、SUM-GDAおよびその拡張は、他の競合する最先端の方法を大幅に改善し、優れています。特に、GDAモジュールには2つの利点があります。1）ペアフレーム内の関係とすべてのペア間の関係をモデル化し、グローバルな注目を集めます。 1つのビデオのすべてのフレームにわたって。 2）ビデオ全体に対する各フレームの重要性を反映しているため、これらのフレームにさまざまな注目が集まっています。 
[ABSTRACT] sum-gdaとその拡張機能は、他の競合状態よりも優れています-大幅な改善により、最新の方法</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-label Co-regularization for Semi-supervised Facial Action Unit
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_6.html">
      <font color="black">Multi-label Co-regularization for Semi-supervised Facial Action Unit
  Recognition</font>
    </a>
  </h2>
  <font color="black">2つのディープニューラルネットワークを使用して、ラベル付きとラベルなしの両方の顔画像のマルチビュー機能を生成し、マルチビュー損失は、2つの機能ジェネレーターに条件付きの独立表現を強制するように設計されています。予測の一貫性を制約するために、 2つのビューでは、2つのビューの予測AU確率分布の距離を最小化することにより、マルチラベル共正則化損失をさらに提案します。さらに、個々のAU間の関係の事前知識は、グラフたたみ込みネットワーク（GCN）を通じて埋め込まれます。ラベルのない大きなデータセットからの有用な情報を活用するため。 
[要約]提案されたアプローチは、auラベルなしで顔画像の大規模なデータセットを効果的に活用できます。これらには、半教師付き顔au認識のためのマルチラベル共正則化が含まれます。2つのビューの予測整合性を制約するために、さらに提案します。 2つのビューの予測されたau確率分布の距離を最小化することによるマルチラベル共正則化損失</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-24">
        <br><font color="black">2019-10-24</font>
      </time>
    </span>
</section>
<!-- paper0: Pruning Convolutional Filters using Batch Bridgeout -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_7.html">
      <font color="black">Pruning Convolutional Filters using Batch Bridgeout</font>
    </a>
  </h2>
  <font color="black">CIFAR画像分類タスクで、一般的なコンピュータービジョンモデルVGGNet、ResNet、およびWide-ResNetで提案された方法を評価します。ただし、トレーニングアルゴリズムが密な重みベクトルになる場合、厳しいトレーニング後の剪定はパフォーマンスの低下を招きます。最新のコンピュータービジョンモデルの容量は急速に増加しており、パラメーターの数はトレーニングセットの適合に必要な数をはるかに超えています。 
[ABSTRACT]このモデルは、結論の実行時のメモリと計算要件を削減するために、ニューラルネットワークをトレーニングするために使用できます。ただし、結果の多くは、スパース性を誘発する確率的正則化スキームであるバッチブリッジアウトに基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Few-shot Font Generation with Localized Style Representations and
  Factorization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_8.html">
      <font color="black">Few-shot Font Generation with Localized Style Representations and
  Factorization</font>
    </a>
  </h2>
  <font color="black">ただし、このようなアプローチでは、さまざまなローカルスタイルを表すモデルが制限されるため、非常に複雑な構造を持つさまざまな数のコンポーネント（「ラジカル」と呼ばれることが多い）で構成される文字の最も複雑な文字システム（中国語など）には適さなくなります。 ..提案されたスタイル表現により、テキストデザインの複雑なローカルの詳細を合成できます。手動のデザインは高価であり、デザイナーの専門知識に敏感であるため、自動少数ショットフォントの生成が強く求められています。 
[ABSTRACT]既存の少数ショットフォント生成メソッドは、いくつかの参照グリフからスタイルとコンテンツ要素を解きほぐすことを目的としています。これらには、各フォントスタイルのユニバーサルスタイル表現を選択することが含まれます。ただし、学習コンポーネント-ユニバーサルスタイルではなく賢明なスタイル表現、提案されている</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Knowledge Enhanced Neural Fashion Trend Forecasting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_9.html">
      <font color="black">Knowledge Enhanced Neural Fashion Trend Forecasting</font>
    </a>
  </h2>
  <font color="black">さらに、ファッション要素の時系列データをかなり複雑なパターンで効果的にモデル化するために、時系列データのモデリングで深い再帰型ニューラルネットワークの機能を利用する、Knowledge EnhancedRecurrent Networkモデル（KERN）を提案します。予測は、学界と業界の両方にとって重要なタスクです。さらに、ファッション要素のトレンドの時系列パターンに影響を与えるファッション分野の内部および外部の知識を活用します。 
[ABSTRACT]まず、複雑な時系列のファッション要素のレコードとユーザー情報を含む大規模なファッショントレンドデータセット（fit）をInstagramから提供します。新しいモデルは、客観的なファッション要素の複雑なパターンを効果的にキャプチャできます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-07">
        <br><font color="black">2020-05-07</font>
      </time>
    </span>
</section>
<!-- paper0: Information-Theoretic Visual Explanation for Black-Box Classifiers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_10.html">
      <font color="black">Information-Theoretic Visual Explanation for Black-Box Classifiers</font>
    </a>
  </h2>
  <font color="black">属性は、（ii）各クラスのサポートと反対の両方の説明を提供し、（iii）関連オブジェクトだけでなく、画像の最も決定的な部分を特定します。さらに、メソッド（iv）は、クラスに依存しない補足説明を提供します。最後に、私たちの方法（v）のアルゴリズムの強化により、定量的評価指標の観点から説明の忠実さが向上します。 
[ABSTRACT] 2つの属性マップを提案します。情報獲得（ig）マップとポイントワイズ相互情報（pmi）マップです。このメソッドは、関連するオブジェクトだけでなく、画像の重要な部分を特定します。画像の重要な部分を特定します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Visible fingerprint of X-ray images of epoxy resins using singular value
  decomposition of deep learning features -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_11.html">
      <font color="black">Visible fingerprint of X-ray images of epoxy resins using singular value
  decomposition of deep learning features</font>
    </a>
  </h2>
  <font color="black">一方、機能マップの行列分解の左特異ベクトルは、ネットワークまたはネットワークアーキテクチャの容量などの変数が変化してもほとんど変化しません。この逆問題のソリューションでは、不均質材料では、畳み込みニューラルネットワークの初期層のフィーチャーマップのすべてのチャネルの特異値分解から得られた固有ベクトルを使用します。エポキシ樹脂のプロセス変数はそれらの機械的特性を変化させますが、特性の視覚的識別これらの材料のサンプルのX線画像の機能は困難です。 
[ABSTRACT]最強のアクティブ化されたチャネルは、特徴的な機能を視覚的に表現します。ただし、実際の設定では画像が十分に堅牢ではありません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-16">
        <br><font color="black">2020-04-16</font>
      </time>
    </span>
</section>
<!-- paper0: Multiplexed Illumination for Classifying Visually Similar Objects -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_12.html">
      <font color="black">Multiplexed Illumination for Classifying Visually Similar Objects</font>
    </a>
  </h2>
  <font color="black">視覚的に類似した人工果物サンプルと実際の果物サンプルへのアプローチを示し、固定光源アプローチと従来のコード選択スキームに比べて著しい改善を示しています。次に、トレーニングされたパターンを適用して、新しいオブジェクトの高速分類を実行します。この作業により、偽造検出、農業および製造における品質管理、および皮膚病変の分類における潜在的なアプリケーションにより、以前は区別が付かなかったオブジェクトの迅速な分類が可能になります。 
[要約]正常に分類できるオブジェクトの範囲を拡張するために多重照明を使用することを提案します。次に、イラストパターンを選択し、結果の画像を使用して分類子をトレーニングする方法を開発します。これらの作業により、以前は区別できなかったオブジェクトをすばやく分類できます。偽造品の検出、農業と製造における品質管理、皮膚病変の分類における潜在的な用途</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Fuzzy Simplicial Networks: A Topology-Inspired Model to Improve Task
  Generalization in Few-shot Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_13.html">
      <font color="black">Fuzzy Simplicial Networks: A Topology-Inspired Model to Improve Task
  Generalization in Few-shot Learning</font>
    </a>
  </h2>
  <font color="black">これらのデータセットを使用して、メトリックベースの少数ショットモデルの故障モードを調査します。固定データセット内の根本的に異なるタスク（たとえば、カテゴリメンバーシップから検出を含むタスクへの移動）にモデルがどれだけ一般化できるかを質問することもできますオブジェクトの方向または数量）。カテゴリのメンバーシップ）。 
[ABSTRACT]この制限に対処しようとする学習アルゴリズムは、データが限られている新しいタスクにうまく一般化するように設計されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Interactive Learning for Semantic Segmentation in Earth Observation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_14.html">
      <font color="black">Interactive Learning for Semantic Segmentation in Earth Observation</font>
    </a>
  </h2>
  <font color="black">合成されたアノテーションを使用した3つのデータセットの実験を通して、アプローチの利点を示し、10回のサンプリングされたクリックでIoUの改善が最大4.7％に達することを示します。したがって、DISCA（継続的な適応を伴うディープイメージセグメンテーション）という名前のフレームワーク内でインタラクティブに改良することを提案します。 ..最後に、ドメイン適応などの追加の問題に直面したときに、私たちのアプローチが特にやりがいのあるものになることを示しています。 
[要約]ドメイン適応などの追加の問題に直面した場合、私たちのアプローチは特にやりがいがあります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Generative Model without Prior Distribution Matching -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_15.html">
      <font color="black">Generative Model without Prior Distribution Matching</font>
    </a>
  </h2>
  <font color="black">Variational Autoencoder（VAE）とそのバリエーションは、いくつかの事前分布（ガウス分布など）を満たすために低次元潜在表現を学習することによる古典的な生成モデルです。この方法の有効性について理論的および実験的サポートを提供します。トポロジープロパティのデータマニホールドの保持と潜在空間での分布マッチングの矛盾。埋め込み幾何学は、幾何学的構造を最大限に保持する単純な正則化オートエンコーダアーキテクチャを使用してトレーニングされます。 
[ABSTRACT]結果として、潜在変数を事前に適合させるのではなく、事前分布を埋め込み分布に一致させることを提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: ApproxNet: Content and Contention-Aware Video Analytics System for
  Embedded Clients -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_16.html">
      <font color="black">ApproxNet: Content and Contention-Aware Video Analytics System for
  Embedded Clients</font>
    </a>
  </h2>
  <font color="black">これにより、新しい動的近似手法を使用して、変化するランタイム条件の下で望ましい推論レイテンシと精度のトレードオフを実現できます。このホワイトペーパーでは、組み込みまたはモバイルクライアント（総称して「センサーデバイス」と呼ぶ）向けのビデオ分析システムであるApproxNetを紹介します。 .. ApproxNetが実行時にこれらの変更にシームレスに適応し、画像およびビデオフレームの分類の問題に対して低い安定したレイテンシを提供し、ResNet 
[CVPR-16]、MCDNN 
[MobiSys-16よりも精度とレイテンシが向上することを示します。 ]、およびMobileNets 
[Google-17]。 
[ABSTRACT]このようなクライアント用のプロトタイプディープニューラルネットワーク（dnns）を作成する上で重要な作業がありました。これらのどれも、監視カメラやar / vrガジェットなどの変化するランタイム条件に適応できません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-28">
        <br><font color="black">2019-08-28</font>
      </time>
    </span>
</section>
<!-- paper0: MAFF-Net: Filter False Positive for 3D Vehicle Detection with
  Multi-modal Adaptive Feature Fusion -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_17.html">
      <font color="black">MAFF-Net: Filter False Positive for 3D Vehicle Detection with
  Multi-modal Adaptive Feature Fusion</font>
    </a>
  </h2>
  <font color="black">ネットワークが各モーダルの機能を適応的に使用できるようにするために、チャネルアテンションメカニズムに基づくマルチモーダル適応機能融合モジュールが提案されています。KITTIデータセットの実験結果は、点群データのみを使用するアプローチよりも偽陽性のフィルタリングに大きな改善を示しています..上記のメカニズムに基づいて、2つのフュージョンテクノロジーがさまざまな使用シナリオに適応するように提案されています。 DenseAttentionFusionは、より困難な誤検知のフィルタリングに適し、全体的なパフォーマンスが向上します。 
[ABSTRACT]ニューヨークベースのマルチモーダル機能自律ネットワークが提案されています。画像情報を使用して、3D検出の誤検知を効果的に削減します。この方法は、競争力のある結果を提供でき、kittiベンチマークと比較して最速です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: LoRRaL: Facial Action Unit Detection Based on Local Region Relation
  Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_18.html">
      <font color="black">LoRRaL: Facial Action Unit Detection Based on Local Region Relation
  Learning</font>
    </a>
  </h2>
  <font color="black">LoRRaLは、1）双方向の長期短期メモリ（BiLSTM）を使用して、ローカルAU機能マップを動的かつ順次にエンコードします。2）自己注意メカニズムを使用して、ローカルの顔領域から対応を動的に計算し、AU機能マップを再集計します。 AU共起と相互排除を考慮して、3）連続状態の最新のホップフィールドネットワークを使用して、ローカルの顔の特徴をエンコードしてより特徴的なAU特徴マップにマッピングします。これらのネットワークはすべて、顔の画像を入力として取り、AUの出現にマッピングします。 。顔のAU間の共起と相互排除を考慮して、この論文では、局所領域関係学習（LoRRaL）を備えた畳み込みニューラルネットワークを提案します。検出..エンドツーエンドの畳み込み表現学習は、顔のアクションユニット（AU）の検出に非常に効果的であることが証明されています。 
[ABSTRACT]コンボリューションニューラルネットワークと局所領域関係学習（lorral）。これらは、顔のau発生検出へのaus間の潜在的な関係をエンドツーエンドで組み合わせたアプローチです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Visual Voice Activity Detection with an Automatically Annotated
  Dataset -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_19.html">
      <font color="black">Learning Visual Voice Activity Detection with an Automatically Annotated
  Dataset</font>
    </a>
  </h2>
  <font color="black">A-VADと顔検出および追跡の組み合わせに基づいて、非常に大規模なデータセット（WildVVAD）を自動的に作成して注釈を付ける新しい方法論を紹介します。徹底的な経験的評価により、提案されたディープV-のトレーニングの利点が示されます。このデータセットを使用したVADモデル。視覚的な音声アクティビティ検出（V-VAD）は、視覚機能を使用して、人が話しているかどうかを予測します。 
[ABSTRACT] v-vadは、オーディオVAD（a-VAD）が非効率的な場合に役立ちます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Zero-Shot Learning -- The Good, the Bad and the Ugly -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_20.html">
      <font color="black">Zero-Shot Learning -- The Good, the Bad and the Ugly</font>
    </a>
  </h2>
  <font color="black">第2に、従来のゼロショット設定だけでなく、より現実的な一般化されたゼロショット設定でも、かなりの数の最先端のメソッドを詳細に比較および分析します。ゼロショットの事前トレーニングテストクラス..このペーパーの目的は3つあります。 
[ABSTRACT]一歩下がって、地域の現状を分析する時がきたと主張します。最初に、ゼロショット学習ベンチマークで合意されていないという事実を踏まえて、最初に新しいベンチマークを定義します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2017-03-13">
        <br><font color="black">2017-03-13</font>
      </time>
    </span>
</section>
<!-- paper0: OPFython: A Python-Inspired Optimum-Path Forest Classifier -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_21.html">
      <font color="black">OPFython: A Python-Inspired Optimum-Path Forest Classifier</font>
    </a>
  </h2>
  <font color="black">さらに、OPFythonはPythonベースのライブラリであるため、C言語よりもフレンドリーな環境と高速なプロトタイピングワークスペースを提供します。このペーパーでは、OPFythonと呼ばれるPythonベースの最適パスフォレストフレームワークを提案します。クラスは元のC言語の実装に基づいています。最近のグラフに触発された分類子は、Optimum-Path Forestとして知られ、サポートベクターマシンに匹敵する最先端の手法であることが証明されており、さらにそれを超えています。一部のタスクでは。 
[要約]このペーパーは、最適なパスフォレストフレームワークを提案します。これは、opfythonとして知られています。オリジナルのC言語に基づいて、そのすべての関数とクラスを提供します。これには、すべての関数が基づくPythonベースのシステムであるopfythonが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-28">
        <br><font color="black">2020-01-28</font>
      </time>
    </span>
</section>
<!-- paper0: Semantics-Preserving Adversarial Training -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_22.html">
      <font color="black">Semantics-Preserving Adversarial Training</font>
    </a>
  </h2>
  <font color="black">実験結果は、SPATが敵対的なロバスト性を改善し、CIFAR-10およびCIFAR-100で最先端の結果を達成することを示しています。このような非セマンティクスを保持する（結果としてあいまいな）敵対的なデータがターゲットのロバストネスを損なうと仮定します。モデル..敵対的トレーニングは、トレーニングデータに敵対的な例を含めることにより、ディープニューラルネットワーク（DNN）の敵対的な堅牢性を向上させる防御技術です。 
[ABSTRACT]敵対的なトレーニング（spat）は、すべてのclass.testの結果で共有されるピクセルの摂動を促進します。テスト結果は、敵対的な例が元のデータとは異なる意味を持つことが多く、意図しないバイアスがモデルに導入されることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: A Simple Yet Effective Method for Video Temporal Grounding with
  Cross-Modality Attention -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_23.html">
      <font color="black">A Simple Yet Effective Method for Video Temporal Grounding with
  Cross-Modality Attention</font>
    </a>
  </h2>
  <font color="black">最後に、これまでの作品では、アクション境界のあいまいさのために避けられないアノテーションバイアスを考慮していません。まず、既存のメソッドのほとんどは、タスクを解決するために複数の複雑なモジュールの組み合わせに依存しています。これらの制限に対処するために、簡単な直感的な構造設計を備えた2分岐Cross-Modality Attention（CMA）モジュール。2つのモダリティを交互に変調して、ローカルとグローバルの両方で情報のマッチングを改善します。 
[ABSTRACT]シンプルでシンプルな2分岐のクロスモダリティの質問を使用して、ローカルとグローバルの両方で情報を照合できます。このモデルは、両方のcharades-staとactivityotaデータセットの最先端の技術を上回ることができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Non-Unique Segmentation with Reward-Penalty Dice Loss -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_24.html">
      <font color="black">Learning Non-Unique Segmentation with Reward-Penalty Dice Loss</font>
    </a>
  </h2>
  <font color="black">実験結果は、RPDLがDCNNモデルのパフォーマンスを、収集した手術データセットの他の損失関数と比較して最大18.4％改善することを示しています。非一意のセグメンテーションタスクを包括的に学習するために、報酬ペナルティダイス損失（RPDL）関数を提案します。ディープたたみ込みニューラルネットワーク（DCNN）の最適化の目的。RPDLは、共通領域を強化し、外部領域にペナルティを課すことにより、DCNNが非一意のセグメンテーションを学習するのに役立ちます。 
[ABSTRACT]セマンティックセグメンテーションのほとんどの研究とアプリケーションは、固有のパフォーマンス問題に対処することに焦点を当てており、すべての入力画像に対して1つのゴールドスタンダード結果しかありません。異なる外科医が同じ患者に対して成功した手術を行う可能性があるため、固有でないセグメンテーションアノテーションがある場合があります少し違う方法</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: What is the Reward for Handwriting? -- Handwriting Generation by
  Imitation Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_25.html">
      <font color="black">What is the Reward for Handwriting? -- Handwriting Generation by
  Imitation Learning</font>
    </a>
  </h2>
  <font color="black">実際、人間の手書きプロセスは、彼らの将来の計画能力によってもサポートされています。たとえば、マルコフモデルなどの近視眼モデルでは生成できないため、「0」のような閉じた軌道を生成する機能が必要です。手書き生成プロセスの分析は重要な問題であり、さまざまな生成モデルで取り組んできましたが、運動学ベースのモデルや確率モデルなど。典型的なRLアルゴリズムでは、報酬関数を手動で定義する必要があります。これは、生成プロセスの制御に非常に重要です。 
[ABSTRACT]この研究では、報酬学習（rl）フレームワークを使用して、慎重な将来計画機能を備えた手書き生成を実現します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Multiple interaction learning with question-type prior knowledge for
  constraining answer search space in visual question answering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_26.html">
      <font color="black">Multiple interaction learning with question-type prior knowledge for
  constraining answer search space in visual question answering</font>
    </a>
  </h2>
  <font color="black">この論文では、異なるタイプからの質問に答える際の行動に基づいて、異なる共同モダリティ方法間の複数の相互作用を活用することにより、VQAを改善するために質問タイプの事前情報を利用する新しいVQAモデルを提案します。2つのベンチマークデータセットでの固体実験つまり、VQA 2.0とTDIUCは、提案された方法が最も競争力のあるアプローチで最高のパフォーマンスを生み出すことを示しています。視覚的質問応答（VQA）にはさまざまなアプローチが提案されています。 
[要約]提案された方法は、最も競争力のあるアプローチで最高のパフォーマンスを提供します。新しい方法は、2つのベンチマークデータセットに基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Hamming OCR: A Locality Sensitive Hashing Neural Network for Scene Text
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_27.html">
      <font color="black">Hamming OCR: A Locality Sensitive Hashing Neural Network for Scene Text
  Recognition</font>
    </a>
  </h2>
  <font color="black">また、フィードフォワードネットワークを削除し、クロスレイヤーパラメーター共有技術を使用することでパラメーターの数を削減する簡略化されたトランスデコーダーを示します。従来の方法と比較して、分類レイヤーと埋め込みレイヤーの両方のパラメーター数はサイズに依存しません。正確性を失わずにストレージ要件を大幅に削減する語彙の数。それは、特に中国語および複数の言語に適用される軽量のテキスト認識モデルの開発を妨げます。 
[ABSTRACT]モデルのサイズはレキシコンの増加に伴い急速に拡大します。軽量のテキスト認識モデルの開発を妨げます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: JRDB: A Dataset and Benchmark of Egocentric Visual Perception for
  Navigation in Human Environments -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_28.html">
      <font color="black">JRDB: A Dataset and Benchmark of Egocentric Visual Perception for
  Navigation in Human Environments</font>
    </a>
  </h2>
  <font color="black">データセットには、15 fpsのステレオ円柱360 $ ^ \ circ $ RGBビデオ、2つのVelodyne 16 Lidarからの3D点群、2つのSick Lidarからのライン3D点群、オーディオ信号、RGB-Dビデオを含む64分の注釈付きマルチモーダルセンサーデータが含まれています30 fpsで、魚眼カメラからの360 $ ^ \ circ $球面画像とロボットの車輪からのエンコーダー値。データセットと注釈とともに、2Dおよび3Dの人の検出と追跡のためのベンチマークとメトリックを起動します。このデータセットは、今後さらに注釈のタイプを拡張する予定です。エゴセントリックロボットビジョン、自律ナビゲーション、および社会に関するすべての知覚的タスクの分野での研究のための新しいデータソースとテストベンチを提供したいと考えています人間環境におけるロボット工学。 
[ABSTRACT]データセットには、200万個を超える境界ボックスが付けられています。2つのベロダイン16ライダーからの3D点群が含まれています。データセットには、テスト-エゴセントリックロボットビジョンの領域の研究用ベンチが含まれています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-25">
        <br><font color="black">2019-10-25</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Feature Learning for Event Data: Direct vs Inverse Problem
  Formulation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_29.html">
      <font color="black">Unsupervised Feature Learning for Event Data: Direct vs Inverse Problem
  Formulation</font>
    </a>
  </h2>
  <font color="black">私たちの実証結果は、イベントデータからの表現学習の両方のアプローチの利点を強調しています。各アプローチの主な利点を特定して示します。理論的には、最適なソリューションの保証、非同期、並列パラメーターの更新の可能性、および計算複雑。 
[ABSTRACT] 2つの一般的な問題の解釈のパフォーマンスを分析します。ローカルイベントデータからの教師付き特徴学習。直接および逆は教師なし特徴学習用です。これらは、最新の方法との比較を提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Attention with Multiple Sources Knowledges for COVID-19 from CT Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_30.html">
      <font color="black">Attention with Multiple Sources Knowledges for COVID-19 from CT Images</font>
    </a>
  </h2>
  <font color="black">広範な実験は、最近のベースラインと比較して私たちのアプローチの優れたパフォーマンスを示しています。最近、CTスキャンに基づくCOVID-19診断にディープネットワークを利用する取り組みが増えています。この手順により、システムのノイズに対する堅牢性だけでなく、局所病変領域に焦点を当てたネットワークを導きます。 
[要約] covid-19の早期診断は時期尚早である可能性があります。これらのアプローチは主に、新しいアーキテクチャの導入、転移学習技術、または大規模データの構築に重点を置いています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Spatial-Temporal Block and LSTM Network for Pedestrian Trajectories
  Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_31.html">
      <font color="black">Spatial-Temporal Block and LSTM Network for Pedestrian Trajectories
  Prediction</font>
    </a>
  </h2>
  <font color="black">この論文では、新しいLSTMベースのアルゴリズムを提案します。2つの公開データセット、すなわちシーン内の各歩行者はノードと見なされ、グラフの埋め込みによって各ノードとその近傍の関係を取得できます。 
[要約]この論文では、新しいlstmベースのアルゴリズムを提案します。問題は、社会的勢力と乱雑なシーンが原因です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-22">
        <br><font color="black">2020-09-22</font>
      </time>
    </span>
</section>
<!-- paper0: ZSCRGAN: A GAN-based Expectation Maximization Model for Zero-Shot
  Retrieval of Images from Textual Descriptions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_32.html">
      <font color="black">ZSCRGAN: A GAN-based Expectation Maximization Model for Zero-Shot
  Retrieval of Images from Textual Descriptions</font>
    </a>
  </h2>
  <font color="black">複数のベンチマークデータセットでの実験により、提案されたモデルは、いくつかの最新のゼロショットテキストから画像検索モデル、および検索に適切に使用されるゼロショット分類およびハッシュモデルよりも優れた性能を発揮することが示されています。クエリとして、私たちのモデルはゼロショット設定で関連画像を取得できます。実際には、取得モデルをゼロショットIR設定を意味する以前に表示されていないクラスに展開する必要がある場合があります。 
[要約]提案されたモデルは、期待値-最大化フレームワークを使用してトレーニングされます。ゼロショットテキストのセットに基づいています。これらのモデルは、ショットシステムを使用してトレーニングされます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: A Unified Weight Learning and Low-Rank Regression Model for Robust
  Complex Error Modeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_33.html">
      <font color="black">A Unified Weight Learning and Low-Rank Regression Model for Robust
  Complex Error Modeling</font>
    </a>
  </h2>
  <font color="black">ランダムノイズの場合、エラー分布に一致するように一般化されたcorrentropy（GC）関数を定義します。回帰ベースのエラーモデルで最も重要な問題の1つは、画像のさまざまな破損や環境の変化によって生じる複雑な表現エラーのモデリングです。ただし、既存の作品は複雑な破損エラーを適切にモデル化できないため、この問題を解決するのに十分なほど堅牢ではありません。 
[ABSTRACT]提案されたモデルは、エラーの分布と構造に非常によく適合できます。エラーのランクを測定するために、gc関数に基づくランクの類似性に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-10">
        <br><font color="black">2020-05-10</font>
      </time>
    </span>
</section>
<!-- paper0: Content-based Propagation of User Markings for Interactive Segmentation
  of Patterned Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_34.html">
      <font color="black">Content-based Propagation of User Markings for Interactive Segmentation
  of Patterned Images</font>
    </a>
  </h2>
  <font color="black">最終的なピクセル分類は、非常に控えめなユーザー入力から取得できます。画像とボリュームの効率的で簡単なセグメンテーションは、実用上非常に重要です。リアルタイムのフィードバックにより、ユーザーは現在の状況に応じて新しいマーキングを戦略的に配置できます。結果。 
[ABSTRACT]私たちの方法では、ユーザーがピクセルのサブセットをインタラクティブにマークすることで、目的の構造を定義できます。最終的なピクセル分類は、非常に控えめなユーザー入力から取得できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-09-06">
        <br><font color="black">2018-09-06</font>
      </time>
    </span>
</section>
<!-- paper0: Augmented Convolutional LSTMs for Generation of High-Resolution Climate
  Change Projections -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_35.html">
      <font color="black">Augmented Convolutional LSTMs for Generation of High-Resolution Climate
  Change Projections</font>
    </a>
  </h2>
  <font color="black">この作業では、統計的ダウンスケーリング用の補助変数に基づく時空間ニューラルアーキテクチャを提示します。局所的な気候変数（温度や降水量など）と複雑な非線形相互依存性、および大規模予測子（圧力場など）が使用の動機となります。ニューラルネットワークベースの超解像アーキテクチャの概要。ESMの高解像度投影を取得するための現在のソリューションには、ローカルスケールで予測を行うために粗いスケールで情報を考慮するダウンスケーリングアプローチが含まれます。 
[ABSTRACT]最新の地球システムモデル（esms）は、数百キロメートルの空間解像度で利用できます。これには、1.15度（115 km近く）のESM出力から0. 25までの降雨のマイクロスケーリングが含まれます度（25 km）。パブリックドメインですべてのコード、処理されたデータセット、トレーニング済みモデルを利用できるようにします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Whole Slide Images based Cancer Survival Prediction using Attention
  Guided Deep Multiple Instance Learning Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_36.html">
      <font color="black">Whole Slide Images based Cancer Survival Prediction using Attention
  Guided Deep Multiple Instance Learning Networks</font>
    </a>
  </h2>
  <font color="black">全体のスライド画像（WSI）から派生したキーパッチまたはクラスターに制限する現在の画像ベースの生存モデルとは異なり、シャムMI-FCNと注意ベースのMILプーリングの両方を導入することにより、ディープアテンションマルチインスタンスサバイバルラーニング（DeepAttnMISL）を提案します。 WSIからイメージング機能を効率的に学習し、WSIレベルの情報を患者レベルに集約します。2つの大きな癌全体のスライド画像データセットで手法を評価し、提案されたアプローチがより効果的で大規模なデータセットに適しており、正確な癌生存予測に寄与する重要なパターンと機能を見つける際のより良い解釈可能性。注意ベースの集計は、最近の生存モデルの集計手法よりも柔軟で適応性があります。 
[要約]複数インスタンス学習（mil）フレームワークは、患者のリスクを予測するために使用できます。個別化医療の提供を支援するためにも使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Hiearchical Multi-Label Classification Applied to Chest X-Ray
  Abnormality Taxonomies -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_37.html">
      <font color="black">Deep Hiearchical Multi-Label Classification Applied to Chest X-Ray
  Abnormality Taxonomies</font>
    </a>
  </h2>
  <font color="black">完全なラベルを使用する場合、平均AUCは0.887と報告されますが、このデータセットではまだ報告されている最高値です。私たちの知る限り、HMLCを医療画像CADに適用したのは初めてです。具体的なパフォーマンス向上を提供する無条件確率の安定したクロスエントロピー損失関数。 
[要約] CADの使いやすさには、高い分類精度と意味のあるモデル予測が重要です。plcoデータセットのcxrアームから異常ラベルを検出するアプローチを示しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-11">
        <br><font color="black">2020-09-11</font>
      </time>
    </span>
</section>
<!-- paper0: Label-Efficient Multi-Task Segmentation using Contrastive Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_38.html">
      <font color="black">Label-Efficient Multi-Task Segmentation using Contrastive Learning</font>
    </a>
  </h2>
  <font color="black">アノテーション付きデータの量が限られている場合、提案手法が最先端の完全監視モデルを含む他のマルチタスク手法よりも優れていることを実験的に示します。この研究では、対照的なマルチタスクセグメンテーションモデルを提案します。学習ベースのサブタスクを実行し、そのパフォーマンスを他のマルチタスクモデルと比較して、トレーニング用のラベル付きデータの数を変化させます。3D医用画像の注釈の取得は、セグメンテーションタスクの自動化の重要性にもかかわらず、費用と時間がかかります。 
[要旨]モデルをさらに拡張して、正規化ブランチを通じてラベルなしデータを利用できるようにします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: RAF-AU Database: In-the-Wild Facial Expressions with Subjective Emotion
  Judgement and Objective AU Annotations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_39.html">
      <font color="black">RAF-AU Database: In-the-Wild Facial Expressions with Subjective Emotion
  Judgement and Objective AU Annotations</font>
    </a>
  </h2>
  <font color="black">自動表情認識に関する作業の多くは、Ekmanの基本的な感情理論に基づいて、特定の数の感情クラスとその誇張された顔の構成（通常は6つの典型的な表情）を含むデータベースに依存しています。この問題に対処するために、RAF-野生のブレンドされた顔の表情に注釈を付けるために、サインベース（つまりAU）と判断ベース（つまり、知覚された感情）のアプローチを採用するAUデータベース。最初に、既存のデータベースのアノテーション方法をレビューし、クラウドソーシングを有望な戦略として特定しました。野生の顔の表情にラベルを付けるため。 
[ABSTRACT]人間の生活の中での顔の表情は、複数の基本的な感情とブレンドできます。顔の表情など、最も複雑な表情は未解決の問題です。人気の機能とマルチラベルを使用して、raf-auのau認識のベースラインを提供しました学習方法</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-12">
        <br><font color="black">2020-08-12</font>
      </time>
    </span>
</section>
<!-- paper0: Perceptual Quality-preserving Black-Box Attack against Deep Learning
  Image Classifiers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_40.html">
      <font color="black">Perceptual Quality-preserving Black-Box Attack against Deep Learning
  Image Classifiers</font>
    </a>
  </h2>
  <font color="black">問題は、画像の品質を損なうことなくニューラルネットワークを誤解させる効率的な攻撃を設計することです。実際のシステムでの数値実験は、ベンチマーク分類タスクと、バイオメトリクスおよびフォレンジックの主要なアプリケーションの両方で、提案されたアプローチの有効性を証明します。作業では、攻撃の効率と敵対的な画像の知覚品質の両方を改善するために、低歪みパスに沿ってブラックボックス攻撃を実行することを提案します。 
[ABSTRACT]詳細な調査により、敵対的攻撃に対する脆弱性が示されています。調査はブラックボックスシステムを分析することによって実施されています。これは、パフォーマンスと知覚トレーニングの改善に使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-02-20">
        <br><font color="black">2019-02-20</font>
      </time>
    </span>
</section>
<!-- paper0: Anisotropic 3D Multi-Stream CNN for Accurate Prostate Segmentation from
  Multi-Planar MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_41.html">
      <font color="black">Anisotropic 3D Multi-Stream CNN for Accurate Prostate Segmentation from
  Multi-Planar MRI</font>
    </a>
  </h2>
  <font color="black">アーキテクチャの2つのバリアントを調査し、それぞれ2つ（デュアルプレーン）および3つ（トリプルプレーン）の画像の向きで機能します。それらを、文献で使用されている標準ベースライン（シングルプレーン）と比較します。セグメンテーション..公正な比較を実現するために、ハイパーパラメーター最適化戦略を採用して、個々のアプローチに最適な構成を選択します。 
[要約] cnnのセグメンテーションアプローチの大部分は、mrスキャンのみを考慮します。進行状況は特にベースで観察できると彼は言います</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Depth-Adapted CNN for RGB-D cameras -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_42.html">
      <font color="black">Depth-Adapted CNN for RGB-D cameras</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、CNNアーキテクチャで測光情報と幾何情報の両方を明確にする新しい一般的な手順を提案します。提示された新しいモデルは、カメラ座標系のX軸とY軸の周りのスケールと回転に対して不変です。深度データは次のように表されます。空間サンプリング位置を調整するための2Dオフセット。 
[ABSTRACT]これらのフィルターは、幾何学的情報を考慮せずに、固定された近傍の測光情報に重みを付けることで空間コヒーレンスを計算します。最先端のアプローチでは、追加のチャネルまたは画像（hha）として深度を使用するか、2d cnnから3dに渡しますcnn</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: CA-Net: Comprehensive Attention Convolutional Neural Networks for
  Explainable Medical Image Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_43.html">
      <font color="black">CA-Net: Comprehensive Attention Convolutional Neural Networks for
  Explainable Medical Image Segmentation</font>
    </a>
  </h2>
  <font color="black">正確な医用画像セグメンテーションは、疾患の診断と治療計画に不可欠です。ISIC2018からの皮膚病変セグメンテーションおよび胎児MRIのマルチクラスセグメンテーションに関する広範な実験により、提案されたCA-Netは、平均セグメンテーションダイススコアを87.77％から大幅に改善しました。 U-Netと比較して、皮膚病変は92.08％、胎盤は84.79％から87.08％、胎児の脳は93.20％から95.88％。医用画像セグメンテーション。 
[ABSTRACT]畳み込みニューラルネットワーク（cnns）は、自動医用画像セグメンテーションの扱いが複雑です。より正確で説明可能な医療処置のために包括的な注意ベースのcnn（ca-net）を提案します。既存のネットワークよりもはるかに優れた説明可能性があると言います注意の重みマップを視覚化することにより</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-22">
        <br><font color="black">2020-09-22</font>
      </time>
    </span>
</section>
<!-- paper0: Accurate Retinal Vessel Segmentation via Octave Convolution Neural
  Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_44.html">
      <font color="black">Accurate Retinal Vessel Segmentation via Octave Convolution Neural
  Network</font>
    </a>
  </h2>
  <font color="black">網膜血管セグメンテーションは、糖尿病、眼科疾患、心血管疾患などのさまざまな疾患の診断とスクリーニングにおける重要なステップです。特徴抽出に標準の畳み込みを利用する他の畳み込みネットワークと比較して、提案された方法は、複数の学習にオクターブ畳み込みとオクターブ転置畳み込みを利用します。 -空間周波数機能により、さまざまなサイズと形状の網膜血管系をより適切にキャプチャできます。オクターブ畳み込みとオクターブ転置畳み込みの両方を統合するOctave UNetと呼ばれる畳み込みニューラルネットワークの新しいアーキテクチャが、UNetのエンコーダーデコーダアーキテクチャに基づいて提案されています。 、後処理ステップなしで単一の順送りで高解像度の血管セグメンテーションを生成できます。 
[要約]提案された方法は、エンコーダー-デコーダーベースのオクターブたたみ込みネットワークの使用を含みます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-28">
        <br><font color="black">2019-06-28</font>
      </time>
    </span>
</section>
<!-- paper0: Robust and efficient post-processing for video object detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_45.html">
      <font color="black">Robust and efficient post-processing for video object detection</font>
    </a>
  </h2>
  <font color="black">私たちの方法は、特に高速で移動するオブジェクトに関して、最先端の特定のビデオ検出器の結果を改善し、リソース要件を低く抑えます。ビデオでのオブジェクト認識は、自動運転知覚、監視タスクなど、多くのアプリケーションにとって重要なタスクです、ウェアラブルデバイス、またはIoTネットワーク。さらに、YOLOなどの効率的な静止画像検出器に適用すると、計算量の多い検出器に匹敵する結果が得られます。 
[ABSTRACT]動画データを使用したオブジェクト認識は、高速オクルージョンまたはまれなオブジェクトポーズのため、静止画像を使用するよりも困難です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Learnergy: Energy-based Machine Learners -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_46.html">
      <font color="black">Learnergy: Energy-based Machine Learners</font>
    </a>
  </h2>
  <font color="black">過去数年間、ディープラーニングアーキテクチャのコンテキストで機械学習手法が広く奨励されてきました。そのため、このホワイトペーパーでは、エネルギーベースのアーキテクチャのコンテキストでPythonにヒントを得たフレームワークを提案し、Learnergyと表記します。基本的に、 LearnergyはPyTorchに基づいて構築されており、よりフレンドリーな環境とより高速なプロトタイピングワークスペース、そして場合によってはCUDA計算の使用を提供し、計算時間を短縮しています。 
[ABSTRACT] learningergyはpytorchに基づいて構築され、よりフレンドリーな環境を提供します。これにより、プロトタイピングワークスペースが高速になり、場合によってはcuda計算が使用されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-16">
        <br><font color="black">2020-03-16</font>
      </time>
    </span>
</section>
<!-- paper0: X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal
  Transformers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_47.html">
      <font color="black">X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal
  Transformers</font>
    </a>
  </h2>
  <font color="black">最後に、画像生成機能をUNITERに追加してX-UNITERを生成することにより、これらのトレーニングの改良点の一般性を示します。X-LXMERTは、LXMERTの拡張であり、次のようなトレーニングの改良点があります。視覚表現の離散化、広範囲の均一なマスキングの使用比率をマスキングし、適切な事前トレーニングデータセットを適切な目的に合わせて調整し、ペイントできるようにします。マスキングされた言語モデルの成功を反映して、ViLBERT、LXMERT、UNITERなどのビジョンと言語の対応物は、最先端のパフォーマンスを実現しています。視覚的な質問応答や視覚的なグラウンディングなど、さまざまなマルチモーダル識別タスク。 
[ABSTRACT] lxmertの画像生成の最先端のトレーニング機能は、最先端のzativeモデルに匹敵します。この研究は、他のモデルを画像キャプションの重要なタスクに統合します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: CLASS: Cross-Level Attention and Supervision for Salient Objects
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_48.html">
      <font color="black">CLASS: Cross-Level Attention and Supervision for Salient Objects
  Detection</font>
    </a>
  </h2>
  <font color="black">次に、顕著なオブジェクトの微細構造と境界を適切に復元できます。5つのデータセットで、13の最先端の方法よりも常に優れています。ただし、既存の方法では十分に対処できない2つの厄介な問題が存在します。区別できない領域と複雑な構造です。 
[要約]この論文は、正確な非音のための新しいレベルレベルネットワークを提案します。これら2つの問題に対処するために、新しいレベルレベルシステムを提案します。これは、ピクセルレベル、領域レベル、およびオブジェクト-レベル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: A Linear Transportation $\mathrm{L}^p$ Distance for Pattern Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_49.html">
      <font color="black">A Linear Transportation $\mathrm{L}^p$ Distance for Pattern Recognition</font>
    </a>
  </h2>
  <font color="black">ただし、それらの計算コストにより、中程度のパターン認識タスクにも適用できない場合があります。これらの距離は、$ \ mathrm {W} ^ p $と同様に、空間的または時間的摂動を含むデータのモデリングにおける強力なツールです。線形を提案しますこれらの距離のバージョンは、線形の$ \ mathrm {TL} ^ p $距離が、信号処理タスクの線形の$ \ mathrm {W} ^ p $距離よりも大幅に向上する一方で、 $ \ mathrm {TL} ^ p $の距離。 
[要約]これらの距離がlに提示されたのはこれが初めてです。 p $ distance。これらは、空間的または熱的ペルシアシアシアシアシアシアシアシアシアシアシアシアシアシアシアシアシアシアシアシアシスを伴うデータのモデリングにおける強力なツールです。これらの結果は、直線的な$ p $＃＃＃距離が大幅に向上する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Scene Graph to Image Generation with Contextualized Object Layout
  Refinement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_50.html">
      <font color="black">Scene Graph to Image Generation with Contextualized Object Layout
  Refinement</font>
    </a>
  </h2>
  <font color="black">ただし、シーングラフは指定が不十分であるため、トレーニングデータの多くのターゲット画像で同じシーングラフが発生することがよくあります。このモデルでは、オブジェクトレイアウトを徐々にアップサンプリング、リファイン、およびコンテキスト化することで、埋め込みから（中間ボックスを予測せずに）レイアウトを直接予測します。これは、画像の生成、関係の実現、オブジェクトの品質の向上につながります。 
[ABSTRACT]教師付き学習を使用して以前の作業でトレーニングされたモデル。目標は各シーングラフの正確なターゲット画像レイアウトを作成することです。この作業では、すべてのオブジェクトレイアウトを一緒に生成することにより、これらの問題を軽減する方法を提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Hierarchical Predictive Coding Models in a Deep-Learning Framework -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_51.html">
      <font color="black">Hierarchical Predictive Coding Models in a Deep-Learning Framework</font>
    </a>
  </h2>
  <font color="black">このプロトコルは、いくつかのかなり強い統計的仮定を持つベイジアン生成モデルから導出されました。神経科学コミュニティに起源はありますが、これらのモデルを研究するために機械学習コミュニティにも取り組みがあります。予測された入力と実際の入力の不一致予測エラーが発生すると、学習した高レベルの表現の予測精度を改善して改善する将来の学習が発生します。 
[要約]研究はカリフォルニア大学の研究者によって行われました。モジュールの接続性と情報転送のパターンに注目しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-07">
        <br><font color="black">2020-05-07</font>
      </time>
    </span>
</section>
<!-- paper0: A Real-time Vision Framework for Pedestrian Behavior Recognition and
  Intention Prediction at Intersections Using 3D Pose Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_52.html">
      <font color="black">A Real-time Vision Framework for Pedestrian Behavior Recognition and
  Intention Prediction at Intersections Using 3D Pose Estimation</font>
    </a>
  </h2>
  <font color="black">対応する研究コミュニティに貢献するために、https：//github.com/Uehwan/VisionForPedestrianから入手できるソースコードを公開しています。提案されたビジョンフレームワークは、トレーニングプロセスなしでTUDデータセットの行動認識タスクで89.3％の精度を実現し、データセットの意図予測で91.28％の精度で新しい最先端のパフォーマンスを実現します。3Dポーズ分析により、堅牢で歩行者の行動の正確な認識と複数のサイトにわたる意図の予測。 
[要約]ビジョンフレームワークは、歩行者の行動の認識と、一般的な横断または非横断の意図の予測のために提案されています。この概念は、目標を確実にすることを目的としていますが、実行する必要はありません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Federated Learning for Computational Pathology on Gigapixel Whole Slide
  Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_53.html">
      <font color="black">Federated Learning for Computational Pathology on Gigapixel Whole Slide
  Images</font>
    </a>
  </h2>
  <font color="black">複数の機関にまたがる医療データのマルチセントリックで協調的な統合は、当然この課題を克服してモデルのパフォーマンスを向上させるのに役立ちますが、モデルが数十万の使用に向けて拡大するにつれて複雑なデータ共有プロセスで発生する可能性のある他の困難の中でプライバシーの懸念によって制限されますギガピクセルの全体のスライド画像..このホワイトペーパーでは、弱監視の注意マルチインスタンス学習と微分プライバシーを使用して、計算病理学におけるギガピクセルの全体のスライド画像のプライバシー保護連合学習を紹介します。組織学全体のスライド画像で、スライドレベルのラベルのみ。 
[要約]ディープラーニングの開発-データセットに基づいて、多くの場合、収集と時間に依存します-コストのかかるキュレーション。これらのデータは、理想的には、そのようなデータセットに存在する不均一性に対応するために、さまざまなソースと患者集団から取得する必要があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Foreseeing Brain Graph Evolution Over Time Using Deep Adversarial
  Network Normalizer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_54.html">
      <font color="black">Foreseeing Brain Graph Evolution Over Time Using Deep Adversarial
  Network Normalizer</font>
    </a>
  </h2>
  <font color="black">テストの進化の軌跡は、選択されたトレーニンググラフとそれに対応する進化の軌跡によって広がります。いくつかの比較方法に対する一連のベンチマークは、提案された方法が単一のベースラインタイムポイントを使用して最低の脳疾患の進化予測誤差を達成したことを示しました。各脳ネットワークを固定中心集団駆動型接続テンプレートの変換として表すための敵対的な脳ネットワークノーマライザを設計します。 
[要約]脳グラフ進化モデルは文献にほとんど存在しません。脳ガンモデルを分析して、テスト脳法の進化を予測できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Leveraging Local and Global Descriptors in Parallel to Search
  Correspondences for Visual Localization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_55.html">
      <font color="black">Leveraging Local and Global Descriptors in Parallel to Search
  Correspondences for Visual Localization</font>
    </a>
  </h2>
  <font color="black">この単純な組み合わせでは、ローカル記述子とグローバル記述子の融合の優位性は達成されていません。2D画像の各点は、2Dと3Dの点の対応を実行するときに、クエリローカル機能とも呼ばれます。1つは、各画像から機能全体を抽出するグローバル記述子です。 。 
[ABSTRACT] 2つの段階はほとんどのメソッドで連続しています。記述子は、各画像のパッチからローカルフィーチャを抽出します。記述子には、ローカルフィーチャを抽出するローカル記述子もあります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Residual Embedding Similarity-Based Network Selection for Predicting
  Brain Network Evolution Trajectory from a Single Observation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_56.html">
      <font color="black">Residual Embedding Similarity-Based Network Selection for Predicting
  Brain Network Evolution Trajectory from a Single Observation</font>
    </a>
  </h2>
  <font color="black">健康な脳ネットワークと無秩序な脳ネットワークの両方での実験は、RESNetsアブレーションバージョンや従来のアプローチと比較して、提案された方法の成功を実証しています。グラフの畳み込みネットワークを介してトポロジー特性を保持しながら、脳ネットワークの高次元性。 
[ABSTRACT] resnetはまず、各データのコンパクトな幾何学的埋め込みを学習します。これにより、脳のコネクトームの密度が維持されます。これは、脳のコネクトームの構成を悪用して保存することができないと言います</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Point Cloud Semantic Segmentation by Learning 3D Object
  Proposal Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_57.html">
      <font color="black">Improving Point Cloud Semantic Segmentation by Learning 3D Object
  Proposal Generation</font>
    </a>
  </h2>
  <font color="black">さらに、DASSを使用して既存の2ステージ検出器の高再現率の提案を生成するパイプラインを提供し、追加された監視信号を使用して3D方向推定機能を改善できることを示します。新しい検出対応3Dセマンティックセグメンテーション（DASS）フレームワークを提案します。補助3Dオブジェクト検出タスクからのローカリゼーション機能を明示的に利用します。SemanticKITTIおよびKITTIオブジェクトデータセットの広範な実験により、DASSは高精度のBEVを維持しながら、画像FOVの幾何学的に類似したクラスの3Dセマンティックセグメンテーション結果を37.8％IoUまで改善できることが示されています。検出結果。 
[要旨] ossは、幾何学的に類似したクラスの3Dセマンティックセグメンテーション結果を最大37まで改善できます。画像fovの8％iou</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-22">
        <br><font color="black">2020-09-22</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic Breast Lesion Classification by Joint Neural Analysis of
  Mammography and Ultrasound -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_58.html">
      <font color="black">Automatic Breast Lesion Classification by Joint Neural Analysis of
  Mammography and Ultrasound</font>
    </a>
  </h2>
  <font color="black">提案されたアプローチは、GoogleNetアーキテクチャに基づいており、2つのトレーニングステップでデータを微調整します。まず、個別のニューラルネットワークをモダリティごとに個別にトレーニングし、高レベルの機能を生成します。この作業では、それぞれのマンモグラフィと超音波画像から乳癌病変を分類するための学習ベースの方法。 
[ABSTRACT]現在、乳房の既存のコンピューター支援診断システムは、一般に単一のモーダルに基づいています。提案されたアプローチは、0のaucを達成します。94。乳房放射線科医にとって有用な意思決定支援ツールになる可能性があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Fast Kernelized Correlation Filters without Boundary Effect -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_59.html">
      <font color="black">Fast Kernelized Correlation Filters without Boundary Effect</font>
    </a>
  </h2>
  <font color="black">非線形カーネルは、既存のCFトラッカーとは理論的に異なるため、nBEKCFで自然に適用できます。境界効果を完全に回避するために、\ emph {real}および\ emph {dense}パッチのセットは、従来のスライディングウィンドウであり、ガウス応答マップに適合するようにnBEKCFをトレーニングするためのトレーニングサンプルとして使用されています。効率。 
[ABSTRACT] acsiiとccimは、トレーニングサンプルの密度と基底の循環構造を完全に活用し、空間ドメインで完全に実行します。nbekcfは、トリックなしでローカリゼーションの精度を高め、その間、より高いfpsで実行します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-06-17">
        <br><font color="black">2018-06-17</font>
      </time>
    </span>
</section>
<!-- paper0: 2D-3D Geometric Fusion Network using Multi-Neighbourhood Graph
  Convolution for RGB-D Indoor Scene Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CV/paper_60.html">
      <font color="black">2D-3D Geometric Fusion Network using Multi-Neighbourhood Graph
  Convolution for RGB-D Indoor Scene Classification</font>
    </a>
  </h2>
  <font color="black">2番目に提案されたレイヤー、ニアレストボクセルプーリングは、よく知られているボクセルプーリングのパフォーマンスを向上させます。NYU-Depth-v2およびSUN RGB-Dデータセットを使用した実験結果は、提案された方法が現在の状態よりも優れていることを示しています。 -art in RGB-D屋内シーン分類タスク。マルチモーダルフュージョンは、シーン分類タスクのパフォーマンス向上に役立つことが証明されています。 
[ABSTRACT]マルチ-近傍グラフの畳み込みは、よりロバストな幾何学的記述子を学習することを目的としています。提案された方法は、rgb-d屋内シーン分類タスクで現在の最新技術よりも優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Cosine Similarity of Multimodal Content Vectors for TV Programmes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_0.html">
      <font color="black">Cosine Similarity of Multimodal Content Vectors for TV Programmes</font>
    </a>
  </h2>
  <font color="black">私たちのベクトル表現は、オーディオ、LSIトピック、字幕のDoc2vec埋め込み用のスペクトル機能とオーディオワードのバッグから構築され、メタデータ用のカテゴリー機能です。推奨を提供します。後期融合類似性マトリックスは、推奨の精度と多様性を大幅に改善します。 
[ABSTRACT]個々のソースからのコンテンツをどのように表現できるかを示します。BBCテレビ番組のデータセットでモデルを示しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: The Persian Dependency Treebank Made Universal -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_1.html">
      <font color="black">The Persian Dependency Treebank Made Universal</font>
    </a>
  </h2>
  <font color="black">私たちのデータは、監視付き解析で85.2のラベル付き添付ファイルFスコアをもたらします。私たちの脱字化されたペルシア語から英語へのパーサー転送実験は、データでトレーニングされた解析モデルがSerajiらの解析モデルよりも2％正確であることを示しています。 。（2016）ラベル付き添付ファイルスコアの観点から。 
[要約]普遍的樹木は人間よりも正確です。当社のデータは、ラベル付きの添付ファイルet-スコア85をもたらします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: The Struggles of Feature-Based Explanations: Shapley Values vs. Minimal
  Sufficient Subsets -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_2.html">
      <font color="black">The Struggles of Feature-Based Explanations: Shapley Values vs. Minimal
  Sufficient Subsets</font>
    </a>
  </h2>
  <font color="black">さらに、説明者が特定の機能ベースの説明を探す必要があるように見える暗黙の仮定にもかかわらず、2つの一般的なクラスの説明者、Shapley説明者と最小限の十分なサブセット説明者が根本的に異なるタイプのグラウンドトゥルース説明を対象とすることを示します。最近、入力フィーチャの関連性の観点からニューラルモデルの予測を説明することに焦点を当てた作品が増えています。これらの調査結果は、説明者の開発と選択の両方で考慮すべき追加の側面をもたらします。 
[ABSTRACT]特定のケースでは、少なくとも2つのグラウンドトゥルース機能ベースの問題があることを示します。ただし、どちらも、意思決定の完全なビューを提供するには不十分である-モデルの作成プロセス</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: ISA: An Intelligent Shopping Assistant -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_3.html">
      <font color="black">ISA: An Intelligent Shopping Assistant</font>
    </a>
  </h2>
  <font color="black">アシスタントは、購入プロセスをユーザーに案内したり、他の同様の製品をユーザーに推奨したりすることもできます。ISAの自然言語処理コンポーネントのエンジンを構築する際には、データ駆動型のアプローチを採用し、エンジンは優れたパフォーマンスを実現しています。 eコマースの中でも、実店舗は依然として多くの人に好まれる目的地です。 
[ABSTRACT] isaはモバイルベースのインテリジェントショッピングアシスタントで、実店舗でのショッピング体験を向上させるように設計されています。ユーザーは製品のバーコードを写真で撮るかスキャンするだけで、アシスタントについてアシスタントに話しかけることができます。製品</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-07">
        <br><font color="black">2020-07-07</font>
      </time>
    </span>
</section>
<!-- paper0: Can Fine-tuning Pre-trained Models Lead to Perfect NLP? A Study of the
  Generalizability of Relation Extraction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_4.html">
      <font color="black">Can Fine-tuning Pre-trained Models Lead to Perfect NLP? A Study of the
  Generalizability of Relation Extraction</font>
    </a>
  </h2>
  <font color="black">これらの調査結果は、将来の改善の機会を強調しています。オープンソースのテストベッドDiagnoseREは、https：//github.com/zjunlp/DiagnoseRE/で入手できます。提案された改善に従って、汎化手法の違いも特徴付けています。 
[ABSTRACT]テストベッドはテスト結果に使用できます。テストはテストベッドで利用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-14">
        <br><font color="black">2020-09-14</font>
      </time>
    </span>
</section>
<!-- paper0: Worst-Case-Aware Curriculum Learning for Zero and Few Shot Transfer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_5.html">
      <font color="black">Worst-Case-Aware Curriculum Learning for Zero and Few Shot Transfer</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、自動化されたカリキュラム学習を使用して、タスク全体でワーストケースを認識する新しい損失のファミリーを最小限に抑える、マルチタスク転移学習へのより不可知論的なアプローチについて説明します。また、ゼロショットおよび少数ショットの転送設定でパフォーマンスが向上します。事前トレーニング済みの言語エンコーダーに基づくマルチタスク転送学習は、さまざまなタスクで最先端のパフォーマンスを実現します。 
[ABSTRACT]標準的な学習方法は、標準的な学習システムに基づいています。標準的なアプローチでは、タスクが等しく代表的であることを確認する必要があります。これらの損失により、外れ値タスクのパフォーマンスが向上します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Crosslingual Topic Modeling with WikiPDA -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_6.html">
      <font color="black">Crosslingual Topic Modeling with WikiPDA</font>
    </a>
  </h2>
  <font color="black">2つのアプリケーションでのWikiPDAの有用性を示します。28のWikipediaエディションでのトピックバイアスの研究と、クロスリンガルの教師付き分類です。 ..人間の評価によると、WikiPDAは単一言語のテキストベースのLDAよりも一貫性のあるトピックを生成するため、無料でクロスリンガル性を提供します。 
[要約]人間の評価によると、wikipdaは、単一言語のテキストベースのldes.wikipdaのゼロショット言語転送能力よりも一貫したトピックを生成し、モデルは新しい言語で再利用されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Persona-Based Empathetic Conversational Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_7.html">
      <font color="black">Towards Persona-Based Empathetic Conversational Models</font>
    </a>
  </h2>
  <font color="black">最後に、ペルソナが共感的な応答に与える影響を調査するために、広範な実験を行います。共感的な会話モデルは、多くのドメインでユーザーの満足度とタスクの結果を改善することが示されています。次に、CoBERTを提案します。CoBERTは、データセットの最先端のパフォーマンス。 
[要約]心理学では、ペルソナが性格と高い相関関係があることが示されています。これは、共感に影響を与えます。さらに、ペルソナベースの共感的な会話に向けた新しいタスクを提案します。これは、ペルソナが共感に与える影響に関する最初の研究です応答する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-26">
        <br><font color="black">2020-04-26</font>
      </time>
    </span>
</section>
<!-- paper0: A Token-wise CNN-based Method for Sentence Compression -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_8.html">
      <font color="black">A Token-wise CNN-based Method for Sentence Compression</font>
    </a>
  </h2>
  <font color="black">同じ入力を与えられたRNNベースのモデルの1つは他のモデルよりわずかに優れていますが、CNNベースのモデルはRNNベースのアプローチよりも10倍高速でした。この問題に対処するため、このホワイトペーパーでは、トークンごとのたたみ込みニューラルネットワークは、CNNベースのモデルと、事前にトレーニングされたトランスフォーマーからの双方向エンコーダー表現（BERT）機能を備えた削除ベースの文の圧縮です。情報。 
[ABSTRACT]現在の方法は、主にデータシステムに基づいています。処理速度が遅いリカレントニューラルネットワーク（rnvo）モデルに基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-End Speech Recognition and Disfluency Removal -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_9.html">
      <font color="black">End-to-End Speech Recognition and Disfluency Removal</font>
    </a>
  </h2>
  <font color="black">ASRモデルをトレーニングして、別の流暢性検出モデルに依存せずに、流暢な発話を流暢な筆記録に直接マッピングできるかどうかを具体的に検討します。エンドツーエンドモデルが流暢な筆記録を直接生成することを学ぶことを示します。ただし、それらのパフォーマンスは、ASRシステムとディスフルエンシー検出モデルで構成されるベースラインパイプラインアプローチよりわずかに劣ります。このホワイトペーパーの調査結果は、エンドツーエンドの音声認識とディスフルエンシーのタスクに関するさらなる研究のベンチマークとして役立ちます。将来的に削除されます。 
[ABSTRACT]新しい論文は、流暢性の除去のためのモデルのタスクを調査することを目的としています。また、asrシステムと流暢さの検出を調査することを目的としています。調査結果は、さらなる研究のベンチマークとして役立ちます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-22">
        <br><font color="black">2020-09-22</font>
      </time>
    </span>
</section>
<!-- paper0: Bootstrapping a Crosslingual Semantic Parser -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_10.html">
      <font color="black">Bootstrapping a Crosslingual Semantic Parser</font>
    </a>
  </h2>
  <font color="black">実験結果は、MTが複数のMTエンジンによる言い換えで増強された場合、正確な解析のために新しい言語でトレーニングデータを概算できることを示しています。MTが不十分な場合を考慮すると、このアプローチを使用すると、完全な翻訳の2％以内の解析精度しか達成できないこともわかります。トレーニングデータの50％...機械翻訳がトレーニングデータの適切な代替物であるかどうかをクエリし、これを拡張して、英語、言い換え、および多言語の事前トレーニング済みモデルとの共同トレーニングを使用したブートストラップを調査します。 
[ABSTRACT]英語などの単一の言語でトレーニングされたセマンティックパーサーを、最小限のアノテーションで新しい言語と複数のドメインに適応させます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-06">
        <br><font color="black">2020-04-06</font>
      </time>
    </span>
</section>
<!-- paper0: A Divide-and-Conquer Approach to the Summarization of Long Documents -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_11.html">
      <font color="black">A Divide-and-Conquer Approach to the Summarization of Long Documents</font>
    </a>
  </h2>
  <font color="black">私たちの最高のモデルは、学術論文の2つの公開されている2つのデータセットで、最先端の技術と同等の結果を達成します。このアプローチが、シーケンス間RNNやトランスフォーマーを含むさまざまな要約モデルとペアになっていることを示します。は、要約パフォーマンスの改善につながる可能性があります。長いドキュメントのニューラル要約のための新しい分割統治法を提示します。 
[要旨]私たちの方法は、ドキュメントの談話構造を利用しています。このアプローチをさまざまな要約モデルと組み合わせることで、要約パフォーマンスのパフォーマンスが向上することを実証しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-13">
        <br><font color="black">2020-04-13</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Hop Fact Checking of Political Claims -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_12.html">
      <font color="black">Multi-Hop Fact Checking of Political Claims</font>
    </a>
  </h2>
  <font color="black">FEVERのほとんどのクレームには証拠文が1つだけ関連付けられており、ラベルを予測するための推論は必要ありません。2つの証拠文を持つ少数のインスタンスでは単純な推論のみが必要です。このホワイトペーパーでは、より複雑なクレーム検証を実行する方法を検討します。証拠チャンク上に複数のホップがある自然発生クレームについて。最初に、クレーム検証のための推論チェーンの小さな注釈付きデータセットPolitiHopを作成します。 
[ABSTRACT]このホワイトペーパーでは、証拠のチャンクに複数のホップがある自然発生のデータセットに対して、より複雑なクレーム検証を実行する方法を研究します。次に、データセットを他の既存のアーキテクチャと比較し、より広範な内外から知識を転送しますポリティホップへの最新のリソース</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-10">
        <br><font color="black">2020-09-10</font>
      </time>
    </span>
</section>
<!-- paper0: On the Power of Unambiguity in Büchi Complementation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_13.html">
      <font color="black">On the Power of Unambiguity in Büchi Complementation</font>
    </a>
  </h2>
  <font color="black">その結果、$ n $状態のB \ &quot;uchiオートマトンと有限のあいまいさがある場合、古典的なランクベースとスライスベースの補完構造によって構築された相補的なB \&quot; uchiオートマトンの状態数は次のようになります。それぞれ、$ 2 ^ {O（n \ log n）} $から$ 2 ^ {O（n）} $に、$ O（（3n）^ n）$から$ O（4 ^ n）$に改善されました。この作業では、各頂点に最大1つの先行ノードがある無限語上の削減実行有向非巡回グラフ（DAG）を利用して、B内オートマトンの補完問題に\ emph {unambiguity}の力を活用します。次に、このタイプの削減実行DAGを\ emph {統合ツール}として使用して、有限のあいまいさのあるB \ &quot;オートマトンのランクベースとスライスベースの補完構成を最適化する方法を示します。 
[要約]次に、このタイプの削減されたくぼみをログとして使用する方法を示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br><font color="black">2020-05-18</font>
      </time>
    </span>
</section>
<!-- paper0: Simultaneous Machine Translation with Visual Context -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_14.html">
      <font color="black">Simultaneous Machine Translation with Visual Context</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、視覚的なコンテキストが役立ち、明示的なオブジェクト領域情報に基づく視覚的に接地されたモデルが一般的に使用されるグローバル機能よりもはるかに優れていることを示しています。連続した入力テキストストリームを、可能な限り低いレイテンシと最高の品質で別の言語に翻訳します。このホワイトペーパーでは、視覚情報の追加が不足しているソースコンテキストを補うことができるかどうかを理解しようとしています。 
[要約]最先端のシミュレーションフレームワークの視覚的機能は、さまざまなマルチモーダルアプローチと視覚的機能の影響を示しています。これらの研究は、マルチモーダルシステムのみが英語から性別のマークされた言語に正しく翻訳できることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-15">
        <br><font color="black">2020-09-15</font>
      </time>
    </span>
</section>
<!-- paper0: Pre-training for Abstractive Document Summarization by Reinstating
  Source Text -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_15.html">
      <font color="black">Pre-training for Abstractive Document Summarization by Reinstating
  Source Text</font>
    </a>
  </h2>
  <font color="black">2つのベンチマーク要約データセット（つまり、CNN / DailyMailとNew York Times）での実験は、3つの目的すべてがベースラインのパフォーマンスを向上させることができることを示しています。抽象ドキュメント要約タスク..このホワイトペーパーでは、ラベルなしテキストでSeq2Seqベースの抽象要約モデルを事前トレーニングできるようにする3つの事前トレーニング目標を示します。 
[ABSTRACT]制限付きの監視された要約データで大規模なseq2seqベースの要約モデルをトレーニングすることは困難です。主な考え方は、元のドキュメントを復元するためにモデルに事前にラベルが付けられることです。これらのタスクは、ベースラインのパフォーマンスを向上させることができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-04">
        <br><font color="black">2020-04-04</font>
      </time>
    </span>
</section>
<!-- paper0: LA-HCN: Label-based Attention for Hierarchical Multi-label
  TextClassification Neural Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_16.html">
      <font color="black">LA-HCN: Label-based Attention for Hierarchical Multi-label
  TextClassification Neural Network</font>
    </a>
  </h2>
  <font color="black">さらに、ローカルとグローバルのドキュメント埋め込みは、それぞれのローカルとグローバルの分類をサポートするために別々に生成されます。私たちの実験では、LA-HCNは、他のニューラルネットワークベースの最新の状態と比較した場合、4つのパブリックHMTCデータセットで最高のパフォーマンスを達成します。 -artアルゴリズム.. LA-HCNとそのバリアントの比較は、提案されたラベルベースの注意モジュールの有効性と、ローカル分類とグローバル分類の組み合わせの使用も示しています。 
[要約] hmtcのほとんどの既存のアルゴリズムは、classifiers.laの設計に焦点を当てています-hcnは、異なるラベルに基づいてテキストから意味のある情報を抽出できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: ZSCRGAN: A GAN-based Expectation Maximization Model for Zero-Shot
  Retrieval of Images from Textual Descriptions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_17.html">
      <font color="black">ZSCRGAN: A GAN-based Expectation Maximization Model for Zero-Shot
  Retrieval of Images from Textual Descriptions</font>
    </a>
  </h2>
  <font color="black">実際には、検索モデルを以前は目に見えなかったクラスに展開する必要がある場合があります。これは、ゼロショットIR設定を意味します。複数のベンチマークデータセットでの実験は、提案されたモデルが快適にいくつかの最先端のゼロショットテキストよりも優れていることを示しています画像検索モデルだけでなく、検索に適切に使用されるゼロショット分類およびハッシュモデルにも対応しています。クエリとしてテキストによる説明を指定すると、モデルはゼロショット設定で関連画像を検索できます。 
[要約]提案されたモデルは、期待値-最大化フレームワークを使用してトレーニングされます。ゼロショットテキストのセットに基づいています。これらのモデルは、ショットシステムを使用してトレーニングされます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: KoBE: Knowledge-Based Machine Translation Evaluation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_18.html">
      <font color="black">KoBE: Knowledge-Based Machine Translation Evaluation</font>
    </a>
  </h2>
  <font color="black">さらなる研究を促進するために、WMT19メトリックトラックデータから18の言語ペアにわたって180万の実体言及を含むデータセットをリリースします。私たちのアプローチは、評価のためにWMT19ベンチマークから18の言語ペアのうち9つの人間の判断との最高の相関を達成参照なし。これは、このタスクの単一の評価方法で最大の勝利数です。4つの言語ペアでは、BLEUよりも人間の判断との相関が高くなります。 
[要約]私たちのアプローチは、各原文と候補の翻訳で見つかったエンティティの言及を根拠に基づいています。また、人間の判断とbleuよりも高い相関を実現します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: RICA: Evaluating Robust Inference Capabilities Based on Commonsense
  Axioms -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_19.html">
      <font color="black">RICA: Evaluating Robust Inference Capabilities Based on Commonsense
  Axioms</font>
    </a>
  </h2>
  <font color="black">私たちのフレームワークとプローブセットは、今後の作業でPTLMの推論能力と言語の変化に対するロバスト性を向上させ、より流動的なコミュニケーションに近づけるのに役立ちます。生成されたプローブセットの実験では、PTLMはランダムな推測よりも性能が高く（微調整を行っても）、統計的バイアスの影響を大きく受け、摂動攻撃に対して堅牢ではないことが示されています。 
[要約] ptlmsの以前の評価は、事実に関する世界の知識、または必要な知識が明示的に提供されたときに説明する能力に焦点を当てています。常識的な証言を行う能力を評価する新しい課題、リカを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-02">
        <br><font color="black">2020-05-02</font>
      </time>
    </span>
</section>
<!-- paper0: Seq2Edits: Sequence Transduction Using Span-level Edit Operations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_20.html">
      <font color="black">Seq2Edits: Sequence Transduction Using Span-level Edit Operations</font>
    </a>
  </h2>
  <font color="black">テキストの正規化、文の融合、および文法エラーの修正について、私たちのアプローチは、各編集操作を人間が読めるタグに関連付けることで説明可能性を向上させます。5つのNLPタスク（テキストの正規化、文の融合、文の分割と言い換え、テキスト簡素化、および文法エラー修正）を行い、全面的に競争力のある結果を報告します。Seq2Editsは、自然言語処理（NLP）タスクのシーケンス編集へのオープンボキャブラリーアプローチであり、入力テキストと出力テキストが高度に重複しています。 
[要旨]私たちの方法は、完全なシーケンスモデルと比較して、ためらいを最大5.2倍高速化します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: A Comparative Study on Structural and Semantic Properties of Sentence
  Embeddings -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_21.html">
      <font color="black">A Comparative Study on Structural and Semantic Properties of Sentence
  Embeddings</font>
    </a>
  </h2>
  <font color="black">これらの結果は、埋め込みベースの関係抽出方法の開発に役立つ情報を提供します。これらの埋め込み方法は、単純な単語の埋め込みの組み合わせからトランスフォーマーベースのBERTスタイルモデルに至るまで、さまざまな手法に対応します。構造的および意味的プロパティの強度の程度。 
[要約]文の埋め込みの使用には多大な労力が費やされています。テキストと構造化された知識の両方を低概念空間に埋め込むことです。これらの間のセマンティックアライメントまたはマッピングを見つけることが可能です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: On the Ability of Self-Attention Networks to Recognize Counter Languages -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_22.html">
      <font color="black">On the Ability of Self-Attention Networks to Recognize Counter Languages</font>
    </a>
  </h2>
  <font color="black">おそらく驚くべきことに、LSTMとは対照的に、トランスフォーマーは、よく知られた複雑さの尺度に従って言語をより複雑にするため、パフォーマンスが低下する通常の言語のサブセットでのみうまく機能します。トランスフォーマーは、多数のNLPタスクで反復モデルに取って代わっています..最初に、n項ブール式、Dyck-1などのよく研究された言語とその一般化を含む、カウンター言語のサブクラスのトランスフォーマーの構築を提供します。 
[ABSTRACT]さまざまな異なる構文プロパティをモデル化する能力の違いはほとんど不明のままです。ただし、さまざまな種類のさまざまなさまざまな異なる言語は不明のままです。テストでは、トランスフォーマーがこのサブクラスでうまく機能し、学習されたメカニズムが強力であることを発見しました私たちの建設と相関しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal
  Transformers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_23.html">
      <font color="black">X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal
  Transformers</font>
    </a>
  </h2>
  <font color="black">最後に、画像生成機能をUNITERに追加してX-UNITERを生成することにより、これらのトレーニングの改良点の一般性を示します。X-LXMERTは、LXMERTの拡張であり、次のようなトレーニングの改良点があります。視覚表現の離散化、広範囲の均一なマスキングの使用比率をマスキングし、適切な事前トレーニングデータセットを適切な目的に合わせて調整し、ペイントできるようにします。これは疑問を投げかけます：これらのモデルは他の方法でテキストの一部から画像を生成できますか？ 
[ABSTRACT] lxmertの画像生成の最先端のトレーニング機能は、最先端のzativeモデルに匹敵します。この研究は、他のモデルを画像キャプションの重要なタスクに統合します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Exploiting Vietnamese Social Media Characteristics for Textual Emotion
  Recognition in Vietnamese -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_24.html">
      <font color="black">Exploiting Vietnamese Social Media Characteristics for Textual Emotion
  Recognition in Vietnamese</font>
    </a>
  </h2>
  <font color="black">私たちの実験的評価は、適切な前処理手法を使用して、多項ロジスティック回帰（MLR）が64.40％という最高のF1スコアを達成し、UIT-VSMECの作成者が作成したCNNモデル（59.74％）よりも4.66％も大幅に向上していることを示しています。 ..データを整理するためにベトナムのソーシャルメディアの特性を調査し、次に感情的なコンテキストを含む可能性のある本質的なフレーズを抽出しました。このホワイトペーパーでは、データの前処理がテキストの感情認識。 
[要約] uitの研究者によって開発されたベトナムのソーシャルメディア感情corpus.systemで行われた実験-vsmec</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Evolution of Part-of-Speech in Classical Chinese -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_25.html">
      <font color="black">Evolution of Part-of-Speech in Classical Chinese</font>
    </a>
  </h2>
  <font color="black">Bisang（2008）は、古典中国語は前分類言語であり、単語の構文上の位置が品詞カテゴリを決定する、と主張しました。最後に、古典中国語と現代中国語の文字埋め込みを整列させ、動詞がより意味の変化を受ける名詞よりも。古典中国語の名詞と動詞の違いをさらに調査します。心理言語学的規範を使用すると、具体性と名詞の用法の間に正の相関関係があります。 
[要約]心理言語学的規範を使用して、具体性と名詞の間に正の相関があることを確認します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Harnessing Multilinguality in Unsupervised Machine Translation for Rare
  Languages -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_26.html">
      <font color="black">Harnessing Multilinguality in Unsupervised Machine Translation for Rare
  Languages</font>
    </a>
  </h2>
  <font color="black">これらの言語の最新の教師なしベースラインをすべて上回り、最大14.4 BLEUの達成を達成しました。この作業では、教師なしシステムを低リソース設定で実用的にするために多言語性が重要であることを示しています。一連のアブレーション研究により、さまざまな程度のデータ品質の下でモデルの堅牢性を確立し、従来の教師なしモデルに対して提案されたアプローチの優れたパフォーマンスをもたらした要因を分析します。 
[ABSTRACT]教師なし翻訳のパフォーマンスが低下し、3.0未満を達成しました。低リソースのまれな言語を含むより現実的な設定では、教師なし翻訳のパフォーマンスが低下します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Comparing Natural Language Processing Techniques for Alzheimer's
  Dementia Prediction in Spontaneous Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_27.html">
      <font color="black">Comparing Natural Language Processing Techniques for Alzheimer's
  Dementia Prediction in Spontaneous Speech</font>
    </a>
  </h2>
  <font color="black">提供された自然発話データセットのテキストトランスクリプトのみを分析し、ADとコントロールの分類およびメンタルミニステート試験スコアの予測のための多数のモデル全体でパフォーマンスを構築および比較します。サポートベクターマシン（SVM）を厳密にトレーニングおよび評価します。勾配ブースティングディシジョンツリー（GBDT）、およびディープラーニングトランスフォーマーベースのモデルと一緒に条件付きランダムフィールド（CRF）。分類メトリック全体で0.81〜0.82のテストセットスコアと4.58のRMSEを示します。 
[ABSTRACT]自然発話タスクによるアルツハイマー病の認識は、広告と関連する表現型の診断と予測のための前処理済みでバランスのとれたデータセットを提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-12">
        <br><font color="black">2020-06-12</font>
      </time>
    </span>
</section>
<!-- paper0: Hierarchical Pre-training for Sequence Labelling in Spoken Dialog -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_28.html">
      <font color="black">Hierarchical Pre-training for Sequence Labelling in Spoken Dialog</font>
    </a>
  </h2>
  <font color="black">OpenSubtitlesで事前トレーニングが行われます：23億ドルを超えるトークンを含む音声ダイアログの大規模なコーパス。DialogActやEmotion / Sentiment識別などのシーケンスのラベル付けタスクは、音声ダイアログシステムの主要なコンポーネントです。\ texttt {SILICONE}はモデルに依存せず、さまざまなサイズの10の異なるデータセットが含まれています。 
[ABSTRACT]新しいベンチマークを使用して、音声対話に適応した一般的な表現を学習します。これらには、音声言語ベンチマークの正式な評価ベンチマークが含まれます。新しい作品はシリコンと呼ばれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Text Classification with Novelty Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_29.html">
      <font color="black">Text Classification with Novelty Detection</font>
    </a>
  </h2>
  <font color="black">このようなペアからの出力確率は、テストインスタンスが見たクラスからのものか、新規/予期しないものかを判断するために使用されます。このアプローチでは、2つのモデルを示します。より効果的なモデルでは、インスタンスのペアの2つの埋め込み行列を使用します。 CNNの2つのチャネル。 
[要約]提案された方法は、インスタンスの2つの埋め込み行列をcnnの2つのチャネルとして使用します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Streamlining Cross-Document Coreference Resolution: Evaluation and
  Modeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/cs.CL/paper_30.html">
      <font color="black">Streamlining Cross-Document Coreference Resolution: Evaluation and
  Modeling</font>
    </a>
  </h2>
  <font color="black">私たちのモデルは、最新の結果を大幅に上回るCD共参照設定に対処するために、ドキュメント内共参照解決のために最近のニューラルモデルを適応および拡張します。評価に続く将来の研究のためにベースライン結果を設定することを目指します。方法論では、このタスクの最初のエンドツーエンドモデルを構築します。クロスドキュメント（CD）共参照解決の最近の評価プロトコルは、一貫性がないか、寛大であることが多く、作品全体で比較できない結果やパフォーマンスの過大評価につながります。 
[ABSTRACT]私たちの主な貢献は、金の言及を想定するのではなく、生のテキストのみへのアクセスを想定し、シングルトン予測を無視して、cd共参照解決における典型的なターゲット設定に対処する実用的な評価方法を提案することです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: Hardware Aware Training for Efficient Keyword Spotting on General
  Purpose and Specialized Hardware -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.AS/paper_0.html">
      <font color="black">Hardware Aware Training for Efficient Keyword Spotting on General
  Purpose and Specialized Hardware</font>
    </a>
  </h2>
  <font color="black">また、8.79 $ \ mu $ WのSotA電力効率を実現するカスタム設計のアクセラレータハードウェアの電力要件を特徴づけ、汎用低電力ハードウェア（マイクロコントローラー）を24倍、特殊用途ASICを16倍にしています。最先端の（SotA）精度と少ないパラメーター数を実現するLegendre Memory Unit（LMU）に基づく新しいKWSニューラルネットワークを構築するための認識トレーニング（HAT）。キーワードスポッティング（KWS）は、電話、ウェアラブル、自動車など、多くのモバイルおよびエッジアプリケーション。 
[要旨] kwsシステムは通常、スマートフォンの「ユーザーw」です。kwsは、車で使用できるスマートフォンベースのデバイスです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-09">
        <br><font color="black">2020-09-09</font>
      </time>
    </span>
</section>
<!-- paper0: DCCRN: Deep Complex Convolution Recurrent Network for Phase-Aware Speech
  Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.AS/paper_1.html">
      <font color="black">DCCRN: Deep Complex Convolution Recurrent Network for Phase-Aware Speech
  Enhancement</font>
    </a>
  </h2>
  <font color="black">最近のいくつかの研究では、複素数値スペクトログラムをトレーニングターゲットとして使用していますが、実数値ネットワークでトレーニングし、マグニチュードとフェーズコンポーネントまたは実数部と虚数部をそれぞれ予測しています。スピーチの向上は、明瞭度の観点からディープラーニングの成功から恩恵を受けています。知覚品質。わずか370万のパラメーターで、Interspeech 2020 Deep Noise Suppression（DNS）チャレンジに提出された当社のDCCRNモデルは、平均オピニオンの観点から、リアルタイムトラックで1位、非リアルタイムトラックで2番目にランク付けされました。スコア（MOS）。 
[要旨]たたみ込み実ネットワークは、たたみ込みニューラルネットワーク（cnn）を使用します。これらのたたみ込み実用語には、たたみ込み部分と長い短期記憶（lstm）が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-01">
        <br><font color="black">2020-08-01</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-End Speech Recognition and Disfluency Removal -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.AS/paper_2.html">
      <font color="black">End-to-End Speech Recognition and Disfluency Removal</font>
    </a>
  </h2>
  <font color="black">ディスフルエンシー検出は通常、自動音声認識（ASR）システムとダウンストリームタスクの間の中間ステップです。個別のディスフルエンシー検出に依存することなく、ASRモデルをトレーニングして、ディスフルエンス音声をフルーエントトランスクリプトに直接マッピングできるかどうかを具体的に調査します。モデル..このペーパーの調査結果は、エンドツーエンドの音声認識と将来の流暢性の除去のタスクに関するさらなる研究のベンチマークとして役立ちます。 
[ABSTRACT]新しい論文は、流暢性の除去のためのモデルのタスクを調査することを目的としています。また、asrシステムと流暢さの検出を調査することを目的としています。調査結果は、さらなる研究のベンチマークとして役立ちます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-22">
        <br><font color="black">2020-09-22</font>
      </time>
    </span>
</section>
<!-- paper0: Narrow-band Deep Filtering for Multichannel Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.AS/paper_3.html">
      <font color="black">Narrow-band Deep Filtering for Multichannel Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">この選択は、従来の広帯域音声強調方法とは対照的です。提案されたモデルの顕著な特徴は、同じパラメータを持つ同じLSTMアーキテクチャが周波数ビン全体でトレーニングされることです。提案されたディープフィルタリングは、音声を区別できます。そして、それらの異なる時間的および空間的特性を利用することによるノイズ：ノイズは比較的定常的であり、チャネル間で弱く相関している一方で、音声は非定常で空間的にコヒーレントです。 
[要約]提案された方法は、狭帯域ディープフィルタリングと呼ばれます。これは、異なる周波数特性と空間特性を利用することにより、音声とノイズを区別できます。これらには、同じ周波数、複雑なstftエントリ、および（平滑化された）空間特性が含まれます。フィルタ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-25">
        <br><font color="black">2019-11-25</font>
      </time>
    </span>
</section>
<!-- paper0: The Freesound Loop Dataset and Annotation Tool -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.AS/paper_4.html">
      <font color="black">The Freesound Loop Dataset and Annotation Tool</font>
    </a>
  </h2>
  <font color="black">自動ループ特性評価からアルゴリズム構成までのアプリケーションで、コミュニティがさらに多くのデータの用途を見つけることを期待しています。FSLDの有用性を示すために、FSLDを使用してテンポとキーを推定し、音楽トラックを生成する短いケーススタディを示します。ループ分離アルゴリズムを評価します。また、開発したオンラインループアノテーターツールをコミュニティに提供します。 
[ABSTRACT] freesoundはクリエイティブコモンズライセンスの下でリリースされたオーディオ録音のコミュニティデータベースです。fsldはfsldを使用してテンポとキーを推定し、音楽トラックを生成し、ループ分離アルゴリズムを評価します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Cough Against COVID: Evidence of COVID-19 Signature in Cough Sounds -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.AS/paper_5.html">
      <font color="black">Cough Against COVID: Evidence of COVID-19 Signature in Cough Sounds</font>
    </a>
  </h2>
  <font color="black">全体的なテストプロトコル内のトリアージステップで使用すると、確認テストの前に個人のリスク層別化を可能にすることにより、追加のサプライや訓練を受けた担当者がいなくても、私たちのツールは5％の疾患有病率でヘルスケアシステムのテスト能力を43％増加できます、または物理インフラストラクチャ。私たちのAIモデルで分析すると、電話で収集された要請された咳の音は、COVID-19ステータスを示す統計的に有意な信号を持っていることを示します（AUC 0.72、t検定、p &lt;0.01、95％CI 0.61-0.83）。これに向けて、微生物学的に確認されたCOVID-19咳音の3,621人から、既知の最大のデータセットを収集します。 
[ABSTRACT]このツールは、追加の供給、訓練を受けた担当者、または物理インフラストラクチャなしで、ヘルスケアシステムのテスト能力を43％増やすことができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-17">
        <br><font color="black">2020-09-17</font>
      </time>
    </span>
</section>
<!-- paper0: Residual Shuffle-Exchange Networks for Fast Processing of Long Sequences -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.AS/paper_6.html">
      <font color="black">Residual Shuffle-Exchange Networks for Fast Processing of Long Sequences</font>
    </a>
  </h2>
  <font color="black">提案されたアーキテクチャは、より長いシーケンスにスケーリングするだけでなく、収束が速くなり、精度が向上します。LAMBADA言語モデリングタスクのShuffle-Exchangeネットワークを凌駕し、MusicNetデータセットで最先端のパフォーマンスを実現しながら、音楽の転写を実現します。パラメーターの数が効率的です。改善されたShuffle-Exchangeネットワークをたたみ込み層と組み合わせ、長いシーケンス処理アプリケーションで有用なビルディングブロックとして確立する方法を示します。 
[ABSTRACT]最近導入されたニューラルシャッフル-交換ネットワークは、geluとレイヤーの正規化を使用した残余ネットワークに基づいています。シャッフルを超えますが、それを超えますが、musicnetデータセットの複雑さを防ぎます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-06">
        <br><font color="black">2020-04-06</font>
      </time>
    </span>
</section>
<!-- paper0: Attention Driven Fusion for Multi-Modal Emotion Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.AS/paper_7.html">
      <font color="black">Attention Driven Fusion for Multi-Modal Emotion Recognition</font>
    </a>
  </h2>
  <font color="black">このアプローチは、感情認識用に調整されたフィルターバンクを学習し、生の音声信号に畳み込みを直接適用する場合と比較して、より効果的な機能を提供します。既存の最先端技術に従って、提案されたシステムのIEMOCAPデータセットでのパフォーマンスを評価します。テキスト処理では、2つのブランチ（DCNNと双方向RNNの後にDCNNが続く）を並行して使用し、相互注意を導入して、Bi-RNNから受信した隠し表現のNグラムレベルの相関を推測します。 
[要約]深い混同ニューラルネットワーク（dcnn）および感情への再帰学習ベースのアプローチ。提案されたシステムは既存の方法よりも優れており、加重精度が3.5％向上しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-23">
        <br><font color="black">2020-09-23</font>
      </time>
    </span>
</section>
<!-- paper0: Comparing Natural Language Processing Techniques for Alzheimer's
  Dementia Prediction in Spontaneous Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-24/eess.AS/paper_8.html">
      <font color="black">Comparing Natural Language Processing Techniques for Alzheimer's
  Dementia Prediction in Spontaneous Speech</font>
    </a>
  </h2>
  <font color="black">自然音声データセットの提供されたテキストの筆記録のみを分析し、ADとコントロールの分類および精神的ミニ状態試験スコアの予測のための多数のモデル全体でパフォーマンスを構築および比較します。最もパフォーマンスの高いモデルは単純な用語頻度であることがわかります-SVMモデルへの入力としての逆ドキュメント周波数（TF-IDF）ベクトライザー、および単純な線形モデルへの埋め込みレイヤーとして使用される場合、事前トレーニング済みのトランスフォーマーベースのモデル「DistilBERT」。サポートベクターマシン（SVM）を厳密にトレーニングおよび評価します。 、勾配ブースティングディシジョンツリー（GBDT）、条件付きランダムフィールド（CRF）、およびディープラーニングトランスフォーマーベースのモデル。 
[ABSTRACT]自然発話タスクによるアルツハイマー病の認識は、広告と関連する表現型の診断と予測のための前処理済みでバランスのとれたデータセットを提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-12">
        <br><font color="black">2020-06-12</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
