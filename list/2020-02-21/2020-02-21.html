<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-02-21の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: iSEGAN: Improved Speech Enhancement Generative Adversarial Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.SD/paper_0.html">
      iSEGAN: Improved Speech Enhancement Generative Adversarial Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      シミュレーション結果は、提案されたアプローチがcGANシステムの音声強調性能を改善することに加えて、安定性の向上と計算労力の削減を示します。さらに、ガンマトーンベースの聴覚フィルタリングレイヤーとトレーニング可能なプリエンファシスレイヤーをさらに改善することを提案しますcGANフレームワークのパフォーマンス。ただし、cGANシステムの安定化とトレーニングは困難であり、依然としてスペクトルエンハンスメントアプローチによって達成されるパフォーマンスには及ばない。 
[要旨]提案されたアプローチは、安定性の改善と労力の削減に加えて、cganシステムの音声強調性能を改善します。新しい提案には、cganを使用して音声強調モデルを改善することが含まれます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: An empirical study of Conv-TasNet -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.SD/paper_1.html">
      An empirical study of Conv-TasNet
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      WSJ0-2mix、LibriTTS、VCTKデータベースからの分離の評価を含むクロスデータセット評価を提案します。そのアーキテクチャは、学習可能なエンコーダー/デコーダーと、この学習スペース上で動作するセパレーターで構成されます。Conv-TasNetは最近音声ソース分離で最先端のパフォーマンスを実現する、提案された波形ベースのディープニューラルネットワーク。 
[概要] connetの構造はconnetによって開発されています。学習可能なエンコーダー/デコーダーとセパレーターが含まれています。結果は、エンコーダーまたはデコーダーの強化により、平均si-snrパフォーマンスが1 db以上向上できることを示しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Convergence-guaranteed Independent Positive Semidefinite Tensor Analysis
  Based on Student's t Distribution -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.SD/paper_2.html">
      Convergence-guaranteed Independent Positive Semidefinite Tensor Analysis
  Based on Student's t Distribution
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験結果から、従来のIPSDTAのコスト関数は単調に増加しない特性を示さないことが明らかになりました。次に、コスト関数の単調な非増加を保証し、安定した収束を提供する新しいパラメーター最適化アルゴリズムを導出します。この方法は、コスト関数の単調な非増加を保証し、ソース分離パフォーマンスにおいて従来のILRMAおよびIPSDTAよりも優れています。 
[ABSTRACT] ipsdtaは、周波数間の相関を考慮に入れることができる最先端のbssメソッドです。しかし、生成モデルは、多変量ガウス分布内に制限されています。コスト関数の単計量非増加を拡張するために提案されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Imputer: Sequence Modelling via Imputation and Dynamic Programming -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.SD/paper_3.html">
      Imputer: Sequence Modelling via Imputation and Dynamic Programming
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      エンドツーエンドの音声認識に適用すると、Imputerは以前の非自己回帰モデルよりも優れており、自己回帰モデルと比較して競争力のある結果を達成します。 LibriSpeechのその他のテストでは、Imputerは11.1 WERを達成し、13.0 WERでCTCと12.5 WERでseq2seqを上回りました。 
[要約]入力者は、反復性のある例示モデルであり、入力または出力トークンの数とは関係なく、一定数の可能なステップのみを必要とします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Learning Style-Aware Symbolic Music Representations by Adversarial
  Autoencoders -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.SD/paper_4.html">
      Learning Style-Aware Symbolic Music Representations by Adversarial
  Autoencoders
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      また、2つの音楽シーケンス間の現実的な補間を作成して、異なるトラックのダイナミクスをスムーズに変更することもできます。本書では、音楽メタデータ情報を考慮したガウス混合をオートエンコーダーの潜在空間の効果的な前処理として使用する方法を示します、最初の音楽敵オートエンコーダー（MusAE）を導入します。生成音楽モデリングで、象徴的な音楽データの効果的な潜在空間を学習するという困難な未解決の問題に対処します。 
[概要]このモデルは、標準の変分オートエンコーダーに基づく最先端のモデルよりも高い再構成精度を備えています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-15">
        <br>2020-01-15
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Wavesplit: End-to-End Speech Separation by Speaker Clustering -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.SD/paper_5.html">
      Wavesplit: End-to-End Speech Separation by Speaker Clustering
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      そして残響（WHAMR！）。 Wavesplitは、ノイズの多い（WHAM！）だけでなく、2つまたは3つのスピーカーのクリーンな混合（WSJ0-2mix、WSJ0-3mix）で以前の最先端技術よりも優れていることを示します。条件。 
[ABSTRACT]モデルは、クラスタリングを通じて話者表現のセットを推測します。モデルは、推測された表現に基づいて各ソース信号を推定します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Disentangled Speech Embeddings using Cross-modal Self-supervision -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.SD/paper_6.html">
      Disentangled Speech Embeddings using Cross-modal Self-supervision
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      「野生の」トーキングヘッドの大規模な視聴覚データセットでメソッドをトレーニングし、標準的な話者認識パフォーマンスのために学習した話者表現を評価することにより、その有効性を示します。 ）両方の表現に共通する低レベルの機能を共有します。 （2）これらの要因を明確に解きほぐすための自然なメカニズムを提供し、コンテンツとアイデンティティの新しい組み合わせをより一般化する可能性を提供し、最終的にはより堅牢なスピーカーアイデンティティ表現を生成します。 -注釈なし---言語コンテンツと話者のアイデンティティの表現。 
[概要]この概念は、自己監視型学習目標によって開発されました。これは、これらの要因を解くための自然なメカニズムを提供します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multilogue-Net: A Context Aware RNN for Multi-modal Emotion Detection
  and Sentiment Analysis in Conversation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.SD/paper_7.html">
      Multilogue-Net: A Context Aware RNN for Multi-modal Emotion Detection
  and Sentiment Analysis in Conversation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      会話の感情分析と感情検出は、現実世界の多くのアプリケーションで重要であり、さまざまなアプリケーションがさまざまな種類のデータを活用して合理的に正確な予測を実現します。提案モデルは、2つのベンチマークデータセットで最先端を実行しますさまざまな精度および回帰メトリックについて。マルチモーダル機能を扱う現在のシステムは、すべてのモダリティ、会話の現在の話し手と聞き手、および利用可能なものの間の関連性と関係を通じて、会話のコンテキストを活用およびキャプチャできません。適切な融合メカニズムによるモダリティ。 
[要約]感情検出と感情分析は特に有用です。現在のシステムは、利用可能なモダリティの特定のサブセットを使用して予測を生成します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-19">
        <br>2020-02-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Sequence-to-sequence Singing Synthesis Using the Feed-forward
  Transformer -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.SD/paper_8.html">
      Sequence-to-sequence Singing Synthesis Using the Feed-forward
  Transformer
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      歌の発音のタイミングが楽譜によって非常に制約されていることを考えると、単純な持続時間モデルの助けを借りておおよその初期アライメントを導き出します。事前のトレーニングデータの必要性を回避するシーケンスツーシーケンス歌唱シンセサイザーを提案します。整列された音声および音響機能..次に、Transformerモデルのフィードフォワードバリアントに基づくデコーダーを使用して、一連の自己注意および畳み込み層が、ターゲットの音響機能に到達するように初期整列の結果を調整します。 
[要約]デコーダーは、トランスモデルのフィードフォワードバリアントに基づいています。一連の自己注意層と畳み込み層は、ターゲットの音響特性に到達するように初期アライメントの結果を調整します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-22">
        <br>2019-10-22
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Federated pretraining and fine tuning of BERT using clinical notes from
  multiple silos -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.CL/paper_0.html">
      Federated pretraining and fine tuning of BERT using clinical notes from
  multiple silos
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      BERTなどの大規模なコンテキスト表現モデルは、近年、自然言語処理（NLP）を大幅に進化させています。この記事では、臨床テキストを使用して、BERTモデルを統合して事前トレーニングと微調整の両方を行うことができることを示しますただし、ヘルスケアのような特定の分野では、プライバシーと規制上の理由により、複数の機関からの多様な大規模テキストデータへのアクセスは非常に困難です。 
[概要]ヘルスケアのような特定の分野では、プライバシーと規制上の理由により、複数の機関からの多様な大規模テキストデータへのアクセスは問題ありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Guiding attention in Sequence-to-sequence models for Dialogue Act
  prediction -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.CL/paper_1.html">
      Guiding attention in Sequence-to-sequence models for Dialogue Act
  prediction
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この作業では、以下を使用してDA分類に合わせて調整されたseq2seqモデルを紹介します。会話型対話に基づいて対話型行為（DA）を予測するタスクは、会話型エージェントの開発における重要なコンポーネントです。 
[ABSTRACT] seq2seqモデルは、複雑なグローバル依存関係を学習することが知られています。線形条件付きランダムフィールド（crf）のみを使用して現在提案されているアプローチは、ローカルタグ依存関係のみをモデル化します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: The Fluidity of Concept Representations in Human Brain Signals -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.CL/paper_2.html">
      The Fluidity of Concept Representations in Human Brain Signals
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちは、この区別を偶然よりもはるかに高い精度で信号からデコードできることを発見しましたが、クラスタリングおよび関係分析に関連する構造化要因ではないことがわかりました。人間の言語処理の認知理論は、多くの場合、具体的概念と抽象的な概念を区別します。 。詳細な比較から、人間の概念表現は、二分されたカテゴリーが捉えることができるよりも流動的であるという印象が得られます。 
[要約]人間の概念の概念は、二分カテゴリーが捉えることができるよりも複雑であることがわかった
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Working Memory Graphs -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.CL/paper_3.html">
      Working Memory Graphs
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの結果は、環境観測を因子分解できる環境では、WMGのTransformerベースのアーキテクチャがサンプル効率を劇的に向上させることを意味します。因子観測空間を特徴とする3つの環境でWMGを評価します：過去の観測に対する複雑な推論を必要とする経路探索環境、BabyAIテキスト命令を含むgridworldレベル、および将来の計画を強調する倉庫番。WMGのTransformerベースのアーキテクチャとファクタリングされた観測スペースの組み合わせにより、すべてのタスクで他のアーキテクチャと比較して学習効率が大幅に向上することがわかります。 
[ABSTRACT]変圧器ベースのモデルは視覚的理由のパフォーマンスを向上させることができます。変圧器ベースのアーキテクチャは、サンプルの効率を劇的に高めることができます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-17">
        <br>2019-11-17
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Imputer: Sequence Modelling via Imputation and Dynamic Programming -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.CL/paper_4.html">
      Imputer: Sequence Modelling via Imputation and Dynamic Programming
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      LibriSpeechのテストその他では、Imputerは11.1 WERを達成し、13.0 WERでCTCおよび12.5 WERでseq2seqをアウトパフォームします。 .. Imputerは、反復生成モデルであり、入力または出力トークンの数に関係なく、一定数の生成ステップのみを必要とします。 
[要約]入力者は、反復性のある例示モデルであり、入力または出力トークンの数とは関係なく、一定数の可能なステップのみを必要とします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Balancing Cost and Benefit with Tied-Multi Transformers -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.CL/paper_5.html">
      Balancing Cost and Benefit with Tied-Multi Transformers
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      次に、デコードを高速化するためにエンコーダおよびデコーダレイヤーの数をアプリオリに選択するメカニズムを提案します。また、モデルの圧縮のためにレイヤーの繰り返しスタックと知識の蒸留を検討します。代わりに、NxM損失からなる単一損失を計算します。損失は、Nエンコーダー層の1つに接続されたMデコーダー層の1つの出力から計算されます。複数のモデルを1つに圧縮して数を動的に選択できる複数のトランスフォーマーをトレーニングする新しい手順を提案および評価しますデコード中のエンコーダーおよびデコーダー層の。 
[概要]ニューラルマシン翻訳の提案されたアプローチのコスト-利益分析を提示します。彼らは、翻訳品質を維持しながらデコードコストを削減することを示します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Revisiting Self-Training for Neural Sequence Generation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.CL/paper_6.html">
      Revisiting Self-Training for Neural Sequence Generation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      パフォーマンスの向上を慎重に検討することで、隠れた状態の摂動（つまり、標準的な機械翻訳とテキスト要約ベンチマークに関する実証研究により、ノイズの多い自己学習がラベルなしデータを効果的に利用し、監視対象のパフォーマンスを改善できることがわかります自己学習は、最も早くて最も簡単な半教師ありの方法の1つです。
[要約]キーアイデアは、モデルの予測と組み合わせたラベルなしデータで元のラベル付きデータセットを補強することです。これが最初です。時間自己訓練は、神経シーケンス生成タスクの監視ベースラインを改善することができます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-30">
        <br>2019-09-30
      </time>
    </span>
  </h3>
</article>
<!-- paper0: The Limitations of Stylometry for Detecting Machine-Generated Fake News -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.CL/paper_7.html">
      The Limitations of Stylometry for Detecting Machine-Generated Fake News
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      したがって、スタイロメトリーはテキストの出所を特定することでなりすましを防ぐことはできますが、正当なLMアプリケーションと誤った情報をもたらすアプリケーションとを区別することはできません。広くスタイロメトリーと呼ばれるアプローチは、人間が書いたテキストのソース属性と誤情報の検出に成功しました。 
[要約]機械生成の誤情報を検出するための研究が提案されています。ただし、この作業では、スタイロメトリーがマシン形成に対して制限されていることを示しています。誤った情報を持ち込む
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-26">
        <br>2019-08-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multi-Agent Reinforcement Learning as a Computational Tool for Language
  Evolution Research: Historical Context and Future Challenges -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.CL/paper_8.html">
      Multi-Agent Reinforcement Learning as a Computational Tool for Language
  Evolution Research: Historical Context and Future Challenges
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      エージェント集団における緊急コミュニケーションの計算モデルは、マルチエージェント強化学習（MARL）の最近の進歩により、機械学習コミュニティに現在関心を集めています。この論文の目標は、言語進化の歴史的文脈内で最近のMARL貢献を位置付けることですしかし、現在の貢献は、言語が前言語物質からどのように出現したかを理解することを目的とした初期の理論的および計算的文献からはまだ比較的切り離されています。 
[ABSTRACT]言語が前言語物質からどのように出現したかを判断するための研究が現在進行中です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Wavesplit: End-to-End Speech Separation by Speaker Clustering -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.CL/paper_9.html">
      Wavesplit: End-to-End Speech Separation by Speaker Clustering
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      そして残響（WHAMR！）。エンドツーエンドの音声分離システムであるWavesplitを紹介します。Wavesplitは、2人または3人のスピーカー（WSJ0-2mix、WSJ0-3mix）のクリーンな混合で以前の最先端技術よりも優れていることを示します。ノイズの多い（WHAM！）
[ABSTRACT]モデルは、クラスタリングを通じて話者表現のセットを推測します。モデルは、推測された表現に条件付けられた各ソース信号を推定します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Contextual Lensing of Universal Sentence Representations -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.CL/paper_10.html">
      Contextual Lensing of Universal Sentence Representations
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      コアユニバーサルマトリックス表現が与えられれば、言語の類似性の概念を少数のレンズパラメーターにフォーカスできることを示します。この作業では、コンテキスト指向のユニバーサル文ベクトルを誘導する方法論であるコンテキストレンズを提案します。レンズのコンテキストの関数として固定長ベクトルを誘導できる適応可能な「レンズ」を備えた、コア、可変長、文マトリックス表現への普遍的な文ベクトルの構築。 
[概要]ジェネリックフォーカスの概念は、固有のコンテキスト化と言語使用の非永続性と対立します。普遍的なセンテンスの構築を、適応可能な「レンズ」を備えたコア、可変長、センテンスマトリックス表現に分割します。複数の言語にわたる文章の翻訳類似性を単一の重み行列にエンコードできることを実証しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: FrameAxis: Characterizing Framing Bias and Intensity with Word Embedding -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.CL/paper_11.html">
      FrameAxis: Characterizing Framing Bias and Intensity with Word Embedding
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの方法は、フレーミングバイアス（各マイクロフレームでのテキストの偏り）とフレーミング強度（各マイクロフレームの使用量）をテキストから定量的に緩和することができ、フレーミングの微妙な特徴を提供します。分野を超えたフレーミングのスケーラブルで微妙な計算分析を可能にします。SemEvalデータセットだけでなく、3つの他のデータセットと人間の評価を使用してアプローチを評価し、FrameAxisが関連するマイクロフレームでドキュメントを確実に特徴付けることができることを実証します。 
[ABSTRACT]私たちの教師なしアプローチは、セマンティック軸のホストを考慮することにより、より詳細な洞察を提供します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multilogue-Net: A Context Aware RNN for Multi-modal Emotion Detection
  and Sentiment Analysis in Conversation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.CL/paper_12.html">
      Multilogue-Net: A Context Aware RNN for Multi-modal Emotion Detection
  and Sentiment Analysis in Conversation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      マルチモーダル機能を扱う現在のシステムは、すべてのモダリティを通じて会話のコンテキスト、会話内の現在の話し手と聞き手、および適切な融合メカニズムを通じて利用可能なモダリティ間の関連性と関係を活用およびキャプチャできません。 model outは、さまざまな精度と回帰メトリックに関する2つのベンチマークデータセットで最新技術を実行します。会話の感情分析と感情検出は、さまざまな種類のデータを活用するさまざまなアプリケーションで重要です。合理的に正確な予測を達成できます。 
[要約]感情検出と感情分析は特に有用です。現在のシステムは、利用可能なモダリティの特定のサブセットを使用して予測を生成します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-19">
        <br>2020-02-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Measuring Social Biases in Grounded Vision and Language Embeddings -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.CL/paper_13.html">
      Measuring Social Biases in Grounded Vision and Language Embeddings
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらのメトリックは、COCO、概念キャプション、およびGoogle画像からの10,228個の画像で拡張言語バイアスベンチマークを拡張することによって作成された、接地バイアス用の最初の新しいデータセットで使用されます。 SEAT）そして、3つの一般化が、バイアス、言語、ビジョンの相互作用についての異なるが重要な質問に答えることを実証します。これは、ビジョンと言語が異なるバイアスに苦しむ可能性があるという事実にもかかわらずです。 
[要約]埋め込みのバイアスのビジョンは、非接地データの場合と同等またはそれ以上に重要であると思われる
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Adversarial Filters of Dataset Biases -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/cs.CL/paper_14.html">
      Adversarial Filters of Dataset Biases
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最適なバイアス削減のための一般化されたフレームワークに配置することにより、AFLiteの理論的理解を提供します。これにより、これらのモデルは、偽のデータセットバイアスをオーバーフィットすることによって、基礎となるタスクではなく、データセットを解決することを学習したかどうかの問題を提起します。敵対的または非配布サンプルでテストすると、パフォーマンスが大幅に低下します。 
[ABSTRACT]フィルター処理されたデータセットでトレーニングされたモデルは、特にベンチマークが偏ったサンプルで過密になっている場合、配信タスクのより良い一般化をもたらします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-10">
        <br>2020-02-10
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: iSEGAN: Improved Speech Enhancement Generative Adversarial Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/eess.AS/paper_0.html">
      iSEGAN: Improved Speech Enhancement Generative Adversarial Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ただし、cGANシステムの安定化とトレーニングは困難であり、スペクトルエンハンスメントアプローチによって達成されるパフォーマンスにはまだ達していません。一般的なニューラルネットワークベースの音声強調システムは、マグニチュードスペクトログラムで動作し、ノイズの多い音声信号とクリーンな音声信号間の位相の不一致を無視します。 
[要旨]提案されたアプローチは、安定性の改善と労力の削減に加えて、cganシステムの音声強調性能を改善します。新しい提案には、cganを使用して音声強調モデルを改善することが含まれます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Photorealistic Lip Sync with Adversarial Temporal Convolutional Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/eess.AS/paper_1.html">
      Photorealistic Lip Sync with Adversarial Temporal Convolutional Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      また、合成顔マップから高解像度のフォトリアルな顔の外観を生成するための画像から画像への変換ベースのアプローチを提案します。実験により、モデルは従来のRNNベースのベースラインよりも精度と速度の点で優れています。対象者の唇の動きと話し方。 
[概要]対象者の唇の動きと話し方がモデルの鍵となります。このモデルは、精度と速度の両方で従来のrnnベースのベースラインよりも優れています。また、最近のニューラルネットワークベースのソリューションのいくつかの既存の問題も解決します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: An empirical study of Conv-TasNet -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/eess.AS/paper_2.html">
      An empirical study of Conv-TasNet
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この論文では、Conv-TasNetの実証的研究を行い、（深い）非線形バリアントに基づくエンコーダー/デコーダーの拡張を提案します。Conv-TasNetは、最近提案された波形ベースのディープニューラルです。さらに、より大きく多様なLibriTTSデータセットを使用して実験し、はるかに大きなデータセットでトレーニングされた場合の調査モデルの一般化機能を調査します。 
[概要] connetの構造はconnetによって開発されています。学習可能なエンコーダー/デコーダーとセパレーターが含まれています。結果は、エンコーダーまたはデコーダーの強化により、平均si-snrパフォーマンスが1 db以上向上できることを示しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Convergence-guaranteed Independent Positive Semidefinite Tensor Analysis
  Based on Student's t Distribution -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/eess.AS/paper_3.html">
      Convergence-guaranteed Independent Positive Semidefinite Tensor Analysis
  Based on Student's t Distribution
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらの問題を解決するために、まず、生成モデルを、さまざまなタイプの信号を処理できるパラメトリック多変量スチューデントのt分布に拡張することを提案します。本稿では、ブラインドソース分離（BSS）問題に対処し、独立した正半正テンソル解析（IPSDTA）の新しい拡張フレームワークを提案します。 
[ABSTRACT] ipsdtaは、周波数間の相関を考慮に入れることができる最先端のbssメソッドです。しかし、生成モデルは、多変量ガウス分布内に制限されています。コスト関数の単計量非増加を拡張するために提案されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Imputer: Sequence Modelling via Imputation and Dynamic Programming -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/eess.AS/paper_4.html">
      Imputer: Sequence Modelling via Imputation and Dynamic Programming
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      対数限界尤度の下限をもたらす、扱いやすい動的プログラミングトレーニングアルゴリズムを示します。LibriSpeechテスト-その他では、Imputerは11.1 WERを達成し、13.0 WERでCTCおよび12.5 WERでseq2seqをアウトパフォームします。入力シーケンスと出力シーケンス間のすべての可能なアライメント、および可能なすべての生成順序をほぼマージナライズします。 
[要約]入力者は、反復性のある例示モデルであり、入力または出力トークンの数とは関係なく、一定数の可能なステップのみを必要とします
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Wavesplit: End-to-End Speech Separation by Speaker Clustering -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/eess.AS/paper_5.html">
      Wavesplit: End-to-End Speech Separation by Speaker Clustering
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      条件..ノイズの多い（WHAM！）だけでなく、2つまたは3つのスピーカーのクリーンな混合（WSJ0-2mix、WSJ0-3mix）でWavesplitが以前の最先端技術よりも優れていることを示します。さらに、シーケンス全体の話者表現は、以前のアプローチと比較して、長くて難しいシーケンスのより堅牢な分離を提供します。 
[ABSTRACT]モデルは、クラスタリングを通じて話者表現のセットを推測します。モデルは、推測された表現に基づいて各ソース信号を推定します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: GCI detection from raw speech using a fully-convolutional network -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/eess.AS/paper_6.html">
      GCI detection from raw speech using a fully-convolutional network
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      提案されたアルゴリズムの性能は、公開されているデータセットを使用して、他の3つの最先端のアプローチと比較され、トレーニング段階で制御された合成または実際の音声信号を使用する影響が調査されます。この問題を克服するために、提案します完全なグラウンドトゥルースを使用して高品質の合成音声でネットワークをトレーニングします。最近、畳み込みニューラルネットワークを使用した新しいアプローチが登場し、有望な結果が得られました。 
[概要]提案された方法は、多くの音声分析および処理アプリケーションで使用され、この目的のためにさまざまなアルゴリズムが提案されています。多くの話者と大きな合成データセットを使用すると、実際の音声と卵信号の小さなデータベースを使用するよりも一般化能力が向上します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-22">
        <br>2019-10-22
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Disentangled Speech Embeddings using Cross-modal Self-supervision -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/eess.AS/paper_7.html">
      Disentangled Speech Embeddings using Cross-modal Self-supervision
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちのアプローチの背後にある鍵となるアイデアは、注釈なしで、言語コンテンツと話者のアイデンティティの表現を解き明かすことです。この論文の目的は、手動で注釈付けされたデータにアクセスすることなく、話者のアイデンティティの表現を学習することです。 
[概要]この概念は、自己監視型学習目標によって開発されました。これは、これらの要因を解くための自然なメカニズムを提供します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Visually Guided Self Supervised Learning of Speech Representations -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/eess.AS/paper_8.html">
      Visually Guided Self Supervised Learning of Speech Representations
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      自己教師付き表現学習は、最近、音声と視覚の両方のモダリティで多くの研究関心を集めています。感情認識の最先端の結果と音声認識の競争力のある結果を達成します。視聴覚音声の文脈における視覚的モダリティ。 
[概要]提案されている教師なしオーディオ機能は、ラベルなし視聴覚スピーチの実質的に無制限のトレーニングデータを活用できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-13">
        <br>2020-01-13
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multilogue-Net: A Context Aware RNN for Multi-modal Emotion Detection
  and Sentiment Analysis in Conversation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/eess.AS/paper_9.html">
      Multilogue-Net: A Context Aware RNN for Multi-modal Emotion Detection
  and Sentiment Analysis in Conversation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      提案されたモデルは、さまざまな精度と回帰メトリックに関する2つのベンチマークデータセットで最先端を実行します。会話の感情分析と感情検出は、さまざまな種類のデータを活用するさまざまなアプリケーションで重要です。この論文では、前述のすべての欠点を考慮に入れ、会話、対話者の状態、および感情によって伝えられる感情のコンテキストを追跡しようとするリカレントニューラルネットワークアーキテクチャを提案します。会話のスピーカー。 
[要約]感情検出と感情分析は特に有用です。現在のシステムは、利用可能なモダリティの特定のサブセットを使用して予測を生成します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-19">
        <br>2020-02-19
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Sequence-to-sequence Singing Synthesis Using the Feed-forward
  Transformer -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/eess.AS/paper_10.html">
      Sequence-to-sequence Singing Synthesis Using the Feed-forward
  Transformer
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      歌の発音のタイミングが楽譜によって高度に制約されていることを考えると、単純な持続時間モデルの助けを借りて、おおよその初期アライメントを導き出します。次に、Transformerモデルのフィードフォワードバリアントに基づくデコーダーを使用して、一連のこのアプローチの利点には、より速い推論と、教師の強制によって訓練された自己回帰モデルに影響を与える露出バイアスの問題を回避することが含まれます。 
[要約]デコーダーは、トランスモデルのフィードフォワードバリアントに基づいています。一連の自己注意層と畳み込み層は、ターゲットの音響特性に到達するように初期アライメントの結果を調整します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-22">
        <br>2019-10-22
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Attention-based ASR with Lightweight and Dynamic Convolutions -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/eess.AS/paper_11.html">
      Attention-based ASR with Lightweight and Dynamic Convolutions
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      従来の隠れマルコフモデルベースのASRと比較した単純なモデルトレーニングにより、シーケンス間モデルを使用したエンドツーエンド（E2E）自動音声認識（ASR）が注目されています。 RNNベースのE2Eモデルよりも、ノイズ/残響の多いタスクを含むさまざまなASRベンチマークで最先端のTransformerと競合するパフォーマンスを備えています。 
[要旨]これらには、自己学習、周波数軸の畳み込みが含まれます。これらには、自己注意と自己観察のテクニックが含まれます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-26">
        <br>2019-12-26
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: The role of CAPG in molecular communication between the embryo and the uterine endometrium: Is its function conserved in species with different implantation strategies? -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/biorxiv.physiology/paper_0.html">
      The role of CAPG in molecular communication between the embryo and the uterine endometrium: Is its function conserved in species with different implantation strategies?
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      発達中のウシ受胎産物（胚および胚外膜）は、この発生段階でタンパク質を産生することがわかっています。妊娠認識シグナルによって。.ユーテリアの妊娠初期におけるこの一般的なプロセスは、マクロファージキャッピングタンパク質（CAPG）などの高度に保存された受胎産物由来タンパク質によって促進される可能性があると仮定しました。 
[ABSTRACT] capgは子宮内膜上皮プラークのトランスクリプトームを変更する機能を共有している可能性があります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: The fish body functions as an airfoil: surface pressures generate thrust during carangiform locomotion -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/biorxiv.physiology/paper_1.html">
      The fish body functions as an airfoil: surface pressures generate thrust during carangiform locomotion
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらの力学は、以前に報告されたヤツメウナギで見られるものとは対照的であり、これは陰圧で推力を生成しますが、波状の力学によってそれを行います。後部領域では、体の形と運動学の微妙な違いにより、マスはブルーギルよりも多くの推力を生成できますが、これらのcar状スイマーの前半体に負圧によって生成される推力は、体および尾びれ上に生成される総推力の28％を含み、前半体の実質的な抗力を大幅に減少させます。 
[概要]翼の形状は、翼のように作用します。形状とピッチングの動きによって生成される推力。これは、体と尾びれに生成される総推力の28％です。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Physiological and cognitive consequences of a daily 26h photoperiod in a primate (M. murinus) -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/biorxiv.physiology/paper_2.html">
      Physiological and cognitive consequences of a daily 26h photoperiod in a primate (M. murinus)
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      光周期サイクルの長さが26時間の場合、安静時の体温とエネルギー消費が有意に高く、認知能力が低いことがわかった。いくつかの生理学的および認知パラメーター（体温、エネルギー消費、酸化ストレス、認知パフォーマンス）..私たちの知る限り、この研究は概日共鳴理論の潜在的なメカニズムを強調する最初のものです。 
[概要] 1972年にピッテンドリグによって概日共鳴理論が提案されました。26時間の光周期レジメンを課す潜在的なコストを評価しました。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Systemic Bone Loss Following Myocardial Infarction in Mice is Mitigated by Treatment with a β3 Adrenergic Receptor Antagonist -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-02-21/biorxiv.physiology/paper_3.html">
      Systemic Bone Loss Following Myocardial Infarction in Mice is Mitigated by Treatment with a β3 Adrenergic Receptor Antagonist
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      同様に、MIマウスでは骨梁容積がコントロールマウスと比較して減少し、この骨損失はβ3拮抗薬治療によって部分的に減衰しました。MIマウスのBMDおよびBMCは、大腿骨および腰椎で減少しました（-6.9％大腿骨BMD、-3.5％腰椎BMD）; {β} 3拮抗薬治療により、この骨量減少反応が減少した（-5.3％大腿骨BMD、-1.2％腰部骨密度）。これらの結果は、MIが全身性骨量減少につながり、この骨量減少が{beta} 3-アンタゴニスト治療。 
[ABSTRACT] miは交感神経系の活性化を介して基礎となるアテローム性動脈硬化を悪化させる可能性があります。骨損失は治療によって部分的に減衰しました。miは循環単球レベルの増加につながりました、研究が見つかりました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-20">
        <br>2020-02-20
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
