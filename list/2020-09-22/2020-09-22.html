<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-09-22の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Correlating Subword Articulation with Lip Shapes for Embedding Aware
  Audio-Visual Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.SD/paper_0.html">
      <font color="black">Correlating Subword Articulation with Lip Shapes for Embedding Aware
  Audio-Visual Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">次に、マルチモーダルEASE（MEASE）のオーディオ機能とビジュアル機能の相補性を利用して、情報の交差する方法で、ノイズの多いスピーチと唇のビデオからオーディオビジュアル埋め込みを抽出します。提案されたサブワードベースのVEASEアプローチは、単語レベルでの従来の埋め込みよりも効果的であることを示しています。電話レベルで。 
[ABSTRACT]提案されたmeaseフレームは、従来の埋め込みよりも効果的です。唇のフレームから視覚的な埋め込みを抽出するためのプロトタイプが初めて作成されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: On Loss Functions and Recurrency Training for GAN-based Speech
  Enhancement Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.SD/paper_1.html">
      <font color="black">On Loss Functions and Recurrency Training for GAN-based Speech
  Enhancement Systems</font>
    </a>
  </h2>
  <font color="black">再発層を含めることの利点についても説明します。全体的に、客観的なメトリック損失関数と平均二乗誤差（MSE）を組み合わせたCRGANモデルは、多くの評価メトリックにわたる比較アプローチよりも優れたパフォーマンスを提供します。この研究では、音声強調のための新しい畳み込み反復GAN（CRGAN）アーキテクチャ。 
[要約]提案されたcrganモデルは、同じロスバーを使用してsota ganベースのモデルよりも優れています。また、他の非ganベースのシステムよりも優れており、音声強調にガンを使用する利点を示唆しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-29">
        <br><font color="black">2020-07-29</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-End Bengali Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.SD/paper_2.html">
      <font color="black">End-to-End Bengali Speech Recognition</font>
    </a>
  </h2>
  <font color="black">ブロックBを使用した私たちの最高のモデルは、WERが13.67で、サイズが41x11と21x11のより大きな畳み込みカーネルを使用した同等のモデルよりも1.39％減少します。2層のブロックAと4層の2つのCNNブロックを提案します。ブロックB。最初のレイヤーは7x3カーネルで構成され、後続のレイヤーは3x3カーネルのみで構成されます。公開されているラージベンガル語ASRトレーニングデータセットを使用して、複雑さと深さの異なる7つのディープニューラルネットワーク構成のパフォーマンスをベンチマークして評価しますベンガル語ASRタスクについて。 
[要約]ベンガル語の研究とリソースはほとんどありません。これらには、音声ベースの音声ベースのオサマビンの広範な使用が含まれます。ただし、研究は成功していません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Graph2Speak: Improving Speaker Identification using Network Knowledge in
  Criminal Conversational Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.SD/paper_3.html">
      <font color="black">Graph2Speak: Improving Speaker Identification using Network Knowledge in
  Criminal Conversational Data</font>
    </a>
  </h2>
  <font color="black">犯罪者の会話データの2つの候補データセット、犯罪現場調査（CSI）、テレビ番組、およびROXANNEシミュレーションデータを紹介します。以前の相互作用の頻度に基づいて候補話者を再ランク付けすることにより、話者識別ベースラインを1.2改善します。 CSIデータでは絶対％（相対1.3％）、会話の精度2.6％絶対（相対3.4％）、ROXANNEシミュレーションではそれぞれ1.1％絶対（相対1.2％）および2％絶対（相対2.5％）データ..犯罪捜査は、話者を特定し、既存の犯罪ネットワークを構築または強化するために、主に音声会話データの収集に依存しています。 
[ABSTRACT]ソーシャルネットワーク分析ツールを使用して主要なキャラクターを識別します。会話情報のメトリックも紹介します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-03">
        <br><font color="black">2020-06-03</font>
      </time>
    </span>
</section>
<!-- paper0: Open-set Short Utterance Forensic Speaker Verification using
  Teacher-Student Network with Explicit Inductive Bias -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.SD/paper_4.html">
      <font color="black">Open-set Short Utterance Forensic Speaker Verification using
  Teacher-Student Network with Explicit Inductive Bias</font>
    </a>
  </h2>
  <font color="black">訓練されたディープスピーカー埋め込みネットワークを小さなターゲットデータセットに対して堅牢になるように進めるために、モデルを微調整の開始点と調整点として利用することにより、事前訓練された学生モデルをフォレンジックターゲットドメインに向けて微調整する新しい戦略を導入します。正則化の参照..提案された目的関数は、短い発話での教師-生徒の学習のパフォーマンスを効率的に改善できること、および私たちの微調整戦略は、事前に訓練されたものに対して明示的な帰納的バイアスを提供することにより、一般的に使用される重み減衰法よりも優れていることを示します。モデル..提案されたアプローチは、1st48-UTD法医学コーパス（管理されていない状態で記録された短い発話からなる実際の殺人調査の新しく確立された自然主義的なデータセット）で評価されます。 
[ABSTRACT]小さなフォレンジックフィールドのデータセットを使用して新しいシステムを開発できます。メソッドは、単語分類の損失、カルバックライブラー、埋め込みの類似性に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: DiffWave: A Versatile Diffusion Model for Audio Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.SD/paper_5.html">
      <font color="black">DiffWave: A Versatile Diffusion Model for Audio Synthesis</font>
    </a>
  </h2>
  <font color="black">DiffWaveは、音声品質の面で強力なWaveNetボコーダーに一致することを示しています（MOS：4.44対4.43）。桁違いに高速に合成します。特に、困難な無条件生成タスクで自己回帰およびGANベースの波形モデルを大幅に上回っています。オーディオ品質と、さまざまな自動および人間による評価からのサンプルの多様性に関して。DiffWaveは、メルスペクトログラム、クラス条件付き生成、無条件生成を条件とするニューラルボコーディングを含む、さまざまな波形生成タスクで高忠実度のオーディオを生成します。 
[ABSTRACT] diffwaveは、さまざまな波形生成タスクで忠実度の高いオーディオを生成します。自己回帰およびガンベースの波形モデルに一致します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Residual Shuffle-Exchange Networks for Fast Processing of Long Sequences -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.SD/paper_6.html">
      <font color="black">Residual Shuffle-Exchange Networks for Fast Processing of Long Sequences</font>
    </a>
  </h2>
  <font color="black">ただし、モデルは非常に複雑で、Gated Recurrent Unitから派生した洗練されたゲーティングメカニズムを含みます。提案されたアーキテクチャは、より長いシーケンスにスケーリングするだけでなく、より速く収束し、より良い精度を提供します。これは、 LAMBADA言語モデリングタスクは、パラメーターの数を効率化しながら、MusicNetデータセットで音楽の文字起こしのための最先端のパフォーマンスを実現します。 
[ABSTRACT]最近導入されたニューラルシャッフル-交換ネットワークは、geluとレイヤーの正規化を使用した残余ネットワークに基づいています。シャッフルを超えますが、それを超えますが、musicnetデータセットの複雑さを防ぎます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-06">
        <br><font color="black">2020-04-06</font>
      </time>
    </span>
</section>
<!-- paper0: Detecting Acoustic Events Using Convolutional Macaron Net -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.SD/paper_7.html">
      <font color="black">Detecting Acoustic Events Using Convolutional Macaron Net</font>
    </a>
  </h2>
  <font color="black">2つの類似したモデルを同期的にトレーニングするMean-Teacherアプローチとは対照的に、モデルの1つがフレームレベルの予測を提供し、他のモデルがクリップレベルの予測を提供する2つの異なるCMNを同期的にトレーニングすることを提案します。 CNNとコンフォーマーの組み合わせを利用する課題の最初の場所では、システムもわずかに0.3％勝っています。提案されたフレームワークに基づいて、システムは、音響シーンおよびイベントの検出と分類（DCASE）のベースラインシステムよりも優れています。 2020年チャレンジタスク4のマージンは10％を超えています。 
[要旨] cnnの新しいシステムは、畳み込みニューラルネットワークとマカロンネット（mn）を組み合わせたもので、畳み込みマカロンnet.cnnと呼ばれます。このプロジェクトは、システムからのデータの欠如に基づいています。新しいシステム、データ疑似ラベル付きのラベル付きモデルを上回る</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Semi-Supervised NMF-CNN For Sound Event Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.SD/paper_8.html">
      <font color="black">Semi-Supervised NMF-CNN For Sound Event Detection</font>
    </a>
  </h2>
  <font color="black">続いて、近似された強くラベル付けされたデータを使用して、2つの異なるCNNが半教師付きフレームワークでトレーニングされます。1つのCNNはクリップレベルの予測に使用され、もう1つはフレームレベルの予測に使用されます。DCASE2020チャレンジタスクでモデルをテストする4つのテストセットで、モデルは44.4％のイベントベースのF1スコアを達成できますが、アンサンブルシステムは46.3％のイベントベースのF1スコアを達成できます。主なアイデアは、NMFを使用して弱くラベル付けされたデータ。 
[要約]このアイデアを使用して、モデルはイベントを達成できます。このアイデアに基づいています。音響シーンとイベントの検出と分類に45.7％のマークを達成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-02">
        <br><font color="black">2020-07-02</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Deep learning achieves radiologist-level performance of tumor
  segmentation in breast MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_0.html">
      <font color="black">Deep learning achieves radiologist-level performance of tumor
  segmentation in breast MRI</font>
    </a>
  </h2>
  <font color="black">私たちは、いくつかの3D深い畳み込みニューラルネットワークアーキテクチャ、入力モダリティ、および調和方法の中から選択しました。テストセットは、4人の放射線科医が独立して腫瘍をセグメント化した250の検査から構成されました。結果の測定は、2DセグメンテーションのDiceスコアであり、ウィルコクソン符号順位検定とTOST手順を使用するネットワークと放射線科医。 
[要約]私たちは、2002年から2014年の間に1つの臨床サイトで受診した12歳から94歳の女性（平均年齢54歳）から遡及的に収集された38、229の乳房検査を活用しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Temporal Huber regularization for DCE-MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_1.html">
      <font color="black">Temporal Huber regularization for DCE-MRI</font>
    </a>
  </h2>
  <font color="black">アプローチは、ラットの脳標本からのシミュレーションおよび実験的な放射状黄金角DCE-MRIデータを使用してテストされます。DCE-MRIの時間的正則化のためのHuberペナルティの使用を提案し、それを全変動、全一般変動および平滑性に基づいて比較します時間的正則化モデル。動的血管造影イメージング（DCE-MRI）は、微小血管構造と組織灌流の研究に使用されます。 
[要約]再構成への正則化の効果は以前の研究と比較されています。結果は、フーバー正則化が全変動ベースのモデルで同様の再構成精度を生成することを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-19">
        <br><font color="black">2020-03-19</font>
      </time>
    </span>
</section>
<!-- paper0: Text Synopsis Generation for Egocentric Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_2.html">
      <font color="black">Text Synopsis Generation for Egocentric Videos</font>
    </a>
  </h2>
  <font color="black">生成された説明の5％（またはそれ以下）のみを含む生成されたテキストの要約は、自然言語処理で確立されたメトリックを使用して、テキストドメインのグラウンドトゥルースの要約と比較されます。次に、弱く監視された目的、正しい説明を特定します。ユーザーは短いテキストを読んでビデオに関する洞察を得ることができ、さらに重要なことに、テキストクエリを使用して大規模なビデオデータベースのコンテンツを効率的に検索できます。 
[要旨]以前のビデオ要約アルゴリズムは、そのようなビデオの閲覧を加速できます。これらの説明は、自己中心的なビデオに基づいています。ネットワークは、各ショットのテキスト説明を生成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-08">
        <br><font color="black">2020-05-08</font>
      </time>
    </span>
</section>
<!-- paper0: A Foreground-background Parallel Compression with Residual Encoding for
  Surveillance Video -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_3.html">
      <font color="black">A Foreground-background Parallel Compression with Residual Encoding for
  Surveillance Video</font>
    </a>
  </h2>
  <font color="black">デコード側では、フォアグラウンドとバックグラウンドの合成とフレーム品質の向上を実現するために、粗密の2段階モジュールが適用されます。このホワイトペーパーでは、フォアグラウンドを抽出して圧縮するビデオ圧縮方法を提案します。実験結果は、HECVデータセットで同じPSNR（36 dB）を達成するために、提案された方法が従来のアルゴリズムH.265よりも69.5％bpp（ビット/ピクセル）少ないことを示しています。 
[要約]提案された方法は、従来のアルゴリズムhよりも69.5％少ないbpp（ビット/ピクセル）を必要とします。 hecvデータセットで同じpsnr（36 db）を達成するには265</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-18">
        <br><font color="black">2020-01-18</font>
      </time>
    </span>
</section>
<!-- paper0: SAR Image Despeckling by Deep Neural Networks: from a pre-trained model
  to an end-to-end training strategy -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_4.html">
      <font color="black">SAR Image Despeckling by Deep Neural Networks: from a pre-trained model
  to an end-to-end training strategy</font>
    </a>
  </h2>
  <font color="black">CNNトレーニングには、優れたトレーニングデータが必要です。スペックルフリー/スペックル破損画像の多くのペアが必要です。スペックルフリー画像の固有の不足を考えると、これはSARアプリケーションの問題です。さまざまな可能なアプローチの中で、畳み込みニューラルに基づく方法ネットワーク（CNN）は最近、SAR画像の復元で最先端のパフォーマンスを発揮することが示されています。 
[ABSTRACT] cnnトレーニングには適切なトレーニングデータが必要です：多くのスペックルフリー/スペックル破損画像のペア。ハイブリッドアプローチは、スペックルの評価に使用されるcnnのトレーニングにも使用されます-アート</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-28">
        <br><font color="black">2020-06-28</font>
      </time>
    </span>
</section>
<!-- paper0: Low-Cost Implementation of Bilinear and Bicubic Image Interpolation for
  Real-Time Image Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_5.html">
      <font color="black">Low-Cost Implementation of Bilinear and Bicubic Image Interpolation for
  Real-Time Image Super-Resolution</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、これらのプラットフォーム、主にモバイルアプリケーション用に2つのハードウェア効率の良い補間方法を提案します。合成および実際の画像シーケンスに関する実験と結果は、提案されたスキームのパフォーマンスを明確に検証します。画像補間は、 SR 
[ABSTRACT]テクニックの重要なステップは、フィールドプログラマブルゲートアレイ（fpga）デバイスで使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Mapping horizontal and vertical urban densification in Denmark with
  Landsat time-series from 1985 to 2018: a semantic segmentation solution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_6.html">
      <font color="black">Mapping horizontal and vertical urban densification in Denmark with
  Landsat time-series from 1985 to 2018: a semantic segmentation solution</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、ディープネットワークの実装とマルチスケールのコンテキスト情報を含めることで、分類とモデルの時空間全体での一般化能力が大幅に向上することを示しています。DeepLabを使用することで、F1スコアを4および10パーセント増加できます。デンマークのFCNとRFと比較した都市の垂直成長の検出に使用します。2014年のデータでトレーニングしたモデルをデンマークの2006年と1995年に適用できるかどうかをテストし、デンマークのデータでトレーニングしたモデルを使用して正確にマッピングできるかどうかを調べます他のヨーロッパの都市。 
[ABSTRACT] deeplabは、十分なトレーニングデータが利用可能な場合、fcnよりも正確な水平および垂直分類を提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-15">
        <br><font color="black">2020-09-15</font>
      </time>
    </span>
</section>
<!-- paper0: Improving distribution and flexible quantization for DCT coefficients -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_7.html">
      <font color="black">Improving distribution and flexible quantization for DCT coefficients</font>
    </a>
  </h2>
  <font color="black">両方を最適化すると、ここでは量子化がほぼ均一になり、テール処理が自動化されます。特に、このような連続分布の場合、最適化された連続\ emph {量子化密度関数} $ q $による量子化アプローチについても説明します。これは、逆CDF（累積分布関数）です。 ）通常の格子上の$ Q $ $ \ {Q ^ {-1}（（i-1 / 2）/ N）：i = 1 \ ldots N \} $は量子化ノードを提供します-最適化された（非-uniform）量子化-さまざまなサイズ$ N $で、レート歪み制御を使用します。歪みのみに対して$ q $を最適化すると、大幅に改善されますが、より均一な分布によりエントロピーが増加します。 
[ABSTRACT]これは、dctカクテルの分布を予測することについても説明されています。これらは、dctブロックの既にデコグされた領域からのものです。これらの要因には、最適化された連続＆mu mu mu強調が含まれます。これらは、最適化された「不均一」量子化を含みます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: WCE Polyp Detection with Triplet based Embeddings -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_8.html">
      <font color="black">WCE Polyp Detection with Triplet based Embeddings</font>
    </a>
  </h2>
  <font color="black">この方法の重要な点は、データセットが小さい場合に画像からの特徴抽出を改善することを目的としたトリプレット損失関数を使用することです。深い畳み込みニューラルネットワークとメトリックを組み合わせた新しいポリープコンピューター支援決定システムを開発しました学習..医師がカプセル内視鏡ビデオを評価するのに必要な時間を短縮するために、自動画像分析手法を使用できますが、これらの手法はまだ研究段階にあります。 
[ABSTRACT]現在の分析は、ビデオのフレームのほぼすべての1つを手動で検査することで行うことができます。これは、面倒でエラーが発生しやすいタスクです。この方法は、ポリープ検出器の結果を視覚的に説明するために使用されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-10">
        <br><font color="black">2019-12-10</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Uni- and Multi-modal Stream Networks for Multimodal Image
  Registration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_9.html">
      <font color="black">Adversarial Uni- and Multi-modal Stream Networks for Multimodal Image
  Registration</font>
    </a>
  </h2>
  <font color="black">画像から画像への変換によってマルチモーダル問題（CTからMRなど）を単峰問題（MRからMRなど）に変換しようとする他の変換ベースの方法とは異なり、この方法は変形フィールドを活用します（i）翻訳されたMR画像と（ii）元のCT画像をデュアルストリーム方式で推定し、それらを融合してより良い登録パフォーマンスを達成する方法を自動的に学習します。この方法は、2つの臨床データセットとは、最先端の従来の方法や学習ベースの方法と比較して有望な結果を示しています。マルチモーダル登録ネットワークは、グラウンドトゥルースの変形なしに、計算効率の高い類似性メトリックによって効果的にトレーニングできます。 
[要旨]変形可能認識方法は変形可能画像登録方法を作成するために使用できます。変形可能画像認識方法を使用する新しい方法に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-06">
        <br><font color="black">2020-07-06</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Automated COVID-19 Grading with Convolutional Neural Networks
  in Computed Tomography Scans: An Ablation Study -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_10.html">
      <font color="black">Improving Automated COVID-19 Grading with Convolutional Neural Networks
  in Computed Tomography Scans: An Ablation Study</font>
    </a>
  </h2>
  <font color="black">これらのコンポーネントの選択は、体系的ではなく実際的であることが多かった。これらのコンポーネントを使用した3D CNNは、105のCTスキャンのテストセットでROC曲線（AUC）の下で0.934の面積を達成し、公的に入手可能なセットで0.923のAUCを達成した。 742 CTスキャン、以前に公開された2D CNNと比較して大幅な改善。このペーパーでは、CT画像からのCOVID-19グレーディングのためのCNNベースのアルゴリズムのパフォーマンスを向上させるさまざまなコンポーネントを特定します。 
[ABSTRACT]これらの研究の多くは、一般的に使用されるコンポーネントから組み立てられたアルゴリズムの初期結果を報告することに焦点を当てていました。これらは、3d ctボリュームの処理に最適ではない場合でも、2d cnnsを使用した複数の研究を含みました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Morphological Reconstruction of Detached Dendritic Spines via Geodesic
  Path Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_11.html">
      <font color="black">Morphological Reconstruction of Detached Dendritic Spines via Geodesic
  Path Prediction</font>
    </a>
  </h2>
  <font color="black">2つの光子顕微鏡データの実験的分析は、平均絶対再構成エラーの観点から、最先端の技術に対して12.5％の改善が見られる私たちの方法の有効性を示しています。蛍光顕微鏡からの樹状突起棘の形態学的再構成は、神経画像解析における重大な未解決の問題。この問題に対処し、脊椎首の再構成の可能性のある経路空間から最適解を求める確率的フレームワークに基づく教師なし経路予測手法を紹介します。 
[ABSTRACT]私たちの方法は、外れ値による偏りを減らすように特別に設計されています。ノイズとコントラストの低下に悩まされている画像から、困難な形状を再構築することに長けています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-19">
        <br><font color="black">2020-03-19</font>
      </time>
    </span>
</section>
<!-- paper0: Image denosing in underwater acoustic noise using discrete wavelet
  transform with different noise level estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_12.html">
      <font color="black">Image denosing in underwater acoustic noise using discrete wavelet
  transform with different noise level estimation</font>
    </a>
  </h2>
  <font color="black">およびsymlet（sym ..多くのアプリケーションでは、画像のノイズ除去と改善は、水中などの色付きノイズが存在する場合の重要なプロセスを表しています。.ピーク信号対ノイズ比（PSNR）と平均二乗誤差は、この結果のパフォーマンス測定を表しています。 
[ABSTRACT]このホワイトペーパーでは、さまざまな基底関数をもつ離散ウェーブレット変換でマルチレベルのノイズパワー推定を使用して、新しい画像ノイズ除去方法を提案します。ドーベチーや双直交表示などのウェーブレットのさまざまなベースの結果この方法で使用するノイズ除去プロセスは、非常に目立つ画像を生成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-19">
        <br><font color="black">2019-04-19</font>
      </time>
    </span>
</section>
<!-- paper0: RF-Based Low-SNR Classification of UAVs Using Convolutional Neural
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_13.html">
      <font color="black">RF-Based Low-SNR Classification of UAVs Using Convolutional Neural
  Networks</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、低信号対雑音比（SNR）領域での無線周波数（RF）フィンガープリントからの無人航空機（UAV）の分類の問題を調査します。両方のRF時間でトレーニングされた畳み込みニューラルネットワーク（CNN）を使用します。 15種類の既製ドローンコントローラーのRF信号のシリーズ画像とスペクトログラム。スペクトログラムを使用したRF信号の時系列表現とは対照的に、目的の周波数間隔、つまり2.4 GHzのみに焦点を合わせることが可能です。 ISM帯域、およびこの帯域外の他の信号成分をフィルターで除去します。 
[要約]コンバージョンはrf時系列画像と15種類の既製のドローンコントローラーrf信号のスペクトログラムでトレーニングされます。snrが減少すると、ディスコ内の情報がノイズで失われるため、このアプローチは劇的に失敗します。封筒がひどく歪んでいる</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-11">
        <br><font color="black">2020-09-11</font>
      </time>
    </span>
</section>
<!-- paper0: Balance Scene Learning Mechanism for Offshore and Inshore Ship Detection
  in SAR Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_14.html">
      <font color="black">Balance Scene Learning Mechanism for Offshore and Inshore Ship Detection
  in SAR Images</font>
    </a>
  </h2>
  <font color="black">さまざまなシーンのサンプル数の大きな不均衡は、合成開口レーダー（SAR）船の検出精度を大幅に低下させます。したがって、この問題を解決するために、このレターでは、SAR画像でのオフショアおよびインショアの船検出のためのバランスシーン学習メカニズム（BSLM）を提案します。 
[ABSTRACT]このレターは、sar画像でのオフショアおよびインショアの船舶検出のためのバランスシーン学習メカニズム（bslm）を提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Impact of lung segmentation on the diagnosis and explanation of COVID-19
  in chest X-ray images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_15.html">
      <font color="black">Impact of lung segmentation on the diagnosis and explanation of COVID-19
  in chest X-ray images</font>
    </a>
  </h2>
  <font color="black">U-Net CNNアーキテクチャを使用して肺のセグメンテーションを実行し、VGG、ResNet、およびInceptionの3つの有名なCNNアーキテクチャを使用して分類を実行しました。異なるソースからCOVID-19 CXR画像データベースを作成することの影響を評価しました。データベースバイアス、および1つのデータベースから別のデータベースへのCOVID-19の一般化。バイアスの少ないシナリオを表しています。肺のセグメンテーションの影響を推定するために、LIMEやGrad-CAMなどの説明可能な人工知能（XAI）を適用しました。 
[要約]このペーパーの主な目的は、cox-19画像を使用したcovid-19自動識別における肺セグメンテーションの影響を示すことです。データベースバイアスと呼ばれるさまざまなソースからcxr画像データベースを作成する効果と、covid-19あるデータベースから別のデータベースへの一般化、偏りの少ないシナリオを表す</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Data-driven regularization parameter selection in dynamic MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_16.html">
      <font color="black">Data-driven regularization parameter selection in dynamic MRI</font>
    </a>
  </h2>
  <font color="black">1つ目は、時間と空間の両方の正則化ドメインで予想されるスパース性を生成するパラメーターペアの2D検索です。アプローチは、シミュレーションおよび実験的なDCE-MRIを使用して評価されます。予想されるスパースレベルは、時間的データの測定データから取得されます。正則化および空間的正則化のための参照画像から。 
[ABSTRACT]モデルは、圧縮センシング（cs）ベースの再構成アプローチの人気につながっています。1つのアプローチでは、データ主導のアプローチが全変動正則化ツールに提案されています。再構成により、正則化ドメインから期待されるスパースレベルが得られます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-03">
        <br><font color="black">2020-04-03</font>
      </time>
    </span>
</section>
<!-- paper0: AdderSR: Towards Energy Efficient Image Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_17.html">
      <font color="black">AdderSR: Towards Energy Efficient Image Super-Resolution</font>
    </a>
  </h2>
  <font color="black">具体的には、加算器の操作では、画像処理タスクに不可欠なアイデンティティマッピングを簡単に学習できません。次に、特徴の分布を調整し、詳細を調整するための学習可能な電源投入を開発します。さらに、ハイパスフィルターの機能では、 AdderNetによって保証されます。 
[ABSTRACT]たとえば、addernetはaddernetを使用して出力機能を計算し、大量のエネルギー消費を回避します。addernetは、写真認識ではなく、added機能を使用して追加機能を計算します。addernetは、cnnベースラインと同等の成功を収めることができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-18">
        <br><font color="black">2020-09-18</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Attention Based Ultra Lightweight Image Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_18.html">
      <font color="black">Multi-Attention Based Ultra Lightweight Image Super-Resolution</font>
    </a>
  </h2>
  <font color="black">包括的な実験は、既存の最先端技術に対する私たちのモデルの優位性を示しています。各FFGには、提案されたマルチアテンションブロック（MAB）のスタックが含まれており、新しい機能フュージョン構造に結合されています。MAFFSRNは、提案された特徴抽出ブロックとして機能する特徴融合グループ（FFG）。 
[ABSTRACT] maffsrnは、機能解決として機能する提案された機能削除グループ（ffgs）で構成されます。コスト効率の高い注意メカニズム（cea）を備えたmabは、複数の注意メカニズムを使用して機能を改良および抽出するのに役立ちます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-29">
        <br><font color="black">2020-08-29</font>
      </time>
    </span>
</section>
<!-- paper0: Reconstruct high-resolution multi-focal plane images from a single 2D
  wide field image -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_19.html">
      <font color="black">Reconstruct high-resolution multi-focal plane images from a single 2D
  wide field image</font>
    </a>
  </h2>
  <font color="black">さらに、MFPINetは、同じボリューム画像を再構成するための現在のリフォーカス方法よりも約24倍高速です。細胞診顕微鏡画像で一連の実験を行い、MFPINetが軸方向のリフォーカスと水平方向の超解像の両方で良好に機能することを示しています。現実的なMFP画像を取得するには提案されたMFPINetは、生成的な敵対的なネットワークフレームワークと、すべてのフォーカルプレーンを一度にポストサンプリングおよびリフォーカスする戦略を採用しています。 
[ABSTRACT] mfpinetは高速のエンドツースキャンネットワーク（mfpinet）です。スキャンに依存することなく、単一の2d低解像度のワイルドファイル画像から高解像度画像を再構築する手法を使用します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Detail reconstruction in binary ghost imaging by using point-by-point
  method -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.IV/paper_20.html">
      <font color="black">Detail reconstruction in binary ghost imaging by using point-by-point
  method</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、ターゲットの認識など、イメージングの詳細に対する要件が高い領域で潜在的なアプリケーションがある可能性があります。数値および実験結果は、従来のゴーストイメージングと比較した場合、この方法によってターゲットの詳細を再構築できることを示しています。異なる2値化手法によるスペックルパターンの違いについては、対応する説明も行います。 
[ABSTRACT]この方法は、画像品質の低下を補正できます。2値化プロセス中の情報の損失も補正できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Feature Flow: In-network Feature Flow Estimation for Video Object
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_0.html">
      <font color="black">Feature Flow: In-network Feature Flow Estimation for Video Object
  Detection</font>
    </a>
  </h2>
  <font color="black">当社のIFF-Netは、既存の方法を上回り、ImageNet VIDで最先端のパフォーマンスを設定します。このホワイトペーパーでは、この事実上のパラダイムを再考し、ビデオオブジェクト検出タスクの欠点を分析します。さらに、 \ textit {self-supervision}に基づく変換残留損失（TRL）。これにより、IFF-Netのパフォーマンスがさらに向上します。 
[ABSTRACT] iff-net）は、ビデオオブジェクト検出用の新しいネットワークです。これには、ビデオオブジェクト検出用の「iffモジュール」が含まれます。これらには、「textbf 
[i] n-network」を含むネットワーク（iff-network）が含まれます。微調整されたネットワークがビデオ検出の負荷を生成することが期待されているという事実に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: When Healthcare Meets Off-the-Shelf WiFi: A Non-Wearable and Low-Costs
  Approach for In-Home Monitoring -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_1.html">
      <font color="black">When Healthcare Meets Off-the-Shelf WiFi: A Non-Wearable and Low-Costs
  Approach for In-Home Monitoring</font>
    </a>
  </h2>
  <font color="black">それらに基づいて、高齢者の行動データ、生理学的データ、および派生情報（たとえば、異常なイベントや基礎疾患）を介護者が直接見ることができます。人間のポーズをキャプチャする一連の信号処理方法とニューラルネットワークを設計しますWifiチャネル状態情報（CSI）から呼吸状態曲線を抽出して抽出します。広範な実験が行われ、その結果によると、既製のWiFiデバイスは、カメラと同様に、壁と正確な呼吸状態を追跡することで、在宅モニタリングのアプローチの有効性と実現可能性を実証します。 
[ABSTRACT]提案されたアプローチは、壁を越えて細かい人間のポーズフィギュアをキャプチャし、オフ-棚のwifiデバイスで詳細な呼吸状態を同時に追跡できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Objects detection for remote sensing images based on polar coordinates -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_2.html">
      <font color="black">Objects detection for remote sensing images based on polar coordinates</font>
    </a>
  </h2>
  <font color="black">P-RSDetメソッドでは、中心点を予測し、1つの極半径と2つの極角を回帰することにより、任意指向のオブジェクト検出を実現できます。DOTA、UCAS-AOD、およびNWPU VHR-10データセットでの実験は、P-RSDetが達成することを示しています。より単純なモデルと少ない回帰パラメーターによる最先端のパフォーマンス。ただし、深層学習に基づく現在の最先端の検出器はすべてデカルト座標でモデル化されています。 
[ABSTRACT]以前の研究では、回転オブジェクトモデリングの問題を処理する際に極座標系が明らかな利点を持っている</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-09">
        <br><font color="black">2020-01-09</font>
      </time>
    </span>
</section>
<!-- paper0: Batch Coherence-Driven Network for Part-aware Person Re-Identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_3.html">
      <font color="black">Batch Coherence-Driven Network for Part-aware Person Re-Identification</font>
    </a>
  </h2>
  <font color="black">最初に、ディープバックボーンモデルの出力から各部分に関連するチャネルを強調表示するバッチコヒーレンスガイダンスチャネルアテンション（BCCA）モジュールを紹介します。したがって、この作業では、バッチコヒーレンス主導という名前のシンプルなフレームワークを提案します。意味的に位置合わせされたパーツの機能を学習しながら、トレーニングとテストの両方の段階で身体のパーツの検出をバイパスするネットワーク（BCD-Net）。トレーニング画像のバッチを使用してチャネルパーツの対応を調査し、BCCAに役立つ新しいバッチレベルの監視信号を課します部品に関連するチャネルを識別するため。 
[ABSTRACT]低品質画像のパーツ検出はシンプルな部品競合ツールですが、シンプルなパーツ追跡ツールは一連の変更にco2 co2を追加します。画像の一部の統計が安定していることが重要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-task Learning with Coarse Priors for Robust Part-aware Person
  Re-identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_4.html">
      <font color="black">Multi-task Learning with Coarse Priors for Robust Part-aware Person
  Re-identification</font>
    </a>
  </h2>
  <font color="black">ATには、トレーニング画像用のボディパーツの位置の粗い事前位置が装備されています。MPNには3つの重要な利点があります。1）推論段階でボディパーツの検出を行う必要がない。 2）モデルは非常にコンパクトで、トレーニングとテストの両方に効率的です。 3）トレーニング段階では、ボディパーツの場所の大まかな事前情報のみが必要であり、簡単に取得できます。コンセプトの転送は、2つの新しい位置合わせ戦略によって実現されます。つまり、ハードパラメーター共有によるパラメーター空間の位置合わせと、クラスごとの方法。 
[ABSTRACT]マルチタスクパーツアウェアネットワーク（mpn）は、歩行者の画像からセマンティックに整列されたパーツレベルの特徴を抽出するように設計されています。上部のボディパーツごとに1つのメインタスク（mt）と1つの補助タスク（at）を構築します同じバックボーンmodel.atsのmtパラメータを最適化することでボディパーツのコンセプトをmtsに転送し、パーツを識別します-バックボーンモデルの関連チャネル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-18">
        <br><font color="black">2020-03-18</font>
      </time>
    </span>
</section>
<!-- paper0: Faster Gradient-based NAS Pipeline Combining Broad Scalable Architecture
  with Confident Learning Rate -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_5.html">
      <font color="black">Faster Gradient-based NAS Pipeline Combining Broad Scalable Architecture
  with Confident Learning Rate</font>
    </a>
  </h2>
  <font color="black">一方、BCNNは幅広いスケーラブルなアーキテクチャであり、そのトポロジは深いものと比較して2つの利点を実現します。主に、シングルステップのトレーニング速度の高速化とメモリ効率の向上を含みます（つまり、DARTSは、勾配ベースの最適なアーキテクチャを発見します。最適化アルゴリズム。これは、BCNNの2つの優位性から同時に恩恵を受けます。アーキテクチャ検索のバッチサイズが大きくなります）。これらはすべて、NASの検索効率の向上に貢献しています。 
[ABSTRACT] b-dartsでは、幅広い畳み込みニューラルネットワーク（bcnn）が、人気のある微分可能なnasアプローチであるdartsのスケーラブルなアーキテクチャとして採用されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-18">
        <br><font color="black">2020-09-18</font>
      </time>
    </span>
</section>
<!-- paper0: Feed-Forward On-Edge Fine-tuning Using Static Synthetic Gradient Modules -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_6.html">
      <font color="black">Feed-Forward On-Edge Fine-tuning Using Static Synthetic Gradient Modules</font>
    </a>
  </h2>
  <font color="black">ロボットが新しいオブジェクトを把握するために学習する必要があるシナリオを1回だけ示して、ロボットの把握シナリオでメソッドをテストしました。代わりに、フォワードパス中に、静的合成勾配モジュール（SGM）が各レイヤーの勾配を予測します。この方法では、標準の逆伝播を使用した場合と同等の結果が得られます。 
[ABSTRACT]これにより、すべてのアクティベーションを保存する必要なく、フィードフォワード方式でモデルをトレーニングできます。これにより、モデルを学習して新しいオブジェクトを把握することができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: A Dataset of Laryngeal Endoscopic Images with Comparative Study on
  Convolution Neural Network Based Semantic Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_7.html">
      <font color="black">A Dataset of Laryngeal Endoscopic Images with Comparative Study on
  Convolution Neural Network Based Semantic Segmentation</font>
    </a>
  </h2>
  <font color="black">目的医用画像分析における解剖学的構造の自動セグメンテーションは、自律診断のほか、さまざまなコンピューターやロボット支援による介入の前提条件です。各メソッドの精度を測定するために、Intersection-over-Union（IoU）評価メトリックが使用されました。 4つの機械学習ベースのメソッドSegNet、UNet、ENet、ErfNetは、人間の喉頭の新しい7クラスのデータセットを監視しながらトレーニングされました。 
[要約]深い畳み込みニューラルネットワークに基づく最近の手法は、以前のヒューリスティック手法よりも優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-07-16">
        <br><font color="black">2018-07-16</font>
      </time>
    </span>
</section>
<!-- paper0: Hybrid Tensor Decomposition in Neural Network Compression -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_8.html">
      <font color="black">Hybrid Tensor Decomposition in Neural Network Compression</font>
    </a>
  </h2>
  <font color="black">後者は最も広く使用されている分解法であり、HTのバリアントであるため、ウェイトマトリックスと畳み込みカーネルをHTとTTの両方の形式に変換します。後者は、ニューラルネットワーク圧縮のハイブリッドテンソル分解の見通しを明らかにします。この現象に基づいて、TTとHTを組み合わせて畳み込み部分と完全に接続された部分を別々に圧縮し、畳み込みニューラルネットワーク（CNN）でTTまたはHT形式のみを使用するよりも高い精度を実現するハイブリッドテンソル分解の戦略を提案します。 
[要約]モデルのサイズが大きくなるため、dnnsの現在の需要が高まっています。dnnsを使用して、ニューラルネットワーク圧縮の新しいシステムを開発できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-29">
        <br><font color="black">2020-06-29</font>
      </time>
    </span>
</section>
<!-- paper0: Modeling Score Distributions and Continuous Covariates: A Bayesian
  Approach -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_9.html">
      <font color="black">Modeling Score Distributions and Continuous Covariates: A Bayesian
  Approach</font>
    </a>
  </h2>
  <font color="black">3つの実験は、私たちのアプローチの正確さと有効性を示しています。最初に、年齢と顔の検証パフォーマンスの関係を調査し、以前の方法がパフォーマンスと信頼性を誇張している可能性があることを発見します。モデルパフォーマンスの多変量表面。 
[ABSTRACT]連続的な共変量に対するモデルのパフォーマンスは、調査が特に困難です。混合モデルを使用して、任意の分布分布とローカル基底関数をキャプチャします。以前の方法は、パフォーマンスと信頼性を誇張している場合があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Conditional Automated Channel Pruning for Deep Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_10.html">
      <font color="black">Conditional Automated Channel Pruning for Deep Neural Networks</font>
    </a>
  </h2>
  <font color="black">この問題に対処するために、条件付き自動チャネルプルーニング（CACP）メソッドを提案して、単一のチャネルプルーニングプロセスを通じて異なる圧縮率の圧縮モデルを取得します。複数の圧縮率を考慮する場合、チャネルプルーニングプロセスを複数回繰り返す必要があります。これは非常に非効率的ですが不必要です。このために、入力として任意の圧縮率を取り、対応する圧縮モデルを出力する条件付きモデルを開発します。 
[要約]これは、複数の圧縮率を考慮する場合、チャネルプルーニングプロセスを複数回繰り返す必要があるため、非常に非効率的ですが不要です。このために、任意の圧縮率を固定値として受け取り、対応する圧縮モデル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Contrastive Clustering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_11.html">
      <font color="black">Contrastive Clustering</font>
    </a>
  </h2>
  <font color="black">インスタンスレベルとクラスターレベルの対照的な損失を同時に最適化することにより、モデルは表現とクラスター割り当てをエンドツーエンドで共同で学習します。特に、CCはCIFAR-10（CIFAR-10）で0.705（0.431）のNMIを達成します。 -100）データセット。これは、最良のベースラインと比較して最大19 \％（39 \％）のパフォーマンスの改善です。そこで、インスタンスレベルとクラスターレベルの対比学習は、正のペアの類似性と負のペアの最小化
[ABSTRACT]データセットに加えて、ポジティブとネガティブのインスタンスペアは、データ拡張によって構築されます。これらは、次に特徴空間に投影されます。これらの例は異なることが示されています。結果は、ccが17の競合クラスタリング手法よりも優れていることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Text Synopsis Generation for Egocentric Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_12.html">
      <font color="black">Text Synopsis Generation for Egocentric Videos</font>
    </a>
  </h2>
  <font color="black">私たちは、各ビデオの長さが3〜5時間で、平均して3000を超えるテキストの説明に関連付けられている、挑戦的なUT Egocentricビデオデータセットのフレームワークを検証します。生成されたテキストの要約（生成された説明の5％（またはそれ以下）のみを含む） 、自然言語処理で確立されたメトリックを使用して、テキストドメインのグラウンドトルースサマリーと比較されます。ユーザーは短いテキストを読んでビデオに関する洞察を得ることができ、さらに重要なことに、テキストクエリを使用して大規模なビデオデータベースのコンテンツを効率的に検索できます。 
[要旨]以前のビデオ要約アルゴリズムは、そのようなビデオの閲覧を加速できます。これらの説明は、自己中心的なビデオに基づいています。ネットワークは、各ショットのテキスト説明を生成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-08">
        <br><font color="black">2020-05-08</font>
      </time>
    </span>
</section>
<!-- paper0: CURL: Contrastive Unsupervised Representations for Reinforcement
  Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_13.html">
      <font color="black">CURL: Contrastive Unsupervised Representations for Reinforcement
  Learning</font>
    </a>
  </h2>
  <font color="black">私たちのコードはオープンソースであり、https：//github.com/MishaLaskin/curl。で入手できます。CURLは、対照学習を使用して生のピクセルから高レベルの機能を抽出し、抽出された機能に加えてポリシー外の制御を実行します。CURLはパフォーマンスが優れています。 DeepMind Control SuiteとAtari Gamesの複雑なタスクに関する以前のピクセルベースの方法（モデルベースとモデルフリーの両方）で、それぞれ100K環境での1.9倍と1.2倍のパフォーマンスの向上とインタラクションステップのベンチマークを示しました。 
[ABSTRACT] curlは、対照学習に基づいて生のピクセルから高レベルの特徴を抽出します。対照学習に基づいて、抽出された特徴に基づいてオフポリシー制御を実行します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-08">
        <br><font color="black">2020-04-08</font>
      </time>
    </span>
</section>
<!-- paper0: A Foreground-background Parallel Compression with Residual Encoding for
  Surveillance Video -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_14.html">
      <font color="black">A Foreground-background Parallel Compression with Residual Encoding for
  Surveillance Video</font>
    </a>
  </h2>
  <font color="black">さらに、フォアグラウンドを圧縮し、それらのパフォーマンスをアブレーションスタディで比較して、ビデオ圧縮における時間情報の重要性を示す2つの異なるスキームを示します。前景と背景の構成とフレーム品質の向上。H.264やH.265などの従来のビデオ圧縮アルゴリズムは、監視ビデオの低情報密度特性を十分に活用していません。 
[要約]提案された方法は、従来のアルゴリズムhよりも69.5％少ないbpp（ビット/ピクセル）を必要とします。 hecvデータセットで同じpsnr（36 db）を達成するには265</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-18">
        <br><font color="black">2020-01-18</font>
      </time>
    </span>
</section>
<!-- paper0: SAR Image Despeckling by Deep Neural Networks: from a pre-trained model
  to an end-to-end training strategy -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_15.html">
      <font color="black">SAR Image Despeckling by Deep Neural Networks: from a pre-trained model
  to an end-to-end training strategy</font>
    </a>
  </h2>
  <font color="black">CNNトレーニングには、優れたトレーニングデータが必要です。スペックルフリー/スペックル破損画像の多くのペアです。さまざまな可能なアプローチの中で、畳み込みニューラルネットワーク（CNN）に基づく方法は、最近、SARの最先端のパフォーマンスに到達することが示されています画像の復元..スペックルの低減は、合成開口レーダー（SAR）画像における長年のトピックです。 
[ABSTRACT] cnnトレーニングには適切なトレーニングデータが必要です：多くのスペックルフリー/スペックル破損画像のペア。ハイブリッドアプローチは、スペックルの評価に使用されるcnnのトレーニングにも使用されます-アート</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-28">
        <br><font color="black">2020-06-28</font>
      </time>
    </span>
</section>
<!-- paper0: SuPer Deep: A Surgical Perception Framework for Robotic Tissue
  Manipulation using Deep Learning for Feature Extraction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_16.html">
      <font color="black">SuPer Deep: A Surgical Perception Framework for Robotic Tissue
  Manipulation using Deep Learning for Feature Extraction</font>
    </a>
  </h2>
  <font color="black">この作業では、外科的知覚の深層学習手法を活用することで課題を克服します。転移学習を活用することで、深層学習ベースのアプローチは、最小限のトレーニングデータと少ない機能エンジニアリングの努力で外科的シーンを完全に知覚する必要があります。フレームワークは、手術道具と組織追跡の機能を開発するために多大な労力を必要とします。 
[要約]外科的知覚フレームワークは、外科用ツールと組織追跡の機能を開発するために多大な労力を必要とします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-07">
        <br><font color="black">2020-03-07</font>
      </time>
    </span>
</section>
<!-- paper0: SSCR: Iterative Language-Based Image Editing via Self-Supervised
  Counterfactual Reasoning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_17.html">
      <font color="black">SSCR: Iterative Language-Based Image Editing via Self-Supervised
  Counterfactual Reasoning</font>
    </a>
  </h2>
  <font color="black">クロスタスクコンシステンシー（CTC）の助けを借りて、これらの反事実的指示を自己監視シナリオでトレーニングします。このような能力は、反事実的思考と、すでに発生したイベントの代替案について考える能力に起因します。トレーニングデータの％、SSCRは完全なデータを使用した場合と同等の結果を達成します。 
[ABSTRACT] ilbieは、指導の前後にある大規模な画像を収集することに挑戦しています。これらの能力は、反事実的な思考と、すでに発生したイベントの代替案について考える能力に起因します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: 3D-FUTURE: 3D Furniture shape with TextURE -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_18.html">
      <font color="black">3D-FUTURE: 3D Furniture shape with TextURE</font>
    </a>
  </h2>
  <font color="black">よく整理された3D-FUTUREを前提として、共同2Dインスタンスセグメンテーションと3Dオブジェクトポーズ推定、画像ベースの3D形状検索、単一画像からの3Dオブジェクト再構成、テクスチャーリカバリなど、広く研究されているいくつかのタスクのベースライン実験を提供します3D形状。データベースでの関連する将来の研究を容易にします。現在の3Dベンチマークの3D CAD形状は、主にオンラインモデルリポジトリから収集されます。このペーパーでは、Texture（3D-FUTURE）を使用した3D家具の形状について説明します。家庭シナリオにおける3D家具形状の大規模リポジトリ。 
[ABSTRACT] 3d-フューチャーには5,000の異なる部屋の3d画像が含まれています。通常、幾何学的なディテールが不十分であり、情報が少ないテクスチャがあります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: ES Attack: Model Stealing against Deep Neural Networks without Data
  Hurdles -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_19.html">
      <font color="black">ES Attack: Model Stealing against Deep Neural Networks without Data
  Hurdles</font>
    </a>
  </h2>
  <font color="black">ESアタックは、ヒューリスティックに生成された合成データを使用して、代替モデルをトレーニングし、最終的には機能的に同等の被害者DNNのコピーを実現します。ディープニューラルネットワーク（DNN）は、機械学習などのさまざまな商用機械学習サービスに不可欠なコンポーネントになっています。サービス（MLaaS）。しかし、ほとんどの既存の作業は、攻撃が成功した場合に被害者のDNNに関する機密のトレーニングデータまたは補助データを取得する必要があるような攻撃の影響を過小評価していました。 
[ABSTRACT]合成APIは、パブリックAPIを通じて盗まれる可能性があります。これらには、データハードルのないモデル盗用攻撃が含まれます。es攻撃は、補助データからの既存のほとんどのモデル盗用よりも優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: WCE Polyp Detection with Triplet based Embeddings -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_20.html">
      <font color="black">WCE Polyp Detection with Triplet based Embeddings</font>
    </a>
  </h2>
  <font color="black">これは、ポリープの外観の多様性、データセットの構造の不均衡、およびデータの不足のため、困難な問題です。この方法の重要な点は、三重項損失関数を使用して、画像からの特徴抽出を改善することです。小さなデータセット..深い畳み込みニューラルネットワークと計量学習を組み合わせた新しいポリープコンピューター支援決定システムを開発しました。 
[ABSTRACT]現在の分析は、ビデオのフレームのほぼすべての1つを手動で検査することで行うことができます。これは、面倒でエラーが発生しやすいタスクです。この方法は、ポリープ検出器の結果を視覚的に説明するために使用されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-10">
        <br><font color="black">2019-12-10</font>
      </time>
    </span>
</section>
<!-- paper0: TexMesh: Reconstructing Detailed Human Texture and Geometry from RGB-D
  Video -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_21.html">
      <font color="black">TexMesh: Reconstructing Detailed Human Texture and Geometry from RGB-D
  Video</font>
    </a>
  </h2>
  <font color="black">実際には、自己適応の短いサンプルシーケンスでモデルをトレーニングし、モデルはその後インタラクティブフレームレートで実行されます。RGBフレーム、キャプチャされた環境マップ、およびRGB-Dトラッキングからの粗いフレームごとのヒューマンメッシュが与えられると、私たちの方法は、高解像度のアルベドテクスチャとともに、時空間的に一貫した詳細なフレームごとのメッシュを再構築します。入射照明を使用することで、ローカルサーフェスジオメトリとアルベドを正確に推定できます。詳細な表面形状と高解像度のテクスチャ推定のために、自己監視された方法で実際のシーケンスにトレーニングされたモデル。 
[ABSTRACT]私たちはtexmeshを合成データと現実世界のデータで検証し、定量的かつ妥当な方法で最先端の技術を上回っていることを示します。私たちの方法は人体の分析に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-01">
        <br><font color="black">2020-08-01</font>
      </time>
    </span>
</section>
<!-- paper0: Distance weighted discrimination of face images for gender
  classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_22.html">
      <font color="black">Distance weighted discrimination of face images for gender
  classification</font>
    </a>
  </h2>
  <font color="black">高次元低サンプルサイズ（HDLSS）の状況での分類と特徴抽出のための距離加重識別の利点を示します。この分析により、男性と女性の人間による識別の推進要因の理解に新たな貢献をすることができます。洞察に富んだ視覚化を通じて分類の解釈を調査し、分類子の判別誤差を調べることにより、距離加重識別とフィッシャーの線形識別、サポートベクターマシン、主成分分析を比較します。 
[ABSTRACT]私たちは、男性と女性の間の人間の差別のドライバーの理解に新たな貢献をすることができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2017-06-15">
        <br><font color="black">2017-06-15</font>
      </time>
    </span>
</section>
<!-- paper0: Learning From Multiple Experts: Self-paced Knowledge Distillation for
  Long-tailed Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_23.html">
      <font color="black">Learning From Multiple Experts: Self-paced Knowledge Distillation for
  Long-tailed Classification</font>
    </a>
  </h2>
  <font color="black">具体的には、提案されたフレームワークには、自己学習型のエキスパート選択とカリキュラムインスタンス選択の2つのレベルの適応学習スケジュールが含まれます。これにより、知識が「学生」に適応的に転送されます。最先端の方法と比較して優れたパフォーマンス。実際のシナリオでは、データは長い裾の分布を示す傾向があり、ディープネットワークのトレーニングの難易度が高くなります。 
[要約]提案されたlfmeフレームワークは、複数の専門家の学習と呼ばれます（lfme）</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-06">
        <br><font color="black">2020-01-06</font>
      </time>
    </span>
</section>
<!-- paper0: A Novel Transferability Attention Neural Network Model for EEG Emotion
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_24.html">
      <font color="black">A Novel Transferability Attention Neural Network Model for EEG Emotion
  Recognition</font>
    </a>
  </h2>
  <font color="black">脳波計（EEG）の感情認識のための既存の方法は、すべてのEEGサンプルに基づいてモデルを常に区別できないようにトレーニングします。結果は、提案されたモデルが最先端のパフォーマンスを達成することを検証します。これは、出力を測定することによって実装できます。複数の脳領域レベルの弁別子と1つの単一のサンプルレベルの弁別器の組み合わせ。 
[要旨]ソース（トレーニング）サンプルの一部は、ターゲット（テスト）サンプルとは非常に異なるため、悪影響をもたらす可能性があります。ただし、脳波サンプルの場合、サンプルのすべての脳領域に転送可能な感情情報が含まれているわけではありませんテストデータに効果的に</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Uni- and Multi-modal Stream Networks for Multimodal Image
  Registration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_25.html">
      <font color="black">Adversarial Uni- and Multi-modal Stream Networks for Multimodal Image
  Registration</font>
    </a>
  </h2>
  <font color="black">画像から画像への変換によってマルチモーダル問題（CTからMRなど）を単峰問題（MRからMRなど）に変換しようとする他の変換ベースの方法とは異なり、この方法は変形フィールドを活用します（i）変換されたMR画像と（ii）元のCT画像をデュアルストリーム方式で推定し、それらを融合してより良い登録パフォーマンスを達成する方法を自動的に学習します。計算機トモグラフィー（CT）画像間の変形可能な画像登録磁気共鳴（MR）イメージングは、多くの画像誘導療法に不可欠です。私たちの方法は2つの臨床データセットで評価され、最先端の従来の学習ベースの方法と比較して有望な結果を示しています。 
[要旨]変形可能認識方法は変形可能画像登録方法を作成するために使用できます。変形可能画像認識方法を使用する新しい方法に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-06">
        <br><font color="black">2020-07-06</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Ensemble Robustness by Collaboratively Promoting and Demoting
  Adversarial Robustness -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_26.html">
      <font color="black">Improving Ensemble Robustness by Collaboratively Promoting and Demoting
  Adversarial Robustness</font>
    </a>
  </h2>
  <font color="black">アンサンブルベースの敵対的トレーニングは、敵対的な攻撃に対する堅牢性を達成するための原則的なアプローチです。したがって、提案されたフレームワークは、敵の転送可能性を削減し、アンサンブルメンバーの多様性を促進する柔軟性を提供します。私たちのアンサンブルアプローチ。この作品では、アンサンブルモデルの委員会モデル間でコラボレーションするためのシンプルで効果的な戦略を提案します。 
[要約]提案された方法は、アンサンブルメンバー間の敵対的な例の転送可能性を制御することです。これは、特定のサンプルの各モデルメンバーに対して定義された安全なセットと安全でないセットを介して実現されるため、転送可能性の定量化と正則化に役立ちます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Object-Independent Human-to-Robot Handovers using Real Time Robotic
  Vision -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_27.html">
      <font color="black">Object-Independent Human-to-Robot Handovers using Real Time Robotic
  Vision</font>
    </a>
  </h2>
  <font color="black">把握の選択と認識のモジュールは、リアルタイムで同時に実行できるため、進行状況を監視できます。ロボットは、対象のオブジェクトに向かって視覚サーボによって制御されます。人間に属していると見なされるピクセルは、把握の候補から除外されます。ポーズ、したがってロボットが人間のパートナーと衝突することなくオブジェクトを安全に拾うことを保証します。 
[ABSTRACT]人体のパーツパーツのセグメンテーションと手/指のセグメンテーションを目的としています。人体のパーツパーツのセグメンテーションからの2つの知覚モジュールを使用し、ロボットセンサーでフィルタリングします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-02">
        <br><font color="black">2020-06-02</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Automated COVID-19 Grading with Convolutional Neural Networks
  in Computed Tomography Scans: An Ablation Study -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_28.html">
      <font color="black">Improving Automated COVID-19 Grading with Convolutional Neural Networks
  in Computed Tomography Scans: An Ablation Study</font>
    </a>
  </h2>
  <font color="black">これらのコンポーネントを備えた3D CNNは、105のCTスキャンのテストセットで0.934のROC曲線（AUC）の下の面積を達成し、742のCTスキャンの公的に利用可能なセットで0.923のAUCを達成しました。 2D CNN ..たとえば、2D CNNを使用したいくつかの研究では、これらが3D CTボリュームの処理に最適ではないかもしれません。2DCNNの代わりに3D CNNを使用することの効果を調査しました。自動的に計算された病変マップを追加のネットワーク入力として提供し、カテゴリー出力ではなく連続を予測します。 
[ABSTRACT]これらの研究の多くは、一般的に使用されるコンポーネントから組み立てられたアルゴリズムの初期結果を報告することに焦点を当てていました。これらは、3d ctボリュームの処理に最適ではない場合でも、2d cnnsを使用した複数の研究を含みました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: MFIF-GAN: A New Generative Adversarial Network for Multi-Focus Image
  Fusion -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_29.html">
      <font color="black">MFIF-GAN: A New Generative Adversarial Network for Multi-Focus Image
  Fusion</font>
    </a>
  </h2>
  <font color="black">一連の実験結果は、MFIF-GANが視覚的知覚、定量分析、および効率においていくつかの代表的な最先端（SOTA）アルゴリズムより優れていることを示しています。このホワイトペーパーでは、新しい生成的敵対的ネットワークを紹介します。マルチフォーカス画像をフォーカスマップに変換し、オールインフォーカス画像をさらに取得するためにMFIF-GANと呼ばれます。ネットワークでは、注意メカニズムとしてSqueeze and Excitation Residual Network（SE-ResNet）モジュールが採用されています。 
[ABSTRACT] mfifは、フォーカス-デフォーカス境界（fdb）.focus-art）とフォーカス-の周りのデフォーカス広がり効果（dse）を解決します-ネットワークで使用されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Separating Content from Style Using Adversarial Learning for Recognizing
  Text in the Wild -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_30.html">
      <font color="black">Separating Content from Style Using Adversarial Learning for Recognizing
  Text in the Wild</font>
    </a>
  </h2>
  <font color="black">背景のスタイルの正規化に加えて、文字パターンを調整して認識タスクを容易にします。このフレームワークを最近の認識方法に統合して、新しい最先端の認識精度を実現できます。したがって、弁別器は、認識機能の混乱に応じてジェネレータを生成し、生成されたパターンを認識しやすくします。 
[ABSTRACT]画像内の複数の文字の生成と認識のために提案された敵対的な学習フレームワーク。ペアのトレーニングサンプルの欠如に基づいて、認識器から識別器に注意マスクを共有するインタラクティブな共同トレーニングスキームを設計します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-13">
        <br><font color="black">2020-01-13</font>
      </time>
    </span>
</section>
<!-- paper0: The High-Quality Wide Multi-Channel Attack (HQ-WMCA) database -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_31.html">
      <font color="black">The High-Quality Wide Multi-Channel Attack (HQ-WMCA) database</font>
    </a>
  </h2>
  <font color="black">高品質のワイドマルチチャネル攻撃データベース（HQ-WMCA）データベースは、以前のワイドマルチチャネル攻撃データベース（WMCA）を拡張し、色、深度、熱、赤外線（スペクトル）、および短波赤外線（スペクトル）、そしてまた幅広い攻撃。 
[ABSTRACT]高品質のワイドマルチチャネル攻撃データベースには、色、深度、熱、赤外線（赤外線）、および短波赤外線（スペクトル）などのより多くのチャネルがあります。高品質のワイドマルチチャネル攻撃データベース（wmca）データベースが利用可能になりました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Balance Scene Learning Mechanism for Offshore and Inshore Ship Detection
  in SAR Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_32.html">
      <font color="black">Balance Scene Learning Mechanism for Offshore and Inshore Ship Detection
  in SAR Images</font>
    </a>
  </h2>
  <font color="black">さまざまなシーンのサンプル数の大きな不均衡は、合成開口レーダー（SAR）船の検出精度を大幅に低下させます。したがって、この問題を解決するために、このレターでは、SAR画像でのオフショアおよびインショアの船検出のためのバランスシーン学習メカニズム（BSLM）を提案します。 
[ABSTRACT]このレターは、sar画像でのオフショアおよびインショアの船舶検出のためのバランスシーン学習メカニズム（bslm）を提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-21">
        <br><font color="black">2020-07-21</font>
      </time>
    </span>
</section>
<!-- paper0: Sequential Graph Convolutional Network for Active Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_33.html">
      <font color="black">Sequential Graph Convolutional Network for Active Learning</font>
    </a>
  </h2>
  <font color="black">新しく注釈が付けられた例と既存の例を併用して、グラフのパラメータが学習され、変更された目的が最小限に抑えられます。プールベースのアクティブラーニング用の新規および一般的な逐次グラフ畳み込みネットワーク（GCN）トレーニングを提案します。公開されている4つの画像分類ベンチマークでの手法と、VAAL、学習損失、CoreSetなどの既存のいくつかの手法との比較。
[要約]これらの例は、ラベルなしとラベル付きの例で表されています。グラフのノードの信頼スコアに基づいて、それらをサブサンプリングして、継承された不確実性が相関する場所に注釈を付けます。この方法は、4つの公開されている画像分類ベンチマークに基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-18">
        <br><font color="black">2020-06-18</font>
      </time>
    </span>
</section>
<!-- paper0: Semi-supervised Semantic Segmentation of Organs at Risk on 3D Pelvic CT
  Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_34.html">
      <font color="black">Semi-supervised Semantic Segmentation of Organs at Risk on 3D Pelvic CT
  Images</font>
    </a>
  </h2>
  <font color="black">新しい設計スキームが導入され、ベースライン残留Uネットアーキテクチャが強化されてパフォーマンスが向上します。次に、半教師付き敵対戦略が導入され、ラベル付きおよびラベルなしの3D CT画像を利用します。骨盤コンピューター断層撮影におけるリスクのある臓器の自動セグメンテーション（CT）画像は、手作業による輪郭作成の時間と労力を節約し、観察者内および観察者間の変動を減らすことにより、放射線治療計画を支援します。 
[ABSTRACT]新しい方法は、100件のトレーニングケース、20件のテストケースのデータセットで評価されます。これは、100件のケースからの距離距離と20件のテスト方法に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: flexgrid2vec: Learning Efficient Visual Representations Vectors -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_35.html">
      <font color="black">flexgrid2vec: Learning Efficient Visual Representations Vectors</font>
    </a>
  </h2>
  <font color="black">不均衡な低サイズのデータセットを使用しますが、flexgrid2vecは、よく知られている基本分類子に対して安定した優れた結果を示します。.各画像を、柔軟な一意のノード位置とエッジ距離のグラフで表します。非常に集中的な計算の必要性、詳細な構造情報を失うリスク、特定の形状やオブジェクトに対するメソッドの特異性などが含まれます。 
[ABSTRACT]既存の視覚表現方法にはいくつかの問題があります。それらには、非常に集中的なコンピューティングの必要性と、詳細な構造情報を失うリスクが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-30">
        <br><font color="black">2020-07-30</font>
      </time>
    </span>
</section>
<!-- paper0: Discriminative Segmentation Tracking Using Dual Memory Banks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_36.html">
      <font color="black">Discriminative Segmentation Tracking Using Dual Memory Banks</font>
    </a>
  </h2>
  <font color="black">ベルとホイッスルなしで、私たちのシンプルでありながら効果的な追跡アーキテクチャは、VOT2016、VOT2018、VOT2019、GOT-10K、およびTrackingNetベンチマークに新しい最先端の技術を設定し、特にVOT2016で0.535と0.506のEAOを達成しますVOT2018 ..既存のテンプレートベースのトラッカーは通常、境界ボックスを使用して各フレームのターゲットをローカライズするため、ピクセル単位の表現の学習とターゲットの複雑で非剛体な変換の処理が制限されます。さらに、私たちのアプローチは、主要なセグメンテーショントラッカーよりも優れています。 2つのビデオオブジェクトセグメンテーションベンチマークDAVIS16およびDAVIS17のD3S。 
[ABSTRACT]外観メモリバンクは、空間的および地域的な非局所的類似性を使用して、外観マスクを現在のフレームに伝播します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: A bisector line field approach to interpolation of orientation fields -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_37.html">
      <font color="black">A bisector line field approach to interpolation of orientation fields</font>
    </a>
  </h2>
  <font color="black">配向フィールドの大域的再構成の問題へのアプローチを提案します。次に、手順をフィンガープリント分析の例で示します。適切に選択されたエネルギー最小化問題に恵まれて、ターゲット配向フィールドの多項式補間を提供しながら、フェーズステップを2倍にします。 
[要約]この方法は、「二等分線フィールド」と呼ばれる幾何学的モデルに基づいています。男性と女性の性器のペアを方向フィールドにマッピングします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-07-26">
        <br><font color="black">2019-07-26</font>
      </time>
    </span>
</section>
<!-- paper0: Denoised Smoothing: A Provable Defense for Pretrained Classifiers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_38.html">
      <font color="black">Denoised Smoothing: A Provable Defense for Pretrained Classifiers</font>
    </a>
  </h2>
  <font color="black">この防御をノイズ除去平滑化と呼び、ImageNetおよびCIFAR-10での広範な実験を通じてその効果を実証します。カスタムトレーニング済みノイズ除去器を既製の画像分類器に付加し、ランダム化平滑化を使用することで、効果的に事前学習済みの分類子を変更せずに、敵対例に対して$ \ ell_p $堅牢であることが保証されている新しい分類子。事前学習済みの画像分類子を$ \ ell_p $敵対攻撃から確実に防御する方法を示します。 
[ABSTRACT]事前学習済みの分類サービスは、証明可能な堅牢なものに変換できます。このアプローチを使用して、alexis、google、aws、およびclarifai画像分類APIを証明できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-04">
        <br><font color="black">2020-03-04</font>
      </time>
    </span>
</section>
<!-- paper0: AdderSR: Towards Energy Efficient Image Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_39.html">
      <font color="black">AdderSR: Towards Energy Efficient Image Super-Resolution</font>
    </a>
  </h2>
  <font color="black">具体的には、加算器の操作では、画像処理タスクに不可欠なアイデンティティマッピングを簡単に学習できません。次に、特徴の分布を調整して詳細を調整するための学習可能なパワーアクティベーションを開発します。いくつかのベンチマークモデルとデータセットで行われた実験は、 AdderNetを使用した画像超解像モデルは、CNNベースラインに匹敵するパフォーマンスと視覚的品質を、エネルギー消費量を約2 $ \ times $削減して実現できます。 
[ABSTRACT]たとえば、addernetはaddernetを使用して出力機能を計算し、大量のエネルギー消費を回避します。addernetは、写真認識ではなく、added機能を使用して追加機能を計算します。addernetは、cnnベースラインと同等の成功を収めることができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-18">
        <br><font color="black">2020-09-18</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Attention Based Ultra Lightweight Image Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_40.html">
      <font color="black">Multi-Attention Based Ultra Lightweight Image Super-Resolution</font>
    </a>
  </h2>
  <font color="black">さらに、費用対効果の高い注意メカニズム（CEA）を備えたMABは、複数の注意メカニズムを使用して機能を改良および抽出するのに役立ちます。包括的な実験は、既存の最新技術に対するモデルの優位性を示しています。MAFFSRN特徴抽出ブロックとして機能する提案された特徴融合グループ（FFG）で構成されます。 
[ABSTRACT] maffsrnは、機能解決として機能する提案された機能削除グループ（ffgs）で構成されます。コスト効率の高い注意メカニズム（cea）を備えたmabは、複数の注意メカニズムを使用して機能を改良および抽出するのに役立ちます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-29">
        <br><font color="black">2020-08-29</font>
      </time>
    </span>
</section>
<!-- paper0: Reconstruct high-resolution multi-focal plane images from a single 2D
  wide field image -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CV/paper_41.html">
      <font color="black">Reconstruct high-resolution multi-focal plane images from a single 2D
  wide field image</font>
    </a>
  </h2>
  <font color="black">さらに、MFPINetは、同じボリューム画像を再構成するための現在のリフォーカス方法よりも約24倍高速です。提案された方法は、高解像度3Dイメージングの速度を大幅に高め、低解像度広視野画像のアプリケーションを拡大する可能性があります。この論文では、スキャンに依存せずに単一の2D低解像度野生フィールド画像から高解像度多焦点面画像を再構築するための高速エンドツーエンド多焦点面画像ネットワーク（MFPINet）を提案します。 
[ABSTRACT] mfpinetは高速のエンドツースキャンネットワーク（mfpinet）です。スキャンに依存することなく、単一の2d低解像度のワイルドファイル画像から高解像度画像を再構築する手法を使用します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic
  Text Exchange -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_0.html">
      <font color="black">Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic
  Text Exchange</font>
    </a>
  </h2>
  <font color="black">パイプラインの成功は、セマンティックテキストエクスチェンジスコア（STES）で測定します。セマンティックコンテンツを調整しながら元のテキストの感情と流暢さを維持する機能です。テキストの意味の変更..私たちの実験は、SMERTIがYelpのレビュー、Amazonのレビュー、およびニュースのヘッドラインのベースラインモデルよりも優れていることを示しています。 
[ABSTRACT]セマンティックテキスト交換スコア（stes）によってパイプラインの成功を測定します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-30">
        <br><font color="black">2019-08-30</font>
      </time>
    </span>
</section>
<!-- paper0: Profile Consistency Identification for Open-domain Dialogue Agents -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_1.html">
      <font color="black">Profile Consistency Identification for Open-domain Dialogue Agents</font>
    </a>
  </h2>
  <font color="black">プロファイルの一貫性の識別の調査を容易にするために、110Kを超える単一ターンの会話とそのKey-Value属性プロファイルを使用して、人間が注釈を付けた大規模なデータセットを作成します。応答とプロファイルの明示的な関係は手動でラベル付けされます。また、キーと値の構造情報は、BERTモデルを強化してプロファイルの一貫性を特定し、強力なベースラインを超えて改善されました。 
[ABSTRACT]人間との一貫した一貫性は、対話の一貫性を向上させるための鍵です。応答とユーザーの間の一貫性を識別する必要はありません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: TED: Triple Supervision Decouples End-to-end Speech-to-text Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_2.html">
      <font color="black">TED: Triple Supervision Decouples End-to-end Speech-to-text Translation</font>
    </a>
  </h2>
  <font color="black">ターゲット文の翻訳損失に加えて、\ methodには、入力から音響特徴を抽出する音響トランスデューサーをガイドする2つの補助監視信号と、ソーストランスクリプションテキストに関連する意味特徴を抽出するセマンティックエンコーダーが含まれています。英語-フランス語と英語-ドイツ語の両方の音声翻訳ベンチマークにおける最先端のパフォーマンス。神経科学に触発されて、人間はさまざまな情報を処理するための知覚システムと認識システムを持っているため、TED、\ textbf {T} ransducer- \ textbf { E} ncoder- \ textbf {D} ecoder、エンドツーエンドの音声からテキストへの翻訳タスクを分離するためのトリプル監視を備えた統合フレームワーク。 
[要約]エンドツーエンドツーテキスト翻訳タスクを分離するために、トリプル監視を備えた統合フレームワークtedを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Measuring Massive Multitask Language Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_3.html">
      <font color="black">Measuring Massive Multitask Language Understanding</font>
    </a>
  </h2>
  <font color="black">さらに悪いことに、道徳や法律などの社会的に重要な主題については、ほぼランダムな精度があります。モデルもパフォーマンスに偏りがあり、いつ間違っているかわからないことがよくあります。しかし、57のタスクすべてで、最良のモデルエキスパートレベルの精度に到達するには、まだかなりの改善が必要です。 
[ABSTRACT]テストは、初等数学、米国の歴史、コンピューターサイエンス、法律などを含む57のタスクをカバーしています。テストするには、テストを使用して、多くのタスクにわたってモデルを分析し、重要な欠点を特定できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-07">
        <br><font color="black">2020-09-07</font>
      </time>
    </span>
</section>
<!-- paper0: NABU $\mathrm{-}$ Multilingual Graph-based Neural RDF Verbalizer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_4.html">
      <font color="black">NABU $\mathrm{-}$ Multilingual Graph-based Neural RDF Verbalizer</font>
    </a>
  </h2>
  <font color="black">NABUは、エンコーダーデコーダーアーキテクチャに基づいており、グラフアテンションネットワークに触発されたエンコーダーとトランスフォーマーをデコーダーとして使用します。ただし、英語が広く対象となっている唯一の言語です。標準ベンチマークWebNLGデータセットの単一言語および多言語設定でNABUを評価します。 
[ABSTRACT] nabuは多言語のグラフベースのニューラルモデルであり、RDFデータをドイツ語、ロシア語、english。に言語化します。ナレッジグラフは言語でマークされているという事実に基づいています。ブルー</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-16">
        <br><font color="black">2020-09-16</font>
      </time>
    </span>
</section>
<!-- paper0: Vector Projection Network for Few-shot Slot Tagging in Natural Language
  Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_5.html">
      <font color="black">Vector Projection Network for Few-shot Slot Tagging in Natural Language
  Understanding</font>
    </a>
  </h2>
  <font color="black">基本的に、このアプローチは、適応バイアスを使用した正規化線形モデルと同等です。具体的には、ベンチマークSNIPSとNERの5ショット設定では、このメソッドは最強の少数ショット学習ベースラインよりも$ 6.30 $と$ 13.79 $ポイントFそれぞれ$ _1 $スコア..対照的な実験は、提案されたベクトル投影ベースの類似性メトリックが他のバリアントを大幅に上回ることができることを示しています。 
[ABSTRACT]新しい研究は、数ショットスロットのタグ付けのための新しいシステムを提案します。それは、単語としての各ターゲットラベル上の文脈的な単語の埋め込みの予測を悪用します-ラベルの類似性</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Generative Imagination Elevates Machine Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_6.html">
      <font color="black">Generative Imagination Elevates Machine Translation</font>
    </a>
  </h2>
  <font color="black">また、分析実験を行い、その結果は、イマジネーションが分解戦略を実行するときに欠落している情報を埋めるのに役立つことを示しています。ソース文から、テキスト表現と想像上の視覚表現の両方に基づいてターゲット文を生成します。ただし、トレーニングと推論のプロセスは、適切に調整された2か国語の文に大きく依存しています。 
[要約]私たちの翻訳モデルは視覚的な想像力の恩恵を受け、テキストを大幅に上回ります-ニューラル機械翻訳のみ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: SDST: Successive Decoding for Speech-to-text Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_7.html">
      <font color="black">SDST: Successive Decoding for Speech-to-text Translation</font>
    </a>
  </h2>
  <font color="black">学習の難しさを軽減するために、エンドツーエンドの\ textbf {S} peech-to-text \ textbf {T}変換タスクのための\ textbf {S}逐次\ textbf {D}エンコーディングを備えた統合フレームワークであるSDSTを提案します。 。実験は、提案された\ methodが以前の最先端のメソッドを大幅に改善することを示しています。このメソッドは、2つの主流のデータセットで検証されています。 
[要約]音声認識とマシンマージンの組み合わせは、ダイレクトテキストに大きな負担をかける-モーダルクロスリンガルマッピング</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Alleviating the Inequality of Attention Heads for Neural Machine
  Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_8.html">
      <font color="black">Alleviating the Inequality of Attention Heads for Neural Machine
  Translation</font>
    </a>
  </h2>
  <font color="black">この現象をマルチヘッド注意の不均衡トレーニングと特定のヘッドへのモデル依存性に関連付けます。その後の経験的分析もこの仮定をサポートし、メソッドの有効性を確認します。最近の研究では、Transformerの注意ヘッドが等しくないことが示されています。 。 
[要約]以前の調査では、複数の言語ペアで翻訳の改善が達成されていることが示されています。実験では、調査の結果が一貫していることが示されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Accent Estimation of Japanese Words from Their Surfaces and
  Romanizations for Building Large Vocabulary Accent Dictionaries -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_9.html">
      <font color="black">Accent Estimation of Japanese Words from Their Surfaces and
  Romanizations for Building Large Vocabulary Accent Dictionaries</font>
    </a>
  </h2>
  <font color="black">漢字）とよみ（簡略化された音声情報）。この手法は、特に一部のカテゴリの単語に対して、高い精度でアクセントを推定できることが実験的に示されています。.著者は、この手法を既存の大きな語彙日本語辞書NEologdに適用し、大きな語彙の日本語アクセント辞書。 
[ABSTRACT]日本語のアクセント辞書は限られた語彙に限定されています。研究者は、限られた情報から単語のアクセントを予測するアクセント推定手法を開発しました。この手法により、精度の高いアクセントを推定できることが実験的に示されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Weakly Supervised Learning of Nuanced Frames for Analyzing Polarization
  in News Media -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_10.html">
      <font color="black">Weakly Supervised Learning of Nuanced Frames for Analyzing Polarization
  in News Media</font>
    </a>
  </h2>
  <font color="black">提案されたサブフレームとその埋め込みを、最小限の監視を使用して、移民、銃規制、中絶の3つのトピックで評価します。イデオロギーの違いを捉え、ニュースメディアで政治的言説を分析するサブフレームの機能を示します。 Boydstun et al。、2014によって提案された幅広いポリシーフレームを、より良い方法で政治イデオロギーの違いを捉えることができるきめの細かいサブフレームに分割することを提案します。 
[ABSTRACT] 2014年に提案された標準のポリシーフレームを破ることをお勧めします。サブフレームは政治的トピックの違いをより適切に捉えることができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Exploring Versatile Generative Language Model Via Parameter-Efficient
  Transfer Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_11.html">
      <font color="black">Exploring Versatile Generative Language Model Via Parameter-Efficient
  Transfer Learning</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、単一の大規模な事前トレーニング済みモデルを使用して、複数のダウンストリーム生成タスクを同時に微調整する効果的な方法を提案します。5つの多様な言語生成タスクの実験では、追加の2-3％を使用するだけで、各タスクのパラメーターを使用すると、モデル全体でモデル全体の微調整のパフォーマンスを維持または向上できます。事前トレーニング済みの生成言語モデルを下流の言語生成タスクに微調整すると、有望な結果が得られます。 
[ABSTRACT]実験は5つの異なる言語生成タスクで実行されました。追加の2〜3％のパラメーターを使用することにより、モデル全体でモデル全体の微調整のパフォーマンスを維持または改善できることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-08">
        <br><font color="black">2020-04-08</font>
      </time>
    </span>
</section>
<!-- paper0: Multitask Pointer Network for Multi-Representational Parsing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_12.html">
      <font color="black">Multitask Pointer Network for Multi-Representational Parsing</font>
    </a>
  </h2>
  <font color="black">結果として生じる2次システムは、単一のモデルから無制限の構成要素ツリーと依存関係ツリーの両方を共同で生成できる最初のパーサーになるだけでなく、両方の構文形式がトレーニング中に互いに利益をもたらし、最先端の精度を達成できることを証明します継続的な英語と中国のペンTreebanksや不連続なドイツのNEGRAとTIGERのデータセットなど、広く使用されているいくつかのベンチマークでは、単一のモデルをトレーニングすることで、入力文を効率的に解析できる遷移ベースのアプローチを提案します。構成要素ツリーと依存関係ツリーの両方で、連続/射影および不連続/非射影構文構造の両方をサポートします。そのために、2つの個別のタスク固有のデコーダーと共通のエンコーダーを備えたポインターネットワークアーキテクチャを開発し、マルチタスク学習戦略に従ってそれらを共同で訓練する。 
[ABSTRACT]デコーダーと一般的なエンコーダーのトレーニングに使用できる「生物学的」を開発します。また、プロジェクトへの潜在的なリンクをテストするように教えることもできます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: DiffWave: A Versatile Diffusion Model for Audio Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_13.html">
      <font color="black">DiffWave: A Versatile Diffusion Model for Audio Synthesis</font>
    </a>
  </h2>
  <font color="black">DiffWaveが音声品質の面で強力なWaveNetボコーダーに一致していることを示しています（MOS：4.44と4.43の比較）。桁違いに高速に合成します。モデルは自己回帰ではなく、ホワイトノイズ信号をMarkovを通じて構造化波形に変換します。合成のステップ数が一定のチェーン。特に、さまざまな自動および人間の評価からのオーディオ品質とサンプルの多様性の点で、困難な無条件生成タスクで自己回帰およびGANベースの波形モデルを大幅に上回ります。 
[ABSTRACT] diffwaveは、さまざまな波形生成タスクで忠実度の高いオーディオを生成します。自己回帰およびガンベースの波形モデルに一致します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Aligning AI With Shared Human Values -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_14.html">
      <font color="black">Aligning AI With Shared Human Values</font>
    </a>
  </h2>
  <font color="black">ETHICSデータセットを使用すると、現在の言語モデルには、基本的な倫理的知識の有望ではあるが不完全な理解があることがわかります。モデルは、さまざまなテキストシナリオに関する広範な道徳的判断を予測します。これには、物理的世界と社会的世界の知識を価値判断に関連付ける必要があります。チャットボットの出力を操作したり、最終的にはオープンエンドの強化学習エージェントを正規化したりできます。 
[ABSTRACT]正義、幸福、義務、美徳、常識の道徳の概念にまたがる新しいベンチマークである倫理データセットを紹介します。これには、物理的および社会的な世界の知識を価値判断に結び付ける必要があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-05">
        <br><font color="black">2020-08-05</font>
      </time>
    </span>
</section>
<!-- paper0: Empathetic Dialogue Generation via Knowledge Enhancing and Emotion
  Dependency Modeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_15.html">
      <font color="black">Empathetic Dialogue Generation via Knowledge Enhancing and Emotion
  Dependency Modeling</font>
    </a>
  </h2>
  <font color="black">有用な外部知識がないと、暗黙の細かい感情を知覚するのが難しくなります。最初に、感情関連の概念の束によって対話コンテキストを強化し、知識強化されたコンテキストグラフを作成します。対話者間の感情的な相互作用がないと、共感のパフォーマンスも制限されます対話の生成。 
[要約]共感的な対話生成のタスクは、この問題に対処するために提案されています。外部の知識の欠如は、インデシデンスを理解することを困難にします。これらには、グラフが含まれます-グラフの意味的および感情的表現を学習するための認識トランスフォーマーエンコーダー</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: ALOHA: Artificial Learning of Human Attributes for Dialogue Agents -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_16.html">
      <font color="black">ALOHA: Artificial Learning of Human Attributes for Dialogue Agents</font>
    </a>
  </h2>
  <font color="black">私たちの予備実験は、ALOHAの2つのバリエーションを、提案されたデータセットと組み合わせると、選択したターゲットキャラクターの正しい対話応答を識別する際にベースラインモデルよりも優れ、キャラクターのアイデンティティ、番組のジャンル、およびコンテキストに関係なく安定していることを示しています詳細なHLAデータと特定のキャラクターの会話データを組み合わせることにより、キャラクタープロファイルをモデル化し、HLAを通じてキャラクターの言語スタイルを学習する能力をダイアログエージェントに提供するデータセットHLA-Chatを提示します。次に、キャラクタースペースマッピング、キャラクターコミュニティの検出、言語スタイルの検索を組み合わせてキャラクター（またはパーソナリティ）固有の言語モデルを構築する3コンポーネントシステムALOHA（人間の属性の人工学習の略）。 
[ABSTRACT]新しいシステムalohaは、文字空間マッピング、文字コミュニティ検出、言語スタイル検索を組み合わせて、特定の言語モデルを構築します。新しいシステムは、人間の属性の人工学習に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-18">
        <br><font color="black">2019-10-18</font>
      </time>
    </span>
</section>
<!-- paper0: Assessing the Severity of Health States based on Social Media Posts -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_17.html">
      <font color="black">Assessing the Severity of Health States based on Social Media Posts</font>
    </a>
  </h2>
  <font color="black">多様なNLUビューは、ユーザーの健康を評価するために、タスクと個々の疾患の両方でその有効性を示します。したがって、患者のソーシャルメディアの投稿から健康状態の重症度を評価できるシステムは、医療専門家を支援します（ HP）ユーザーの投稿に優先順位を付けます。ユーザーの健康状態の重大度を評価するために、テキストコンテンツとコンテキスト情報の両方をモデル化するマルチビュー学習フレームワークを提案します。 
[ABSTRACT]新しい調査によると、オンラインピアサポートは専門家の介入なしでは効果が限定的であることが示されています。システムは自然言語理解のさまざまな側面を使用して、ユーザーの健康状態の重症度を特定します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Filtering before Iteratively Referring for Knowledge-Grounded Response
  Selection in Retrieval-Based Chatbots -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_18.html">
      <font color="black">Filtering before Iteratively Referring for Knowledge-Grounded Response
  Selection in Retrieval-Based Chatbots</font>
    </a>
  </h2>
  <font color="black">また、知識グラウンディングプロセスを視覚化することにより、FIREがより解釈可能であることも示します。この方法では、コンテキストフィルターと知識フィルターが最初に構築され、グローバルおよび双方向の注意によってそれぞれ知識認識コンテキスト表現とコンテキスト認識知識表現を導き出します。 ..実験結果は、元のペルソナと修正されたペルソナを含むPERSONA-CHATデータセットで2.8％と4.1％を超えるマージン、およびCMU_DoGデータセットで3.1％を超えるマージンで、FIREが以前の方法よりも優れていることを示しています。 
[ABSTRACT]この論文では、このタスクを繰り返し参照（発火）する前に、filteringという名前のメソッドを提案します。さらに、会話に関係のないエントリは、ナレッジフィルターによって破棄されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-30">
        <br><font color="black">2020-04-30</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Robustness and Generality of NLP Models Using Disentangled
  Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_19.html">
      <font color="black">Improving Robustness and Generality of NLP Models Using Disentangled
  Representations</font>
    </a>
  </h2>
  <font color="black">これらの表現は、異なるロジット$ l $ sにマッピングされ、そのアンサンブルは最終予測$ y $を作成するために使用されます。それらの成功にもかかわらず、ニューラルモデルはロバスト性と一般性の両方に欠けています。異なる出力;あるドメインでトレーニングしたモデルのパフォーマンスは、別のドメインでテストすると劇的に低下します。このホワイトペーパーでは、絡み合い表現学習の観点からNLPモデルの堅牢性と一般性を向上させる方法を紹介します。 
[要約]ニューラルモデルは、堅牢性と一般性の両方に欠けています。ニューロンへの小さな摂動は、まったく異なる出力をもたらす可能性があります。1つのタスクでトレーニングされたモデルのパフォーマンスは、別のドメインでテストすると大幅に低下します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Modality-Transferable Emotion Embeddings for Low-Resource Multimodal
  Emotion Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/cs.CL/paper_20.html">
      <font color="black">Modality-Transferable Emotion Embeddings for Low-Resource Multimodal
  Emotion Recognition</font>
    </a>
  </h2>
  <font color="black">実験は、私たちのモデルがほとんどの感情カテゴリで最先端のパフォーマンスを達成することを示しています。次に、これらの埋め込みを視覚空間と音響空間に転送する2つのマッピング関数が学習されます。各モダリティについて、モデルは表現距離を計算します入力シーケンスとターゲットの感情の間で、距離に基づいて予測を行います。 
[要約]モダリティ-感情を埋め込んだ転送可能なモデルを提案します。これらは視覚空間と音響空間に埋め込まれます。各モデルは、どのモダリティでも目に見えない感情に適応できます。モデルは、ゼロショットおよび少数ショットシナリオの既存のベースラインよりも優れています目に見えない感情のために</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: Correlating Subword Articulation with Lip Shapes for Embedding Aware
  Audio-Visual Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.AS/paper_0.html">
      <font color="black">Correlating Subword Articulation with Lip Shapes for Embedding Aware
  Audio-Visual Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">さらに、アーティキュレーションの場所レベルでの視覚的な埋め込みは、アーティキュレーションの場所と唇の形状との間の高い相関を利用して、電話レベルでのパフォーマンスよりもさらに優れたパフォーマンスを示します。最後に、オーディオとビジュアルの両方の埋め込みを組み込んだ提案されたMEASEフレームワークは、最高の視覚のみおよび音声のみのEASEシステムで得られるものよりも大幅に優れた音声品質と了解度。シミュレートされた付加ノイズによって破損したTCD-TIMITコーパスの実験は、提案されたサブワードベースのVEASEアプローチが従来の埋め込みよりも効果的であることを示しています。単語レベル。 
[ABSTRACT]提案されたmeaseフレームは、従来の埋め込みよりも効果的です。唇のフレームから視覚的な埋め込みを抽出するためのプロトタイプが初めて作成されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: DCASENET: A joint pre-trained deep neural network for detecting and
  classifying acoustic scenes and events -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.AS/paper_1.html">
      <font color="black">DCASENET: A joint pre-trained deep neural network for detecting and
  classifying acoustic scenes and events</font>
    </a>
  </h2>
  <font color="black">音響シーンおよびイベント文献における多様な相互関連タスク間でターゲットタスクを実行するシングルタスクディープニューラルネットワークが開発されています。この研究では、音響シーン分類、オーディオの3つのタスクを実行できる統合ディープニューラルネットワークを提案します。タグ付け、およびサウンドイベントの検出。このようなタスクを組み合わせるために調査する研究はほとんどありませんが、作業はその予備段階にあります。 
[ABSTRACT]提案されたシステムdcasenetは、競合する結果を持つ任意のタスクに直接使用できます。または、ターゲットタスクにさらに微調整できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: On Loss Functions and Recurrency Training for GAN-based Speech
  Enhancement Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.AS/paper_2.html">
      <font color="black">On Loss Functions and Recurrency Training for GAN-based Speech
  Enhancement Systems</font>
    </a>
  </h2>
  <font color="black">全体的に、客観的なメトリックの損失関数と平均二乗誤差（MSE）を組み合わせたCRGANモデルは、多くの評価メトリックにわたる比較アプローチに対して最高のパフォーマンスを提供します。反復層を含めることの利点も調査されています。最近の研究では、は、音声強化に生成的敵対的ネットワーク（GAN）を使用することが可能ですが、これらのアプローチは、最新の（SOTA）非GANベースのアプローチと比較されていません。 
[要約]提案されたcrganモデルは、同じロスバーを使用してsota ganベースのモデルよりも優れています。また、他の非ganベースのシステムよりも優れており、音声強調にガンを使用する利点を示唆しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-29">
        <br><font color="black">2020-07-29</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-End Speaker-Dependent Voice Activity Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.AS/paper_3.html">
      <font color="black">End-to-End Speaker-Dependent Voice Activity Detection</font>
    </a>
  </h2>
  <font color="black">スイッチボードコーパスから生成された会話型電話データセットで実験が実行されます。このホワイトペーパーでは、この問題に対処するためのエンドツーエンドのニューラルネットワークに基づくアプローチを提案します。これにより、話者のアイデンティティをモデリングプロセスに明示的に取り入れます。また、以前に提案したセグメントレベルのメトリックを使用して、より包括的な分析を行いました。 
[ABSTRACT]単純な目標は、音声内の無音セグメントを削除することですが、より一般的なVADシステムは、無関係なセグメントをすべて削除できます。このタスクは実際のアプリケーションでは非常に一般的であり、通常はスピーカーの検証を実行することで実装されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Accent Estimation of Japanese Words from Their Surfaces and
  Romanizations for Building Large Vocabulary Accent Dictionaries -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.AS/paper_4.html">
      <font color="black">Accent Estimation of Japanese Words from Their Surfaces and
  Romanizations for Building Large Vocabulary Accent Dictionaries</font>
    </a>
  </h2>
  <font color="black">漢字）とよみ（簡略化された音声情報）。日本語の音声合成（TTS）では、入力文にアクセント情報を追加する必要があります。この手法により、アクセントを高い精度で推定できることが実験的に示されています。特にいくつかのカテゴリーの単語では。 
[ABSTRACT]日本語のアクセント辞書は限られた語彙に限定されています。研究者は、限られた情報から単語のアクセントを予測するアクセント推定手法を開発しました。この手法により、精度の高いアクセントを推定できることが実験的に示されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-End Bengali Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.AS/paper_5.html">
      <font color="black">End-to-End Bengali Speech Recognition</font>
    </a>
  </h2>
  <font color="black">公開されている大規模なベンガル語ASRトレーニングデータセットを使用して、ベンガル語ASRタスクの複雑さと深さが異なる7つのディープニューラルネットワーク構成のパフォーマンスをベンチマークし、評価します。2つのCNNブロック、2層ブロックAと4層を提案します。ブロックBは、最初のレイヤーが7x3カーネルで構成され、後続のレイヤーが3x3カーネルのみで構成されます。ブロックBを使用した最高のモデルは、WERが13.67で、大きな畳み込みカーネルを使用する同等のモデルよりも1.39％減少します。サイズは41x11と21x11です。 
[要約]ベンガル語の研究とリソースはほとんどありません。これらには、音声ベースの音声ベースのオサマビンの広範な使用が含まれます。ただし、研究は成功していません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Graph2Speak: Improving Speaker Identification using Network Knowledge in
  Criminal Conversational Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.AS/paper_6.html">
      <font color="black">Graph2Speak: Improving Speaker Identification using Network Knowledge in
  Criminal Conversational Data</font>
    </a>
  </h2>
  <font color="black">犯罪者の会話データの2つの候補データセット、犯罪シーン調査（CSI）、テレビ番組、およびROXANNEシミュレーションデータを紹介します。犯罪者調査のコンテキストでの会話の正確さの測定基準も紹介します。以前の対話の頻度に基づいて、話者識別のベースラインを1.2％絶対（相対値1.3％）、会話精度をCSIデータで2.6％絶対値（相対値3.4％）、および1.1％絶対値（相対値1.2％）改善します）、およびROXANNEシミュレーションデータでそれぞれ2％絶対（相対2.5％）。 
[ABSTRACT]ソーシャルネットワーク分析ツールを使用して主要なキャラクターを識別します。会話情報のメトリックも紹介します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-03">
        <br><font color="black">2020-06-03</font>
      </time>
    </span>
</section>
<!-- paper0: Open-set Short Utterance Forensic Speaker Verification using
  Teacher-Student Network with Explicit Inductive Bias -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.AS/paper_7.html">
      <font color="black">Open-set Short Utterance Forensic Speaker Verification using
  Teacher-Student Network with Explicit Inductive Bias</font>
    </a>
  </h2>
  <font color="black">目的関数は、話者分類損失、カルバックライブラーダイバージェンス、埋め込みの類似性をまとめて考慮します。提案された目的関数は、短い発話に対する教師と生徒の学習のパフォーマンスを効率的に改善でき、微調整戦略が一般的に優れていることを示します。事前トレーニング済みモデルに明示的な誘導バイアスを提供することにより、重み減衰法を使用しました。トレーニング済みのディープスピーカー埋め込みネットワークを小さなターゲットデータセットに対してロバストになるように進めるために、事前トレーニング済みの微調整に新しい戦略を導入します正規化における微調整の開始点および参照としてモデルを利用することによる、フォレンジックターゲットドメインに対する学生モデル。 
[ABSTRACT]小さなフォレンジックフィールドのデータセットを使用して新しいシステムを開発できます。メソッドは、単語分類の損失、カルバックライブラー、埋め込みの類似性に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: DiffWave: A Versatile Diffusion Model for Audio Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.AS/paper_8.html">
      <font color="black">DiffWave: A Versatile Diffusion Model for Audio Synthesis</font>
    </a>
  </h2>
  <font color="black">DiffWaveは、音声品質の面で強力なWaveNetボコーダーに一致することを示しています（MOS：4.44対4.43）。桁違いに高速に合成します。この作業では、条件付きおよび無条件の波形生成のための多目的な拡散確率モデルであるDiffWaveを提案します。 ..データの尤度に対する変分限界のバリアントを最適化することにより、効率的にトレーニングされます。 
[ABSTRACT] diffwaveは、さまざまな波形生成タスクで忠実度の高いオーディオを生成します。自己回帰およびガンベースの波形モデルに一致します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: A Deep Learning Based Analysis-Synthesis Framework For Unison Singing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.AS/paper_9.html">
      <font color="black">A Deep Learning Based Analysis-Synthesis Framework For Unison Singing</font>
    </a>
  </h2>
  <font color="black">私たちは主観的なリスニングテストを使用して、品質、メロディーへの順守、知覚されたユニゾンの程度など、提案された合成システムの知覚的要因を評価します。この論文では、合唱の文脈におけるユニゾン歌唱の研究を紹介します。最近提案されたディープラーニングベースの方法論を利用して、ユニゾン混合の録音における個々の歌手の基本周波数（F0）分布を分析します。分析に基づいて、アカペラ入力からユニゾン信号を合成するためのシステムを提案し、ユニゾンミックスを表す単一の音声プロトタイプ。 
[要約]ユニゾンの混合の分析は分析に基づいています。アカペラ入力からユニゾン信号を合成するシステムを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Residual Shuffle-Exchange Networks for Fast Processing of Long Sequences -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.AS/paper_10.html">
      <font color="black">Residual Shuffle-Exchange Networks for Fast Processing of Long Sequences</font>
    </a>
  </h2>
  <font color="black">最近導入されたニューラルシャッフル交換ネットワークは、計算効率の高い代替手段を提供し、O（n log n）時間での長期依存性のモデリングを可能にします。提案されたアーキテクチャは、より長いシーケンスにスケーリングするだけでなく、より速く収束し、より良い精度を提供します.. LAMBADA言語モデリングタスクのShuffle-Exchangeネットワークを凌駕し、パラメーターの数を効率化しながら、MusicNetデータセットで最先端のパフォーマンスを実現し、音楽の文字起こしを実現します。 
[ABSTRACT]最近導入されたニューラルシャッフル-交換ネットワークは、geluとレイヤーの正規化を使用した残余ネットワークに基づいています。シャッフルを超えますが、それを超えますが、musicnetデータセットの複雑さを防ぎます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-06">
        <br><font color="black">2020-04-06</font>
      </time>
    </span>
</section>
<!-- paper0: Detecting Acoustic Events Using Convolutional Macaron Net -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.AS/paper_11.html">
      <font color="black">Detecting Acoustic Events Using Convolutional Macaron Net</font>
    </a>
  </h2>
  <font color="black">CNNとConformerの組み合わせを使用する課題の最初の場所と比較すると、システムは0.3％もわずかに勝っています。2つの類似したモデルを同期的にトレーニングするMean-Teacherアプローチとは対照的に、2つの異なるトレーニングを行うことを提案しますモデルの1つがフレームレベルの予測を提供し、他のモデルがクリップレベルの予測を提供するCMNを同期的に提供します。提案されたフレームワークに基づいて、システムは音響シーンおよびイベントの検出と分類（DCASE）のベースラインシステムよりも優れています。 2020年チャレンジタスク4のマージンは10％を超えています。 
[要旨] cnnの新しいシステムは、畳み込みニューラルネットワークとマカロンネット（mn）を組み合わせたもので、畳み込みマカロンnet.cnnと呼ばれます。このプロジェクトは、システムからのデータの欠如に基づいています。新しいシステム、データ疑似ラベル付きのラベル付きモデルを上回る</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Semi-Supervised NMF-CNN For Sound Event Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.AS/paper_12.html">
      <font color="black">Semi-Supervised NMF-CNN For Sound Event Detection</font>
    </a>
  </h2>
  <font color="black">事後出力の平均化を通じてモデルを統合することにより、イベントベースのF1スコアを48.6％に増やすことができます。DCASE2020チャレンジタスク4テストセットでモデルをテストすることにより、モデルは44.4のイベントベースのF1スコアを達成できます。 ％一方で、我々のアンサンブルシステムは、46.3％のイベントベースのF1スコアを達成できます。その後、近似の強くラベル付けされたデータを使用して、2つの異なるCNNが半教師付きフレームワークでトレーニングされ、1つのCNNがクリップレベルの予測ともう1つはフレームレベルの予測用です。 
[要約]このアイデアを使用して、モデルはイベントを達成できます。このアイデアに基づいています。音響シーンとイベントの検出と分類に45.7％のマークを達成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-02">
        <br><font color="black">2020-07-02</font>
      </time>
    </span>
</section>
<!-- paper0: Light Convolutional Neural Network with Feature Genuinization for
  Detection of Synthetic Speech Attacks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-22/eess.AS/paper_13.html">
      <font color="black">Light Convolutional Neural Network with Feature Genuinization for
  Detection of Synthetic Speech Attacks</font>
    </a>
  </h2>
  <font color="black">この点で、本物の音声のみの特性を使用して、畳み込みニューラルネットワーク（CNN）でトランスフォーマーを学習する特徴正規化と呼ばれる新しい方法を提案します。既存のスプーフィング対策のほとんどは、攻撃の性質が行われたときにうまく機能しますASVspoof 2019論理アクセスコーパスは、提案された方法を評価するために使用されます。 
[ABSTRACT]合成音声では、asvシステムを不正アクセスから保護することが重要です。提案された機能の正規化ベースのlcnnシステムは、他の状態よりも優れています-最先端のなりすまし対策</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
