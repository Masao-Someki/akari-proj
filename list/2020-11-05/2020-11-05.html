<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-11-05の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: IEEE SLT 2021 Alpha-mini Speech Challenge: Open Datasets, Tracks, Rules
  and Baselines -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.SD/paper_0.html">
      <font color="black">IEEE SLT 2021 Alpha-mini Speech Challenge: Open Datasets, Tracks, Rules
  and Baselines</font>
    </a>
  </h2>
  <font color="black">ただし、このアプローチでは、実際のアプリケーションシナリオ、特にエコーデータで、シミュレートされたデータと記録されたデータの間に不一致が生じる可能性があります。この課題では、データ駆動型の方法、特にディープラーニングを促進するために、かなりの音声、キーワード、エコー、ノイズのコーパスをオープンソース化します。 KWSとSSLに関する学習アプローチ。さらに、研究者が成果をすばやく評価し、モデルを最適化するためのルール、評価方法、およびベースラインを示します。 
[概要]多くの出版物が、近年のオープンソースデータセットでのディープラーニングベースのkwsとsslの大幅な改善を報告しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Single channel voice separation for unknown number of speakers under
  reverberant and noisy settings -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.SD/paper_1.html">
      <font color="black">Single channel voice separation for unknown number of speakers under
  reverberant and noisy settings</font>
    </a>
  </h2>
  <font color="black">提案されたアプローチは、話者分類ブランチとともに最適化されたいくつかの分離ヘッドで構成されています。さらに、最大5人の異なる話者が同時に話す新しいノイズの多い残響データセットを提示します。分類ブランチは、各ヘッドが異なる数のスピーカーを分離することに特化しています。 
[概要]提案されたアプローチは、話者分類ブランチを備えた分離されたヘッドで構成されています。システムは話者の数を推定し、各ヘッドは異なる数の分離に特化しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Frustratingly Easy Noise-aware Training of Acoustic Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.SD/paper_2.html">
      <font color="black">Frustratingly Easy Noise-aware Training of Acoustic Models</font>
    </a>
  </h2>
  <font color="black">環境ノイズと残響は、自動音声認識（ASR）システムのパフォーマンスに悪影響を及ぼします。この論文では、ハイブリッドASRの音響モデルのノイズを意識したトレーニングのための発話レベルのノイズベクトルを提案します。文献で提案されているベースの適応ベースラインであり、私たちの方法が両方のデータセットでそれらを上回っていることを示しています。 
[概要]文献で提案されているいくつかの埋め込みベースの適応ベースラインを実装し、両方のデータセットでメソッドがそれらよりも優れていることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: AudVowelConsNet: A Phoneme-Level Based Deep CNN Architecture for
  Clinical Depression Diagnosis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.SD/paper_3.html">
      <font color="black">AudVowelConsNet: A Phoneme-Level Based Deep CNN Architecture for
  Clinical Depression Diagnosis</font>
    </a>
  </h2>
  <font color="black">音素ユニット、特にディープラーニングによるうつ病認識のための母音と子音の音響特性を調査します。自動評価システムに音素レベルの音声コンポーネントを組み込むことに焦点を当てたものはほとんどありません。うつ病は、患者のに悪影響を与える一般的で深刻な気分障害です。日常業務で正常に機能する能力。 
[ABSTRACT]音声は、うつ病の診断における強力なツールであることが証明されています。うつ病とその重症度レベルを検出するためのさまざまな音響特性の調査に焦点を当てた機械学習の研究</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-30">
        <br><font color="black">2020-10-30</font>
      </time>
    </span>
</section>
<!-- paper0: DESNet: A Multi-channel Network for Simultaneous Speech Dereverberation,
  Enhancement and Separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.SD/paper_4.html">
      <font color="black">DESNet: A Multi-channel Network for Simultaneous Speech Dereverberation,
  Enhancement and Separation</font>
    </a>
  </h2>
  <font color="black">実験によると、非リバーブ化の場合、提案されたDESNetは音声強調と分離においてDCCRNとほとんどの最先端の構造よりも優れていますが、デリバーブされたシナリオでは、DESNetはカスケードされたWPE-DCCRNネットワークよりも改善されています。論文では、音声の残響除去、強調、分離を同時に行うためのマルチチャネルネットワーク（DESNet）を提案します。さらに、音声アンミキシングとニューラルネットワークベースの加重予測の構造として、新しいディープコンプレックス畳み込み反復ネットワーク（DCCRN）を使用します。エラー（WPE）は、音声の残響除去のために事前にカスケードされます。 
[概要]提案された変更に加えて、マルチチャネル機能の注意技術を採用します。また、ネットワークのトレーニングのための段階的なsnr戦略と損失を紹介します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Correlation based Multi-phasal models for improved imagined speech EEG
  recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.SD/paper_5.html">
      <font color="black">Correlation based Multi-phasal models for improved imagined speech EEG
  recognition</font>
    </a>
  </h2>
  <font color="black">ニューラルネットワークを使用するバイフェーズ共通表現学習モジュールは、分析フェーズとサポートフェーズの間の相関と再現性をモデル化するように設計されています。次に、トレーニングされた相関ネットワークを使用して、分析フェーズの識別機能を抽出します。これらの機能はさらに進んでいます。ガウス混合ベースの隠れマルコフモデルやディープニューラルネットワークなどの機械学習モデルを使用して、5つのバイナリ音声カテゴリに分類されます。 
[要約]提案されたアプローチは、デコード中に多相データが利用できないことをさらに処理します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Can We Trust Deep Speech Prior? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.SD/paper_6.html">
      <font color="black">Can We Trust Deep Speech Prior?</font>
    </a>
  </h2>
  <font color="black">理論上の明らかな利点にもかかわらず、深い生成モデルによって生成される可能性は必ずしも音声品質と一致しないため、深い事前分布は慎重に使用する必要があると主張します。注意深い分析により、この問題はに深く根ざしていることが示されました。深い生成モデルの柔軟性と最尤（ML）トレーニングの性質との不調和..最近、深いスピーチ事前確率に基づくスピーチエンハンスメント（SE）が、非負の変分自動エンコーダーなど、大きな注目を集めています。行列因数分解（VAE-NMF）アーキテクチャ。 
[概要]これは、低ランクの共分散を持つガウス分布などの浅いモデルによるクリーンな音声を表す従来のアプローチと比較されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Prosodic Representation Learning and Contextual Sampling for Neural
  Text-to-Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.SD/paper_7.html">
      <font color="black">Prosodic Representation Learning and Contextual Sampling for Neural
  Text-to-Speech</font>
    </a>
  </h2>
  <font color="black">この論文では、文脈的に適切な韻律を用いた神経音声合成のための新しい2段階トレーニングプロセスでトレーニングされたモデルであるKathakaを紹介します。強いベースラインと比較した場合、自然性において統計的に有意な相対的改善$ 13.2 \％$を示します。記録..また、サンプリング手法のバリエーションに関するアブレーション研究を実施し、それぞれの場合でベースラインを超えて統計的に有意な改善を示しています。 
[概要]ステージiでは、melから韻律分布を学習します-トレーニング中に利用可能なスペクトログラム。これを行うには、テキストにbertを使用し、テキストから抽出されたpardicツリーにグラフ-注意ネットワークを使用します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Cross-Lingual Machine Speech Chain for Javanese, Sundanese, Balinese,
  and Bataks Speech Recognition and Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.SD/paper_8.html">
      <font color="black">Cross-Lingual Machine Speech Chain for Javanese, Sundanese, Balinese,
  and Bataks Speech Recognition and Synthesis</font>
    </a>
  </h2>
  <font color="black">次に、インドネシア語のASRとTTSを、テキストのみまたは音声データのみのクロスリンガルマシン音声チェーンフレームワークで利用して、民族言語のASRとTTSを開発し、これらの民族言語の音声テキストデータのペアの必要性を排除します。私たちは、これらのインドネシア語の音声認識と合成の開発に焦点を当てています：Javanese、Sundanese、Balinese、およびBataks ..最初に、監視付きトレーニングで標準インドネシア語のASRとTTSを別々にトレーニングします。 
[ABSTRACT] s2stシステムには、教師ありトレーニングに大きく依存する機械翻訳（mt）、音声認識（asr）、合成（tts）が必要です。フレームワークは当初、民族言語内でのみ実装されていました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Incremental Machine Speech Chain Towards Enabling Listening while
  Speaking in Real-time -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.SD/paper_9.html">
      <font color="black">Incremental Machine Speech Chain Towards Enabling Listening while
  Speaking in Real-time</font>
    </a>
  </h2>
  <font color="black">この作業では、機械がリアルタイムで話しているときに聞くことができるようにするための増分機械音声チェーンを提案します。実験結果は、提案されたフレームワークが、非インクリメンタルベーシックマシンスピーチチェーン..したがって、長い発話に遭遇すると、大幅な遅延が発生します。 
[概要]リアルタイムで、人間はちょっと話していることをリアルタイムで聞くことができます。聞き取りに遅れがあると、話し続けることができなくなります。インクリメンタルasrとインクリメンタルniances（itts）を構築します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Weighted Data Spaces for Correlation-Based Array Imaging in Experimental
  Aeroacoustics -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.SD/paper_10.html">
      <font color="black">Weighted Data Spaces for Correlation-Based Array Imaging in Experimental
  Aeroacoustics</font>
    </a>
  </h2>
  <font color="black">この一般的なクラスのビームフォーマーには、従来のビームフォーミング、（ロバスト）アダプティブビームフォーミング、シェーディングを使用したビームフォーミングなど、多くのよく知られた方法が含まれています。この記事では、周波数領域での相関測定に基づく空力音響イメージング方法について説明します。任意に相関するノイズをカバーする測定プロセスのモデル。 
[ABSTRACT]この分野の標準的な方法は、推定された相関行列が加法性ホワイトノイズと重ね合わされることを前提としています。相関データの共分散行列は、4次モーメントで与えられます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-27">
        <br><font color="black">2020-05-27</font>
      </time>
    </span>
</section>
<!-- paper0: Augmenting Images for ASR and TTS through Single-loop and Dual-loop
  Multimodal Chain Framework -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.SD/paper_11.html">
      <font color="black">Augmenting Images for ASR and TTS through Single-loop and Dual-loop
  Multimodal Chain Framework</font>
    </a>
  </h2>
  <font color="black">実験結果は、シングルループとデュアルループの両方のマルチモーダルチェーンフレームワークにより、ASRとTTSが画像のみのデータセットを使用してパフォーマンスを向上できることを明らかにしました。さらに、このフレームワークのパフォーマンスは、シングルスピーカーの人工音声データでのみ調査されました。この研究では、画像生成（IG）を使用してマルチモーダルマシンチェーンフレームワークを刷新し、マルチスピーカーの自然音声データでシングルループおよびデュアルループアーキテクチャを使用してASRおよびTTSの画像データを拡張する可能性を調査します。 
[ABSTRACT]研究者はマルチモーダル学習学習システムを刷新しました。このフレームワークは画像検索（ir）モデルに依存していたため、トレーニング中にすでにわかっているより多くの画像の処理に限定されていました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Sequence-to-Sequence Learning via Attention Transfer for Incremental
  Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.SD/paper_12.html">
      <font color="black">Sequence-to-Sequence Learning via Attention Transfer for Incremental
  Speech Recognition</font>
    </a>
  </h2>
  <font color="black">より薄いまたはより浅いモデルを使用する代わりに、教師モデルの元のアーキテクチャを維持しながら、シーケンスを短くする（エンコーダーとデコーダーの状態が少ない）代替の学生ネットワークを設計します。主な理由の1つは、モデルが増分ステップを実行し、現在の短い音声セグメントに合わせた文字起こしを学習します。注意ベースのシーケンス間自動音声認識（ASR）は、入力シーケンス全体を受信した後に出力が生成されるため、長い発話を認識するために大幅な遅延が必要です。 
[概要]これは、インクリメンタル音声認識（isr）の理解が不足しているためです。isrの元の注意アーキテクチャに基づくasrを使用することは可能です。代わりに、学生は現在の入力ショート間の同じ配置を模倣することを学びます。音声セグメント</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Fast Data-Driven Learning of MRI Sampling Pattern for Large Scale
  Problems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.IV/paper_0.html">
      <font color="black">Fast Data-Driven Learning of MRI Sampling Pattern for Large Scale
  Problems</font>
    </a>
  </h2>
  <font color="black">2つのデータセットがテストされました。1つは高解像度イメージング用の脳画像、もう1つは軟骨の定量的マッピング用の膝画像です。オプションで、再構成の品質を損なうことなくスキャン時間をほぼ半分にすることができます。特定のMRI問題に対する効果的なサンプリングと再構成のペア。 
[ABSTRACT]低音は、より大きなspとより大きなデータセットを使用して、さまざまな再構築方法の効果的なspを迅速に学習するために使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: RAIN: A Simple Approach for Robust and Accurate Image Classification
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.IV/paper_1.html">
      <font color="black">RAIN: A Simple Approach for Robust and Accurate Image Classification
  Networks</font>
    </a>
  </h2>
  <font color="black">最後に、RdmDUモジュールは、ディープ超解像ネットワークなどの詳細拡張モデルを使用してアップサンプリングを実行します。RAINは新しいランダム化拡張スキームを導入します。既存の敵対的防御方法の大部分は、コストをかけて堅牢性を実現することが示されています。予測精度を犠牲にすること。 
[概要]精度の大幅な低下は、機械学習アルゴリズムの信頼性に影響を与えます。これは、システムの精度の大幅な低下の結果です。結果により、予測精度が低下することが示されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-24">
        <br><font color="black">2020-04-24</font>
      </time>
    </span>
</section>
<!-- paper0: Noise Reduction to Compute Tissue Mineral Density and Trabecular Bone
  Volume Fraction from Low Resolution QCT -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.IV/paper_2.html">
      <font color="black">Noise Reduction to Compute Tissue Mineral Density and Trabecular Bone
  Volume Fraction from Low Resolution QCT</font>
    </a>
  </h2>
  <font color="black">トレーニングエラーとテストエラーの比較により、過剰適合に対する高いロバスト性が明らかになりました。BMDおよびボクセル単位の密度の評価に対する効果は示されていませんが、フィルターは、フィルター処理されていないデータに関してTMDおよびBV / TVの計算を完全に改善しました。低解像度TMDおよびBV / TVの二乗平均および精度誤差は、初期値の17％未満に減少しました。 
[概要]椎骨-ファントム研究には、シミュレートされた不一致のctノイズと3つの異なる管電流の9回の繰り返しによる高解像度の末梢および臨床ctスキャンが含まれていました。トレーニングとテストエラーの比較により、過剰適合に対するノイズの多い損失が明らかになりました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: BGGAN: Bokeh-Glass Generative Adversarial Network for Rendering
  Realistic Bokeh -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.IV/paper_3.html">
      <font color="black">BGGAN: Bokeh-Glass Generative Adversarial Network for Rendering
  Realistic Bokeh</font>
    </a>
  </h2>
  <font color="black">実験によると、私たちの方法は高品質のボケ効果をレンダリングし、すべてのスマートフォンチップセットで1.9秒で1つの$ 1024 \ times 1536 $ピクセル画像を処理できます。DSLRはこの種の効果を自然に簡単にレンダリングできます。 AIM 2020 Rendering Realistic Bokeh Challenge Track 1 \＆Track 2. 
[ABSTRACT] glass-netは、複雑なハードウェアに依存しないボケ画像を生成します。これは、スマートフォンで使用できるデジタルジェネレーターに基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Improved anomaly detection by training an autoencoder with skip
  connections on images corrupted with Stain-shaped noise -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.IV/paper_4.html">
      <font color="black">Improved anomaly detection by training an autoencoder with skip
  connections on images corrupted with Stain-shaped noise</font>
    </a>
  </h2>
  <font color="black">クリーンな画像のみがトレーニングに利用できる一般的なシナリオでは、アイデンティティマッピングへのネットワークの収束を防ぐために、合成ノイズモデルでそれらを破壊し、その目的のために元のステインノイズモデルを導入することを提案します。私たちのアプローチの関連性を実証し、私たちの検証は、ピクセルおよび画像ごとの異常検出の両方について、MVTec ADデータセットに対するパフォーマンスを比較することにより、再構成ベースの方法の最初の一貫した評価を提供します。このモデルが再構成に有利であることを示します。実際の欠陥の外観に関係なく、任意の実世界の画像からのクリーンな画像の抽出。 
[要約]私たちのアプローチの関連性を示すことに加えて、私たちの検証は、再構築ベースの方法の最初の一貫した評価を提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-29">
        <br><font color="black">2020-08-29</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Anomaly Detection for X-Ray Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.IV/paper_5.html">
      <font color="black">Unsupervised Anomaly Detection for X-Ray Images</font>
    </a>
  </h2>
  <font color="black">私たちの方法は、診断の効率を高め、重要な領域を見逃すリスクを減らします。したがって、異常を検出し、これらの方法の出力をどのように説明できるかを示すために、教師なし学習に最先端のアプローチを採用します。異常と間違われることが多いノイズの影響を低減するために、強力な前処理パイプラインを導入しています。 
[要約]異常のない画像でトレーニングされた方法は、異常の治療に使用できます。これは、医師が手のX線画像を評価するのに役立ちます。方法は、異常を発症するリスクを減らすために開発できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-29">
        <br><font color="black">2020-01-29</font>
      </time>
    </span>
</section>
<!-- paper0: Do Noises Bother Human and Neural Networks In the Same Way? A Medical
  Image Analysis Perspective -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.IV/paper_6.html">
      <font color="black">Do Noises Bother Human and Neural Networks In the Same Way? A Medical
  Image Analysis Perspective</font>
    </a>
  </h2>
  <font color="black">実験結果は、提案されたフレームワークが人間の視覚のノイズ除去ネットワークよりも良い結果を達成できることを示しています。最近、多くの医療ノイズ除去方法は、定量的および定性的に、有意なアーチファクト低減結果とノイズ除去を示しました。この論文では、アプリケーションを紹介します。 -以下のニューラルネットワークのノイズ除去に焦点を当てたガイド付きノイズ除去フレームワーク。 
[概要]これらのアプリケーションのいくつかは、医用画像を事前に分析するために提案されています。これにより、臨床評価中に放射線科医により多くの情報がもたらされます。ただし、既存の方法は人間の視覚を中心に開発されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Non contrast Doppler Microvessel Image Reconstruction by a semi Nonrigid
  Motion Compensation and Localized Clutter Filtering; a Qualitative and
  Quantitative Evaluation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.IV/paper_7.html">
      <font color="black">Non contrast Doppler Microvessel Image Reconstruction by a semi Nonrigid
  Motion Compensation and Localized Clutter Filtering; a Qualitative and
  Quantitative Evaluation</font>
    </a>
  </h2>
  <font color="black">シミュレーション研究によりアルゴリズムの性能を評価することに成功しました。しかし、呼吸や血管の脈動などのさまざまな理由により、動きは微小血管系イメージングの本質的な部分です。超音波微小血管用の堅牢で計算効率の高い動き補償アルゴリズムを提案しました。イメージング。 
[概要]動き補償には、組織クラッターフィルタリングの使用が含まれます。このような動きの一部は、空間的クラッタリングを使用して処理されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Segmentation of Defective Skulls from CT Data for Tissue Modelling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.IV/paper_8.html">
      <font color="black">Segmentation of Defective Skulls from CT Data for Tissue Modelling</font>
    </a>
  </h2>
  <font color="black">このアプリケーションドメインでのCNNセグメンテーション実験は、これまで単純なパッチベースのCNNアーキテクチャに限定されていたため、最初にエンコーダ-デコーダアーキテクチャの使用によってセグメンテーションの精度が大幅に向上することを示します。次に、セグメンテーションの数を示します。通常は手動で修正する必要があるアーティファクトは、CNNトレーニングに境界項を追加し、グラフカットを使用してセグメンテーションをグローバルに最適化することでさらに減らすことができます。この作業では、カスタム頭蓋インプラント設計のための欠陥のある頭蓋骨の自動セグメンテーションの方法を示します。および3D印刷の目的。 
[ABSTRACT]ディープラーニングベースの手法では、最初の最初の構造のスペクトルを含む十分なデータセットを作成できません。通常は手動で修正する必要があるセグメンテーションアーティファクトの数は、cnnトレーニングに境界項を追加することでさらに減らすことができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-20">
        <br><font color="black">2019-11-20</font>
      </time>
    </span>
</section>
<!-- paper0: Viewing angle of holographic image reconstructed from digital hologram
  with enhanced numerical aperture -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.IV/paper_9.html">
      <font color="black">Viewing angle of holographic image reconstructed from digital hologram
  with enhanced numerical aperture</font>
    </a>
  </h2>
  <font color="black">開口数（NA）が強化されたデジタルホログラムは、フレネル回折領域でレプリカフリンジパターンを形成し、それぞれの複数の画像を再構成します。複数の画像は、サブアパーチャ以外のホログラムの全開口数のNAとして表される同じ視野角を持ちます。レプリカフリンジパターン..再構成された複数の画像の視野角は、光子場の運動量とその空間分解能の間の量子力学的不確実性の関係から解釈されます。 
[概要]ホログラフィック複数画像の視野角は、量子力学的不確実性から解釈されます。複数の画像は、異なる形状を使用して撮影されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-27">
        <br><font color="black">2020-04-27</font>
      </time>
    </span>
</section>
<!-- paper0: Geometric Solution of Image Degradation by Diffraction in Lensless
  Sensing and Microscopy -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.IV/paper_10.html">
      <font color="black">Geometric Solution of Image Degradation by Diffraction in Lensless
  Sensing and Microscopy</font>
    </a>
  </h2>
  <font color="black">シャドウイメージングの提案されたアプローチの重要な利点は、被写界深度の改善と、センサーからサンプルまでの作動距離の大幅な増加です。この原理は、直径が約6〜9マイクロメートルの赤血球のレンズレスイメージングによって実験的に検証されています。高度に発散するビームを利用して、変換された投影空間でこれらの点拡散関数の空間範囲を比較的小さくすることができます。これにより、視覚情報の空間的な混合を解除できます。 
[要約]この方法は、すべての光学画像が回折パターンであり、その逆もあるという確実性に基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: FDRN: A Fast Deformable Registration Network for Medical Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.IV/paper_11.html">
      <font color="black">FDRN: A Fast Deformable Registration Network for Medical Images</font>
    </a>
  </h2>
  <font color="black">実験結果は、私たちのFDRNが、ダイススコアおよび正規化相互相関（NCC）において、調査された方法よりも定性的および定量的に優れていることを示しています。LPBA40脳MRIデータセットで既存の最先端の登録方法と比較しました。さらに、FDRNは、特定のタイプの医療画像や解剖学的構造に限定されない、画像登録のための一般化されたフレームワークです。 
[概要]提案されたfdrnは、コンパクトなエンコーダー-デコーダー構造を誇っています。他の解剖学的構造やct画像にも適用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: An artificial intelligence system for predicting the deterioration of
  COVID-19 patients in the emergency department -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.IV/paper_12.html">
      <font color="black">An artificial intelligence system for predicting the deterioration of
  COVID-19 patients in the emergency department</font>
    </a>
  </h2>
  <font color="black">実際の臨床現場でのパフォーマンスを検証するために、パンデミックの最初の波の間にニューヨーク大学ランゴーンヘルスにディープニューラルネットワークの予備バージョンをサイレントに展開しました。これにより、リアルタイムで正確な予測が生成されました。ディープニューラルネットワーク胸部X線画像の有益な領域を抽出して、臨床医が予測を解釈するのを支援し、リーダー研究で2人の放射線科医と同等のパフォーマンスを発揮します。コロナウイルス病2019（COVID-19）のパンデミック、救急部門での患者の迅速かつ正確なトリアージ意思決定に情報を提供するために重要です。 
[ABSTRACT]胸部X線画像から学習するディープニューラルネットワーク。システムは、臨床医が予測を解釈するのに役立つ有益な領域を抽出します。システムは、医療従事者が予測を解釈するのにも役立ちます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-04">
        <br><font color="black">2020-08-04</font>
      </time>
    </span>
</section>
<!-- paper0: Video Generative Adversarial Networks: A Review -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.IV/paper_13.html">
      <font color="black">Video Generative Adversarial Networks: A Review</font>
    </a>
  </h2>
  <font color="black">GANモデルのバリエーションは、一般に、いくつかの調査論文である程度取り上げられていますが、私たちの知る限りでは、これは最先端のビデオGANモデルをレビューする最初の調査論文の1つです。次に、このペーパーでは、当初はビデオドメイン用に開発されなかったが、複数のビデオGANバリエーションで採用された、GANフレームワークの主な改善点を要約します。条件モデルは、条件のタイプに従って、オーディオ、テキスト、ビデオ、およびにさらにグループ化されます。画像。 
[概要]このペーパーは、現在のビデオガンモデルの主な課題と制限に焦点を当てて作成されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Robust Super-Resolution of Real Faces using Smooth Features -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.IV/paper_14.html">
      <font color="black">Robust Super-Resolution of Real Faces using Smooth Features</font>
    </a>
  </h2>
  <font color="black">最近の研究のいくつかは、Generative Adversarial Network（GAN）を使用して、実際の画像のデータセットからこれらの劣化をモデル化しようとしています。実際の低解像度（LR）の顔画像には、既知のダウンサンプリングカーネルではキャプチャできないほど多様で複雑な劣化が含まれています。信号に依存しないノイズ..合成的に劣化したLR画像を生成し、対応する実際の高解像度（HR）画像とともに使用して、ピクセル単位の損失と敵対的損失の組み合わせを使用して超解像度（SR）ネットワークをトレーニングします。 
[概要]バイキュービックにダウンサンプリングされたクリーンな画像を実際の劣化画像に変換する劣化ガンをトレーニングします。この方法は、広範囲のノイズ、ブラー、圧縮アーティファクト、およびノイズに対してロバストである必要があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic identification of fossils and abiotic grains during carbonate
  microfacies analysis using deep convolutional neural networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.IV/paper_15.html">
      <font color="black">Automatic identification of fossils and abiotic grains during carbonate
  microfacies analysis using deep convolutional neural networks</font>
    </a>
  </h2>
  <font color="black">薄片のミクロ相同定に基づく記述岩石学的分析は、堆積環境の解釈と古生態学的再構築に広く使用されています。骨格断片の形態学的および微細構造の多様性を区別するには、ミクロ相の化石形態型に関する広範な事前知識と顕微鏡下での長いトレーニングセッションが必要です。機械分類器は、この課題に対処するのに役立ちます。 
[概要]この研究では、1、149の参照と独自の資料からの公開データを含むマイクロフェイシー画像データセットを収集しました。ツールは、人間の分類器と同等の再現性とバイアス回避で高精度を示しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-24">
        <br><font color="black">2020-09-24</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Learning Discriminative Representations for Fine-Grained Diabetic
  Retinopathy Grading -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_0.html">
      <font color="black">Learning Discriminative Representations for Fine-Grained Diabetic
  Retinopathy Grading</font>
    </a>
  </h2>
  <font color="black">眼科医は、疾患の重症度レベルを判断するために、眼底画像の識別部分に焦点を当てる必要があります。クラス間の順序情報を活用するために、順序回帰法を使用してソフトラベルを取得します。実験結果は優れたパフォーマンスを示しています。 2つのパブリックIDRiDおよびDeepDRデータセットに対する提案された方法の評価。 
[概要]ディープラーニングは医療画像解析で大きな成功を収めています。近年、ディープラーニングは成功を収めています。実際、drの自動画像グレーディングはきめ細かい分類タスクと見なされています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Global Image Segmentation Process using Machine Learning algorithm &
  Convolution Neural Network method for Self- Driving Vehicles -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_1.html">
      <font color="black">Global Image Segmentation Process using Machine Learning algorithm &
  Convolution Neural Network method for Self- Driving Vehicles</font>
    </a>
  </h2>
  <font color="black">実験結果は73％の平均IOUを達成します。この画像セグメンテーション方法の計画を標準化し、目視検査システムを理解するための最先端の方法の開発をさらに提案しました。サンプリングでは、地方都市のデータセットサンプルと検証を想定しています。 Python言語を使用してJupyterNotebookで実行されるプロセス。 
[概要]論文の主な目的は、画像セグメンテーションプロセスと畳み込みニューラルネットワーク法を使用して入力画像を分割し、視覚の効率的な結果を得ることです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-26">
        <br><font color="black">2020-10-26</font>
      </time>
    </span>
</section>
<!-- paper0: RAIN: A Simple Approach for Robust and Accurate Image Classification
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_2.html">
      <font color="black">RAIN: A Simple Approach for Robust and Accurate Image Classification
  Networks</font>
    </a>
  </h2>
  <font color="black">最後に、RdmDUモジュールは、ディープ超解像ネットワークなどの詳細拡張モデルを使用してアップサンプリングを実行します。具体的には、RAINは、ランダム化小循環シフト（RdmSCS）とランダム化ダウンアップサンプリング（RdmDU）の2つの新しいランダム化モジュールで構成されます。 RAINは、新しいランダム化強化スキームを導入します。 
[概要]精度の大幅な低下は、機械学習アルゴリズムの信頼性に影響を与えます。これは、システムの精度の大幅な低下の結果です。結果により、予測精度が低下することが示されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-24">
        <br><font color="black">2020-04-24</font>
      </time>
    </span>
</section>
<!-- paper0: Noise Reduction to Compute Tissue Mineral Density and Trabecular Bone
  Volume Fraction from Low Resolution QCT -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_3.html">
      <font color="black">Noise Reduction to Compute Tissue Mineral Density and Trabecular Bone
  Volume Fraction from Low Resolution QCT</font>
    </a>
  </h2>
  <font color="black">BMDおよびボクセル単位の密度の評価に対する効果は示されていませんが、フィルターは、フィルター処理されていないデータに関してTMDおよびBV / TVの計算を完全に改善しました。低解像度TMDおよびBV /のルート平均二乗および精度エラーTVは初期値の17％未満に減少しました。提案されたアーキテクチャはしきい値と回転不変であり、一度に広範囲の画像解像度に適用可能であり、さらなる微細構造パラメータの正確な計算に役立つ可能性があります。 
[概要]椎骨-ファントム研究には、シミュレートされた不一致のctノイズと3つの異なる管電流の9回の繰り返しによる高解像度の末梢および臨床ctスキャンが含まれていました。トレーニングとテストエラーの比較により、過剰適合に対するノイズの多い損失が明らかになりました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Mucko: Multi-Layer Cross-Modal Knowledge Reasoning for Fact-based Visual
  Question Answering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_4.html">
      <font color="black">Mucko: Multi-Layer Cross-Modal Knowledge Reasoning for Fact-based Visual
  Question Answering</font>
    </a>
  </h2>
  <font color="black">このプロセスを複数回スタックすることにより、モデルは反復推論を実行し、すべての質問指向の証拠を分析することによって最適な答えを予測します。多層グラフ表現に加えて、モダリティを意識した異種グラフ畳み込みネットワークを提案して、与えられた質問に最も関連するさまざまなレイヤー..FVQAタスクで新しい最先端のパフォーマンスを実現し、広範な実験でモデルの有効性と解釈可能性を示します。 
[概要]視覚的、意味的、事実的な機能にはfvqaソリューションが必要です。新しいモデルは、きめ細かい選択なしですべてのタイプの情報を組み合わせます。これにより、fvqaタスクでの最新のパフォーマンスが作成されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-16">
        <br><font color="black">2020-06-16</font>
      </time>
    </span>
</section>
<!-- paper0: 3D-LaneNet+: Anchor Free Lane Detection using a Semi-Local
  Representation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_5.html">
      <font color="black">3D-LaneNet+: Anchor Free Lane Detection using a Semi-Local
  Representation</font>
    </a>
  </h2>
  <font color="black">結果は、元の3D-LaneNetと比較して大幅な改善を示しています。これは、複雑なレーントポロジ、曲率、および表面ジオメトリへのより良い一般化に起因する可能性があります。合成データと実世界データの両方を使用して、3D-LaneNet +の有効性を示します。この組み合わせにより3Dが可能になります。 -LaneNet +は、元の3D-LaneNetのように、レーンアンカー、非最大抑制、およびレーンモデルフィッティングの使用を回避します。 
[概要]最近提案された3d-lanenetに従い、これまでサポートされていなかったレーントポロジの検出を可能にするために拡張します。3d-lanenetの有効性を示しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-01">
        <br><font color="black">2020-11-01</font>
      </time>
    </span>
</section>
<!-- paper0: BGGAN: Bokeh-Glass Generative Adversarial Network for Rendering
  Realistic Bokeh -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_6.html">
      <font color="black">BGGAN: Bokeh-Glass Generative Adversarial Network for Rendering
  Realistic Bokeh</font>
    </a>
  </h2>
  <font color="black">実験によると、私たちの方法は高品質のボケ効果をレンダリングし、すべてのスマートフォンチップセットで1.9秒で1つの$ 1024 \ times 1536 $ピクセル画像を処理できます。さらに、インスタンスの正規化（IN）がネットワークに再実装され、 IN付きのtfliteモデルはスマートフォンGPUで高速化できます。ただし、センサーの制限により、スマートフォンは深度効果のある画像を直接キャプチャすることはできません。 
[概要]ガラス-ネットは複雑なハードウェアに依存せずにボケ画像を生成します。これはスマートフォンで使用できるデジタルジェネレーターに基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Improved anomaly detection by training an autoencoder with skip
  connections on images corrupted with Stain-shaped noise -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_7.html">
      <font color="black">Improved anomaly detection by training an autoencoder with skip
  connections on images corrupted with Stain-shaped noise</font>
    </a>
  </h2>
  <font color="black">このモデルは、実際の欠陥の外観に関係なく、任意の実世界の画像からのクリーンな画像の再構成に有利であることを示します。私たちのアプローチの関連性を示すことに加えて、検証は、再構成ベースの方法の最初の一貫した評価を提供します。ピクセル単位と画像単位の両方の異常検出について、MVTec ADデータセットに対するパフォーマンスを比較します。このアプローチでは、異常検出は従来、再構成の残差、または再構成の不確実性に依存します。 
[要約]私たちのアプローチの関連性を示すことに加えて、私たちの検証は、再構築ベースの方法の最初の一貫した評価を提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-29">
        <br><font color="black">2020-08-29</font>
      </time>
    </span>
</section>
<!-- paper0: Fairness in Biometrics: a figure of merit to assess biometric
  verification systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_8.html">
      <font color="black">Fairness in Biometrics: a figure of merit to assess biometric
  verification systems</font>
    </a>
  </h2>
  <font color="black">機械学習ベース（ML）システムは、過去10年間、日常生活のいくつかのインスタンスに影響を与える無数のシナリオで主に展開されています。この作業では、バイオメトリクスの公平性の側面について説明します。次に、フェイスバイオメトリクスを使用したユースケースは、性別と人種の人口統計を調査する3つの公開データセットを使用して、この新しいメリットの数値と比較していくつかのシステムが評価される場所で示されています。 
[概要]いわゆる公平性の不一致率（fdr）は、複数の生体認証システムに基づいています。顔の生体認証を使用したユースケースを示し、複数のシステムを比較して評価します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Hyperspectral classification of blood-like substances using machine
  learning methods combined with genetic algorithms in transductive and
  inductive scenarios -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_9.html">
      <font color="black">Hyperspectral classification of blood-like substances using machine
  learning methods combined with genetic algorithms in transductive and
  inductive scenarios</font>
    </a>
  </h2>
  <font color="black">私たちの実験では、GAをグリッド検索による従来のモデル最適化と比較します。結果は、GAベースのモデル最適化により、バンドの数を減らし、GSベースの参照モデルよりも優れた正確な分類器を作成できることを示しています。テストデータと同様の例にアクセスできます。検証セットの重要性を強調する実験でこれを説明します。 
[概要]血液と5つの視覚的に類似した物質を使用した7つのハイパースペクトル画像の法医学データセットをテストします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Anomaly Detection for X-Ray Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_10.html">
      <font color="black">Unsupervised Anomaly Detection for X-Ray Images</font>
    </a>
  </h2>
  <font color="black">したがって、エンドツーエンドの方法で不確実な最終診断を提供する解釈不可能なブラックボックスシステムに焦点を当てる代わりに、異常のない画像でトレーニングされた教師なし方法を使用して、医師が手のX線画像を評価するのを支援する方法を調査します..医療（画像）データのラベルを取得するには、希少で高価な専門家が必要です。私たちの方法は、診断の効率を高め、重要な領域を見逃すリスクを減らします。 
[要約]異常のない画像でトレーニングされた方法は、異常の治療に使用できます。これは、医師が手のX線画像を評価するのに役立ちます。方法は、異常を発症するリスクを減らすために開発できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-29">
        <br><font color="black">2020-01-29</font>
      </time>
    </span>
</section>
<!-- paper0: Do Noises Bother Human and Neural Networks In the Same Way? A Medical
  Image Analysis Perspective -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_11.html">
      <font color="black">Do Noises Bother Human and Neural Networks In the Same Way? A Medical
  Image Analysis Perspective</font>
    </a>
  </h2>
  <font color="black">実験結果は、提案されたフレームワークが人間の視覚のノイズ除去ネットワークよりも優れた結果を達成できることを示しています。ディープラーニングは、ノイズ除去、分類、セグメンテーションなどを含む医用画像ですでにその力を実証しています。この論文では、アプリケーションを紹介します-次のニューラルネットワークのノイズ除去に焦点を当てたガイド付きノイズ除去フレームワーク。 
[概要]これらのアプリケーションのいくつかは、医用画像を事前に分析するために提案されています。これにより、臨床評価中に放射線科医により多くの情報がもたらされます。ただし、既存の方法は人間の視覚を中心に開発されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: A Deep Temporal Fusion Framework for Scene Flow Using a Learnable Motion
  Model and Occlusions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_12.html">
      <font color="black">A Deep Temporal Fusion Framework for Scene Flow Using a Learnable Motion
  Model and Occlusions</font>
    </a>
  </h2>
  <font color="black">このように、私たちのアプローチは、さまざまなシーンフロー推定器に高速マルチフレーム拡張を提供します。これは、基礎となるデュアルフレームアプローチよりも優れています。2番目のステップでは、ニューラルネットワークが共通の参照フレームからの双方向シーンフロー推定を組み合わせます。 、オクルージョンマスクの洗練された推定値と自然な副産物を生成します。私たちの仕事は、オクルージョンの問題を克服するために、マルチフレーム設定でシーンフロー推定値を時間的に融合するための新しいデータ駆動型アプローチを提案します。 
[ABSTRACT]モーション分析は、オクルージョンと視野外モーションが制限要因であることを示しています。しかし、代わりに、一定のモーションモデルに依存せず、データからモーションの一般的な時間的関係を学習します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-03">
        <br><font color="black">2020-11-03</font>
      </time>
    </span>
</section>
<!-- paper0: Defense-guided Transferable Adversarial Attacks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_13.html">
      <font color="black">Defense-guided Transferable Adversarial Attacks</font>
    </a>
  </h2>
  <font color="black">転送可能性をさらに促進するために、max-min理論を使用して変換された値を決定します。このような問題を解決するために、敵の攻撃と防御の両方に有益な入力変換に触発されたmax-minフレームワークを設計します。Imagenetでの広範な実験防御に基づく転送可能な攻撃により、転送可能性が大幅に向上することを示しています。 
[概要]これらの例は未知のモデルへの転送が困難であり、転送性の低いいくつかの方法が提案されています。この方法は、深いモデルのロバスト性を評価するためのベンチマークとなることが期待されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: S3-Net: A Fast and Lightweight Video Scene Understanding Network by
  Single-shot Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_14.html">
      <font color="black">S3-Net: A Fast and Lightweight Video Scene Understanding Network by
  Single-shot Segmentation</font>
    </a>
  </h2>
  <font color="black">CityScapes、UCF11、HMDB51、およびMOMENTSデータセットを使用した実験では、提案されたS3-NetがUCF11の3D-CNNベースのアプローチと比較して8.1％の精度向上、6.9倍のストレージ削減、CityScapesでの22.8FPSの推論速度を達成することを示しています。 GTX1080Ti GPU .. S3-Netと呼ばれる提案されたネットは、ターゲットサブシーンをすばやく見つけてセグメント化し、その一方で、LSTMベースの時空間モデルへの入力として構造化された時系列の意味的特徴を抽出します。 S3-Netは、エッジコンピューティング用に軽量化することを目的としています。 
[ABSTRACT] s3-netは、インターネットに取り組むための高度なモデルと呼ばれます。高速のシングルスピードモデルモデルを使用します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: SD-Measure: A Social Distancing Detector -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_15.html">
      <font color="black">SD-Measure: A Social Distancing Detector</font>
    </a>
  </h2>
  <font color="black">カメラからの人々の距離と彼らの間の距離を概算するための本物のアルゴリズムの助けを借りて、社会的距離のガイドラインが遵守されているかどうかを判断します。提案されたフレームワークは、マスクR-CNNディープニューラルネットワークを活用してビデオ内の人々を検出しますフレーム..フレームワークは、カスタムビデオ映像データセット（CVFD）およびカスタム個人画像データセット（CPID）でテストされたときに、低い誤警報率と併せて高い精度値を達成し、社会的距離のガイドラインが実践されているかどうかを判断する上でその有効性を示しました。 。 
[ABSTRACT]ビデオ映像から社会的距離を検出するために開発されたシステム。社会的距離が実践されているかどうかを識別するために開発されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: CPM R-CNN: Calibrating Point-guided Misalignment in Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_16.html">
      <font color="black">CPM R-CNN: Calibrating Point-guided Misalignment in Object Detection</font>
    </a>
  </h2>
  <font color="black">COCOデータセットの十分な評価によると、CPM R-CNNは、前述のミスアライメントを調整することにより、ローカリゼーションの精度を向上させるのに効率的であることが実証されています。より高速なR-CNNとFPNを備えたResNet-101に基づくグリッドR-CNNにより、私たちのアプローチは、ホイッスルとベルなしで、検出mAPをそれぞれ3.3％と1.5％大幅に改善できます。 
[ABSTRACT]ポイント-アンカーベースの方法に導入されたガイド付きアプローチ。この方法は、笛やベルなしで、検出マップをそれぞれ3.3％と1.5％大幅に改善できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-07">
        <br><font color="black">2020-03-07</font>
      </time>
    </span>
</section>
<!-- paper0: Segmentation of Defective Skulls from CT Data for Tissue Modelling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_17.html">
      <font color="black">Segmentation of Defective Skulls from CT Data for Tissue Modelling</font>
    </a>
  </h2>
  <font color="black">このアプリケーションドメインでのCNNセグメンテーション実験は、これまで単純なパッチベースのCNNアーキテクチャに限定されていたため、最初に、エンコーダ-デコーダアーキテクチャの使用によってセグメンテーションの精度が大幅に向上することを示します。最後に、提案された方法を使用することを示します。 、臨床応用に十分な精度の3Dセグメンテーションは、2D CNNアーキテクチャとそれに対応する3Dアーキテクチャで実現できます。この作業では、カスタム頭蓋インプラント設計と3D印刷の目的で欠陥のある頭蓋骨を自動セグメンテーションする方法を紹介します。 
[ABSTRACT]ディープラーニングベースの手法では、最初の最初の構造のスペクトルを含む十分なデータセットを作成できません。通常は手動で修正する必要があるセグメンテーションアーティファクトの数は、cnnトレーニングに境界項を追加することでさらに減らすことができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-20">
        <br><font color="black">2019-11-20</font>
      </time>
    </span>
</section>
<!-- paper0: Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh
  Recovery from a 2D Human Pose -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_18.html">
      <font color="black">Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh
  Recovery from a 2D Human Pose</font>
    </a>
  </h2>
  <font color="black">入力としての2D人間のポーズは、2つのドメイン間で比較的均一な幾何学的特性を持ちながら、重要な人体の関節情報を提供します。また、提案されたシステムは、粗いtoでGraphCNNを使用してメッシュトポロジを十分に活用しながら、表現の問題を回避します。 -細かい方法..上記の弱点を克服するために、2D人間のポーズから直接人間のメッシュ頂点の3D座標を推定する新しいグラフ畳み込みニューラルネットワーク（GraphCNN）ベースのシステムであるPose2Meshを提案します。 
[概要]最初の弱点は、列車データとテストデータの間の画像の外観が異なるため、外観ドメインギャップの問題です。pose2meshは、3dを推定する新しいグラフ畳み込みニューラルネットワーク（graphcnn）ベースのシステムです。 2D人間ポーズから直接人間メッシュエッジの座標</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Channel Planting for Deep Neural Networks using Knowledge Distillation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_19.html">
      <font color="black">Channel Planting for Deep Neural Networks using Knowledge Distillation</font>
    </a>
  </h2>
  <font color="black">ネットワークプルーニングは、ネットワークからの冗長で不要なパラメータを減らすことができます。これらの方法で得られる小規模なネットワークのパフォーマンスは、事前定義されたネットワークによって制限されます。STL-10データセットの場合、同等のパフォーマンスを達成できることを示します。大規模なネットワークと比較して7％のパラメーターであり、少量のデータによって引き起こされる過剰適合を低減します。 
[ABSTRACT]ネットワークパフォーマンスは、ネットワークパフォーマンスを低下させることなくネットワークのサイズを圧縮することが示されています。サイズを圧縮するための複数の方法が提案されています。これには、植えられたチャネルをトレーニングするための知識蒸留法の使用が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Image Compositing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_20.html">
      <font color="black">Deep Image Compositing</font>
    </a>
  </h2>
  <font color="black">私たちの方法は、前景画像と背景画像の両方のコンテキスト情報と色情報の活用を最適化するためにエンドツーエンドでトレーニングできます。最適化では、合成品質が考慮されます。具体的には、ラプラシアンピラミッドブレンディング、高密度接続マルチ異なるスケールで前景画像と背景画像からの情報を効果的に融合するために、ストリーム融合ネットワークが提案されています。実験は、提案された方法が高品質の合成を自動的に生成し、定性的および定量的に既存の方法よりも優れていることを示しています。 
[概要]提案された方法は、ユーザー入力なしで高品質の合成を自動的に生成できます。前景と背景の画像からの情報を異なるスケールに融合するように設計されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Low cost enhanced security face recognition with stereo cameras -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_21.html">
      <font color="black">Low cost enhanced security face recognition with stereo cameras</font>
    </a>
  </h2>
  <font color="black">この記事では、ほとんどの認識アーキテクチャにおける現在のセキュリティの脆弱性の解決に貢献しようとする顔認識の代替案について説明します。提示されたソフトウェアは、ステレオセットアップの助けを借りて顔の深度マップを作成し、従来の認識プログラムよりも高いレベルのセキュリティを提供します。 ..市場に出回っている現在の低コストの顔認証ソフトウェアは、深度情報が不足しているため、顔の印刷された画像にだまされる可能性があります。 
[概要]人物のアイデンティティと顔の深度マップの分析は、深い畳み込みニューラルネットワークを介して処理されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Visual Representations for Transfer Learning by Suppressing
  Texture -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_22.html">
      <font color="black">Learning Visual Representations for Transfer Learning by Suppressing
  Texture</font>
    </a>
  </h2>
  <font color="black">私たちの方法は、転移学習タスクに特に効果的であり、5つの標準的な転移学習データセットでパフォーマンスの向上が見られました。これらの問題に対処するために、異方性拡散に基づく古典的な方法を使用して、テクスチャが抑制された画像を使用したトレーニングを強化することを提案します。 Sketch-ImageNetデータセット、DTDデータセット、および顕著性マップを使用した追加の視覚分析で最大11.49 \％）は、私たちのアプローチが、より良い転送よりも優れた表現の学習に役立つことを示唆しています。 
[概要]この簡単な方法は、重要なエッジ情報を保持し、同時にテクスチャを抑制するのに役立ちます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-03">
        <br><font color="black">2020-11-03</font>
      </time>
    </span>
</section>
<!-- paper0: Graph Based Temporal Aggregation for Video Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_23.html">
      <font color="black">Graph Based Temporal Aggregation for Video Retrieval</font>
    </a>
  </h2>
  <font color="black">これらの問題を克服するために、検索対象のすべてのビデオからのフレームの組み合わせセットから無向グラフを構築する、画像クエリによるビデオ検索の新しいアプローチを提案します。このグラフのノード機能は、ビデオ検索のタスクで使用されます。 ..この調査では、ResNet-152とResNet-50の2つの異なるResNetモデルを使用します。 
[概要]この分野での作業のほとんどは、vseなどの手法を使用したテキストクエリによるビデオ検索です。これらのアプローチは、データセットの外部からのクエリでは珍しくなく、大規模なビデオデータセットでは適切に拡張できません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: General Data Analytics With Applications To Visual Information Analysis:
  A Provable Backward-Compatible Semisimple Paradigm Over T-Algebra -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_24.html">
      <font color="black">General Data Analytics With Applications To Visual Information Analysis:
  A Provable Backward-Compatible Semisimple Paradigm Over T-Algebra</font>
    </a>
  </h2>
  <font color="black">新しいパラダイムのパフォーマンスとその下位互換性を示すために、視覚的なパターン分析のためのいくつかの標準的なアルゴリズムを一般化します。t代数では、すべてではないにしても、多くのアルゴリズムがこの新しい半単純なパラダイムを使用して簡単な方法で一般化されます。実験公開データセットでは、一般化されたアルゴリズムが標準的なアルゴリズムと比べて遜色がないことが示されています。 
[概要]たとえば、視覚的なパターン分析のための一連の新しいアルゴリズムを一般化します。新しいツールのパフォーマンスとその下位互換性を示します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-31">
        <br><font color="black">2020-10-31</font>
      </time>
    </span>
</section>
<!-- paper0: Effective Fusion Factor in FPN for Tiny Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_25.html">
      <font color="black">Effective Fusion Factor in FPN for Tiny Object Detection</font>
    </a>
  </h2>
  <font color="black">一連の実験と分析の後、統計的手法によって特定のデータセットの融合係数の有効値を推定する方法を探ります。包括的な実験は、TinyPersonやTinyCityPersonsなどの小さなオブジェクト検出データセットで実行されます。コードとモデルは解放されます。 
[ABSTRACT] fpnは、小さなオブジェクトの検出に適応するための新しい概念です。これは、各レイヤーに分散されたオブジェクトの数に基づいています。適切な融合係数でfpnを構成すると、ネットワークは大幅なパフォーマンスの向上を実現できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Filter Pruning using Hierarchical Group Sparse Regularization for Deep
  Convolutional Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_26.html">
      <font color="black">Filter Pruning using Hierarchical Group Sparse Regularization for Deep
  Convolutional Neural Networks</font>
    </a>
  </h2>
  <font color="black">提案された方法は、CIFAR-10のResNetのパラメータを50％以上削減でき、テストサンプルの精度はわずか0.3％低下することが示されています。また、TinyImageNet-200のResNetのパラメータは34％削減され、ベースラインネットワーク..以前の作業で、階層グループのスパース正則化が、不要なチャネルに接続されたフィルターが自動的にゼロに近いスパースネットワークを取得するのに効果的であることが示されています。 
[要約]この論文では、ベティ正則化を使用したフィルター剪定法を提案します。これは、ランダムに選択されたトレーニングサンプルの分類損失の増加に基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: From Generalized zero-shot learning to long-tail with class descriptors -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_27.html">
      <font color="black">From Generalized zero-shot learning to long-tail with class descriptors</font>
    </a>
  </h2>
  <font color="black">DRAGONは、新しいベンチマークで最先端のモデルよりも優れています。（1）サンプルごとにヘッドクラスへのバイアスを修正することを学習します。 （2）クラス記述からの情報を融合して、テールクラスの精度を向上させます。また、既存の学習に基づいて、クラス記述を使用したロングテール学習用の新しいベンチマークCUB-LT、SUN-LT、AWA-LTを導入します。 with-attributesデータセットとクラス記述子を備えたImagenet-LTのバージョン。 
[ABSTRACT] dragon、後期-ロングテール学習のための融合アーキテクチャ-クラス記述子を使用。既存の学習に基づいて構築された新しいベンチマーク-descriptions.gfslの既存のベンチマークに関する新しいsota</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-05">
        <br><font color="black">2020-04-05</font>
      </time>
    </span>
</section>
<!-- paper0: Leveraging Temporal Joint Depths for Improving 3D Human Pose Estimation
  in Video -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_28.html">
      <font color="black">Leveraging Temporal Joint Depths for Improving 3D Human Pose Estimation
  in Video</font>
    </a>
  </h2>
  <font color="black">ビデオの各フレームで推定された2Dポーズから3Dポーズを予測するアプローチの有効性は、3D人間ポーズ推定で実証されています。ただし、人物の外観情報のない2Dポーズは、関節の深さに関して多くのあいまいさがあります。本論文では、ビデオの各フレームの3Dポーズを推定し、時間情報を考慮してそれを改良することを提案します。 
[概要]提案されたアプローチは、ジョイントの深さの不確実性を低減します。提案されたソリューションは、2Dモデルの問題を低減します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: The Forchheim Image Database for Camera Identification in the Wild -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_29.html">
      <font color="black">The Forchheim Image Database for Camera Identification in the Wild</font>
    </a>
  </h2>
  <font color="black">カメラ識別方法の評価におけるFODBの有用性を示します。この目的のために、多くのデータセットが提案されています。画像の出所は、犯罪捜査やジャーナリズムの事実確認における重要な知識を表すことができます。 
[概要]現在、多数のデータセットが提案されています。この目的のために、12を超えるデータセットがあります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: CoT-AMFlow: Adaptive Modulation Network with Co-Teaching Strategy for
  Unsupervised Optical Flow Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_30.html">
      <font color="black">CoT-AMFlow: Adaptive Modulation Network with Co-Teaching Strategy for
  Unsupervised Optical Flow Estimation</font>
    </a>
  </h2>
  <font color="black">最近、教師なしオプティカルフロー推定が研究のホットスポットになっています。トレーニングパラダイムについては、2つのネットワークが同時に挑戦領域について互いに教え合い、精度をさらに向上させる共同ティーチング戦略を採用しています。オプティカルフロー情報を使用して、周囲の動きを推定します。 
[ABSTRACT]オプティカルフロー情報を使用して、周囲の動きを推定できます。ただし、教師なしアプローチは、部分的に遮蔽された領域やテクスチャの少ない領域では信頼できないことがよくあります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning Framework to Detect Face Masks from Video Footage -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_31.html">
      <font color="black">Deep Learning Framework to Detect Face Masks from Video Footage</font>
    </a>
  </h2>
  <font color="black">ビデオ映像での顔のマスクの検出は、主に、マスクされた領域に顔のランドマークがないため、マスク自体が顔検出アルゴリズムのオクルージョンとして動作するため、困難な作業です。これらの顔の画像と手がかりは、マスクされた領域を識別するためのオブジェクト検出器としてMobileNetV2アーキテクチャを利用するネオテリック分類器。提案されたフレームワークは、COVID-19安全プロトコルに準拠しながら公共スペースでの人々の動きをキャプチャするビデオのコレクションであるデータセットでテストされました。 
[概要]提案されたフレームワークは、mtcnn顔検出モデルを利用しています。これは、covid-19安全プロトコルに準拠しながら、公共スペースでの人々の動きをキャプチャするデータセットでテストされています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: FDRN: A Fast Deformable Registration Network for Medical Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_32.html">
      <font color="black">FDRN: A Fast Deformable Registration Network for Medical Images</font>
    </a>
  </h2>
  <font color="black">LPBA40脳MRIデータセットで既存の最先端の登録方法との比較を行いました。実験結果は、ダイススコアと正規化相互相関（NCC）において、FDRNが調査した方法よりも定性的および定量的に優れていることを示しています。特に、提案されたFDRNは、コンパクトなエンコーダ-デコーダ構造を備えており、深い監視、付加的な転送、および残差学習を活用します。 
[概要]提案されたfdrnは、コンパクトなエンコーダー-デコーダー構造を誇っています。他の解剖学的構造やct画像にも適用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Pixel-wise Dense Detector for Image Inpainting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_33.html">
      <font color="black">Pixel-wise Dense Detector for Image Inpainting</font>
    </a>
  </h2>
  <font color="black">複数の公開データセットでの実験は、提案されたフレームワークの優れたパフォーマンスを示しています。このような位置情報により、ジェネレータはアーティファクトに注意を払い、さらに強化します。さらに重要なことに、重み基準を使用して、検出器の出力を再構成損失に明示的に挿入します。 、手動操作ではなく、敵対的損失と再構築損失の重みを自動的にバランスさせます。 
[概要]ジェネレーターはエンコーダー-デコーダーアーキテクチャに従って欠落領域を埋めます。弱い教師あり学習を使用する検出器は、アーティファクトの位置をピクセル単位でローカライズします。このようなデータは、敵対的損失と再構築損失の重みのバランスを取るために必要です。手動ではなく自動</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Registration Loss Learning for Deep Probabilistic Point Set Registration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_34.html">
      <font color="black">Registration Loss Learning for Deep Probabilistic Point Set Registration</font>
    </a>
  </h2>
  <font color="black">3DMatchおよびKittiデータセットに対して広範な実験を実行します。これは、確率的登録が完全に微分可能であり、その結果が真にエンドツーエンドの学習フレームワークであるために可能です。実験は、私たちのアプローチが学習した機能と学習戦略の統合により、Kittiの最先端を上回ります。 
[概要]このツールは、登録損失学習戦略（rll）によって作成されました。これは、登録エラーを重みによって損失として直接使用します。結果はgithubで入手できます。 com</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: An artificial intelligence system for predicting the deterioration of
  COVID-19 patients in the emergency department -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_35.html">
      <font color="black">An artificial intelligence system for predicting the deterioration of
  COVID-19 patients in the emergency department</font>
    </a>
  </h2>
  <font color="black">3,661人の患者からのデータを使用してトレーニングされた当社のAI予測システムは、96時間以内の劣化を予測するときに、受信者動作特性曲線（AUC）の下の面積0.786（95％CI：0.745-0.830）を達成します。実際の臨床設定では、パンデミックの最初の波の間にニューヨーク大学ランゴーンヘルスに深部神経ネットワークの予備バージョンを静かに展開し、リアルタイムで正確な予測を生成しました。深部神経ネットワークは胸部Xの有益な領域を抽出します-臨床医が予測を解釈するのを支援し、読者研究で2人の放射線技師と同等のパフォーマンスを発揮する光線画像。 
[ABSTRACT]胸部X線画像から学習するディープニューラルネットワーク。システムは、臨床医が予測を解釈するのに役立つ有益な領域を抽出します。システムは、医療従事者が予測を解釈するのにも役立ちます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-04">
        <br><font color="black">2020-08-04</font>
      </time>
    </span>
</section>
<!-- paper0: Video Generative Adversarial Networks: A Review -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_36.html">
      <font color="black">Video Generative Adversarial Networks: A Review</font>
    </a>
  </h2>
  <font color="black">データセット、適用された損失関数、および評価メトリックの包括的なリストは、補足資料で提供されます。次に、ビデオGANモデルの包括的なレビューが、条件の有無に応じて2つの主要な部門で提供されます。次に、このペーパーでは、当初はビデオドメイン用に開発されていなかったが、複数のビデオGANバリエーションで採用されたGANフレームワークの主な改善点を要約しています。 
[概要]このペーパーは、現在のビデオガンモデルの主な課題と制限に焦点を当てて作成されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: SSGP: Sparse Spatial Guided Propagation for Robust and Generic
  Interpolation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_37.html">
      <font color="black">SSGP: Sparse Spatial Guided Propagation for Robust and Generic
  Interpolation</font>
    </a>
  </h2>
  <font color="black">これらのアイデアを拡張し、オプティカルフロー、シーンフロー、深度補完などの多数の補間問題に適用できる一般的なクロスドメインアーキテクチャを作成します。密なターゲット解像度に向けた疎なピクセル情報の補間は、複数の分野にわたるアプリケーションを見つけます。コンピュータービジョンで..私たちの実験では、提案されたスパース空間ガイド伝搬（SSGP）の概念が、特殊なアルゴリズムと比較して、堅牢性、精度、または速度の向上を実現することを示しています。 
[ABSTRACT]モーションフィールドの補間は、ターゲット画像から抽出されたエッジ情報を利用するモデルベースの補間を適用します。私たちの仕事は、高密度データの高密度ガイダンスの問題に取り組む深度補完の最新トレンドに触発されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-21">
        <br><font color="black">2020-08-21</font>
      </time>
    </span>
</section>
<!-- paper0: Handwriting Classification for the Analysis of Art-Historical Documents -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_38.html">
      <font color="black">Handwriting Classification for the Analysis of Art-Historical Documents</font>
    </a>
  </h2>
  <font color="black">デジタル化されたアーカイブには、何百万ものドキュメントに何世代にもわたる学者の知識が含まれ、保存されています。視覚的構造に基づいて、抽出されたテキストフラグメント（数字、日付、単語など）にラベルを付ける手書き分類モデルを提案します。この目的のために、テキスト分類のためのいくつかの深層学習ベースのモデルを開発して比較します。 
[要約]これらのアーカイブのサイズには自動分析が必要です。これには複数の言語で書かれたドキュメントが含まれ、認識モデルを作成するためのトレーニングデータが不足しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: An Improved Attention for Visual Question Answering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_39.html">
      <font color="black">An Improved Attention for Visual Question Answering</font>
    </a>
  </h2>
  <font color="black">アテンション結果とクエリの関係を決定できるエンコーダ-デコーダフレームワーク内にアテンションオンアテンション（AoA）モジュールを組み込みます。また、視覚情報とテキスト情報の両方を組み合わせるマルチモーダルフュージョンモジュールを提案します。 VQAを解決するための改善された注意ベースのアーキテクチャを提案します。 
[概要]エンコーダー-デコーダーフレームワーク内にアテンションオンアテンション（aoa）モジュールを組み込みます。アテンション結果とクエリの関係を判別できます。次に、2つを乗算することにより、最終的なアテンション情報を生成するために別のアテンションを追加します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Robust Super-Resolution of Real Faces using Smooth Features -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_40.html">
      <font color="black">Robust Super-Resolution of Real Faces using Smooth Features</font>
    </a>
  </h2>
  <font color="black">この論文では、特徴抽出モジュールがLR画像からロバストな特徴を抽出し、SRモジュールがこれらのロバストな特徴のみを使用してHR推定を生成する、2モジュールの超解像ネットワークを提案します。この補間されたLR画像は、超解像ネットワークを端から端までトレーニングするのは、対応するHRの対応物です。最近の研究のいくつかは、Generative Adversarial Network（GAN）を使用して、実際の画像のデータセットからこれらの劣化をモデル化しようとしています。 
[概要]バイキュービックにダウンサンプリングされたクリーンな画像を実際の劣化画像に変換する劣化ガンをトレーニングします。この方法は、広範囲のノイズ、ブラー、圧縮アーティファクト、およびノイズに対してロバストである必要があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: VEGA: Towards an End-to-End Configurable AutoML Pipeline -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_41.html">
      <font color="black">VEGA: Towards an End-to-End Configurable AutoML Pipeline</font>
    </a>
  </h2>
  <font color="black">c）ディープラーニングフレームワークの一般的なコンポーネントを統合インターフェースに抽象化します。VEGAはhttps://github.com/huawei-noah/vegaでオープンソース化されています。ただし、統合されたAutoMLシステムの設計には、構成可能性、スケーラビリティ、統合性、およびプラットフォームの多様性。 
[ABSTRACT] automlは、us.vegaで開発できるautomlシステムであり、自動データ拡張、モデル圧縮、完全トレーニングなど、automlの複数のモジュールで動作します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-03">
        <br><font color="black">2020-11-03</font>
      </time>
    </span>
</section>
<!-- paper0: Visual Localization Under Appearance Change: Filtering Approaches -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_42.html">
      <font color="black">Visual Localization Under Appearance Change: Filtering Approaches</font>
    </a>
  </h2>
  <font color="black">合成データセットと実際のデータセットの実験結果は、提案された方法が、大幅な外観変化の下での視覚的位置特定に関するタスクについて、最先端技術（つまり、深層学習ベースのポーズ回帰アプローチ）よりも優れた結果を達成することを示しています。テストシーケンスの時間的連続性を活用することで、視覚的なローカリゼーションが大幅に向上します。定性的および定量的です。私たちのアプローチは、画像を単一のベクトルとして表現するためのエンコード手法を使用したローカル機能に依存しています。 
[要約]ビデオをカメラにアップロードできますが、カメラでキャプチャできます。カメラカメラカメラキャプチャの結果として撮影できます。これは、カメラのビデオをキャプチャするために使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-11-20">
        <br><font color="black">2018-11-20</font>
      </time>
    </span>
</section>
<!-- paper0: DAIS: Automatic Channel Pruning via Differentiable Annealing Indicator
  Search -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_43.html">
      <font color="black">DAIS: Automatic Channel Pruning via Differentiable Annealing Indicator
  Search</font>
    </a>
  </h2>
  <font color="black">さらに、DAISは、剪定のスパース性を制御し、モデルのパフォーマンスを向上させるために、事前の構造知識に基づいてさまざまな正則化を設計します。畳み込みニューラルネットワークは、効率的な展開に対する大きな計算オーバーヘッドにもかかわらず、コンピュータービジョンタスクの実行に大きな成功を収めました。具体的には、DAISは緩和します。二値化されたチャネルインジケーターは連続的であり、2レベルの最適化を介してインジケーターとモデルパラメーターの両方を共同で学習します。 
[ABSTRACT]アニーリングインジケーター検索（dais）が新しい論文で作成されました。これは、連続モデルとターゲットの2値化モデルの間の無視できない不一致のバランスを取ることを目的としています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Weed Density and Distribution Estimation for Precision Agriculture using
  Semi-Supervised Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_44.html">
      <font color="black">Weed Density and Distribution Estimation for Precision Agriculture using
  Semi-Supervised Learning</font>
    </a>
  </h2>
  <font color="black">したがって、提案されたアプローチは、広範なラベル付きデータを必要とせずに、さまざまな植物種に一般化することが示されています。その後、雑草の感染領域は、微調整されたCNNを使用して識別され、手作りの機能を設計する必要がなくなります。この雑草密度と分布は、自律型ロボットを使用して感染地域を選択的に処理するためのサイト固有の雑草管理システムで役立ちます。 
[概要]除草剤を無制限に使用して雑草を除去すると、生物多様性が変化し、環境汚染を引き起こします。この雑草の密度と分布は、自律型ロボットを使用して感染地域を選択的に処理するための特定の雑草管理システムであるサイトで役立ちます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Muti-view Mouse Social Behaviour Recognition with Deep Graphical Model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_45.html">
      <font color="black">Muti-view Mouse Social Behaviour Recognition with Deep Graphical Model</font>
    </a>
  </h2>
  <font color="black">この問題に対処するために、ここでは、ビュー固有のビュー共有サブ構造を共同で学習する新しいマルチビュー潜在的注意および動的識別モデルを提案します。前者は各ビューの一意のダイナミクスをキャプチャし、後者はビュー間の相互作用をエンコードします。 ..標準CRMI13およびマルチビューパーキンソン病マウス行動（PDMB）データセットの実験結果は、モデルが他の最先端技術よりも優れており、不均衡なデータ問題に効果的に対処していることを示しています。マウスの社会的行動の説明、げっ歯類の観察のためのマルチビュービデオ録画の使用はますます多くの注目を集めています。 
[概要]シングルカメラのビデオ録画は主に分析に使用されますが、社会的行動の理解は依然として課題です。これは、データソース間の対応が不足しているためです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Semi-supervised Facial Action Unit Intensity Estimation with Contrastive
  Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_46.html">
      <font color="black">Semi-supervised Facial Action Unit Intensity Estimation with Contrastive
  Learning</font>
    </a>
  </h2>
  <font color="black">この目的のために、特徴抽出器と時間モジュールを組み合わせた時空間モデルを2段階で学習する半教師あり学習アプローチを提案します。わずか2ドルで作業する場合、この方法が既存の方法よりも優れていることを実験的に検証します。 DISFAとBP4Dの両方のデータセットに対してランダムに選択されたデータの％$は、ラベル付けされたフレームを慎重に選択せずに、以前のアプローチでは依然として時間のかかるタスクが必要です。私たちの知る限り、監視されていない方法。 
[概要]私たちの方法では、キーフレームを手動で選択する必要はありません。わずか2ドルで最先端の結果を生成します。既存の方法を半ば実行します。ラベル付きフレームを慎重に選択しなくても、時間のかかる作業です。以前のアプローチではまだ必要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-03">
        <br><font color="black">2020-11-03</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic identification of fossils and abiotic grains during carbonate
  microfacies analysis using deep convolutional neural networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_47.html">
      <font color="black">Automatic identification of fossils and abiotic grains during carbonate
  microfacies analysis using deep convolutional neural networks</font>
    </a>
  </h2>
  <font color="black">高性能ワークステーションを使用して、4つの古典的な深畳み込みニューラルネットワーク（DCNN）を実装しました。これは、過去数年にわたってコンピュータービジョンで非常に効率的であることが証明されています。ただし、次のような類似した形態のサンプルでは問題がありました。二枚貝、腕足動物、貝虫類にもかかわらず、0.88の精度が得られました。この要件は、堆積学者や古生物学者、特に初心者に特定の課題を引き起こします。 
[概要]この研究では、1、149の参照と独自の資料からの公開データを含むマイクロフェイシー画像データセットを収集しました。ツールは、人間の分類器と同等の再現性とバイアス回避で高精度を示しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-24">
        <br><font color="black">2020-09-24</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Multimodality Learning for UAV Video Aesthetic Quality Assessment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_48.html">
      <font color="black">Deep Multimodality Learning for UAV Video Aesthetic Quality Assessment</font>
    </a>
  </h2>
  <font color="black">実験結果は、私たちの方法がビデオ分類法およびビデオ美学のための従来のSVMベースの方法よりも優れていることを明らかにしています。ドローンカメラによってキャプチャされた6,000のUAVビデオショットでデータセットを構築します。無人航空機（UAV）の数が増えているにもかかわらず、空中ビデオでは、空中写真の美的品質を向上させるための貴重な情報を提供できる空中ビデオの美学に焦点を当てた研究が不足しています。 
[概要]この記事では、UAVビデオの美的品質評価のためのディープマルチモダリティ学習の方法を紹介します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Few-Shot Font Generation with Deep Metric Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_49.html">
      <font color="black">Few-Shot Font Generation with Deep Metric Learning</font>
    </a>
  </h2>
  <font color="black">ここでは、より良いスタイルの特徴を抽出するためのシンプルで強力なフレームワークを提案しました。このフレームワークは、スタイルエンコーダに深いメトリック学習を導入します。白黒および形状が特徴的なフォントデータセットを使用して実験を行い、提案されたフレームワークの有効性を示しました。 。 
[ABSTRACT]日本語フォントは、少数のフォントサンプルから自動的に生成されます。これらのグリフは、スケルトン、輪郭、セリフなどの一貫した特性を持つことが期待されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Crack Detection as a Weakly-Supervised Problem: Towards Achieving Less
  Annotation-Intensive Crack Detectors -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_50.html">
      <font color="black">Crack Detection as a Weakly-Supervised Problem: Towards Achieving Less
  Annotation-Intensive Crack Detectors</font>
    </a>
  </h2>
  <font color="black">これにより、実際の亀裂検出システムを展開する際に、データ注釈のコストが大幅に低下します。低品質の注釈でトレーニングされた監視対象モデルの予測とピクセルの明るさに基づく予測を組み合わせることで、フレームワークは注釈の品質による影響を受けにくくなります。実験的結果は、提案されたフレームワークが低品質の注釈が提供された場合でも高い検出精度を維持することを示しています。 
[概要]重要な分野での最近の研究により、検出精度が大幅に向上しました。しかし、最近の研究では、提案されたフレームワークが高い検出精度を維持していることが示されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: A Bayesian Inference Framework for Procedural Material Parameter
  Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_51.html">
      <font color="black">A Bayesian Inference Framework for Procedural Material Parameter
  Estimation</font>
    </a>
  </h2>
  <font color="black">最適化によるパラメーターの点推定の計算に加えて、私たちのフレームワークは、マルコフ連鎖モンテカルロアプローチを使用して、もっともらしい材料パラメーターの空間をサンプリングし、ユーザーが選択できるもっともらしい一致のコレクションを提供し、離散および連続の両方を効率的に処理しますモデルパラメータ..フレームワークの有効性を実証するために、さまざまな材料の手続き型モデル（壁石膏、革、木材、異方性ブラシ金属、層状メタリックペイント）を合成および実際のターゲット画像の両方に適合させます。手続き型材料モデルは、その柔軟性、コンパクトさ、および簡単な編集性のおかげで、多くのアプリケーションで注目を集めています。 
[概要]写真からの手続き情報の逆レンダリング問題を調査し、問題の統一されたビューを提示します。コレクションの有効性を示すために、壁のしっくい、革、木、異方性など、さまざまな材料のモデルを適合させます。つや消しメタルとレイヤードメタリックペイント</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-02">
        <br><font color="black">2019-12-02</font>
      </time>
    </span>
</section>
<!-- paper0: Realtime CNN-based Keypoint Detector with Sobel Filter and CNN-based
  Descriptor Trained with Keypoint Candidates -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_52.html">
      <font color="black">Realtime CNN-based Keypoint Detector with Sobel Filter and CNN-based
  Descriptor Trained with Keypoint Candidates</font>
    </a>
  </h2>
  <font color="black">コーナーポイントをキーポイントとして検出するために、SobelNetのトレーニングプロセスのガウス損失を設計します。キーポイントの位置は、CNNの出力マップで非最大抑制（NMS）プロセスを実行した後に取得されます。検出器と記述子は並行して機能します。 
[ABSTRACT] sobelnetとdesnetはキーポイントを検出し、高密度のローカル記述子を計算できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Affine invariant triangulations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CV/paper_53.html">
      <font color="black">Affine invariant triangulations</font>
    </a>
  </h2>
  <font color="black">Springer、1993]は、$ S $の共分散行列の逆行列を使用して、$ A_ {S} $で表されるアフィン不変ノルムと、$ {DT} _ {A_ {S}} 
[で表されるアフィン不変三角測量を定義します。 S] $ ..さらに、ポイントセット$ S $とポリゴン$ P $の頂点のさまざまなアフィン不変ソート方法を提供します。これらは、既知のアルゴリズムと組み合わせて、$ S $の他のアフィン不変三角測量法を取得できます。と$ P $ ..私たちの仕事は、ニールソンによる方法
[アフィン不変三角測量の特性化]に基づいています。 
[概要]さらに、ポイントセット$ sのさまざまなafブランケットソート方法を提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Extracting Chemical-Protein Interactions via Calibrated Deep Neural
  Network and Self-training -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_0.html">
      <font color="black">Extracting Chemical-Protein Interactions via Calibrated Deep Neural
  Network and Self-training</font>
    </a>
  </h2>
  <font color="black">いくつかの生物医学記事から化学物質とタンパク質の間の相互作用を抽出することは、医薬品開発や医薬品の副作用の予測など、生物医学研究の多くの分野で重要です。私たちのモデルは、最初に、事前に訓練された言語理解モデルを使用して入力シーケンスをエンコードします。これは、混合トレーニングと信頼ペナルティ損失の追加という2つのキャリブレーション方法を使用してトレーニングされます。最後に、推定された不確実性を使用して抽出された拡張データを使用してモデルが再トレーニングされます。 
[ABSTRACT]ディープニューラルネットワーク（dnn）モデルは、この問題に対処するために適用されています。データの不確実性を予測し、これらのモデルの信頼性を向上させるために、「キャリブレーション」メソッドが使用されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Mucko: Multi-Layer Cross-Modal Knowledge Reasoning for Fact-based Visual
  Question Answering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_1.html">
      <font color="black">Mucko: Multi-Layer Cross-Modal Knowledge Reasoning for Fact-based Visual
  Question Answering</font>
    </a>
  </h2>
  <font color="black">多層グラフ表現に加えて、モダリティ対応の異種グラフ畳み込みネットワークを提案して、特定の質問に最も関連するさまざまな層からの証拠をキャプチャします。このプロセスを複数回スタックすることにより、モデルは反復推論を実行し、予測します。すべての質問指向の証拠を分析することによる最適な回答。事実に基づく視覚的質問回答（FVQA）は、画像に関する質問に回答するために、目に見えるコンテンツを超えた外部知識を必要とします。これは、一般的なVQAを達成するために困難ですが、不可欠です。 
[概要]視覚的、意味的、事実的な機能にはfvqaソリューションが必要です。新しいモデルは、きめ細かい選択なしですべてのタイプの情報を組み合わせます。これにより、fvqaタスクでの最新のパフォーマンスが作成されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-16">
        <br><font color="black">2020-06-16</font>
      </time>
    </span>
</section>
<!-- paper0: Indic-Transformers: An Analysis of Transformer Language Models for
  Indian Languages -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_2.html">
      <font color="black">Indic-Transformers: An Analysis of Transformer Language Models for
  Indian Languages</font>
    </a>
  </h2>
  <font color="black">最後に、インドの言語のモデリングを処理するための効果的な戦略を提示し、コミュニティのモデルチェックポイントをリリースします：https：//huggingface.co/neuralspace-reverie ..一方、インドの言語は、そのようなベンチマークでは過小評価されています。 。インドの言語でのパフォーマンスを具体的に評価するために、ヒンディー語、ベンガリ語、テルグ語での複数のダウンストリームタスクに関する広範な実験を通じてこれらの言語モデルを分析します。 
[概要]いくつかのインドの言語は多言語トランスフォーマーモデルのトレーニングに含まれていますが、それらはそのような作業の主な焦点ではありませんでした</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: A BERT-based Dual Embedding Model for Chinese Idiom Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_3.html">
      <font color="black">A BERT-based Dual Embedding Model for Chinese Idiom Prediction</font>
    </a>
  </h2>
  <font color="black">中国のイディオムは、通常、古代の物語から派生した特別な固定フレーズであり、その意味はしばしば非常に慣用的で非構成的です。次に、各候補イディオムの埋め込みを、コンテキストプーリングによるコンテキスト内のすべてのトークンの非表示表現と照合します。 、最初に、各候補イディオムの埋め込みを、コンテキスト内の空白に対応する非表示の表現と照合します。 
[概要]中国のイディオム予測タスクは、空白のコンテキストが与えられた候補のセットから正しいイディオムを選択することです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: TransQuest: Translation Quality Estimation with Cross-lingual
  Transformers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_4.html">
      <font color="black">TransQuest: Translation Quality Estimation with Cross-lingual
  Transformers</font>
    </a>
  </h2>
  <font color="black">私たちの評価は、提案された方法が、WMTからのデータセットでトレーニングされたときに、現在のオープンソース品質推定フレームワークを上回る最先端の結果を達成することを示しています。さらに、フレームワークは、特に低-リソース言語、非常に競争力のある結果を得ることができます。この論文では、クロスリンガルトランスフォーマーに基づく単純なQEフレームワークを提案し、それを使用して2つの異なるニューラルアーキテクチャを実装および評価します。 
[概要]これらのメソッドの大部分は、トレーニングを受けた言語ペアでのみ機能し、新しい言語ペアの再トレーニングが必要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-01">
        <br><font color="black">2020-11-01</font>
      </time>
    </span>
</section>
<!-- paper0: "Thy algorithm shalt not bear false witness": An Evaluation of
  Multiclass Debiasing Methods on Word Embeddings -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_5.html">
      <font color="black">"Thy algorithm shalt not bear false witness": An Evaluation of
  Multiclass Debiasing Methods on Word Embeddings</font>
    </a>
  </h2>
  <font color="black">人工知能アプリケーションの広大な開発と採用により、これらのアルゴリズムの公平性に関する研究が増加しました。具体的には、この手法は、測定された宗教的バイアスを平均82,42％、96,78％、54,76減少させることができます。それぞれ3つの単語埋め込みセットの％..広く使用されている3つの単語埋め込み、つまりWord2Vec、GloVe、ConceptNetの宗教的バイアスの除去を調査することにより、推奨される方法はConceptorDebiasingであることが示されています。 
[要約]調査によると、単語の埋め込みには社会的バイアスが残っています。これには、ハードバイアス除去、ソフトウェットバイアス除去、および概念的バイアス除去が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-30">
        <br><font color="black">2020-10-30</font>
      </time>
    </span>
</section>
<!-- paper0: An Empirical Study of Contextual Data Augmentation for Japanese Zero
  Anaphora Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_6.html">
      <font color="black">An Empirical Study of Contextual Data Augmentation for Japanese Zero
  Anaphora Resolution</font>
    </a>
  </h2>
  <font color="black">CDAは、テキスト分類や機械翻訳など、他のいくつかの自然言語処理タスクでうまく機能することが報告されています。コンテキストデータ拡張（CDA）と呼ばれる、ラベル付きのデータを生成する最先端のデータ拡張方法を採用しています。事前にトレーニングされた言語モデルを使用してインスタンスをトレーニングします。この調査では、CDAに関する2つの未踏の問題、つまり、データ拡張の計算コストを削減する方法と、生成されたデータの品質を確保する方法について説明します。 
[要約]提案された方法は、従来のcdaと比較して、拡張されたトレーニングデータの品質を向上させることができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-02">
        <br><font color="black">2020-11-02</font>
      </time>
    </span>
</section>
<!-- paper0: Data Augmentation for End-to-end Code-switching Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_7.html">
      <font color="black">Data Augmentation for End-to-end Code-switching Speech Recognition</font>
    </a>
  </h2>
  <font color="black">さらに、提案されたすべてのアプローチを最近人気のあるSpecAugmentと組み合わせることができ、追加のゲインを得ることができます。コードスイッチングエンドツーエンド自動音声認識（ASR）モデルのトレーニングには通常、大量のデータが必要ですが、コード-スイッチングデータは制限されることがよくあります。具体的には、既存のコードスイッチングデータとのオーディオスプライシング、および単語翻訳または単語挿入によって生成された新しいコードスイッチングテキストとのTTSです。 
[概要]コードスイッチングデータ拡張のために3つの新しいアプローチが提案されています。3つの提案されたアプローチはすべて、コードスイッチングasrを個別に大幅に改善します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Optimizing Transformer for Low-Resource Neural Machine Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_8.html">
      <font color="black">Optimizing Transformer for Low-Resource Neural Machine Translation</font>
    </a>
  </h2>
  <font color="black">IWSLT14トレーニングデータのさまざまなサブセットでの実験では、低リソース条件下でのTransformerの有効性は、ハイパーパラメータ設定に大きく依存していることが示されています。一方、Transformerモデルは多くの言語ペアで大幅な改善を達成し、事実上のものになっています。主流のアーキテクチャ、低リソース条件下でのその機能はまだ完全には調査されていません。私たちの実験では、低リソース条件に最適化されたTransformerを使用すると、Transformerのデフォルト設定を使用する場合と比較して最大7.3BLEUポイントの変換品質が向上することが示されています。 
[概要]トランスフォーマーモデルは、多くの言語ペアで大幅な改善を達成しましたが、低リソース条件下でのその機能はまだ十分に調査されていません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: PCP Theorems, SETH and More: Towards Proving Sub-linear Time
  Inapproximability -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_9.html">
      <font color="black">PCP Theorems, SETH and More: Towards Proving Sub-linear Time
  Inapproximability</font>
    </a>
  </h2>
  <font color="black">サブリニア時間アルゴリズムに関する新たな研究作業を考慮すると、サブリニアPCP定理は、サブリニア時間近似アルゴリズムの研究を導く上で重要です。サブ二次時間の近似不可能性のための分散PCPフレームワークを考案しました。サブリニア時間PCPを証明するための\ cite {Abboud2017}の証明手法と、それを使用して既存と新規の両方の近似不可能な結果を証明する方法を示します。 
[要約]これらの結果は、劣線形pcpの力を示しています。劣線形性の劣性性は解決されていません。結果は、劣性性が機能しないことを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic Identification of Types of Alterations in Historical
  Manuscripts -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_10.html">
      <font color="black">Automatic Identification of Types of Alterations in Historical
  Manuscripts</font>
    </a>
  </h2>
  <font color="black">文字などの歴史的写本の改変は、有望な研究分野を表しています。ここで提案する方法は、デジタル学術版ベルリン知識人で実施された実験に基づいて開発されており、alterLDAはラベル付きデータの改変の認識において高いパフォーマンスを実現します。一方では、それらはテキストの構成を理解するのに役立ちます。 
[要約]ラベルのないデータについて、alterldaを適用すると、著者、編集者、その他の原稿寄稿者の行動の変更に関する興味深い新しい洞察が得られます。これについて、そのような結果を達成するために従うべき方法論の手順を詳しく説明します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-20">
        <br><font color="black">2020-03-20</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Sparse Prototypes for Text Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_11.html">
      <font color="black">Learning Sparse Prototypes for Text Generation</font>
    </a>
  </h2>
  <font color="black">これは、（1）プロトタイプ選択分布にスパース性を誘発する事前確率を課し、（2）償却された変分推論を利用して、プロトタイプ検索関数を学習することによって実現されます。プロトタイプ駆動型テキスト生成では、最初にから選択する非パラメトリックモデルを使用します。文「プロトタイプ」のライブラリを作成し、プロトタイプを変更して出力テキストを生成します。さらに興味深いことに、プロトタイプ選択のスパース性を変化させると、学習したプロトタイプがさまざまな粒度でセマンティクスと構文をキャプチャできることを示します。属性は、生成するプロトタイプを指定することで制御できます。 
[概要]以前の言語モデルでは、テキストコーパス全体を保存してインデックスを作成できましたが、これらのメソッドは、キャプチャしてインデックスを作成する必要があるため、テスト時には非効率的です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-29">
        <br><font color="black">2020-06-29</font>
      </time>
    </span>
</section>
<!-- paper0: Chinese Grammatical Correction Using BERT-based Pre-trained Model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_12.html">
      <font color="black">Chinese Grammatical Correction Using BERT-based Pre-trained Model</font>
    </a>
  </h2>
  <font color="black">（2020）中国語の文法エラー訂正タスクのエンコーダー-デコーダーモデルに..近年、事前にトレーニングされたモデルが広く研究され、いくつかのダウンストリームタスクがそれらの利用から恩恵を受けています。エラータイプを分析し、次のように結論付けます。文レベルのエラーはまだ解決されていません。 
[要約]この研究では、bertベースの事前トレーニング済みモデルを組み込んだ2つの方法の有効性を検証します。また、エラータイプを分析し、文レベルのエラーはまだ対処されていないと結論付けます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Investigating Novel Verb Learning in BERT: Selectional Preference
  Classes and Alternation-Based Syntactic Generalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_13.html">
      <font color="black">Investigating Novel Verb Learning in BERT: Selectional Preference
  Classes and Alternation-Based Syntactic Generalization</font>
    </a>
  </h2>
  <font color="black">後者の場合、言語オブジェクトの不完全な選択ネットワークでBERTを微調整し、証明されていないがもっともらしい動詞/オブジェクトのペアを期待するかどうかを尋ねます。深層学習モデルの構文能力を調査した以前の研究では、文法の一般化と、トレーニング中にモデルがさらされる証拠の量。言語交代テストの場合、モデルは遷移性バイアスと一致する動作を示すことがわかります。数回見られる動詞は直接目的語を取ると予想されます。ただし、直接目的語で見られる動詞は、一時的に発生することは想定されていません。 
[要約]モデルは、新しい単語学習ツールを使用して、バートの数ショットの学習機能をテストします。口頭テストでは、モデルが推移性バイアスと一致する動作を示すことがわかります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Program Enhanced Fact Verification with Verbalization and Graph
  Attention Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_14.html">
      <font color="black">Program Enhanced Fact Verification with Verbalization and Graph
  Attention Network</font>
    </a>
  </h2>
  <font color="black">上記のフレームワークをサポートするために、マージン損失に基づく新しいトレーニング戦略で最適化されたプログラム選択モジュールを提案し、より正確なプログラムを生成します。これは、最終的な検証結果の向上に効果的であることが示されています。実験結果は、提案されたフレームワークを示しています。ベンチマークデータセットTABFACTで、74.4％の精度という新しい最先端のパフォーマンスを実現します。その上に、言語化されたプログラム実行からのさまざまな証拠ソースを融合するように設計されたグラフ注意検証ネットワークを構築します。最終的な検証の決定を行うためのプログラム構造、および元のステートメントとテーブル。 
[要約]提案されたシステムは、さまざまな証拠のソースを融合するように設計されています。これには、プログラムが含まれます-強化された言語化とグラフ注意ネットワーク（progvgat）。提案されたフレームワークは、ベンチマークデータセットタブファクトで74％の精度を達成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-06">
        <br><font color="black">2020-10-06</font>
      </time>
    </span>
</section>
<!-- paper0: Hybrid Supervised Reinforced Model for Dialogue Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_15.html">
      <font color="black">Hybrid Supervised Reinforced Model for Dialogue Systems</font>
    </a>
  </h2>
  <font color="black">このモデルは、非反復ベースラインよりも優れたパフォーマンス、学習速度、および堅牢性を実現します。これは、人間と機械の相互作用を、相互作用コンテキストを埋め込んだ潜在表現にモデル化して、ディスカッションをガイドすることに基づいています。このペーパーでは、反復ハイブリッドモデルとトレーニングについて説明します。 Deep Recurrent Q-Networks（DRQN）に基づくタスク指向の対話システムの手順。 
[要約]モデルは、対話管理に必要な両方のタスクに対応します。これは、非再発ベースラインからのデータに基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Probing Multilingual BERT for Genetic and Typological Signals -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_16.html">
      <font color="black">Probing Multilingual BERT for Genetic and Typological Signals</font>
    </a>
  </h2>
  <font color="black">1）言語距離を使用して言語ツリーを推測および評価し、カルテットツリー距離の点で参照ファミリーツリーに近いことを確認します。2）距離行列回帰分析を実行し、言語距離が系統発生によって最もよく説明できることを確認します。構造的要因によって最悪であり、3）言語学的アプローチに基づいて公開されたランク付けされたリストと有意に相関する通時的意味の安定性（言語間表現の変動性に基づく）を測定するための新しい尺度を提示します。私たちの結果は、クロスリンガルテキスト表現..多言語BERT（mBERT）のレイヤーを調べて、100の言語にわたる系統的および地理的言語信号を調べ、mBERT表現に基づいて言語距離を計算します。 
[概要]言語ツリーを推測および評価するために言語距離を使用します。カルテットツリー距離の点で、参照ファミリーツリーに近いことがわかります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Transformer based Data Augmentation with Subword Units for
  Morphologically Rich Online ASR -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_17.html">
      <font color="black">Deep Transformer based Data Augmentation with Subword Units for
  Morphologically Rich Online ASR</font>
    </a>
  </h2>
  <font color="black">Transformerで生成されたテキストによるデータ拡張は言語の分離にはうまく機能しますが、形態学的に豊富な言語で語彙の爆発的な増加を引き起こすことを示します。語彙のサイズとメモリ要件..したがって、サブワードベースのニューラルテキスト拡張と呼ばれる新しい方法を提案します。この方法では、生成されたテキストを統計的に導出されたサブワードに再トークン化します。 
[ABSTRACT]サブワードベースのニューラルテキスト拡張は新しい方法です。これらには、生成されたテキストを統計的に導出されたサブワードに再トークン化することが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Diversity Aware Relevance Learning for Argument Search -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_18.html">
      <font color="black">Diversity Aware Relevance Learning for Argument Search</font>
    </a>
  </h2>
  <font color="black">私たちの経験的評価は、必要なデータが少なくても、私たちのアプローチが引数検索タスクの大幅な改善につながることを示しています。最先端の方法は、クレームと前提の間の明示的なマッピングに依存しているため、利用可能な大規模なものを利用できません。面倒で費用のかかる手動注釈のない施設のコレクション。それらの多様性アプローチは、選択された施設がすべての側面をカバーすることを直接保証しないクラスタリングを介して重複を削除することに依存しています。 
[ABSTRACT]新しい作業により、引数検索の問題に対する新しいマルチステップアプローチが導入されました。これには、クレームと前提の間の明示的なマッピングに依存することが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Prosodic Representation Learning and Contextual Sampling for Neural
  Text-to-Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_19.html">
      <font color="black">Prosodic Representation Learning and Contextual Sampling for Neural
  Text-to-Speech</font>
    </a>
  </h2>
  <font color="black">記録と比較した場合、強いベースラインよりも自然度が$ 13.2 \％$の統計的に有意な相対的改善を示しています。また、サンプリング手法のバリエーションについてアブレーション研究を実施し、いずれの場合もベースラインに対して統計的に有意な改善を示しています。 。ステージIでは、トレーニング中に利用可能なメルスペクトログラムから文レベルでの韻律分布を学習します。 
[概要]ステージiでは、melから韻律分布を学習します-トレーニング中に利用可能なスペクトログラム。これを行うには、テキストにbertを使用し、テキストから抽出されたpardicツリーにグラフ-注意ネットワークを使用します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: An Improved Attention for Visual Question Answering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_20.html">
      <font color="black">An Improved Attention for Visual Question Answering</font>
    </a>
  </h2>
  <font color="black">モーダル内およびモーダル間の依存関係をキャプチャする注意は、これらの課題に対処するためにおそらく最も広く使用されているメカニズムとして浮上しています。VQA-v2ベンチマークデータセットでの広範な実験は、私たちの方法が最先端のパフォーマンスを達成することを示しています。 。また、視覚情報とテキスト情報の両方を組み合わせるマルチモーダル融合モジュールを提案します。 
[概要]エンコーダー-デコーダーフレームワーク内にアテンションオンアテンション（aoa）モジュールを組み込みます。アテンション結果とクエリの関係を判別できます。次に、2つを乗算することにより、最終的なアテンション情報を生成するために別のアテンションを追加します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Collaborative Training of GANs in Continuous and Discrete Spaces for
  Text Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_21.html">
      <font color="black">Collaborative Training of GANs in Continuous and Discrete Spaces for
  Text Generation</font>
    </a>
  </h2>
  <font color="black">私たちの方法では、オートエンコーダーを使用して暗黙のデータ多様体を学習し、連続空間での敵対的訓練の学習目標を提供します。2つの敵対的訓練間の協調的相互作用により、異なる空間でのテキスト表現が効果的に正規化されます。1行の研究でこの問題が解決されます。強化学習（RL）を採用し、離散アクション空間で直接次の単語のサンプリングポリシーを最適化する。 
[要約]この問題を解決するために一連の研究が使用されています。これには、介入アクションスペースで直接次の単語サンプリングポリシーを最適化することが含まれます。このような方法は、微分不可能な個別プロセスを回避するために使用されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-16">
        <br><font color="black">2020-10-16</font>
      </time>
    </span>
</section>
<!-- paper0: Cross-Lingual Machine Speech Chain for Javanese, Sundanese, Balinese,
  and Bataks Speech Recognition and Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_22.html">
      <font color="black">Cross-Lingual Machine Speech Chain for Javanese, Sundanese, Balinese,
  and Bataks Speech Recognition and Synthesis</font>
    </a>
  </h2>
  <font color="black">次に、インドネシア語のASRとTTSを、テキストのみまたは音声データのみのクロスリンガルマシン音声チェーンフレームワークで利用して、民族言語のASRとTTSを開発し、これらの民族言語の音声テキストデータのペアの必要性を排除します。私たちは、これらのインドネシア語の音声認識と合成の開発に焦点を当てています：Javanese、Sundanese、Balinese、およびBataks ..最初に、監視付きトレーニングで標準インドネシア語のASRとTTSを別々にトレーニングします。 
[ABSTRACT] s2stシステムには、教師ありトレーニングに大きく依存する機械翻訳（mt）、音声認識（asr）、合成（tts）が必要です。フレームワークは当初、民族言語内でのみ実装されていました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Conversation Graph: Data Augmentation, Training and Evaluation for
  Non-Deterministic Dialogue Management -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_23.html">
      <font color="black">Conversation Graph: Data Augmentation, Training and Evaluation for
  Non-Deterministic Dialogue Management</font>
    </a>
  </h2>
  <font color="black">会話グラフ（ConvGraph）を提案します。これは、データの拡張、マルチリファレンストレーニング、および非決定論的エージェントの評価に利用できるダイアログのグラフベースの表現です。さらに、従来のトレーニング信号推論は非決定論的には適していません。エージェントの振る舞い、すなわち。 3つのデータセットにわたる内在的および外的評価は、ConvGraphを使用したデータ拡張および/またはマルチリファレンストレーニングにより、対話の成功率を最大6.4％向上させることができることを示しています。 
[要約]データセットのサイズは制限されていることがよくあります。これらの測定にはディスカッションアクションが必要です。これらには、同一のダイアログ状態で有効な複数のアクションが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-29">
        <br><font color="black">2020-10-29</font>
      </time>
    </span>
</section>
<!-- paper0: Experiencers, Stimuli, or Targets: Which Semantic Roles Enable Machine
  Learning to Infer the Emotions? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_24.html">
      <font color="black">Experiencers, Stimuli, or Targets: Which Semantic Roles Enable Machine
  Learning to Infer the Emotions?</font>
    </a>
  </h2>
  <font color="black">（ターゲット）..（経験者）、「この感情の原因は何ですか？」これらの質問に答えるには、テキスト内のこれらの役割のフィラーを制御された方法でマスクすることにより、少なくとも1つの意味的役割で注釈が付けられた5つの利用可能なデータセットで感情分類モデルをトレーニングし、複数のコーパス、刺激、およびターゲットにわたって感情情報を伝達することがわかります。経験者は交絡因子と見なされる可能性があります。 
[要約]役割の位置について考えることで、分類の決定を改善できます。これらのgの意味的役割のどれが、分類子が感情を推測できるかは不明です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-03">
        <br><font color="black">2020-11-03</font>
      </time>
    </span>
</section>
<!-- paper0: Incremental Machine Speech Chain Towards Enabling Listening while
  Speaking in Real-time -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_25.html">
      <font color="black">Incremental Machine Speech Chain Towards Enabling Listening while
  Speaking in Real-time</font>
    </a>
  </h2>
  <font color="black">この作業では、機械がリアルタイムで話しているときに聞くことができるようにするための増分機械音声チェーンを提案します。したがって、長い発話に遭遇すると大幅な遅延が発生します。実験結果は、提案されたフレームワークが遅延を減らすことができることを示しています。非インクリメンタルな基本的な機械の音声チェーンに匹敵するパフォーマンスを維持しながら、長い発話のため。 
[概要]リアルタイムで、人間はちょっと話していることをリアルタイムで聞くことができます。聞き取りに遅れがあると、話し続けることができなくなります。インクリメンタルasrとインクリメンタルniances（itts）を構築します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: PheMT: A Phenomenon-wise Dataset for Machine Translation Robustness on
  User-Generated Contents -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_26.html">
      <font color="black">PheMT: A Phenomenon-wise Dataset for Machine Translation Robustness on
  User-Generated Contents</font>
    </a>
  </h2>
  <font color="black">異文化コミュニケーションにNMTをより有効に活用するために、最も有望な方向性の1つは、これらの表現を正しく処理するモデルを開発することです。ただし、既存の研究では、NMTは、次のようなかなりのノイズを伴う特定の種類の入力に依然として苦労していることが示唆されています。インターネット上のユーザー生成コンテンツ（UGC）..ニューラルマシン翻訳（NMT）は、ニュースドメインからのテキストなどのクリーンな入力を翻訳するときに、その品質が大幅に向上することを示しています。 
[概要]既存の調査によると、mi5は、インターネット上のユーザー生成コンテンツ（ugc）など、かなりのノイズを伴うクリーンな入力に依然として苦労しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Revisiting Low Resource Status of Indian Languages in Machine
  Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_27.html">
      <font color="black">Revisiting Low Resource Status of Indian Languages in Machine
  Translation</font>
    </a>
  </h2>
  <font color="black">私たちの仕事を通じて、ピボット言語の選択やコーパスサイズの反復的な増分増加の効果などの設計の選択も評価します。自動化されたフレームワークを提供することに加えて、私たちの仕事はまた、既存のものと比較して比較的大きなコーパスを生成しますインドの言語で利用できるコーパス..私たちのパイプラインは、ベースラインNMTシステム、検索モジュール、および政府によるプレスリリースなどの公開されているWebサイトでの作業に使用される調整モジュールで構成されています。 
[概要]この取り組みへの主な貢献は、上記のパイプラインを使用してコーパスのサイズを改善するインクリメンタルメソッドを取得することです。また、インドの言語で利用可能な既存のコーパスと比較して、比較的大きなコーパスを生成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Augmenting Images for ASR and TTS through Single-loop and Dual-loop
  Multimodal Chain Framework -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_28.html">
      <font color="black">Augmenting Images for ASR and TTS through Single-loop and Dual-loop
  Multimodal Chain Framework</font>
    </a>
  </h2>
  <font color="black">ただし、そのフレームワークには依然として大量のペアリングされていない（音声またはテキスト）データが必要です。次に、プロトタイプのマルチモーダルマシンチェーンを調査して、ペアリングされていない大量のデータの必要性をさらに減らし、ASRまたはTTSを改善できるようにしました。音声またはテキストデータが利用可能でした。さらに、このフレームワークのパフォーマンスは、単一スピーカーの人工音声データでのみ調査されました。 
[ABSTRACT]研究者はマルチモーダル学習学習システムを刷新しました。このフレームワークは画像検索（ir）モデルに依存していたため、トレーニング中にすでにわかっているより多くの画像の処理に限定されていました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Sequence-to-Sequence Learning via Attention Transfer for Incremental
  Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_29.html">
      <font color="black">Sequence-to-Sequence Learning via Attention Transfer for Incremental
  Speech Recognition</font>
    </a>
  </h2>
  <font color="black">私たちの実験では、認識プロセスの開始時間を約1.7秒遅らせることで、終了まで待つ必要があるパフォーマンスと同等のパフォーマンスを達成できることが示されています。より薄いモデルやより浅いモデルを使用する代わりに、代替の学生ネットワークを設計します。は、教師モデルの元のアーキテクチャを維持しますが、シーケンスは短くなります（エンコーダとデコーダの状態はほとんどありません）。注意の転送を使用して、学生ネットワークは、現在の入力短い音声セグメントと文字起こしの間の同じ配置を模倣することを学習します。 
[概要]これは、インクリメンタル音声認識（isr）の理解が不足しているためです。isrの元の注意アーキテクチャに基づくasrを使用することは可能です。代わりに、学生は現在の入力ショート間の同じ配置を模倣することを学びます。音声セグメント</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Neural text normalization leveraging similarities of strings and sounds -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/cs.CL/paper_30.html">
      <font color="black">Neural text normalization leveraging similarities of strings and sounds</font>
    </a>
  </h2>
  <font color="black">結果は、単語文字列の類似性を活用することでスペルミスや略語を処理し、音の類似性を考慮して音声置換や強調文字を処理することに成功したことを示しました。単語文字列と音の両方の類似性を考慮したモデルを実験的に比較しました。単語の文字列または音の類似性のみを考慮したモデル、および類似性のないモデルをベースラインとして..提案されたモデルがベースラインよりも高いF $ _1 $スコアを達成したように。 
[概要]単語と音の類似性を考慮したモデルを実験的に比較しました。結果は、提案されたモデルがベースラインよりも高いf $ 1 $スコアを達成したことを示しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: IEEE SLT 2021 Alpha-mini Speech Challenge: Open Datasets, Tracks, Rules
  and Baselines -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.AS/paper_0.html">
      <font color="black">IEEE SLT 2021 Alpha-mini Speech Challenge: Open Datasets, Tracks, Rules
  and Baselines</font>
    </a>
  </h2>
  <font color="black">IEEE Spoken Language Technology Workshop（SLT）2021 Alpha-mini Speech Challenge（ASC）は、ヒューマノイドロボットのキーワードスポッティング（KWS）と音源位置（SSL）の研究を改善することを目的としています。多くの出版物は、深層学習に基づく大幅な改善を報告しています。近年のオープンソースデータセットでのKWSとSSL。ただし、このアプローチでは、実際のアプリケーションシナリオ、特にエコーデータで、シミュレートされたデータと記録されたデータの間に不一致が生じる可能性があります。 
[概要]多くの出版物が、近年のオープンソースデータセットでのディープラーニングベースのkwsとsslの大幅な改善を報告しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Single channel voice separation for unknown number of speakers under
  reverberant and noisy settings -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.AS/paper_1.html">
      <font color="black">Single channel voice separation for unknown number of speakers under
  reverberant and noisy settings</font>
    </a>
  </h2>
  <font color="black">提案されたアプローチは、話者分類ブランチとともに最適化されたいくつかの分離ヘッドで構成されています。未知の数の話者の音声分離のための統一されたネットワークを提示します。分離は、すべての間のパラメータ共有とともに、時間領域で実行されます。分離ヘッド。 
[概要]提案されたアプローチは、話者分類ブランチを備えた分離されたヘッドで構成されています。システムは話者の数を推定し、各ヘッドは異なる数の分離に特化しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Frustratingly Easy Noise-aware Training of Acoustic Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.AS/paper_2.html">
      <font color="black">Frustratingly Easy Noise-aware Training of Acoustic Models</font>
    </a>
  </h2>
  <font color="black">文献で提案されているいくつかの埋め込みベースの適応ベースラインを実装し、私たちの方法が両方のデータセットでそれらよりも優れていることを示します。AMIとAurora-4での実験を通じて、この単純な適応手法が6〜7％の相対WER改善をもたらす可能性があることを示します。 ..この論文では、ハイブリッドASRにおける音響モデルのノイズ認識トレーニングのための発話レベルのノイズベクトルを提案します。 
[概要]文献で提案されているいくつかの埋め込みベースの適応ベースラインを実装し、両方のデータセットでメソッドがそれらよりも優れていることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: AudVowelConsNet: A Phoneme-Level Based Deep CNN Architecture for
  Clinical Depression Diagnosis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.AS/paper_3.html">
      <font color="black">AudVowelConsNet: A Phoneme-Level Based Deep CNN Architecture for
  Clinical Depression Diagnosis</font>
    </a>
  </h2>
  <font color="black">音素ユニットの音響特性、特にディープラーニングによるうつ病認識のための母音と子音を調査します。ディープネットワークを介した母音と子音の音声特性の融合は、単一空間ネットワークや最先端のディープラーニングを大幅に上回ります。 DAIC-WOZデータベースへのアプローチ..自動評価システムに音素レベルの音声コンポーネントを組み込むことに焦点を当てたものはほとんどありません。 
[ABSTRACT]音声は、うつ病の診断における強力なツールであることが証明されています。うつ病とその重症度レベルを検出するためのさまざまな音響特性の調査に焦点を当てた機械学習の研究</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-30">
        <br><font color="black">2020-10-30</font>
      </time>
    </span>
</section>
<!-- paper0: Data Augmentation for End-to-end Code-switching Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.AS/paper_4.html">
      <font color="black">Data Augmentation for End-to-end Code-switching Speech Recognition</font>
    </a>
  </h2>
  <font color="black">具体的には、既存のコードスイッチングデータとのオーディオスプライシング、および単語の翻訳または単語の挿入によって生成された新しいコードスイッチングテキストとのTTSです。さらに、提案されたすべてのアプローチは、最近人気のあるSpecAugmentと組み合わせることができ、追加ゲインはこの論文では、コードスイッチングデータ拡張のための3つの新しいアプローチが提案されています。 
[概要]コードスイッチングデータ拡張のために3つの新しいアプローチが提案されています。3つの提案されたアプローチはすべて、コードスイッチングasrを個別に大幅に改善します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: One-shot conditional audio filtering of arbitrary sounds -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.AS/paper_5.html">
      <font color="black">One-shot conditional audio filtering of arbitrary sounds</font>
    </a>
  </h2>
  <font color="black">さらに、コンディショニングエンコーダーによって学習された表現は、ラベルを使用せずにトレーニングされているにもかかわらず、埋め込みスペースで音響的に類似した音をクラスター化することを示します。ソース分離ネットワークと共同で学習されたコンディショニングエンコーダーモデルを使用して、トレーニングされたモデルは、トレーニング中に見られなかったものも含め、任意の音源をフィルタリングするように「構成」できます。Librispeechでトレーニングした場合、2つのスピーカーの混合から1つの音声を分離すると、モデルは14.0dBのSI-SDR改善を達成します。 
[概要]サウンドフィルターを使用すると、サウンドクラスラベルを使用せずにモデルをトレーニングできます。条件付けエンコーダーによって学習された表現が、音響的に類似したサウンドをクラスター化することを示します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: DESNet: A Multi-channel Network for Simultaneous Speech Dereverberation,
  Enhancement and Separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.AS/paper_6.html">
      <font color="black">DESNet: A Multi-channel Network for Simultaneous Speech Dereverberation,
  Enhancement and Separation</font>
    </a>
  </h2>
  <font color="black">この論文では、音声の残響除去、強調、分離を同時に行うためのマルチチャネルネットワーク（DESNet）を提案します。さらに、新しいディープコンプレックス畳み込みリカレントネットワーク（DCCRN）を、音声アンミキシングとニューラルネットワークベースの構造として使用します。加重予測誤差（WPE）は、音声の残響除去のために事前にカスケードされます。また、最終的なパフォーマンスをさらに向上させるために、ネットワークのトレーニングのための段階的なSNR戦略とシンフォニックロスを紹介します。 
[概要]提案された変更に加えて、マルチチャネル機能の注意技術を採用します。また、ネットワークのトレーニングのための段階的なsnr戦略と損失を紹介します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Correlation based Multi-phasal models for improved imagined speech EEG
  recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.AS/paper_7.html">
      <font color="black">Correlation based Multi-phasal models for improved imagined speech EEG
  recognition</font>
    </a>
  </h2>
  <font color="black">これらの機能は、ガウス混合ベースの隠れマルコフモデルやディープニューラルネットワークなどの機械学習モデルを使用して、5つのバイナリ音韻カテゴリにさらに分類されます。地形の視覚化と結果ベースの推論は、この論文で提案されている多相相関モデリングアプローチが想像された音声のEEG認識パフォーマンス..次に、トレーニングされた相関ネットワークを使用して、分析フェーズの識別機能を抽出します。 
[要約]提案されたアプローチは、デコード中に多相データが利用できないことをさらに処理します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Can We Trust Deep Speech Prior? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.AS/paper_8.html">
      <font color="black">Can We Trust Deep Speech Prior?</font>
    </a>
  </h2>
  <font color="black">理論上の明らかな利点にもかかわらず、深い生成モデルによって生成される可能性は必ずしも音声品質と一致しないため、深い事前分布は慎重に使用する必要があると主張します。注意深い分析により、この問題はに深く根ざしていることが示されました。深い生成モデルの柔軟性と最尤（ML）トレーニングの性質との不調和..最近、深いスピーチ事前確率に基づくスピーチエンハンスメント（SE）が、非負の変分自動エンコーダーなど、大きな注目を集めています。行列因数分解（VAE-NMF）アーキテクチャ。 
[概要]これは、低ランクの共分散を持つガウス分布などの浅いモデルによるクリーンな音声を表す従来のアプローチと比較されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Transformer based Data Augmentation with Subword Units for
  Morphologically Rich Online ASR -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.AS/paper_9.html">
      <font color="black">Deep Transformer based Data Augmentation with Subword Units for
  Morphologically Rich Online ASR</font>
    </a>
  </h2>
  <font color="black">Transformerで生成されたテキストを使用したデータ拡張は言語の分離には有効ですが、形態学的に豊富な言語で語彙が爆発的に増加することを示します。したがって、生成されたテキストを再トークン化するサブワードベースのニューラルテキスト拡張と呼ばれる新しい方法を提案します。統計的に導出されたサブワードに..MorfessorとBPEの統計サブワードトークナイザーを比較し、両方の方法が語彙サイズとメモリ要件を大幅に削減しながらWERを大幅に改善できることを示します。 
[ABSTRACT]サブワードベースのニューラルテキスト拡張は新しい方法です。これらには、生成されたテキストを統計的に導出されたサブワードに再トークン化することが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Learning in your voice: Non-parallel voice conversion based on speaker
  consistency loss -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.AS/paper_10.html">
      <font color="black">Learning in your voice: Non-parallel voice conversion based on speaker
  consistency loss</font>
    </a>
  </h2>
  <font color="black">本論文では、並列音声コーパスがトレーニングに利用できない場合のトレーニングシナリオと変換シナリオの不一致を解決するための新しい音声変換戦略を提案します。提案された方法の優位性は、主観的リスニングテストと客観的測定で示されます。トレーニングプロセスでは他の話者のID情報を使用するため、トレーニングの哲学は音声変換プロセスの目的と自然に一致します。 
[概要]提案されたモデルは、オートエンコーダと解きほぐしのフレームワークに基づいています。音声信号自体を自然に再構築しながら、アイデンティティとコンテンツの表現を抽出するように設計されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Prosodic Representation Learning and Contextual Sampling for Neural
  Text-to-Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.AS/paper_11.html">
      <font color="black">Prosodic Representation Learning and Contextual Sampling for Neural
  Text-to-Speech</font>
    </a>
  </h2>
  <font color="black">録音と比較した場合、強いベースラインよりも自然度が$ 13.2 \％$の統計的に有意な相対的改善を示しています。この論文では、文脈的に適切な神経音声合成のための新しい2段階トレーニングプロセスでトレーニングされたモデルであるKathakaを紹介します。韻律..また、サンプリング手法のバリエーションについてアブレーション研究を実施し、いずれの場合もベースラインに対して統計的に有意な改善を示しています。 
[概要]ステージiでは、melから韻律分布を学習します-トレーニング中に利用可能なスペクトログラム。これを行うには、テキストにbertを使用し、テキストから抽出されたpardicツリーにグラフ-注意ネットワークを使用します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Modal Transformers Utterance-Level Code-Switching Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.AS/paper_12.html">
      <font color="black">Multi-Modal Transformers Utterance-Level Code-Switching Detection</font>
    </a>
  </h2>
  <font color="black">オーディオネットワークと音声ネットワークは、初期畳み込み、Bi-LSTM、およびトランスフォーマーエンコーダーレイヤーで構成されます。トランスフォーマーエンコーダーレイヤーは、自己注意を使用して分類を改善するための重要で関連性のある機能を選択するのに役立ちます。神経層とトランスフォーマーがより良いパフォーマンスを得るのを助けることを示します。 
[概要]私たちのモデルは、音素シーケンスを処理する音素ネットワークと、コードを処理するオーディオネットワーク（a）で構成されています-スイッチド。発話の音素シーケンスをmfcc機能とともに利用すると、コードのパフォーマンスが向上します-スイッチ検出</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Cross-Lingual Machine Speech Chain for Javanese, Sundanese, Balinese,
  and Bataks Speech Recognition and Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.AS/paper_13.html">
      <font color="black">Cross-Lingual Machine Speech Chain for Javanese, Sundanese, Balinese,
  and Bataks Speech Recognition and Synthesis</font>
    </a>
  </h2>
  <font color="black">次に、インドネシア語のASRとTTSを、テキストのみまたは音声データのみのクロスリンガルマシン音声チェーンフレームワークで利用して、民族言語のASRとTTSを開発します。これらの民族言語の音声テキストデータのペアは不要です。最近、マシン音声連鎖メカニズムは、ASRとTTSが半監視学習で互いに支援できるようにするために提案されました。最初に、監視トレーニングで標準インドネシア語のASRとTTSを別々にトレーニングします。 
[ABSTRACT] s2stシステムには、教師ありトレーニングに大きく依存する機械翻訳（mt）、音声認識（asr）、合成（tts）が必要です。フレームワークは当初、民族言語内でのみ実装されていました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Incremental Machine Speech Chain Towards Enabling Listening while
  Speaking in Real-time -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.AS/paper_14.html">
      <font color="black">Incremental Machine Speech Chain Towards Enabling Listening while
  Speaking in Real-time</font>
    </a>
  </h2>
  <font color="black">私たちの実験結果は、提案されたフレームワークが、非インクリメンタルな基本的な機械音声チェーンと同等のパフォーマンスを維持しながら、長い発話による遅延を減らすことができることを明らかにしています。この作業では、マシンが聴けるようにするためのインクリメンタルな機械音声チェーンを提案します。リアルタイムで話す。具体的には、短期間のループを通じて両方のシステムを一緒に改善させることにより、インクリメンタルASR（ISR）とインクリメンタルTTS（ITTS）を構築します。 
[概要]リアルタイムで、人間はちょっと話していることをリアルタイムで聞くことができます。聞き取りに遅れがあると、話し続けることができなくなります。インクリメンタルasrとインクリメンタルniances（itts）を構築します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Multi-task Network for Delay Estimation and Echo Cancellation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.AS/paper_15.html">
      <font color="black">Deep Multi-task Network for Delay Estimation and Echo Cancellation</font>
    </a>
  </h2>
  <font color="black">実験結果は、提案された方法が信頼できる遅延推定を行い、エコーリターンロスエンハンスメント（ERLE）と音声品質の知覚評価（PESQ）の点で、一貫性のないエコーパス遅延シナリオで既存の最先端のソリューションよりも優れていることを示唆しています。提案されたアーキテクチャは、エコー信号と拡張信号を別々に推定する2つの畳み込み再帰ネットワーク（CRNN）と、エコーパス遅延を推定する完全接続（FC）ネットワークで構成されます。参照遅延の不整合により、の収束が遅くなります。適応型フィルターであり、トレーニングセットの「見えない」参照遅延のために深層学習モデルのパフォーマンスも低下させます。 
[要約]提案された方法は、ref-遅延推定とエコーキャンセレーションタスクの両方に対処するために提案されています。これは、conと呼ばれるマルチタスクネットワークの開発に基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Weighted Data Spaces for Correlation-Based Array Imaging in Experimental
  Aeroacoustics -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.AS/paper_16.html">
      <font color="black">Weighted Data Spaces for Correlation-Based Array Imaging in Experimental
  Aeroacoustics</font>
    </a>
  </h2>
  <font color="black">この一般的なクラスのビームフォーマには、従来のビームフォーミング、（ロバスト）アダプティブビームフォーミング、シェーディングを使用したビームフォーミングなど、多くのよく知られた方法が含まれています。周波数領域での相関測定に基づく方法。 
[ABSTRACT]この分野の標準的な方法は、推定された相関行列が加法性ホワイトノイズと重ね合わされることを前提としています。相関データの共分散行列は、4次モーメントで与えられます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-27">
        <br><font color="black">2020-05-27</font>
      </time>
    </span>
</section>
<!-- paper0: Augmenting Images for ASR and TTS through Single-loop and Dual-loop
  Multimodal Chain Framework -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.AS/paper_17.html">
      <font color="black">Augmenting Images for ASR and TTS through Single-loop and Dual-loop
  Multimodal Chain Framework</font>
    </a>
  </h2>
  <font color="black">実験結果は、シングルループとデュアルループの両方のマルチモーダルチェーンフレームワークにより、ASRとTTSが画像のみのデータセットを使用してパフォーマンスを向上できることを明らかにしました。さらに、このフレームワークのパフォーマンスは、シングルスピーカーの人工音声データでのみ調査されました。残念ながら、このフレームワークは画像検索（IR）モデルに依存していたため、トレーニング中にすでにわかっている画像のみを処理するように制限されていました。 
[ABSTRACT]研究者はマルチモーダル学習学習システムを刷新しました。このフレームワークは画像検索（ir）モデルに依存していたため、トレーニング中にすでにわかっているより多くの画像の処理に限定されていました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Robust Speaker Extraction Network Based on Iterative Refined Adaptation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.AS/paper_18.html">
      <font color="black">Robust Speaker Extraction Network Based on Iterative Refined Adaptation</font>
    </a>
  </h2>
  <font color="black">ほとんどのスピーカー抽出システムは、トレーニング時間中にテストスピーカーに遭遇したことを前提として、満足のいくパフォーマンスを実現します。WSJ0-2mix-extrおよびWHAM！での実験。補助ネットワークによってエンコードされた最初の話者の埋め込みが与えられると、抽出ネットワークはターゲットスピーカーの潜在的な表現を取得できます。これは補助ネットワークにフィードバックされ、抽出ネットワークのより正確なガイダンスを提供するための洗練された埋め込みを取得します。 
[概要]ほとんどのスピーカー抽出システムは、トレーニング時間中にテストスピーカーに遭遇したことに基づいて満足のいくパフォーマンスを達成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Sequence-to-Sequence Learning via Attention Transfer for Incremental
  Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-05/eess.AS/paper_19.html">
      <font color="black">Sequence-to-Sequence Learning via Attention Transfer for Incremental
  Speech Recognition</font>
    </a>
  </h2>
  <font color="black">薄いモデルや浅いモデルを使用する代わりに、教師モデルの元のアーキテクチャを維持しながら、シーケンスを短くする（エンコーダとデコーダの状態が少ない）代替の学生ネットワークを設計します。注意の伝達を使用して、学生ネットワークは現在の入力短音声セグメントと文字起こしの間の同じアラインメント..1つの主な理由は、モデルが増分ステップを決定し、現在の短音声セグメントとアラインメントする文字起こしを学習する必要があるためです。 
[概要]これは、インクリメンタル音声認識（isr）の理解が不足しているためです。isrの元の注意アーキテクチャに基づくasrを使用することは可能です。代わりに、学生は現在の入力ショート間の同じ配置を模倣することを学びます。音声セグメント</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
