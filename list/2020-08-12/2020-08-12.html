<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-08-12の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Unsupervised Speech Decomposition via Triple Information Bottleneck -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_0.html">
      <font color="black">Unsupervised Speech Decomposition via Triple Information Bottleneck</font>
    </a>
  </h2>
  <font color="black">最近、最先端の音声変換システムにより、話者に依存する情報と独立した情報のもつれを解くことができる音声表現が可能になりました。この論文では、注意深く設計された3つの情報のボトルネック。残りの音声コンポーネントのもつれをさらに解くのは、各コンポーネントの明示的な注釈がない場合、決定が難しい問題であり、取得が困難で費用がかかります。 
[要旨]これらのシステムは音色をほぐすことができるだけですが、ピッチ、リズム、コンテンツに関する情報は依然依存しています。システムはgithubで公開されています。 com / auspicious3000 / speechsplit。ラベルは公開されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-23">
        <br><font color="black">2020-04-23</font>
      </time>
    </span>
</section>
<!-- paper0: Transfer Learning for Improving Singing-voice Detection in Polyphonic
  Instrumental Music -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_1.html">
      <font color="black">Transfer Learning for Improving Singing-voice Detection in Polyphonic
  Instrumental Music</font>
    </a>
  </h2>
  <font color="black">このミスマッチを減らすために、転移学習を使用して、人工の音声と音楽のトレーニングセットから学習した知識を、小さいが一致するポリフォニックデータセットに転送します。つまり、伴奏付きの歌声を歌います。エンドポイントと個別のインストゥルメンタルミュージッククリップを人工的に追加して、ポリフォニックボーカルをシミュレートしてボーカル/非ボーカル検出器をトレーニングします。ただし、フレームレベルのラベル付けには時間がかかり、手間がかかります。そのため、適切にラベル付けされたデータセットを歌うことはほとんどできません。 -音声検出（S-VD）。 
[要約]転移学習による提案されたデータ拡張方法は、s-vdパフォーマンスを改善し、f-スコアを89.5％から93. 2％に改善できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Data Efficient Voice Cloning from Noisy Samples with Domain Adversarial
  Training -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_2.html">
      <font color="black">Data Efficient Voice Cloning from Noisy Samples with Domain Adversarial
  Training</font>
    </a>
  </h2>
  <font color="black">前者は少数のターゲットスピーカーデータを使用して、直接モデル更新を通じてマルチスピーカーモデルをターゲットスピーカーの音声に転送しますが、後者では、数秒だけのターゲットスピーカーの音声が、モデルを更新せずにターゲットスピーカーの音声を合成するマルチスピーカーモデル。ただし、ユーザーが提供するサンプルには、実際のアプリケーションでは必然的に音響ノイズが含まれる場合があります。実験により、スピーカーの適応とエンコーディングの両方について、提案されたアプローチは、騒々しい話者のサンプルは、明らかに最先端の音声強調モジュールを採用した方法よりも優れています。 
[ABSTRACT]ターゲット音声のスピーカー適応は、複数のスピーカーからトレーニングされたモデルの一種です。2つの方法には、スピーカーデータと話すデータが必要です。ただし、両方のスピーカーデータが必要です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-10">
        <br><font color="black">2020-08-10</font>
      </time>
    </span>
</section>
<!-- paper0: PoCoNet: Better Speech Enhancement with Frequency-Positional Embeddings,
  Semi-Supervised Conversational Data, and Biased Loss -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_3.html">
      <font color="black">PoCoNet: Better Speech Enhancement with Frequency-Positional Embeddings,
  Semi-Supervised Conversational Data, and Biased Loss</font>
    </a>
  </h2>
  <font color="black">音声品質を維持するように偏った新しい損失関数は、最適化が音声品質に関する人間の知覚的意見とよりよく一致するのに役立ちます。半教師付きの方法は、ノイズの多いデータセットを事前に強化して会話トレーニングデータの量を増やし、実際の録音のパフォーマンスを向上させるのに役立ちます。新規のPoCoNetアーキテクチャーは、周波数ポジショナルエンベディングを使用して、初期層で周波数依存機能をより効率的に構築できる畳み込みニューラルネットワークです。 
[要約]複数の革新により、音声強調のためのより大きな大規模ニューラルネットワークが実現します。半教師付き方法は、ノイズの多いデータセットを事前に強化することにより会話トレーニングデータの量を増やし、実際の録音のパフォーマンスを向上させます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Learning For Sequence-to-sequence Text-to-speech For
  Low-resource Languages -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_4.html">
      <font color="black">Unsupervised Learning For Sequence-to-sequence Text-to-speech For
  Low-resource Languages</font>
    </a>
  </h2>
  <font color="black">さらに、提案された方法を仮説の低リソース言語に拡張し、客観的評価を使用してこの方法の有効性を検証します。具体的には、最初にベクトル量子化変分オートエンコーダー（VQ-VAE）を使用して、教師なし言語単位を抽出します。大規模な、公に発見された、文字起こしされていない音声から。次に、次を使用して、シーケンスからシーケンスへのTTSモデルを事前トレーニングします。 <unsupervised linguistic units, audio>ペア。 
[ABSTRACT]これらのモデルは、人間に近い音声を正確に大きく生成できます-書き起こされたテキストコーパス。現在、新しい教師なし事前トレーニングメカニズムの準備をしています。次に、シーケンス-教師付き言語ユニットを事前トレーニングします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Spectrum and Prosody Conversion for Cross-lingual Voice Conversion with
  CycleGAN -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_5.html">
      <font color="black">Spectrum and Prosody Conversion for Cross-lingual Voice Conversion with
  CycleGAN</font>
    </a>
  </h2>
  <font color="black">また、スペクトルマッピングと韻律マッピングの2つのCycleGANパイプラインをトレーニングすることも提案します。クロスリンガル音声変換に関するこれまでの研究は、主にF0転送の線形変換によるスペクトル変換に焦点を当てています。連続ウェーブレット変換（CWT）分解の使用を提案します。 F0モデリング用。 
[ABSTRACT]たとえば、2つのスピーカーのパラレルデータの必要性を排除します。これは、クロスリンガル音声変換における韻律の最初の研究です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Transformer with Bidirectional Decoder for Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_6.html">
      <font color="black">Transformer with Bidirectional Decoder for Speech Recognition</font>
    </a>
  </h2>
  <font color="black">双方向デコーダー（STBD）を使用して提案された音声変換を実証するために、AISHELL-1データセットに対して広範な実験を行います。具体的には、提案された変換の出力には、左から右へのターゲットと右から左への移動が含まれます。 target ..この作業では、異なる方向のコンテキストを同時に利用する双方向音声変換を導入します。 
[ABSTRACT]私たちは双方向デコーダー（stbd）を使用して、提案された音声変換器を示しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Exploring Optimal DNN Architecture for End-to-End Beamformers Based on
  Time-frequency References -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_7.html">
      <font color="black">Exploring Optimal DNN Architecture for End-to-End Beamformers Based on
  Time-frequency References</font>
    </a>
  </h2>
  <font color="black">結果のモデルはW-Netビームフォーマーと呼ばれ、2つのコンポーネントが含まれています。 1つ目は、2つ目がビーム形成フィルターを推定するために使用する時間周波数参照を計算します。両方のアプローチが効果的です。ただし、それらの一般化可能性には盲点があります。現在、最良の方法は、ディープニューラルネットワーク（DNN）を利用した一般化固有値と最小分散の歪みのない応答ビームフォーマーの変形と、使用されるDNNベースのフィルター推定方法です。ビームフォーミングフィルターを直接計算します。 
[ABSTRACT]現在、最良の方法は、ディープニューラルネットワーク（dnn）およびdnnベースのフィルター-ビームフォーミングフィルターを直接計算するために使用される推定方法です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-23">
        <br><font color="black">2020-05-23</font>
      </time>
    </span>
</section>
<!-- paper0: Coswara -- A Database of Breathing, Cough, and Voice Sounds for COVID-19
  Diagnosis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_8.html">
      <font color="black">Coswara -- A Database of Breathing, Cough, and Voice Sounds for COVID-19
  Diagnosis</font>
    </a>
  </h2>
  <font color="black">これに向けて、この論文では、Coswaraと呼ばれる呼吸音、つまり咳、呼吸、声のデータベースを作成（および分析）する際の初期の取り組みを示します。COVID-19の顕著な症状には、咳や呼吸困難があります。 、この方法は高価で時間がかかり、社会的距離に違反します。 
[要約] covid、呼吸音の現在の標準的な方法は、有用な洞察を提供し、診断ツールの設計を可能にします。サウンドサンプルは、Webサイトアプリケーションを使用して世界中のクラウドソーシングで収集されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-21">
        <br><font color="black">2020-05-21</font>
      </time>
    </span>
</section>
<!-- paper0: A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal
  Transformer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_9.html">
      <font color="black">A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal
  Transformer</font>
    </a>
  </h2>
  <font color="black">パフォーマンスは、私たちのモデルが優れたパフォーマンスを達成する挑戦的なActivityNetキャプションデータセットで実証されています。高密度ビデオキャプションタスクでオーディオおよびビジュアルモダリティを使用して提案されたモデルの有効性を示しますが、モジュールは2つのモダリティをシーケンスからシーケンスへのタスク..既存の方法は、オーディオトラックを完全に無視しながら、主に視覚機能のみを利用してこのタスクに取り組みます。 
[ABSTRACT]バイモーダルトランスフォーマーモデルは、シンプルなプロポーザル生成モジュールの特徴エクストラクターとして使用できます。事前トレーニング済みのバイモーダルエンコーダーは、バイモーダルモデルの一部です。フィーチャーの作成にも使用できます。提案生成のためのキャプター</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-17">
        <br><font color="black">2020-05-17</font>
      </time>
    </span>
</section>
<!-- paper0: Alzheimer's Dementia Detection from Audio and Text Modalities -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_10.html">
      <font color="black">Alzheimer's Dementia Detection from Audio and Text Modalities</font>
    </a>
  </h2>
  <font color="black">話し言葉の流暢さが低いことはアルツハイマー病の患者では一般的なパターンであるため、リズムの特徴も提案されます。音響波形と内容の両方の特徴が抽出されると、音声処理によるアルツハイマー型認知症の自動検出が強化されます。x-ベクトル、i-ベクトル、および統計的音声ベースの機能機能が評価されます。 
[ABSTRACT]提出されたシステムは、患者の音声とメッセージの両方からアルツハイマー病のパターンを検出することを目的としています。これらは、音声とテキストシステムからそれらを識別することを目的としています。ソフトウェアは、アルツハイマー病のパターンを識別するために使用されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Meta-Learning for Short Utterance Speaker Recognition with Imbalance
  Length Pairs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_11.html">
      <font color="black">Meta-Learning for Short Utterance Speaker Recognition with Imbalance
  Length Pairs</font>
    </a>
  </h2>
  <font color="black">この問題を解決するために、不均衡な長さのペアのメタ学習フレームワークを導入します。さらに、特定のエピソードのクラスのみを最適化することは、目に見えないクラスの識別的埋め込みを学習するには不十分な場合があるため、モデルを強制して両方のクラスを分類します。サポートおよびトレーニングセットのクラスセット全体に対するクエリセット。コードはhttps://github.com/seongmin-kye/meta-SRで入手できます。 
[ABSTRACT]既存の話者認識モデルは短い発話では不十分に機能します。既存のモデル認識モデルを使用してスキルをテストできます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-06">
        <br><font color="black">2020-04-06</font>
      </time>
    </span>
</section>
<!-- paper0: SpeechBERT: An Audio-and-text Jointly Learned Language Model for
  End-to-end Spoken Question Answering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_12.html">
      <font color="black">SpeechBERT: An Audio-and-text Jointly Learned Language Model for
  End-to-end Spoken Question Answering</font>
    </a>
  </h2>
  <font color="black">エンドツーエンドモデルはASRの前にオーディオデータから情報を抽出できることが示されたため、このモデルは、応答スパンにASRエラーを含むデータセットに対して次のテキスト質問応答（TQA）モデルでASRをカスケードする従来のアプローチよりも優れていました。生成されたエラー..エンドツーエンドのSQAの可能性に加えて、SpeechBERTは、他の多くの音声言語理解タスクでも、多くのテキスト処理タスクのBERTと同様に考慮することができます。.提案されたエンドツーエンドモデルを理解するときカスケードアーキテクチャにより、さらに優れたパフォーマンスが達成されました。 
[要約]成功したbertモデルの一部として、音声と質問のスピーチベルトモデルを提案します。提案されたエンドツーエンドを組み立てる一方で、十分に準備された効果もありました。成功したモデルからの学習に加えて、これらは成功した</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-25">
        <br><font color="black">2019-10-25</font>
      </time>
    </span>
</section>
<!-- paper0: Exploring Aligned Lyrics-Informed Singing Voice Separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_13.html">
      <font color="black">Exploring Aligned Lyrics-Informed Singing Voice Separation</font>
    </a>
  </h2>
  <font color="black">実験結果は、モデルが音声アクティビティ情報だけでなく、整列された歌詞からの音声内容も使用できることを示しています。この論文では、整列された歌詞を歌声分離のパフォーマンスを改善するための追加情報として利用する方法を提案します。パフォーマンスの向上が、情報に基づいて整列された歌詞にある音声コンテンツに実際に起因するものであるかどうかという問題が残っています。 
[ABSTRACT]正しい歌詞でトレーニングされたモデルは、実際には通知されなかったモデルよりも優れたパフォーマンスをもたらします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Exploring the Best Loss Function for DNN-Based Low-latency Speech
  Enhancement with Temporal Convolutional Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_14.html">
      <font color="black">Exploring the Best Loss Function for DNN-Based Low-latency Speech
  Enhancement with Temporal Convolutional Networks</font>
    </a>
  </h2>
  <font color="black">最も適切な方法は、データセットの規模とタスクの種類によって異なります。問題のない音声エンコーダ（PASE）機能を使用して、STFTベースの方法と損失関数を提案し、小さいデータセットの主観品質を改善します。また、低レイテンシバージョンのTasNetを実装します。これは、DNSチャレンジに提出し、オープンソーシングによって公開しました。 
[ABSTRACT]時間-周波数マスキングはdnn音声強調に広く使用されていますが、tasnetやtasnetなどの時間領域法も提案されています。この方法は、音声バンクの需要データセットに効果的です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-23">
        <br><font color="black">2020-05-23</font>
      </time>
    </span>
</section>
<!-- paper0: Surgical Mask Detection with Convolutional Neural Networks and Data
  Augmentations on Spectrograms -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_15.html">
      <font color="black">Surgical Mask Detection with Convolutional Neural Networks and Data
  Augmentations on Spectrograms</font>
    </a>
  </h2>
  <font color="black">ここでのアイデアは、記述不足の小さなトレーニングデータセットの特徴分布へのモデルの過剰適合を減らすことです。人間の声のサンプルにおけるサージカルマスク検出のバイナリ分類タスクに対するデータ拡張の影響を示します（ComParE Challenge 2020）。 ..多くの研究分野で、ラベル付きデータセットを取得するのは困難です。 
[ABSTRACT]これは、データ拡張がトレーニングデータの欠如を克服することを約束する場所です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: S-vectors: Speaker Embeddings based on Transformer's Encoder for
  Text-Independent Speaker Verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_16.html">
      <font color="black">S-vectors: Speaker Embeddings based on Transformer's Encoder for
  Text-Independent Speaker Verification</font>
    </a>
  </h2>
  <font color="black">この構造からのスピーカーの埋め込みをs-vectorsと名付けました。基本的なアーキテクチャとして、自己注意に基づいて構築されたトランスフォーマーのエンコーダー構造を使用し、スピーカー分類タスクを実行するようにトレーニングしました。は、モデルのさまざまな層からsベクトルを導出する効果を調査しました。 
[ABSTRACT] x-erzliesは、時間遅延ニューラルネットワーク（tdnn）を使用して取得されます。これらは、自己注意に基づいて構築されたトランスフォーマーのエンコーダー構造を基本アーキテクチャとして使用して構築されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Neural PLDA Modeling for End-to-End Speaker Verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_17.html">
      <font color="black">Neural PLDA Modeling for End-to-End Speaker Verification</font>
    </a>
  </h2>
  <font color="black">最近、ニューラルPLDA（NPLDA）と呼ばれるスピーカー検証におけるバックエンドモデリングのためのニューラルネットワークアプローチを提案しました。このPLDA（NPLDA）は、生成PLDAモデルの尤度比スコアが判別類似度関数として提起され、スコア関数の学習可能なパラメーターが次のように最適化されます。検証コスト。ディープラーニングモデルは教師あり分類問題で大幅な進歩を遂げましたが、話者認識などの外れの検証タスクへのこれらのモデルの適用は、特徴埋め込みの導出に限定されていました。アートx-ベクトルPLDAベースの話者検証システムは、検証スコアの計算に確率的線形判別分析（PLDA）に基づく生成モデルを使用します。 
[要約] npldaネットワークを使用した提案されたe2eモデルは、モデルよりも大幅に改善されています。提案されたモデルは、他のさまざまなモデルとリンクされています。新しいモデルは、テスト用のモデルとして使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: PlugSonic: a web- and mobile-based platform for binaural audio and sonic
  narratives -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_18.html">
      <font color="black">PlugSonic: a web- and mobile-based platform for binaural audio and sonic
  narratives</font>
    </a>
  </h2>
  <font color="black">PlugSonicの主な目的は技術の民主化です。 PlugSonicユーザー-機関であれ市民であれ-はすべて、3Dサウンドスケープとソニックナラティブの作成、処理、体験に必要な機器を与えられています。特定のデバイス、外部ツール（ソフトウェアおよび/またはハードウェア）、専門知識、またはカスタム開発を必要としません。PlugSonic内のオーディオ処理はWeb Audio APIと3D Tune-In Toolkitに基づいていますが、サウンドスケープの探索は物理的なスペースは、AppleのARKitを使用して取得されます。それは、PLUGGY EUプロジェクト（遺産認識と参加のためのプラグイン可能なソーシャルプラットフォーム）の一部として開発され、オーディオ効果を編集および適用するPlugSonicサンプル、およびPlugSonic Soundscapeの2つの主要アプリケーションで構成されます。 、バイノーラルサウンドスケープを作成して体験します。 
[ABSTRACT] plugsonicサウンドスケープは、pluggy eu project.itの一部として作成されました。2つの主要なアプリケーションで構成されています：プラグインサンプル、オーディオエフェクトを編集および適用する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Why Did the x-Vector System Miss a Target Speaker? Impact of Acoustic
  Mismatch Upon Target Score on VoxCeleb Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_19.html">
      <font color="black">Why Did the x-Vector System Miss a Target Speaker? Impact of Acoustic
  Mismatch Upon Target Score on VoxCeleb Data</font>
    </a>
  </h2>
  <font color="black">選択した音響機能の1次および2次統計の距離を線形混合効果モデルの予測子として使用し、標準のカルディxベクトルシステムがASVブラックボックスを形成します。解釈的な機械学習に沿って、登録とテスト発話の音響的不一致に対するASV検出スコアの依存性をモデル化します。現代の自動話者検証（ASV）は、ディープニューラルネットワークを通じて実装された機械学習に大きく依存しています。 
[要約]ターゲットスピーカーのミスを予測する不一致要因を特定することを目的としています。これらの要因を使用して、潜在的なスピーカーミスを検出します（誤った拒否）</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Investigation of End-To-End Speaker-Attributed ASR for Continuous
  Multi-Talker Recordings -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_20.html">
      <font color="black">Investigation of End-To-End Speaker-Attributed ASR for Continuous
  Multi-Talker Recordings</font>
    </a>
  </h2>
  <font color="black">元のE2E SA-ASRと提案された方法を、モノラルLibriCSSデータセットで包括的に調査します。具体的には、E2E SA-ASRモデルの内部スピーカー表現を使用して、スピーカーのカウントとクラスタリングを実行し、プロファイルがスピーカーインベントリから欠落しているスピーカー。関連するスピーカープロファイルを備えた元のE2E SA-ASRと比較して、提案された方法は、事前のスピーカー知識なしに近いパフォーマンスを実現します。 
[要約]提案された方法は、事前の話者の知識なしに近いスタートを達成しました。e2esa-asrトレーニング-マルチトーカー録音を処理できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Acoustic effects of medical, cloth, and transparent face masks on speech
  signals -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_21.html">
      <font color="black">Acoustic effects of medical, cloth, and transparent face masks on speech
  signals</font>
    </a>
  </h2>
  <font color="black">ほとんどのマスクはラペルマイクにほとんど影響を与えません。これは、既存の音声補強システムと支援聴取システムがマスクとの口頭によるコミュニケーションに効果的である可能性があることを示唆しています。すべてのマスクは1 kHzを超える周波数を減衰し、その減衰は話し手の前で最大になり、特にマスクの種類、特に異なる素材や織りの布マスクの間にはかなりのばらつきがあります。 
[ABSTRACT]この調査では、さまざまなフェイスマスクによって引き起こされる音響減衰を調べます。これには、ヘッド型のスピーカーとライブの人間の話し手が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Efficient Integration of Multi-channel Information for
  Speaker-independent Speech Separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_22.html">
      <font color="black">Efficient Integration of Multi-channel Information for
  Speaker-independent Speech Separation</font>
    </a>
  </h2>
  <font color="black">また、低チャネルネットワーク用にトレーニングされたパラメーターを高チャネルネットワークの初期値として適用する転移学習フレームワークである、チャネル順次転移学習も提案します。ディープラーニングベースの方法では、過去数年間の音声分離のパフォーマンスは、音声分離のためにマルチチャネル信号を統合する方法について未解決の問題として残っています。提案された方法は、マルチチャネルのディープクラスタリングよりもパフォーマンスが高く、パフォーマンスの数がマイク。 
[要約]時間領域の音声分離ネットワークに基づいてマルチチャネル情報を統合する2つの方法を提案します。オープンベースのwsj0-2mixデータセットの空間バージョンを使用して方法を評価しました。後期のパフォーマンス-フュージョン方式は、スピーカー間の角度の違いに関係なく、シングルチャネル方式よりも常に高い</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-23">
        <br><font color="black">2020-05-23</font>
      </time>
    </span>
</section>
<!-- paper0: Bunched LPCNet : Vocoder for Low-cost Neural Text-To-Speech Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.SD/paper_23.html">
      <font color="black">Bunched LPCNet : Vocoder for Low-cost Neural Text-To-Speech Systems</font>
    </a>
  </h2>
  <font color="black">提案されたバンチング手法を使用すると、LPCNetは、Deep Convolutional TTS（DCTTS）音響モデルと組み合わせて、モバイルデバイスで実行したときにベースラインランタイムより2.19倍向上し、TTS平均オピニオンスコアの減少は0.1未満である（MOS）。これらの手法は次のとおりです。1）LPCNetが推論ごとに複数のオーディオサンプルを生成できるようにするサンプルバンチング。および2）ビットバンチング。LPCNetの最終層での計算を削減します。LPCNetは、線形予測とディープニューラルネットワークモジュールを組み合わせて計算の複雑さを低く抑える効率的なボコーダーです。 
[ABSTRACT]低コストのlpcnetボコーダーベースのニューラルテキストからオーディオへのシステム。新しい調査では、ベースラインネットワークに対して2.19倍の改善が見られました。0未満です。tts平均オピニオンスコア（mos）の減少</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Exposing Deep-faked Videos by Anomalous Co-motion Pattern Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_0.html">
      <font color="black">Exposing Deep-faked Videos by Anomalous Co-motion Pattern Detection</font>
    </a>
  </h2>
  <font color="black">最近のディープラーニングベースのビデオ合成アプローチ、特に「DeepFake」などのIDを偽造できるアプリケーションでは、セキュリティ上の大きな懸念が生じています。このような結合パターンは、ビデオコンテンツに依存しないローカルモーション機能全体でマイニングされているため、インスタンスごとの変動も大幅に軽減できます。さまざまなコンテンツを含むビデオの一般化可能性を高めるために、ビデオ内の複数の特定の空間位置の時間的動きをモデル化して、協調運動パターンと呼ばれる堅牢で信頼性の高い表現を抽出します。 
[ABSTRACT]新しい方法を使用して、ディープフェイクビデオを公開できます。これらのタイプのコンジョイントパターンは、ローカルモーション機能全体でマイニングされます。これらには、ビデオコンテンツに依存しないモーション機能が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to See Through Obstructions with Layered Decomposition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_1.html">
      <font color="black">Learning to See Through Obstructions with Layered Decomposition</font>
    </a>
  </h2>
  <font color="black">具体的には、2つの層の密なオプティカルフローフィールドの推定と、深いたたみ込みニューラルネットワークを介してフローワープ画像から各層を再構築することを交互に行います。学習ベースの層再構築により、フロー推定の潜在的なエラーと脆性に対応できます。明るさの一貫性などの仮定。反射とフェンスの除去に関する多くの困難なシナリオに関する私たちの結果は、提案された方法の有効性を示しています。 
[ABSTRACT]新しい方法により、フロー推定の潜在的なエラーと輝度などの脆弱な仮定に対応できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: AtrialJSQnet: A New Framework for Joint Segmentation and Quantification
  of Left Atrium and Scars Incorporating Spatial and Shape Information -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_2.html">
      <font color="black">AtrialJSQnet: A New Framework for Joint Segmentation and Quantification
  of Left Atrium and Scars Incorporating Spatial and Shape Information</font>
    </a>
  </h2>
  <font color="black">しかし、自動セグメンテーションは、画像品質が悪いため、さまざまなLA形状、薄い壁、および周囲の強化された領域により、依然として困難です。後期ガドリニウム拡張磁気共鳴イメージング（LGE MRI）からの左心房（LA）および心房瘢痕のセグメンテーション）は、臨床診療における重要なタスクです。LAとLAの瘢痕間の固有の相関関係を利用するために、明示的な表面投影による形状注意（SA）のメカニズムを提案します。 
[要旨]コードと結果は、原稿が受理されると公にリリースされます。コードのリリース時に公開されます。これは、年末までに公開される予定です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Measles Rash Identification Using Residual Deep Convolutional Neural
  Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_3.html">
      <font color="black">Measles Rash Identification Using Residual Deep Convolutional Neural
  Network</font>
    </a>
  </h2>
  <font color="black">その結果、ますます多くの米国の医療専門家と公衆がこの病気を見たことがない。私たちの画像データセットでは、モデルは分類精度95.2％、感度81.7％、特異度97.1％に達し、モデルを示しているはしかの発生を抑えるために、はしかの正確な検出を促進するのに効果的です。はしかの診断を支援するために、さまざまな皮膚の状態の1300以上の画像を収集しました。将来的に電話アプリケーションを作成することを目的とした皮膚の状態。 
[概要]はしかの予防接種が何十年も成功したため、2000年に麻疹は撲滅されたと宣言されました。その結果、2019年に麻疹が再燃し、1、282例が確認されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br><font color="black">2020-05-18</font>
      </time>
    </span>
</section>
<!-- paper0: Thick Cloud Removal of Remote Sensing Images Using Temporal Smoothness
  and Sparsity-Regularized Tensor Optimization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_4.html">
      <font color="black">Thick Cloud Removal of Remote Sensing Images Using Temporal Smoothness
  and Sparsity-Regularized Tensor Optimization</font>
    </a>
  </h2>
  <font color="black">したがって、リモートセンシング画像を有効に活用するには、厚い雲と雲の影を取り除くこと、および雲で汚染されたピクセルを回復することが不可欠です。したがって、希薄ノルムは、雲と雲の影の希薄性を高めるために使用され、一方向全変動（UTV）正則化を適用して、一方向の滑らかさを確保します。TSSTOの基本的な考え方は、厚い雲と雲の影がまばらであるだけでなく、画像の水平方向と垂直方向にも滑らかであり、クリーンな画像は滑らかであるということです。画像間の時間方向。 
[ABSTRACT]一連の実験は、シミュレーションされたクラウドと実際のクラウドの両方で行われます-さまざまなセンサーからのさまざまな解像度の汚染された画像</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning Methods for Solving Linear Inverse Problems: Research
  Directions and Paradigms -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_5.html">
      <font color="black">Deep Learning Methods for Solving Linear Inverse Problems: Research
  Directions and Paradigms</font>
    </a>
  </h2>
  <font color="black">今日、ディープラーニングの急速な発展により、線形逆問題を解決するための新たな展望がもたらされます。この問題は、適切に設計されたさまざまなネットワークアーキテクチャにより、多くのアプリケーションで最先端のパフォーマンスをもたらします。線形逆問題は、さまざまな科学分野の開発。さまざまなアプリケーションで線形逆問題のさまざまなバリアントを解決するために、無数の試みが行われてきました。 
[要約]さまざまな線形逆問題を解決するために複数の試みが行われました。このホワイトペーパーでは、ディープラーニングの開発の進捗状況を包括的に調査します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-27">
        <br><font color="black">2020-07-27</font>
      </time>
    </span>
</section>
<!-- paper0: Left Ventricular Wall Motion Estimation by Active Polynomials for Acute
  Myocardial Infarction Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_6.html">
      <font color="black">Left Ventricular Wall Motion Estimation by Active Polynomials for Acute
  Myocardial Infarction Detection</font>
    </a>
  </h2>
  <font color="black">提案されたアルゴリズムは、心臓病専門医が急性MIの初期徴候を診断するのを支援するために、LV壁セグメントで発生する真の壁の動きを定量化します。さらに、医療専門家は、「最大運動変位」プロットは、壁運動とLV Ejection-Fraction（LVEF）をより適切に評価するのに役立ちます。心エコー図（エコー）は、心筋梗塞（MI ）または一般的に心臓発作として知られています。 
[要約]提案されたアプローチは、左心室（lv）壁の全体的な動きを正確かつロバストに推定できます。これにより、医療専門家は、色分けされたセグメントを通じてエコー画像の視覚化機能を強化できます。この方法は、最初のパブリックエコーです。カタールのハマド医療法人病院の医師が作成したデータベースコレクション</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: TextureWGAN: Texture Preserving WGAN with MLE Regularizer for Inverse
  Problems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_7.html">
      <font color="black">TextureWGAN: Texture Preserving WGAN with MLE Regularizer for Inverse
  Problems</font>
    </a>
  </h2>
  <font color="black">MSEベースの方法は、ベースライン画像とCNNによって生成された画像の間のすべてのピクセルのユークリッド距離を最小化し、画像テクスチャなどのピクセルの空間情報を無視します。この方法は、超解像、画像ノイズ除去、および画像再構成..また、画像テクスチャを評価するために、1次および2次の統計的画像テクスチャ分析を実施しました。 
[要約]すべての提案された方法で、最も一般的で効果的な方法は、平均二乗誤差（mse）を使用した畳み込みニューラルネットワーク（cnn）です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: COVID-19 detection using Residual Attention Network an Artificial
  Intelligence approach -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_8.html">
      <font color="black">COVID-19 detection using Residual Attention Network an Artificial
  Intelligence approach</font>
    </a>
  </h2>
  <font color="black">感染した患者を隔離できるため、COVID-19の効果的なテストは発生を制御するために重要です。このホワイトペーパーでは、人工知能を使用してCOVID-19を検出する手法を紹介します。胸部X線画像のデータセットを収集し、トレーニングしました胸部X線を分類するためのいくつかの一般的なディープコンボリューションニューラルネットワークベースのモデル（VGG、MobileNet、Xception、DenseNet、InceptionResNet）。 
[ABSTRACT] covidのテスト-19は発生を制御するために重要です。ウイルスは急速に感染します。繁殖数は2ドルです。2-2から7ウイルスの効果的なテストは、発生を制御するために不可欠です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-26">
        <br><font color="black">2020-06-26</font>
      </time>
    </span>
</section>
<!-- paper0: Quantitative Analysis of Image Classification Techniques for
  Memory-Constrained Devices -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_9.html">
      <font color="black">Quantitative Analysis of Image Classification Techniques for
  Memory-Constrained Devices</font>
    </a>
  </h2>
  <font color="black">畳み込みニューラルネットワーク（CNN）は、画像分類の最先端技術ですが、通常はメモリフットプリントが大きくなります。しかし、より複雑なマルチクラスおよびマルチチャネル画像分類の可能性はまだありません。これは、メモリが不足していることが多い組み込みデバイスに依存するアプリケーションでの有用性を制限します。 
[要約]これらは、光学式文字認識で最大98.2％の精度に達することが示されています。ただし、メモリフットプリントはわずか6 kbです。これらは、cifar-10を使用した画像分類と比較されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-11">
        <br><font color="black">2020-05-11</font>
      </time>
    </span>
</section>
<!-- paper0: SAFRON: Stitching Across the Frontier for Generating Colorectal Cancer
  Histology Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_10.html">
      <font color="black">SAFRON: Stitching Across the Frontier for Generating Colorectal Cancer
  Histology Images</font>
    </a>
  </h2>
  <font color="black">SAFRONを使用して生成されたサンプルの高解像度画像は、https：//warwick.ac.uk/TIALab/SAFRONで入手できます。合成画像は、アノテーションの限られた可用性のコンテキストでディープラーニングアルゴリズムの開発と評価に使用できます。このために、大腸腺癌の大組織画像タイルの条件付き生成敵対ネットワークに基づいて、提案されたSAFRONフレームワークをトレーニングします。 （CRAG）およびDigestPathデータセット。 
[要約]合成データは、結腸直腸癌組織画像の腺セグメンテーションの標準アルゴリズムのパフォーマンスを大幅に向上させることができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Key-Nets: Optical Transformation Convolutional Networks for Privacy
  Preserving Vision Sensors -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_11.html">
      <font color="black">Key-Nets: Optical Transformation Convolutional Networks for Privacy
  Preserving Vision Sensors</font>
    </a>
  </h2>
  <font color="black">キーネットに適した光学変換のための5つの十分な条件を提供し、一般化された確率行列（スケール、バイアス、フラクショナルピクセルシャフリングなど）がこれらの条件を満たしていることを示しています。は、顔の識別とオブジェクトの検出のために光学的に変換された画像に直接微調整されたネットワークのユーティリティ/プライバシーのトレードオフです。 
[ABSTRACT]キー-ネットは、光学的ホモラティカルに基づく最初の実用的で効率的でプライバシーを保護するビジョンセンサーです。それなしでは、光学変換された画像に直接微調整されたネットワークのユーティリティ/プライバシーのトレードオフがあることを示すように設計されています顔識別とオブジェクト検出</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Detecting Urban Dynamics Using Deep Siamese Convolutional Neural
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_12.html">
      <font color="black">Detecting Urban Dynamics Using Deep Siamese Convolutional Neural
  Networks</font>
    </a>
  </h2>
  <font color="black">変化の検出は、コンピュータビジョンとリモートセンシングの分野で急速に成長している分野です。提案された検出機能は、全体的な精度（95.8）、カッパメジャー（72.5）、再現率（76.5）、精度（77.7）の観点から測定されました。 ）、F1メジャー（77.1）。この作業では、シャムCNNと呼ばれる畳み込みニューラルネットワーク（CNN）のバリアントを設計および開発して、異なる時間にキャプチャされたメケル市のSentinel-2時間画像のペアから特徴を抽出し、都市化による変化を検出する：建物と道路。 
[要約]モデルを使用して、都市化が進行しているさまざまな時期におけるメケルや他の都市の変化を検出できます。このモデルは、これらの対策のほとんどの点で優れたパフォーマンスを達成しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Comparative study of deep learning methods for the automatic
  segmentation of lung, lesion and lesion type in CT scans of COVID-19 patients -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_13.html">
      <font color="black">Comparative study of deep learning methods for the automatic
  segmentation of lung, lesion and lesion type in CT scans of COVID-19 patients</font>
    </a>
  </h2>
  <font color="black">結果として得られたバイナリ病変は、91.3 mlの平均絶対体積誤差でセグメント化されました。COVID-19に関する最近の研究では、CTイメージングが、疾患の理解に役立つことに加えて、疾患の進行を評価し、診断を支援するための有用な情報を提供することを示唆しています。さまざまな方法を組み合わせることで、肺セグメンテーション、バイナリ病変セグメンテーション、マルチクラス病変セグメンテーションのテストセット全体のパフォーマンスを向上させることができ、その結果、平均ダイススコアはそれぞれ0.982、0.724、0.469になります。 
[要約]ディープラーニングを使用して、胸部CTスキャンを使用したcovid-19の迅速かつ正確な定量化を提供する研究が提案されています。これらには、オープンソースや自社開発のアルゴリズムを含む、複数の複数のタイプの研究が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-29">
        <br><font color="black">2020-07-29</font>
      </time>
    </span>
</section>
<!-- paper0: ARPM-net: A novel CNN-based adversarial method with Markov Random Field
  enhancement for prostate and organs at risk segmentation in pelvic CT images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_14.html">
      <font color="black">ARPM-net: A novel CNN-based adversarial method with Markov Random Field
  enhancement for prostate and organs at risk segmentation in pelvic CT images</font>
    </a>
  </h2>
  <font color="black">セグメンテーションネットワークは、新しく設計されたMRFブロックをマルチ残差Uネットのバリエーションに統合します。弁別器は、元のCTと予測/グラウンドトゥルースの積を入力として受け取り、入力を偽/実数に分類します。Aセグメンテーションネットワークと弁別器ネットワークは共同でトレーニングされ、セグメンテーションネットワークのみが予測に使用されました。 
[ABSTRACT]無傷の前立腺癌患者110名の計画ctと構造のデータセットを遡及的に選択し、10倍の交差検証で分割しました。弁別子は、元のctと等高線の予測の積を偽/実数にして、入力を分類します偽物または本物に</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-modal segmentation of 3D brain scans using neural networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_15.html">
      <font color="black">Multi-modal segmentation of 3D brain scans using neural networks</font>
    </a>
  </h2>
  <font color="black">方法：3D MRI（MPRAGE、DWI、FLAIR）およびCTスキャンをセグメント化するように、深い畳み込みニューラルネットワークをトレーニングします。トレーニングラベルは、MPRAGEコントラストで取得され、他のイメージングモダリティに登録されます。破損した入力スキャンを特定するためにドロップアウトサンプリングが実装されていますまたは低品質のセグメンテーション。 
[要約]合計851 mri / ctスキャンの大規模なデータベースがニューラルネットワークのトレーニングに使用されます。セグメンテーション品質は、合計27の解剖学的構造のダイスメトリックを使用して定量化されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: AHP-Net: adaptive-hyper-parameter deep learning based image
  reconstruction method for multilevel low-dose CT -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_16.html">
      <font color="black">AHP-Net: adaptive-hyper-parameter deep learning based image
  reconstruction method for multilevel low-dose CT</font>
    </a>
  </h2>
  <font color="black">AHP-Netは、事前にフレームレットフィルターバンクに構築された学習可能な画像を使用して半二次分割スキームを展開し、さまざまなノイズレベルのハイパーパラメーターを自動的に調整するネットワークを学習します。臨床スキャンを使用した広範な実験的評価により、AHP-Netが従来の異なるノイズレベルのマルチレベルLDCTのためのMBIR技術と最先端のディープラーニングベースの方法。したがって、この作業は、適応型ハイパーパラメーターDLベースの画像再構成法（AHP-Net）を開発することを目的としています。異なるノイズレベルのマルチレベルLDCTを処理できます。 
[ABSTRACT] ahp-netは、さまざまなノイズレベルのマルチレベルldctを処理できる単一のユニバーサルトレーニングモデルを提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Adaptive Object Detection with Dual Multi-Label Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_17.html">
      <font color="black">Adaptive Object Detection with Dual Multi-Label Prediction</font>
    </a>
  </h2>
  <font color="black">本論文では、マルチラベルオブジェクト認識をデュアル補助タスクとして活用することにより、適応型オブジェクト検出のための新しいエンドツーエンドの教師なしディープドメイン適応モデルを提案します。さらに、オブジェクト検出を支援する予測整合性正則化メカニズムを紹介します。これは、マルチラベル予測結果を補助正則化情報として使用して、オブジェクト認識タスクとオブジェクト検出タスクの間で一貫したオブジェクトカテゴリの発見を保証します。いくつかのベンチマークデータセットで実験が行われ、提案されたモデルが状態より優れていることが示されています。最先端の比較方法。 
[ABSTRACT]モデルはマルチラベル予測を組み合わせて、各画像のオブジェクトカテゴリ情報を明らかにします。次に、予測結果を使用して条件付きの敵対的なグローバルフィーチャアライメントを実行します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-29">
        <br><font color="black">2020-03-29</font>
      </time>
    </span>
</section>
<!-- paper0: Implanting Synthetic Lesions for Improving Liver Lesion Segmentation in
  CT Exams -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_18.html">
      <font color="black">Implanting Synthetic Lesions for Improving Liver Lesion Segmentation in
  CT Exams</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、合成病変を埋め込むと、さまざまなアーキテクチャを考慮してセグメンテーションパフォーマンスが向上するだけでなく（最大約12 \％）、この改善はさまざまな画像合成ネットワーク間で一貫していることを示しています。病変の変動性を総合的に高めると、サイズ、密度、形状、および位置は、CTスライスでの肝臓病変セグメンテーションのセグメンテーションモデルのパフォーマンスを向上させるようです。この現象は、さまざまな可能性の中で、積極的なデータ増強方法を使用して、病変セグメンテーションアルゴリズムに固有のバイアスを追加できます。 
[要約]データセット内の病変の変化は、さまざまな種類の病変の有病率にも依存します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Temporally Coherent Embeddings for Self-Supervised Video Representation
  Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_19.html">
      <font color="black">Temporally Coherent Embeddings for Self-Supervised Video Representation
  Learning</font>
    </a>
  </h2>
  <font color="black">私たちは、複数の挑戦的なベンチマーク（Kinetics400、UCF101、HMDB51）を使用して、ビデオアクション認識のダウンストリームタスクで自己管理学習TCEモデルを徹底的に分析および評価します。TCEを使用して、大量のラベル付けされていないビデオデータから堅牢な表現を学習します。シンプルだが効果的な2D-CNNバックボーンとRGBストリーム入力のみ、TCE事前トレーニング済み表現は、UCF101で事前トレーニング済みのすべての以前の自己監視2D-CNNおよび3D-CNNよりも優れています。 
[要約]提案された方法は、ラベル付けされていないビデオデータを使用して、埋め込みスペースに時間的一貫性を明示的に適用します。さらに、隣接するフレームが互いに近くに存在し、ビデオが互いに分離されるように、ビデオをエンコードするようにtceモデルをトレーニングします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-21">
        <br><font color="black">2020-03-21</font>
      </time>
    </span>
</section>
<!-- paper0: 3D FLAT: Feasible Learned Acquisition Trajectories for Accelerated MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.IV/paper_20.html">
      <font color="black">3D FLAT: Feasible Learned Acquisition Trajectories for Accelerated MRI</font>
    </a>
  </h2>
  <font color="black">圧縮センシング（CS）は、k空間（空間座標の物理空間に対して二重のフーリエドメイン）をサブサンプリングして、取得を大幅に加速することを提案します。画像再構成のための深層学習ベースのアプローチの最近の成功に触発され、イメージングシステムの学習ベースの設計では、MRIでの3D非デカルト加速軌道のデータ駆動型設計のための新しいプロトコルである3D FLATを導入します。MRIの最も重要な欠点は、取得時間が長く、標準的なプラクティスでの使用が禁止されていることです。一部のアプリケーション。 
[ABSTRACT] mri mri mriスキャンは、一部のアプリケーションの標準的な実践では使用できませんでした。ただし、圧縮スペースの利点は十分に活用されていません。これらの座標は、mriマシンの厳しい制約に従う軌道を生成しません実際に課された</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Adding Seemingly Uninformative Labels Helps in Low Data Regimes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_0.html">
      <font color="black">Adding Seemingly Uninformative Labels Helps in Low Data Regimes</font>
    </a>
  </h2>
  <font color="black">エキスパートデータが少ない場合にこれらの利益が増加することを明らかにし、さらなる調査を通じていくつかの興味深い特性を明らかにします。ここで紹介する新しいデータセットであるCSAW-Sに関する調査結果を示し、2つのパブリックデータセットでそれらを確認します。この作業では、マンモグラフィ画像での腫瘍のセグメンテーションである、入手が困難な専門家の注釈を必要とするタスクを検討します。 
[ABSTRACT]データ量の少ない設定では、一見すると情報が少ないように見えるラベルでエキスパートアノテーションを補完することにより、パフォーマンスを改善できることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-20">
        <br><font color="black">2020-07-20</font>
      </time>
    </span>
</section>
<!-- paper0: PROFIT: A Novel Training Method for sub-4-bit MobileNet Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_1.html">
      <font color="black">PROFIT: A Novel Training Method for sub-4-bit MobileNet Models</font>
    </a>
  </h2>
  <font color="black">ImageNetでMobileNet-v1、v2、およびv3を量子化することにより、提案された方法を評価し、4ビット量子化が完全なベースラインに匹敵する（1.48％以内の精度で）精度を提供することを報告します。AIWQ問題を緩和するために、他の層よりも強い不安定性の問題によって重みが影響を受ける層を凍結しようとするプログレッシブフリーズ反復トレーニング（PROFIT）と呼ばれる新しいトレーニング方法。増え続けるため、4ビット以下の精度のモバイルモデルが必要です。モバイルデバイスのエネルギー効率の向上に対する需要。 
[ABSTRACT]提案された方法は、最先端の方法よりもパフォーマンスが大幅に優れています。12の86％のトップ1の精度</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Exposing Deep-faked Videos by Anomalous Co-motion Pattern Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_2.html">
      <font color="black">Exposing Deep-faked Videos by Anomalous Co-motion Pattern Detection</font>
    </a>
  </h2>
  <font color="black">特に「DeepFake」などのIDを偽造できるアプリケーションでの最近のディープラーニングベースのビデオ合成アプローチは、セキュリティ上の大きな懸念を引き起こしています。このホワイトペーパーでは、特に、偽のビデオ..このような結合パターンは、ビデオコンテンツから独立しているローカルモーション機能全体でマイニングされるため、インスタンスごとの変動も大幅に軽減できます。 
[ABSTRACT]新しい方法を使用して、ディープフェイクビデオを公開できます。これらのタイプのコンジョイントパターンは、ローカルモーション機能全体でマイニングされます。これらには、ビデオコンテンツに依存しないモーション機能が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: The Umbrella software suite for automated asteroid detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_3.html">
      <font color="black">The Umbrella software suite for automated asteroid detection</font>
    </a>
  </h2>
  <font color="black">ライブラリに基づいて、デスクトッププログラム（ViaNearby）とWebサーバー（Webrella）の両方からアクセス可能な検出パイプラインも実装しました。これは、Wideでのいくつかの小惑星調査のほぼリアルタイムのデータ削減に成功しています。アイザックニュートン望遠鏡のフィールドカメラ。小惑星の検出、検証、識別、および報告用のアンブレラソフトウェアスイートを紹介します。このホワイトペーパーでは、使用可能なインターフェイスとアルゴリズムに焦点を当て、ライブラリについて説明します。小惑星検出のベンチマークとしてEURONEARプロジェクトで使用されている、厳選されたフィールドセットのデスクトップバージョン。 
[ABSTRACT] umbrella2はオープンソースのモジュール式ライブラリで、処理パイプラインのすべてのステップのアルゴリズムとインターフェイスが含まれています。ウェブベースのライブラリは、euronearプロジェクトで小惑星検出ベンチマークとして使用されている厳選された一連のフィールドに基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to See Through Obstructions with Layered Decomposition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_4.html">
      <font color="black">Learning to See Through Obstructions with Layered Decomposition</font>
    </a>
  </h2>
  <font color="black">具体的には、2つのレイヤーの密なオプティカルフローフィールドの推定と、深い畳み込みニューラルネットワークを介してフローワープされた画像から各レイヤーを再構築することを交互に行います。合成で生成されたデータのトレーニングが実際の画像にうまく転送されることを示します。反射とフェンスの除去の多くの挑戦的なシナリオは、提案された方法の有効性を示しています。 
[ABSTRACT]新しい方法により、フロー推定の潜在的なエラーと輝度などの脆弱な仮定に対応できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Attention-based 3D Object Reconstruction from a Single Image -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_5.html">
      <font color="black">Attention-based 3D Object Reconstruction from a Single Image</font>
    </a>
  </h2>
  <font color="black">そのため、ローカルリージョンに基づく機能ではなく補完的な入力機能を活用するために、ネットワークエンコーダー内の自己注意の概念を適用し、エンコーダーがグローバル情報を抽出するのを支援します。また、私たちのアプローチがはるかに一貫性のあるメッシュを生成でき、現在の最先端技術よりも一般化力が向上していることを確認しています。このアプローチにより、メッシュIoUの5.05％、通常の一貫性の0.83％でオリジナルの作業を改善できました。 Chamfer-L1距離の10倍以上。 
[ABSTRACT]コンピュータービジョンコミュニティは、オブジェクトとシーンの3D表面全体を再構築する機能の開発に多大な努力を費やしました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Accurate Bounding-box Regression with Distance-IoU Loss for Visual
  Tracking -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_6.html">
      <font color="black">Accurate Bounding-box Regression with Distance-IoU Loss for Visual
  Tracking</font>
    </a>
  </h2>
  <font color="black">ほとんどの既存の追跡方法は、分類器とマルチスケール推定を使用してターゲットの状態を推定することに基づいています。さらに、オンラインでトレーニングされ、共役勾配ベースの戦略で最適化され、リアルタイムを保証する分類コンポーネントを導入します追跡速度..包括的な実験結果は、リアルタイムの追跡速度でありながら、最先端のトラッカーと比較して、DIoUTrackが競争力のある追跡精度を達成していることを示しています。 
[要約]この論文では、距離-iou（diou）損失に基づく新しい追跡方法を提案することによって問題に対処します。提案された追跡方法は、ターゲット推定コンポーネントとターゲット分類コンポーネントで構成されています。dioutrackは利点を維持できます2つの境界ボックスの中心点間の距離を最小化しながら、iou損失によって提供されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-03">
        <br><font color="black">2020-07-03</font>
      </time>
    </span>
</section>
<!-- paper0: Robust Long-Term Object Tracking via Improved Discriminative Model
  Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_7.html">
      <font color="black">Robust Long-Term Object Tracking via Improved Discriminative Model
  Prediction</font>
    </a>
  </h2>
  <font color="black">（3）より特徴的な学習のためのバックグラウンド拡張：検索領域に含まれていないさまざまなバックグラウンドを拡張して、バックグラウンドクラッタでよりロバストなモデルをトレーニングします。トラッカーRLT-DiMPは、次の3つの側面でSuperDiMPを改善します。（1 ）ランダム消去を使用した不確実性の低減：モデルをロバストにするために、ランダムな小さな長方形の領域を確実に消去した後、複数の画像からの一致を利用します。次に、それに応じてモデルの追跡状態を修正します。 
[ABSTRACT] superdimpは、提案されたprdimpのsuperdimpリグレッサを標準のdimp分類子と組み合わせます。superdimpは、superdimpと呼ばれる短期モデルに基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Extension of JPEG XS for Two-Layer Lossless Coding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_8.html">
      <font color="black">Extension of JPEG XS for Two-Layer Lossless Coding</font>
    </a>
  </h2>
  <font color="black">その結果、JPEG XSとの互換性を維持しながら、元の画像を可逆的に復元できます。JPEGXSと互換性のある2層の可逆画像符号化方式が提案されています。JPEGXSは、静止画像符号化の新しい国際標準です。非常に低いレイテンシと非常に低い複雑さの特性。 
[要約]提案された方法は、jpeg xtに似た2層構造を持っています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Cluster under Domain Shift -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_9.html">
      <font color="black">Learning to Cluster under Domain Shift</font>
    </a>
  </h2>
  <font color="black">具体的には、トレーニング時に新しい情報理論上の損失を最適化することを提案します。これは、ドメインアライメントレイヤーと相まって、モデルがドメイン固有の機能を破棄しながらセマンティックラベルを正しく検出することを保証します。この作業では、この仮定を克服し、ソースデータとターゲットデータの両方に注釈がない場合に、ソースドメインからターゲットドメインに知識を転送する問題に対処します。いくつかのドメイン適応ベンチマークを考慮して、提案されたアプローチをさまざまな設定で評価し、この方法で少数のターゲットサンプルが存在する場合でも、関連するセマンティック情報を自動的に検出し、複数のドメイン適応ベンチマークで最先端の結果を生成します。 
[ABSTRACT]この概念は、ディープクラスタリングの最近の作品から発想を得ています。複数のソースドメインから収集されたデータを使用して、ドメインを構築します-分析クラスタリングモデル。その後、洗練されます。ターゲットデータが利用可能になると、モデルをターゲットドメインに適合させることができますソースデータにアクセスできない</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Stereo Matchability in Disparity Regression Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_10.html">
      <font color="black">Learning Stereo Matchability in Disparity Regression Networks</font>
    </a>
  </h2>
  <font color="black">提案されたディープステレオマッチング機能（DSM）フレームワークは、品質を保証しながら、マッチング結果を改善したり、計算を高速化したりできます。シーンフローとKITTIステレオデータセットを使用して、最先端の学習ベースのステレオ法に対する提案されたフレームワークの有効性を実証します。 
[ABSTRACT]提案されたディープステレオマッチング機能（dsded）フレームワークは、品質を保証しながら、マッチング結果を改善するか、計算を高速化できます。提案されたネットワークを使用して、ピクセル単位のマッチング機能を考慮したステレオマッチングネットワークを開発できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: GeLaTO: Generative Latent Textured Objects -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_11.html">
      <font color="black">GeLaTO: Generative Latent Textured Objects</font>
    </a>
  </h2>
  <font color="black">プロキシは、対応するニューラルテクスチャで個別にラスタライズされ、アルファマップを含む出力の写実的な画像を生成するU-Netを使用して合成されます。疎なビューのセットから複雑なオブジェクトを再構築することにより、アプローチの有効性を示します。また、これらの粗いプロキシは、基本的なオブジェクトジオメトリが眼鏡のように簡単にモデル化できる場合や、車などのより複雑なカテゴリのニューラルネットワークを使用して生成できる場合に手作りできることも示しています。 
[要約]ジェラートモデルは、一連の薄い形状のプロキシと学習神経テクスチャを組み合わせています。プロキシは、対応する神経テクスチャで個別にラスタライズされ、オーネットを許可します。これにより、アルファマップを含む出力の写実的な画像が生成されます。古典的な方法を使用して再構築することが難しい眼鏡フレームの実画像のデータセット</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Tensor Sparse PCA and Face Recognition: A Novel Approach -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_12.html">
      <font color="black">Tensor Sparse PCA and Face Recognition: A Novel Approach</font>
    </a>
  </h2>
  <font color="black">実験結果は、テンソルスパースPCAと任意の分類システムの組み合わせが、常に最高の精度のパフォーマンス指標に到達するとは限らないことを示しています。いくつか例を挙げると、軍事、金融、公安など、多くのアプリケーションがあります。スパースPCAメソッドと1つの特定の分類システムの組み合わせの精度は、PCAメソッドと1つの特定の分類システムの組み合わせの精度より常に優れており、分類システム自体の精度よりも常に優れています。 
[ABSTRACT]このシステムには、軍事、金融、公安などの多くのアプリケーションがあります。データの有効性をテストするために使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-12">
        <br><font color="black">2019-04-12</font>
      </time>
    </span>
</section>
<!-- paper0: AtrialJSQnet: A New Framework for Joint Segmentation and Quantification
  of Left Atrium and Scars Incorporating Spatial and Shape Information -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_13.html">
      <font color="black">AtrialJSQnet: A New Framework for Joint Segmentation and Quantification
  of Left Atrium and Scars Incorporating Spatial and Shape Information</font>
    </a>
  </h2>
  <font color="black">この作業では、新しいフレームワーク、すなわち、LAセグメンテーション、LA表面への瘢痕投影、および瘢痕定量化がエンドツーエンドスタイルで同時に実行されるAtrialJSQnetを開発します。具体的には、SAスキームは共同LAセグメンテーションと瘢痕定量化を実行するマルチタスクアーキテクチャ。以前の方法は通常、2つのタスクを個別に解決し、LAと瘢痕の間の固有の空間関係を無視していました。 
[要旨]コードと結果は、原稿が受理されると公にリリースされます。コードのリリース時に公開されます。これは、年末までに公開される予定です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Online Continual Learning under Extreme Memory Constraints -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_14.html">
      <font color="black">Online Continual Learning under Extreme Memory Constraints</font>
    </a>
  </h2>
  <font color="black">継続的学習（CL）は、過去の経験から得られた知識を保持しながら、新しいタスクを順次学習する人間の能力をエミュレートするエージェントを開発することを目的としています。 MC-OCLの問題は、より高いメモリオーバーヘッドを必要とする以前の蒸留法と同等の精度を実現します。すべてではないにしても、ほとんどの以前のCL法はこれらの制約に違反しているため、MC-OCLのアルゴリズムソリューションを提案します。バッチレベル蒸留（BLD）、正規化ベースのCLアプローチ。データストリームから学習するために安定性と可塑性のバランスを効果的に保ちながら、蒸留によって古いタスクを解決する機能を維持します。 
[ABSTRACT]継続的な学習（mc-ocl）は、可能なアルゴリズムが破局的な忘却を回避するために使用できるメモリオーバーヘッドに厳しい制約を課します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-04">
        <br><font color="black">2020-08-04</font>
      </time>
    </span>
</section>
<!-- paper0: MHSA-Net: Multi-Head Self-Attention Network for Occluded Person
  Re-Identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_15.html">
      <font color="black">MHSA-Net: Multi-Head Self-Attention Network for Occluded Person
  Re-Identification</font>
    </a>
  </h2>
  <font color="black">広範なアブレーション研究を通じて、構造化された自己注意ブランチと注意競争メカニズムの両方がMHSA-Netのパフォーマンス向上に貢献していることを確認しました。MHSAMは主要な地元の人の情報を適応的にキャプチャし、画像の効果的な多様性埋め込みを生成します。人のマッチング..私たちのMHSA-Netは、特にオクルージョンのある画像で最先端のパフォーマンスを実現します。 
[ABSTRACT] mhsa-紙には2つの新しいコンポーネントが含まれています：マルチヘッド自己注意ブランチ（mhsab）と注意競合メカニズム（acm）</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-10">
        <br><font color="black">2020-08-10</font>
      </time>
    </span>
</section>
<!-- paper0: DTVNet: Dynamic Time-lapse Video Generation via Single Still Image -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_16.html">
      <font color="black">DTVNet: Dynamic Time-lapse Video Generation via Single Still Image</font>
    </a>
  </h2>
  <font color="black">テスト段階では、1つの入力画像のみに基づいて、同じコンテンツでさまざまなモーション情報を持つビデオを異なる\ emph {正規化モーションベクトル}で生成できます。DVGには、モーションとコンテンツストリームから学習するモーションストリームとコンテンツストリームが含まれています。共有コンテンツの機能を学習し、それぞれ対応するモーションでビデオフレームを構築するためのエンコーダーとデコーダーと同様に、それぞれ画像。\ emph {モーションストリーム}は、統合する複数の\ emph {適応インスタンス正規化}（AdaIN）レイヤーを導入します。リニアレイヤーで処理されるマルチレベルのモーション情報。 
[要約]提案されたdtvnetは2つのサブモジュールで構成されています。dvgには、モーションと単一の画像から学習するモーションストリームとコンテンツストリームが含まれています。共有コンテンツ機能を学習するためのエンコーダーとデコーダーも含まれています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_17.html">
      <font color="black">PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions</font>
    </a>
  </h2>
  <font color="black">特に、Wide ResNetと比較して、私たちの方法は12.6％のパラメーターのみを使用してより良い結果をもたらします。実装では、PDOの数値スキームを使用してシステムを離散化し、ほぼ等変の畳み込み（PDO-eConvs）を導出します。この作業では、この問題は、畳み込みと偏微分演算子（PDO）の間の接続から処理します。 
[ABSTRACT]理論的には、pdosを変換し、より一般的な連続グループである$ n $ -------- story.butと同等のシステムを提案します。問題、対応する等分散-変換グループの保持は非常に限られています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-20">
        <br><font color="black">2020-07-20</font>
      </time>
    </span>
</section>
<!-- paper0: Deep CNNs Meet Global Covariance Pooling: Better Representation and
  Generalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_18.html">
      <font color="black">Deep CNNs Meet Global Covariance Pooling: Better Representation and
  Generalization</font>
    </a>
  </h2>
  <font color="black">これは、共分散間のパワーユークリッドメトリックと見なすこともでき、それらのジオメトリを効果的に活用します。これらの課題に対処するために、グローバルなマトリックスパワー正規化COVariance（MPN-COV）プーリングを提案します。MPN-COVネットワークの高速トレーニングについては、反復行列平方根正規化を実装し、MPN-COVに固有のGPUの非友好的な固有分解を回避します。 
[要約]深いCNNへのグローバルな共分散プールには2つの課題があります。これらの課題には、グローバルガウス埋め込みネットワークからの証言が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-15">
        <br><font color="black">2019-04-15</font>
      </time>
    </span>
</section>
<!-- paper0: A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal
  Transformer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_19.html">
      <font color="black">A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal
  Transformer</font>
    </a>
  </h2>
  <font color="black">パフォーマンスは、私たちのモデルが優れたパフォーマンスを達成する挑戦的なActivityNetキャプションデータセットで実証されます。高密度のビデオキャプションタスクでオーディオとビジュアルのモダリティを使用して提案されたモデルの有効性を示しますが、モジュールは2つのモダリティをシーケンスからシーケンスへのタスク。また、バイモーダルトランスフォーマーの一部として事前トレーニングされたバイモーダルエンコーダーが、単純な提案生成モジュールの特徴抽出器として使用できることを示します。 
[ABSTRACT]バイモーダルトランスフォーマーモデルは、シンプルなプロポーザル生成モジュールの特徴エクストラクターとして使用できます。事前トレーニング済みのバイモーダルエンコーダーは、バイモーダルモデルの一部です。フィーチャーの作成にも使用できます。提案生成のためのキャプター</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-17">
        <br><font color="black">2020-05-17</font>
      </time>
    </span>
</section>
<!-- paper0: Compressing 3DCNNs Based on Tensor Train Decomposition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_20.html">
      <font color="black">Compressing 3DCNNs Based on Tensor Train Decomposition</font>
    </a>
  </h2>
  <font color="black">VIVAチャレンジ、UCF11、およびUCF101データセットに基づく複数のコントラスト実験に照らして、TT分解は3DCNNを約100倍圧縮することができ、大幅な精度の損失はなく、広範な現実世界のシナリオでのアプリケーションを可能にします。 3D畳み込みカーネルをTT形式でテンソル化し、より高い圧縮率を達成するために適切なTTランクを選択する方法を調査します。3次元の畳み込みニューラルネットワーク（3DCNN）は、ビデオや3D点群認識など、多くのタスクに適用されています。 
[要約] 3dcnnsのスペースの複雑さは、一般に2dcnnのスペースの複雑さよりも大きい。ミニニューラルネットワーク（2dcnn）は、3dcnnモデルを縮小できる</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-08">
        <br><font color="black">2019-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: Thick Cloud Removal of Remote Sensing Images Using Temporal Smoothness
  and Sparsity-Regularized Tensor Optimization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_21.html">
      <font color="black">Thick Cloud Removal of Remote Sensing Images Using Temporal Smoothness
  and Sparsity-Regularized Tensor Optimization</font>
    </a>
  </h2>
  <font color="black">したがって、スパース性ノルムは、雲と雲の影のスパース性を高めるために使用され、単方向の全変動（UTV）レギュライザーが適用されて、一方向の滑らかさを保証します。一連の実験は、シミュレートされた画像と実際の雲で汚染された画像の両方で行われます。異なるセンサーから、異なる解像度で、そして結果は、質的および量的視点の両方から雲と雲の影を取り除くための提案されたTSSTO方法の可能性を実証します。リモートセンシング画像では、雲の影を伴う厚い雲の存在が高い確率ですイベント。後続の処理の品質に影響を与え、アプリケーションのシナリオを制限する可能性があります。 
[ABSTRACT]一連の実験は、シミュレーションされたクラウドと実際のクラウドの両方で行われます-さまざまなセンサーからのさまざまな解像度の汚染された画像</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Orientation Distributions for Object Pose Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_22.html">
      <font color="black">Learning Orientation Distributions for Object Pose Estimation</font>
    </a>
  </h2>
  <font color="black">私たちの評価では、さまざまな種類のオブジェクトの不確実性を推定するために、手法を多数のベースラインアプローチと比較します。ロボットが現実の世界でロバストに動作するには、不確実性を認識している必要があります。この方法により、最高のパフォーマンスが得られます対称性が不明なオブジェクトに対して、対称性と非対称性の両方のオブジェクトを正確にモデリングします。対称性の注釈は必要ありません。 
[ABSTRACT]私たちの方法を使用して、既存の姿勢推定器を拡張できることを示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-02">
        <br><font color="black">2020-07-02</font>
      </time>
    </span>
</section>
<!-- paper0: KBGN: Knowledge-Bridge Graph Network for Adaptive Vision-Text Reasoning
  in Visual Dialogue -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_23.html">
      <font color="black">KBGN: Knowledge-Bridge Graph Network for Adaptive Vision-Text Reasoning
  in Visual Dialogue</font>
    </a>
  </h2>
  <font color="black">さらに、視覚的対話の推論の手掛かりは、モーダル内のエンティティとモーダル間のブリッジから明確に引き出すことができます。古典的なアプローチは、現在の質問、ビジョンの知識、テキストの知識の統合により多くの注意を払い、クロスモーダル情報.. VisDial v1.0およびVisDial-Qデータセットの実験結果は、モデルが既存のモデルよりも最新の結果で優れていることを示しています。 
[ABSTRACT]従来のアプローチでは、ビジョンの知識とテキストの知識の統合により多くの注意を払っています。代わりに、新しい研究では、最新のモデルを使用して既存のモデルよりもパフォーマンスが優れていることが示されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: ClimAlign: Unsupervised statistical downscaling of climate variables via
  normalizing flows -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_24.html">
      <font color="black">ClimAlign: Unsupervised statistical downscaling of climate variables via
  normalizing flows</font>
    </a>
  </h2>
  <font color="black">低（1度の緯度/経度）と高（1/4と1/8度）の解像度でグリッド化された毎日の気温と降水量の値からなる2つのデータセットのいくつかの異なるメトリックを使用して、メソッドの実行可能性を評価します。ダウンスケーリングはランドマークです気候科学と気象学のタスクであり、目標は、粗いスケールの時空間データを使用して、より細かいスケールで値を推定することです。GitHubで、一般にアクセス可能な方法の実装と、比較に使用されるベースラインを提供しています。 
[要旨]監視なしの厳格なダウンスケーリングの新しい方法であるclimalignを提示します。これは、低再スケーリングのフローの正規化における最近の作業の適応を使用します。この方法は、既存の監視付きの統計的ダウンスケーリング方法に匹敵する予測パフォーマンスを実現します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: TCL: an ANN-to-SNN Conversion with Trainable Clipping Layers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_25.html">
      <font color="black">TCL: an ANN-to-SNN Conversion with Trainable Clipping Layers</font>
    </a>
  </h2>
  <font color="black">ただし、最近のいくつかの調査では、SNNを直接トレーニングする代わりにANNをSNNに変換することでこの課題に対処できることが示されていますが、SNNのレイテンシが大きいと、アプリケーションが制限され、Imagenetなどの大規模なデータセットでは問題が発生しやすくなります。 SNNでは精度と待ち時間の間にトレードオフの関係があるため、この問題を克服することは困難です。TCLを従来のデータ正規化手法と組み合わせることにより、VGGに対してそれぞれ71.12％および73.38％（ImageNetで）を取得します。 ANNからSNNへの変換後、250サイクルのレイテンシ制約で-16およびRESNET-34。 
[ABSTRACT] snnbでは、精度とレイテンシの間にトレードオフの関係があり、snnsでは、不正確なデータとsnns.weの間にニューラルリンクがあり、71。12％と73. 38％（ on imagenet）vgg-16およびresnet-34の場合、annからsnnへの変換後</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Better Surgical Instrument Segmentation in Endoscopic Vision:
  Multi-Angle Feature Aggregation and Contour Supervision -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_26.html">
      <font color="black">Towards Better Surgical Instrument Segmentation in Endoscopic Vision:
  Multi-Angle Feature Aggregation and Contour Supervision</font>
    </a>
  </h2>
  <font color="black">第2に、エンドツーエンドのトレーニングステージでは、補助輪郭監視を使用してモデルが境界認識を学習するように誘導し、セグメンテーションマスクの輪郭形状をより正確にします。ネットワーク（DNN）モデルは近年設計されています。提案された方法は、外科医の手術から収集された新規の副鼻腔手術データセットのアブレーション実験で検証され、da Vinci Xiで収集されたパブリックデータセットの既存の方法と比較されます。ロボット。 
[ABSTRACT]近年、ますます多くのディープニューラルネットワーク（dnn）モデルが設計されています。これらのモデルは、限られた回転に基づいています-dnnの不変性のパフォーマンス。提案された方法は、新しいsinusのアブレーション実験で検証されています。 da vinci xiロボットで収集された公開データセットの既存のメソッド</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-25">
        <br><font color="black">2020-02-25</font>
      </time>
    </span>
</section>
<!-- paper0: PiNet: Attention Pooling for Graph Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_27.html">
      <font color="black">PiNet: Attention Pooling for Graph Classification</font>
    </a>
  </h2>
  <font color="black">グラフレベルの分類にグラフの畳み込み演算を利用するための一般化された微分可能な注意ベースのプーリングメカニズムであるPiNetを提案します。標準的な化学情報学データセットのアート手法。 
[要約]同型グラフクラスの識別において、他のグラフニューラルネットワークよりも高いサンプル効率と優れたパフォーマンスを実証しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Efficient Crowd Counting via Structured Knowledge Transfer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_28.html">
      <font color="black">Efficient Crowd Counting via Structured Knowledge Transfer</font>
    </a>
  </h2>
  <font color="black">これらの群集カウントモデルを解放するために、十分にトレーニングされた教師ネットワークの構造化された知識を完全に活用して、軽量でありながら非常に効果的な学生ネットワークを生成する、新しい構造化知識転送（SKT）フレームワークを提案します。元のモデルのパラメーターと計算コストの$ 6 \％$である、蒸留されたVGGベースのモデルは、Nvidia 1080 GPUで少なくとも6.5 $ \ times $の高速化を実現し、最先端のパフォーマンスを実現します。コードとモデルは{\ url {https://github.com/HCPLab-SYSU/SKT}}で入手できます。 
[ABSTRACT]私たちの学生ネットワークは、コンパクトで効果的な機能を学ぶために、教師ネットワークから層-賢い層および層を超えた知識を引き出すことができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-23">
        <br><font color="black">2020-03-23</font>
      </time>
    </span>
</section>
<!-- paper0: Left Ventricular Wall Motion Estimation by Active Polynomials for Acute
  Myocardial Infarction Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_29.html">
      <font color="black">Left Ventricular Wall Motion Estimation by Active Polynomials for Acute
  Myocardial Infarction Detection</font>
    </a>
  </h2>
  <font color="black">いわゆるHMC-QUデータベースは、今後の関連する研究のベンチマークとして機能します。この方法の出力は、エコー技術者が心エコー図記録の品質を評価および改善するのにさらに役立ちます。これにより、医療専門家はさらに色分けされたセグメントによるエコー画像の強化された視覚化機能と「最大運動変位」プロットにより、壁の運動とLV駆出率（LVEF）をより適切に評価できます。 
[要約]提案されたアプローチは、左心室（lv）壁の全体的な動きを正確かつロバストに推定できます。これにより、医療専門家は、色分けされたセグメントを通じてエコー画像の視覚化機能を強化できます。この方法は、最初のパブリックエコーです。カタールのハマド医療法人病院の医師が作成したデータベースコレクション</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Transfer Learning for Protein Structure Classification and Function
  Inference at Low Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_30.html">
      <font color="black">Transfer Learning for Protein Structure Classification and Function
  Inference at Low Resolution</font>
    </a>
  </h2>
  <font color="black">最後に、高解像度、低解像度、NMRで決定された構造が共通の特徴空間に生息していることを確認し、ドメイン間のマッピングが解像度を高めるための理論的基礎を提供します。構造決定は、分子レベルでタンパク質機能を理解するための鍵です..したがって、我々は低解像度での高速、低コストのタンパク質構造分類の概念実証を提供します。 
[ABSTRACT]新しい研究は、低解像度で高速、低コストのタンパク質構造分類の概念実証を提供することを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: COVID-19 detection using Residual Attention Network an Artificial
  Intelligence approach -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_31.html">
      <font color="black">COVID-19 detection using Residual Attention Network an Artificial
  Intelligence approach</font>
    </a>
  </h2>
  <font color="black">私たちの仕事は、臨床診療におけるAI支援アプリケーションの適応を高めるのに役立ちます。このホワイトペーパーでは、人工知能を使用してCOVID-19を検出する手法を紹介します。患者を隔離することができます。 
[ABSTRACT] covidのテスト-19は発生を制御するために重要です。ウイルスは急速に感染します。繁殖数は2ドルです。2-2から7ウイルスの効果的なテストは、発生を制御するために不可欠です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-26">
        <br><font color="black">2020-06-26</font>
      </time>
    </span>
</section>
<!-- paper0: Quantitative Analysis of Image Classification Techniques for
  Memory-Constrained Devices -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_32.html">
      <font color="black">Quantitative Analysis of Image Classification Techniques for
  Memory-Constrained Devices</font>
    </a>
  </h2>
  <font color="black">これは、メモリが不足していることが多い組み込みデバイスに依存するアプリケーションでの有用性を制限します。しかし、より複雑なマルチクラスおよびマルチチャネル画像分類の可能性はまだ決定されていません。最近、大きな進歩がありましたProtoNN、Bonsai、FastGRNNアルゴリズムなどの新しい貢献により、このようなメモリに制約のあるデバイスの画像分類の分野で。 
[要約]これらは、光学式文字認識で最大98.2％の精度に達することが示されています。ただし、メモリフットプリントはわずか6 kbです。これらは、cifar-10を使用した画像分類と比較されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-11">
        <br><font color="black">2020-05-11</font>
      </time>
    </span>
</section>
<!-- paper0: Rethinking Pseudo-LiDAR Representation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_33.html">
      <font color="black">Rethinking Pseudo-LiDAR Representation</font>
    </a>
  </h2>
  <font color="black">さらに、PatchNetの疑似LiDARデータは画像表現として編成されているため、既存の2D CNN設計を簡単に利用して、入力データから深い特徴を抽出し、3D検出パフォーマンスを向上させることができます。このホワイトペーパーでは、深さを調査し、疑似LiDAR表現の有効性がデータ表現自体ではなく座標変換に由来することを確認します。コードはhttps://github.com/xinzhuma/patchnetで入手できます。 
[要約]研究者のチームは、patchという名前の画像ベースのCNN検出器を開発しました-net.itは、疑似ライダーとしてベースの3D検出器としてインスタンス化できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Sharp Multiple Instance Learning for DeepFake Video Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_34.html">
      <font color="black">Sharp Multiple Instance Learning for DeepFake Video Detection</font>
    </a>
  </h2>
  <font color="black">さらに、S-MILを従来のDeepFake画像検出タスクに適合させて、単一フレームデータセットで最先端のパフォーマンスを実現することもできます。部分的に操作された顔を正確に組み込むことができるインスタンスを生成するには、時空間エンコードインスタンスは、フレーム内およびフレーム間の不整合を完全にモデル化するように設計されています。これは、検出パフォーマンスをさらに向上させるのに役立ちます。理論的分析は、従来のMILでの勾配の消失がS-MILで緩和されることを証明します。 
[ABSTRACT]既存の方法は、主に単一フレームの検出用に設計されています。しかし、これはディープフェイク攻撃者に潜在的な高いリスクを残しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Deep UAV Localization with Reference View Rendering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_35.html">
      <font color="black">Deep UAV Localization with Reference View Rendering</font>
    </a>
  </h2>
  <font color="black">評価では、ディープ6DoF-ICLKアルゴリズムが、トレーニング不可能なものよりもはるかに優れていることを示しています。環境の変化の下での調整を学習するために、アーキテクチャは、高解像度で複数年にわたるマップを使用してトレーニングされます。このフィールド、リアルタイムレンダリングエンジン、および付随するデータセットは、この出版物とともにリリースされます。 
[ABSTRACT]リアルタイムレンダリングエンジンは、光学画像と深度画像を生成します。リアルタイムマッピングエンジンを使用して、環境の変化を学習します。ソフトウェアは、車両のリアルタイムマップを使用します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Confronting the Constraints for Optical Character Segmentation from
  Printed Bangla Text Image -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_36.html">
      <font color="black">Confronting the Constraints for Optical Character Segmentation from
  Printed Bangla Text Image</font>
    </a>
  </h2>
  <font color="black">私たちが提案するアルゴリズムは、スキャンまたはキャプチャされた画像の理想的なケースと非理想的なケースの両方から文字をセグメント化して、持続可能な結果をもたらすことができます。これらの手順は、印刷画像を変換の準備ができるようにするための精度と持続可能な結果への扉を開きます。完全に機能するためには、システムは前処理やセグメンテーションなどのいくつかの重要な方法を実行する必要があります。 
[ABSTRACT]光学式文字認識システムは、印刷された画像を食用テキストに変換するように設計されています。印刷データを食品に変換するために使用できます。プロジェクトは現在、英国で開発されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-18">
        <br><font color="black">2020-03-18</font>
      </time>
    </span>
</section>
<!-- paper0: SAFRON: Stitching Across the Frontier for Generating Colorectal Cancer
  Histology Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_37.html">
      <font color="black">SAFRON: Stitching Across the Frontier for Generating Colorectal Cancer
  Histology Images</font>
    </a>
  </h2>
  <font color="black">SAFRONを使用して生成されたサンプルの高解像度画像は、https：//warwick.ac.uk/TIALab/SAFRONで入手できます。この課題に対処するために、形態学的特徴を維持しながら継ぎ目の境界アーティファクトを最小限に抑えながら、グランドトゥルースアノテーションから現実的な大きな組織画像タイルを構築するSAFRONという新しいフレームワークを提案します。合成画像は、深部の開発と評価に使用できます。アノテーションの限られた可用性のコンテキストでの学習アルゴリズム。 
[要約]合成データは、結腸直腸癌組織画像の腺セグメンテーションの標準アルゴリズムのパフォーマンスを大幅に向上させることができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Surgical Mask Detection with Convolutional Neural Networks and Data
  Augmentations on Spectrograms -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_38.html">
      <font color="black">Surgical Mask Detection with Convolutional Neural Networks and Data
  Augmentations on Spectrograms</font>
    </a>
  </h2>
  <font color="black">ここでのアイデアは、説明不足の小さなトレーニングデータセットの特徴分布へのモデルの過剰適合を減らすことです。人間の声のサンプルでのサージカルマスク検出のバイナリ分類タスクに対するデータ拡張の影響を示します（ComParE Challenge 2020）。 ..結果は、ComParEによって提供されたベースラインのほとんどがパフォーマンスが優れていることを示しています。 
[ABSTRACT]これは、データ拡張がトレーニングデータの欠如を克服することを約束する場所です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Key-Nets: Optical Transformation Convolutional Networks for Privacy
  Preserving Vision Sensors -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_39.html">
      <font color="black">Key-Nets: Optical Transformation Convolutional Networks for Privacy
  Preserving Vision Sensors</font>
    </a>
  </h2>
  <font color="black">現代のカメラは、ターゲットアプリケーションとしてコンピュータービジョンや機械学習を使用して設計されていません。キーネットに適した光学変換のための5つの十分な条件を提供し、その一般化された確率行列を示します（たとえば、最後に、 netは、Hill暗号を使用した準同型暗号化と同等であり、メモリとランタイムに上限があり、ユーザー指定のプライバシーパラメータを使用して2次スケーリングされます。 homoratically.itは、それなしではネットワークのユーティリティ/プライバシーのトレードオフが微調整されることを示すように設計されています-顔識別とオブジェクト検出のために光学的に変換された画像に直接調整されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Detecting Urban Dynamics Using Deep Siamese Convolutional Neural
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_40.html">
      <font color="black">Detecting Urban Dynamics Using Deep Siamese Convolutional Neural
  Networks</font>
    </a>
  </h2>
  <font color="black">モデルは、これらのメジャーのほとんどの点で優れたパフォーマンスを達成しており、都市化が進行しているさまざまな時期におけるMekelleおよび他の都市の変化を検出するために使用できます。提案された検出機能は、全体的な精度（95.8）の観点から測定されました。 、カッパメジャー（72.5）、リコール（76.5）、精度（77.7）、F1メジャー（77.1）。変更の検出は、コンピュータービジョンとリモートセンシングの分野で急速に成長している分野です。 
[要約]モデルを使用して、都市化が進行しているさまざまな時期におけるメケルや他の都市の変化を検出できます。このモデルは、これらの対策のほとんどの点で優れたパフォーマンスを達成しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Comparative study of deep learning methods for the automatic
  segmentation of lung, lesion and lesion type in CT scans of COVID-19 patients -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_41.html">
      <font color="black">Comparative study of deep learning methods for the automatic
  segmentation of lung, lesion and lesion type in CT scans of COVID-19 patients</font>
    </a>
  </h2>
  <font color="black">一般に、さまざまな病変タイプを区別するタスクはより難しく、平均絶対容積の差は152 mlであり、統合およびすりガラスの不透明度についてそれぞれ平均ダイススコアが0.369および0.523でした。さまざまな方法を組み合わせることで、肺セグメンテーション、バイナリ病変セグメンテーション、マルチクラス病変セグメンテーションの全体的なテストセットのパフォーマンスは、それぞれ平均ダイススコアが0.982、0.724、0.469になります。結果として得られたバイナリ病変は、91.3 mlの平均絶対ボリュームエラーでセグメント化されました。 
[要約]ディープラーニングを使用して、胸部CTスキャンを使用したcovid-19の迅速かつ正確な定量化を提供する研究が提案されています。これらには、オープンソースや自社開発のアルゴリズムを含む、複数の複数のタイプの研究が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-29">
        <br><font color="black">2020-07-29</font>
      </time>
    </span>
</section>
<!-- paper0: ARPM-net: A novel CNN-based adversarial method with Markov Random Field
  enhancement for prostate and organs at risk segmentation in pelvic CT images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_42.html">
      <font color="black">ARPM-net: A novel CNN-based adversarial method with Markov Random Field
  enhancement for prostate and organs at risk segmentation in pelvic CT images</font>
    </a>
  </h2>
  <font color="black">弁別器は、元のCTと予測/グラウンドトゥルースの積を入力として受け取り、その入力を偽/実数に分類します。方法：無傷の前立腺癌患者110人のCTと構造データセットの計画を遡及的に選択し、10に分けました。 fold cross-validation ..セグメンテーションネットワークは、新しく設計されたMRFブロックを複数の残留Uネットのバリエーションに統合します。 
[ABSTRACT]無傷の前立腺癌患者110名の計画ctと構造のデータセットを遡及的に選択し、10倍の交差検証で分割しました。弁別子は、元のctと等高線の予測の積を偽/実数にして、入力を分類します偽物または本物に</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: TransNet V2: An effective deep network architecture for fast shot
  transition detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_43.html">
      <font color="black">TransNet V2: An effective deep network architecture for fast shot
  transition detection</font>
    </a>
  </h2>
  <font color="black">自動ショット遷移検出アプローチはすでに20年以上にわたって調査されていますが、効果的な普遍的な人間レベルのモデルはまだ提案されていません。非常に効率的な分析のためにコミュニティがすぐに利用できるように、モデルのトレーニング済みインスタンスが提供されています大規模なビデオアーカイブの数。このホワイトペーパーでは、尊敬すべきベンチマークで最先端のパフォーマンスを実現するディープネットワークTransNet V2の現在のバージョンを紹介します。 
[ABSTRACT]ビデオストレージの潜在的な多様性は、依然として誤ヒットと誤解の両方につながる可能性があります。データ認識は、大規模なビデオアーカイブの分析のためにコミュニティで簡単に使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Topic Adaptation and Prototype Encoding for Few-Shot Visual Storytelling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_44.html">
      <font color="black">Topic Adaptation and Prototype Encoding for Few-Shot Visual Storytelling</font>
    </a>
  </h2>
  <font color="black">実験結果は、トピック適応とプロトタイプエンコーディング構造が、BLEUとMETEORメトリックの少数ショットモデルに相互にメリットをもたらすことを示しています。さらに、トピック内導出の機能をモデル化するプロトタイプエンコーディング構造をさらに提案します。人間が物語を語るとき、我々はトピック適応汎化の能力をモデル化するトピック適応型ストーリーテラーを提案します。 
[ABSTRACT]これまでの研究は、大量の人間の注釈付きデータに依存する複雑なモデルの設計に重点を置いていました。実際には、勾配ベースのメタ学習アルゴリズムを適用して、トピックからトピックにすばやく適応する能力をモデルに与えますトピック</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-modal segmentation of 3D brain scans using neural networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_45.html">
      <font color="black">Multi-modal segmentation of 3D brain scans using neural networks</font>
    </a>
  </h2>
  <font color="black">トレーニングラベルは、MPRAGEコントラストで取得され、他の画像モダリティに登録されます。結果：最高の平均ダイススコアは、$ T_1 $加重MPRAGE（$ 85.3 \ pm4.6 \、\％$）で見つかります。方法：深い畳み込みニューラルネットワークは、3D MRI（MPRAGE、DWI、FLAIR）およびCTスキャンをセグメント化するようにトレーニングされています。 
[要約]合計851 mri / ctスキャンの大規模なデータベースがニューラルネットワークのトレーニングに使用されます。セグメンテーション品質は、合計27の解剖学的構造のダイスメトリックを使用して定量化されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Fast and Accurate Optical Flow based Depth Map Estimation from Light
  Fields -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_46.html">
      <font color="black">Fast and Accurate Optical Flow based Depth Map Estimation from Light
  Fields</font>
    </a>
  </h2>
  <font color="black">その時空間エッジ対応フィルタリングプロパティのおかげで、取得するさまざまな視差マップの推定値は非常に一貫しており、高速かつ単純な集約ステップで単一の視差マップを作成し、それを深度マップに変換できます。オプティカルフロー推定器は、ライトフィールドの角度次元に沿って撮影された一連の画像に適用され、いくつかの視差マップ推定を生成します。視差マップ推定は一貫しているため、各視差推定から深度マップを作成し、次に3D空間のさまざまな深度マップを集約して、単一の密な深度マップを作成します。 
[要約]この論文では、既存のオプティカルフロー推定方法に基づいて、ライトフィールドから新しい深度推定方法を提案します。推定値は一貫しており、各視差推定値から深度マップを作成し、単一の密な深度マップを作成するための3Dスペース</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Hardware-Centric AutoML for Mixed-Precision Quantization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_47.html">
      <font color="black">Hardware-Centric AutoML for Mixed-Precision Quantization</font>
    </a>
  </h2>
  <font color="black">私たちのフレームワークは、さまざまなリソース制約（つまり、レイテンシ、エネルギー、モデルサイズ）の下でのさまざまなハードウェアアーキテクチャ（つまり、エッジアーキテクチャとクラウドアーキテクチャ）に対する最適なポリシーが大幅に異なることを明らかにしています。FLOPやモデルなどのプロキシ信号に依存するのではなくサイズ、ハードウェアシミュレータを使用してRLエージェントへの直接フィードバック信号（レイテンシとエネルギー）を生成します。従来の方法と比較すると、フレームワークは完全に自動化されており、さまざまなニューラルネットワークアーキテクチャとハードウェアアーキテクチャの量子化ポリシーを特化できます。 
[ABSTRACT]アルゴリズム-自動化された自動量子化（haq）フレームワークは完全に自動化されており、さまざまなニューラルネットワークアーキテクチャとハードウェアアーキテクチャに合わせて量子化ポリシーを特化できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: HydraMix-Net: A Deep Multi-task Semi-supervised Learning Approach for
  Cell Detection and Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_48.html">
      <font color="black">HydraMix-Net: A Deep Multi-task Semi-supervised Learning Approach for
  Cell Detection and Classification</font>
    </a>
  </h2>
  <font color="black">モデルはマルチタスク学習方式でトレーニングされ、分類ローカライゼーションのためのノイズ耐性のある関節損失があり、単純なディープモデルとは対照的に、限られたデータが与えられたときにパフォーマンスが向上します。トレーニング用のラベル付きセットを使用します。DLBCLデータでは、ラベル付きの例を100個だけ指定した場合に70 \％の精度を達成する単純なCNNとは対照的に、80 \％の精度を達成します。 
[要約]このホワイトペーパーでは、半教師付きディープマルチタスク分類とローカリゼーションアプローチhydramix-netを提案します。これらのアイデアは、モデルを削減するためにさらにシャープになり、トレーニング用のラベル付きセットと混合されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: R-MNet: A Perceptual Adversarial Network for Image Inpainting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_49.html">
      <font color="black">R-MNet: A Perceptual Adversarial Network for Image Inpainting</font>
    </a>
  </h2>
  <font color="black">さらに、特徴空間で計算された新しい損失関数を提案し、敵対訓練と組み合わせた有効なピクセルのみを対象とします。顔画像の修復は広く研究されている問題であり、近年、ジェネレーティブ敵対ネットワークの導入により、フィールド。公開されているデータセットでメソッドを評価し、最新のメソッドと比較します。 
[要旨]顔認識ソフトウェアを使用して新しい画像を作成できます。この方法は、高解像度の修復タスクに一般化できることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Invertible Neural BRDF for Object Inverse Rendering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_50.html">
      <font color="black">Invertible Neural BRDF for Object Inverse Rendering</font>
    </a>
  </h2>
  <font color="black">新しいニューラルネットワークベースのBRDFモデルとオブジェクトの逆レンダリング、つまり、既知のジオメトリのオブジェクトの単一の画像からの反射と自然照明の同時推定のためのベイズフレームワークを紹介します。このモデルを可逆ニューラルBRDFと呼びます。モデル（iBRDF）。このモデルを調整することにより、実際の反射率の潜在空間を抽出します。これにより、事前に強い反射率が直接得られます。 
[ABSTRACT]結果は、ディープニューラルネットワークがラジオメトリック逆問題の解決に役立つ新しい方法を示しています。ディープニューラルシステムがラジオメトリック逆に関する質問への回答にどのように役立つかを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-10">
        <br><font color="black">2020-08-10</font>
      </time>
    </span>
</section>
<!-- paper0: TextRay: Contour-based Geometric Modeling for Arbitrary-shaped Scene
  Text Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_51.html">
      <font color="black">TextRay: Contour-based Geometric Modeling for Arbitrary-shaped Scene
  Text Detection</font>
    </a>
  </h2>
  <font color="black">この作業では、任意形状のテキスト検出方法、つまりTextRayを提案します。これは、トップダウンの輪郭ベースの幾何学的モデリングとシングルショットアンカーフリーフレームワーク内で幾何学的パラメーターの学習を実行します。幾何学的モデリングは、極座標の下で実行されます。複雑な幾何学的レイアウトを統一表現にエンコードする、形状空間とパラメーター空間の間の双方向マッピングスキームを備えたシステム。任意の形状のテキスト検出は、大きなアスペクト比、さまざまなスケール、ランダムな回転などのテキストの複雑な幾何学的レイアウトのため、困難な作業です。と曲線形状。 
[抽象]幾何学的モデリングは極座標系で実行されます。形状空間と単一ピースの対数の間の双方向マッピングスキームに基づいています。これらは視覚的なマッピングに基づいており、複雑な幾何学的レイアウトを統一表現に提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Visual Imitation Made Easy -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_52.html">
      <font color="black">Visual Imitation Made Easy</font>
    </a>
  </h2>
  <font color="black">学習パフォーマンスを向上させるために、さまざまなデータ拡張を採用し、その効果の広範な分析を提供します。最後に、以前は目に見えなかったオブジェクトを使用した実際のロボットシナリオを評価することにより、インターフェイスの有用性を示し、プッシュで87％の成功率を達成します。スタッキングの成功率は62％です。ロボットのビデオはhttps://dhiraj100892.github.io/Visual-Imitation-Made-Easyで入手できます。 
[ABSTRACT]模倣用の代替インターフェースにより、データ収集プロセスが簡素化され、ロボットへの転送が容易になります。両方のタスクで、標準の動作クローンを使用して、以前に収集されたオフラインのデモから実行可能なポリシーを学習します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Domain Adaptive Medical Image Segmentation via Adversarial Learning of
  Disease-Specific Spatial Patterns -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_53.html">
      <font color="black">Domain Adaptive Medical Image Segmentation via Adversarial Learning of
  Disease-Specific Spatial Patterns</font>
    </a>
  </h2>
  <font color="black">適応プロセスには継続的な監視が必要ですが、ターゲットドメインのグラウンドトゥルースマスクの存在を想定できないため、適応プロセスを監視する2つの新しいメトリックと、セグメンテーションアルゴリズムを安定した方法でトレーニングするための戦略を提案します。ターゲットドメインからのいくつかのラベルのない画像でディープネットワークを再調整すると、セグメンテーションの精度が大幅に向上することを示します。医療画像では、多中心データの不均一性により、ディープラーニングベースの方法の適用性が妨げられ、適用時にパフォーマンスが大幅に低下します目に見えないデータドメインのモデル、たとえば
[ABSTRACT]ネットワークを拒否してアーキテクチャを適用しますが、ありそうもないパターンを拒否します。これらは2種類の異なる脳スキャンに基づいています。また、アルゴリズムを使用して、疾患-敵対的な特定の領域をキャプチャします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-25">
        <br><font color="black">2020-01-25</font>
      </time>
    </span>
</section>
<!-- paper0: A Study of Efficient Light Field Subsampling and Reconstruction
  Strategies -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_54.html">
      <font color="black">A Study of Efficient Light Field Subsampling and Reconstruction
  Strategies</font>
    </a>
  </h2>
  <font color="black">これらの戦略を実際のデータセットと合成データセットの両方で評価し、結果から最適な選択戦略を考案します。入力の角密度の影響も評価されます。これらは、圧縮などの将来のライトフィールドリサーチに適用できます。 、角度超解像、およびカメラシステムの設計。 
[要約]機能強化のためにいくつかのアプローチが提案されていますが、この領域ではビュー選択戦略が十分に検討されていません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: BREEDS: Benchmarks for Subpopulation Shift -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_55.html">
      <font color="black">BREEDS: Benchmarks for Subpopulation Shift</font>
    </a>
  </h2>
  <font color="black">次に、対応するシフトが人間のベースラインを取得することで扱いやすいことを検証します。これにより、既存の大規模データセット内でソースを正確に制御および特徴付けできる現実的な分布シフトを合成できます。https：/で利用可能なコードとデータ/github.com/MadryLab/BREEDS-Benchmarks。 
[ABSTRACT]一連のサブポピュレーションシフトベンチマークを作成します。これらのサブポピュレーションシフトはデータサブポピュレーションに基づいています。これらのモデルは標準モデルアーキテクチャの感度を測定します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Adaptive Object Detection with Dual Multi-Label Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_56.html">
      <font color="black">Adaptive Object Detection with Dual Multi-Label Prediction</font>
    </a>
  </h2>
  <font color="black">さらに、マルチラベル予測結果を補助正則化情報として使用し、オブジェクト認識タスクとオブジェクト検出タスクの間で一貫したオブジェクトカテゴリ検出を確実にする、オブジェクト検出を支援する予測整合性正則化メカニズムを導入します。実験は、いくつかのベンチマークデータセットと結果は、提案されたモデルが最新の比較方法よりも優れていることを示しています。このホワイトペーパーでは、マルチラベルを活用することで、適応型オブジェクト検出のための新しいエンドツーエンドの教師なしディープドメイン適応モデルを提案します。デュアル補助タスクとしてのオブジェクト認識。 
[ABSTRACT]モデルはマルチラベル予測を組み合わせて、各画像のオブジェクトカテゴリ情報を明らかにします。次に、予測結果を使用して条件付きの敵対的なグローバルフィーチャアライメントを実行します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-29">
        <br><font color="black">2020-03-29</font>
      </time>
    </span>
</section>
<!-- paper0: Fully-Automated Packaging Structure Recognition in Logistics
  Environments -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_57.html">
      <font color="black">Fully-Automated Packaging Structure Recognition in Logistics
  Environments</font>
    </a>
  </h2>
  <font color="black">私たちの方法のトレーニングと評価には、現実的なロジスティクス画像のカスタムデータセットを使用します。パッケージ構造の認識を完全に自動化する方法を提案します。単一の画像に基づいて、1つまたは複数の輸送ユニットがローカライズされ、それぞれに対して、輸送ユニット、特性、パッケージユニットの総数と配置が認識されます。このソリューションでは、テストケースの約85％でパッケージ構造を正しく認識できることを示しています。最も一般的なパッケージタイプに焦点を当てています。 
[要約]私たちのアルゴリズムは、ディープラーニングモデル、より正確には畳み込みニューラルネットワーク（画像のセグメンテーションなど）、およびコンピュータービジョン手法とヒューリスティックコンポーネントに基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Unified Representation Learning for Cross Model Compatibility -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_58.html">
      <font color="black">Unified Representation Learning for Cross Model Compatibility</font>
    </a>
  </h2>
  <font color="black">顔識別でCMCに対処するための既存のアプローチはありますが、埋め込みモデルの分布が大幅に変化する、より困難な設定では機能しません。異なる埋め込みモデル間の相互互換性により、ビジュアル検索システムは、アイデンティティを再認識せずに正しく認識して取得できます。プライバシーの問題のため、通常は利用できないユーザー画像の-エンコード。広範な実験により、提案されたソリューションは、顔認識や人物の再識別といったさまざまな困難な視覚的検索シナリオに対して、以前のアプローチよりもはるかに優れていることが示されています。 
[ABSTRACT]異なる埋め込みモデル間の相互互換性により、ビジュアル検索システムはアイデンティティを正しく認識して取得できます。プライバシーの問題により、既存のモデルは使用できません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Implanting Synthetic Lesions for Improving Liver Lesion Segmentation in
  CT Exams -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_59.html">
      <font color="black">Implanting Synthetic Lesions for Improving Liver Lesion Segmentation in
  CT Exams</font>
    </a>
  </h2>
  <font color="black">合成病変を埋め込むと、さまざまなアーキテクチャを考慮したセグメンテーションパフォーマンスが向上するだけでなく（最大約12 \％）、この改善はさまざまな画像合成ネットワーク間で一貫していることがわかります。この現象により、病変セグメンテーションアルゴリズムに固有のバイアスが加わり、サイズ、密度、形状、および位置の点で病変の変動性を総合的に高めると、CTスライスでの肝病変セグメンテーションのセグメンテーションモデルのパフォーマンスが向上するように思われます。 
[要約]データセット内の病変の変化は、さまざまな種類の病変の有病率にも依存します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Curiosity -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_60.html">
      <font color="black">Adversarial Curiosity</font>
    </a>
  </h2>
  <font color="black">他のモデルベースおよびモデルフリーの探索戦略と比較して、私たちの敵対的な好奇心アプローチを使用して、シミュレートされた環境でダウンストリームタスクのパフォーマンスが向上することを示します。これらのスケーラビリティの問題は、判別ネットワークによって与えられるスコアを最小化する敵対的な好奇心の方法で対処します。 
[ABSTRACT]私たちは、ロボット操作の予測にスケーリングするための敵対的好奇心メソッドの能力を実証しました-ロボット工学タスクで使用されるパイプラインの計画</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-13">
        <br><font color="black">2020-03-13</font>
      </time>
    </span>
</section>
<!-- paper0: Temporally Coherent Embeddings for Self-Supervised Video Representation
  Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_61.html">
      <font color="black">Temporally Coherent Embeddings for Self-Supervised Video Representation
  Learning</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、TCE：自己監視ビデオ表現学習のための一時的にコヒーレントな埋め込みについて説明します。複数の挑戦的なベンチマーク（Kinetics400、UCF101、HMDB51）を使用したビデオアクション認識のダウンストリームタスクで、自己監視学習TCEモデルを徹底的に分析および評価します。シンプルだが効果的な2D-CNNバックボーンとRGBストリーム入力のみにより、TCE事前トレーニング済み表現は、UCF101で事前トレーニング済みのすべての以前の自己監視2D-CNNおよび3D-CNNよりも優れています。 
[要約]提案された方法は、ラベル付けされていないビデオデータを使用して、埋め込みスペースに時間的一貫性を明示的に適用します。さらに、隣接するフレームが互いに近くに存在し、ビデオが互いに分離されるように、ビデオをエンコードするようにtceモデルをトレーニングします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-21">
        <br><font color="black">2020-03-21</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Recognizing Unseen Categories in Unseen Domains -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_62.html">
      <font color="black">Towards Recognizing Unseen Categories in Unseen Domains</font>
    </a>
  </h2>
  <font color="black">歴史的にZSLとDGのタスクは分離して取り組んでいますが、この作業は、それらを共同で解決するという野心的な目標、すなわちZSL、DG、およびZSL + DGに取り組むための全体的なアルゴリズムであるCuMix（見えないドメインの目に見えないカテゴリを認識するためのCurriculumMixup）を提示します。 
[ABSTRACT]ゼロショット学習（zsl）は、意味論的シフトに対処することです。ドメイン適応の主な課題は、ドメインシフトです。dgを理解しなくても、目に見えないドメインで目に見えない視覚的概念を特定できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: Keypoint Autoencoders: Learning Interest Points of Semantics -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_63.html">
      <font color="black">Keypoint Autoencoders: Learning Interest Points of Semantics</font>
    </a>
  </h2>
  <font color="black">スパースキーポイントで形状を分類する下流のタスクは、選択されたキーポイントの特徴を示すために行われます。しかし、既存の方法は、選択されたポイントのセマンティクスを無視するため、ダウンストリームタスクのパフォーマンスが低下します。スパースキーポイントの選択を差別化するため、ソフトキーポイント提案は、入力ポイント間の加重平均を計算することによって採用されます。 
[ABSTRACT]提案には、キーポイントを検出するための教師なし学習方法であるキーポイントオートエンコーダが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Semi-supervised semantic segmentation needs strong, varied perturbations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_64.html">
      <font color="black">Semi-supervised semantic segmentation needs strong, varied perturbations</font>
    </a>
  </h2>
  <font color="black">実装：https://github.com/Britefury/cutmix-semisup-seg ..最近提案されたCutOutおよびCutMixの拡張技術の適応されたバリアントは、標準で最先端の半教師付きセマンティックセグメンテーション結果を生成することがわかりますデータセット..整合性の正則化は、半教師付き分類問題で画期的な結果をもたらしたアプローチのクラスを表します。 
[ABSTRACT]事前の作業により、クラスターの仮定がその成功にとって重要であることが確立されました。次に、そのような低密度の領域なしで信頼できるパフォーマンスを確保するための鍵として、拡張の選択を特定します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-06-05">
        <br><font color="black">2019-06-05</font>
      </time>
    </span>
</section>
<!-- paper0: 3D FLAT: Feasible Learned Acquisition Trajectories for Accelerated MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_65.html">
      <font color="black">3D FLAT: Feasible Learned Acquisition Trajectories for Accelerated MRI</font>
    </a>
  </h2>
  <font color="black">概念実証として行われた実験結果は、3D FLATが、放射状、星型のスタック、または2D学習軌道（軌道のみで進化する軌道）などの標準的な軌道と比較して、所定の読み出し時間でより高い画質を達成することを示唆しています。 3Dに沿って完全にサンプリングしながら2D平面）。圧縮センシング（CS）は、k空間（空間座標の物理空間に対して二重のフーリエドメイン）をサブサンプリングして、取得を大幅に加速します。ただし、圧縮の利点センシングは完全には活用されていません。 CSを介して取得されるサンプリング密度のほとんどは、実際に課されたMRIマシンの厳しい制約に従う軌跡を生成しません。 
[ABSTRACT] mri mri mriスキャンは、一部のアプリケーションの標準的な実践では使用できませんでした。ただし、圧縮スペースの利点は十分に活用されていません。これらの座標は、mriマシンの厳しい制約に従う軌道を生成しません実際に課された</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Real-Time Sign Language Detection using Human Pose Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_66.html">
      <font color="black">Real-Time Sign Language Detection using Human Pose Estimation</font>
    </a>
  </h2>
  <font color="black">人間の姿勢推定に基づいてオプティカルフローの特徴を抽出し、線形分類子を使用して、これらの特徴が80％の精度で有意義であることを示し、DGS Corpusで評価します。軽量のリアルタイム手話検出モデルを提案します。ビデオ会議でのそのようなケースの必要性を特定します。ビデオ会議アプリケーションでの使用の可能性を実証するために、ブラウザで手話検出にデモアプリケーションを説明します。 
[ABSTRACT]人間の姿勢の推定に基づいてオプティカルフローの特徴を抽出します。これらの特徴は80％の精度で有意であり、DGSコーパスで評価されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Reinforced Wasserstein Training for Severity-Aware Semantic Segmentation
  in Autonomous Driving -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_67.html">
      <font color="black">Reinforced Wasserstein Training for Severity-Aware Semantic Segmentation
  in Autonomous Driving</font>
    </a>
  </h2>
  <font color="black">Wasserstein距離の地上メトリックは、特定のタスクの経験に従って事前に定義できます。CamVidとCityscapesの両方のデータセットでの実験により、Wasserstein損失の有効性が証明されました。最適化の観点から、地上メトリックを設定することをさらに提案します。事前定義された地上計量の増加関数として。 
[ABSTRACT]ディープネットワークは大幅な進歩を遂げました。segnet、enet、fcn、deeplabネットワークは、プラグインの効果に従って適応できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Cross-Modal Hierarchical Modelling for Fine-Grained Sketch Based Image
  Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_68.html">
      <font color="black">Cross-Modal Hierarchical Modelling for Fine-Grained Sketch Based Image
  Retrieval</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、スケッチ固有の階層を構築し、それらを利用してスケッチを対応する階層レベルで写真と一致させることができる新しいネットワークを設計します。この階層構造は、多くの場合視覚的に区別されます。画像検索クエリとしてのスケッチは理想的です細かい視覚的詳細をキャプチャする際のテキストの代替。 
[ABSTRACT]以前の成功は、写真ではなくスケッチのユニークな特性に取り組むことの重要性を示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-29">
        <br><font color="black">2020-07-29</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Generative Grammars for Human Activity Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_69.html">
      <font color="black">Adversarial Generative Grammars for Human Activity Prediction</font>
    </a>
  </h2>
  <font color="black">提案された敵対的文法は、最先端のアプローチよりも優れており、以前の研究よりもはるかに正確かつ将来的に予測することができます。私たちの敵対的文法は、データ分布から確率的な生産ルールを学習できるように設計されています、潜在的な非ターミナル表現と組み合わせて使用します。推論中に複数のプロダクションルールを選択できるため、さまざまな予測結果が得られ、多くのもっともらしい未来を効率的にモデリングできます。 
[要約]提案されたモデルは、複数の異なる将来の活動を予測するために使用できます。将来の活動を予測するのに役立つように設計することができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Text as Neural Operator: Image Manipulation by Text Instruction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CV/paper_70.html">
      <font color="black">Text as Neural Operator: Image Manipulation by Text Instruction</font>
    </a>
  </h2>
  <font color="black">この目的のために、モデルは生成プロセスを分解して、変更を適用する場所（空間領域）と方法（テキスト演算子）を見つけます。提案されたモデルは、3つのデータセットの最近のベースラインに対して良好に機能することを示しています。重要なアイデアは、画像の機能をローカルに変更する神経演算子としての言語。 
[ABSTRACT]提案されたモデルは、3つのデータセットの最近のベースラインに対して良好に機能することを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: A Comparison of Synthetic Oversampling Methods for Multi-class Text
  Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_0.html">
      <font color="black">A Comparison of Synthetic Oversampling Methods for Multi-class Text
  Classification</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、k最近傍アルゴリズム、サポートベクターマシンアルゴリズム、および3種類のニューラルネットワーク（フィードフォワードネットワーク、Long Short-term Memory（LSTM）、および双方向LSTM）について説明します。 ..少数派クラスの2つの例を選択し、それらに基づいて新しい例を生成することで構成されます。 
[ABSTRACT] smoteアルゴリズムは、最も人気のあるオーバーサンプリング手法の1つに基づいています。作成者は、システムを2つの変更（ボーダーラインsmoteとadasyn）およびランダムオーバーサンプリングテクニックと比較します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Context Reinforced Neural Topic Modeling over Short Texts -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_1.html">
      <font color="black">Context Reinforced Neural Topic Modeling over Short Texts</font>
    </a>
  </h2>
  <font color="black">次に、モデルは、トピックを埋め込み空間で多変量ガウス分布またはガウス混合分布として扱うことにより、事前トレーニング済みの単語の埋め込みを活用します。普及しているトピックマイニングツールの1つであるニューラルトピックモデリングは、トレーニングの効率性と強力な汎化能力の利点に多くの関心を集めています。 
[要約]各短いテキストにコンテキストがないことが問題である可能性があります。モデルを使用して新しいモデルを開発できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: A Neural Generative Model for Joint Learning Topics and Topic-Specific
  Word Embeddings -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_2.html">
      <font color="black">A Neural Generative Model for Joint Learning Topics and Topic-Specific
  Word Embeddings</font>
    </a>
  </h2>
  <font color="black">実験結果は、提案されたモデルが、単語の類似性の評価と単語の意味の明確化の両方で、単語レベルの埋め込み方法よりも優れていることを示しています。感情分類として。トレーニング済みモデルは、単語をトピック依存の埋め込みにマップします。これにより、自然に単語多義性の問題に対処します。 
[ABSTRACT]トレーニング済みモデルは、単語をトピックに最適な埋め込みにマップします。また、モデルは、既存の神経トピックモデルと比較して、より一貫した主題を学習しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Analysing the Effect of Clarifying Questions on Document Ranking in
  Conversational Search -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_3.html">
      <font color="black">Analysing the Effect of Clarifying Questions on Document Ranking in
  Conversational Search</font>
    </a>
  </h2>
  <font color="black">調査結果に基づいて、単純なヒューリスティックベースの字句ベースラインを導入します。これは、既存の単純なベースラインを大幅に上回ります。質問とユーザーの回答の明確化のさまざまな側面がランキングの質にどのように影響するかを、定量的および定性的に調査します。そのような混合イニシアチブの設定に存在する明示的なフィードバックに基づいて、説明の会話ラウンド全体のいくつかのきめの細かい処理が必要であることに注意してください。 
[ABSTRACT]システムはユーザーに明確な質問をすることができる必要があります。明確な質問で会話型検索データセットの語彙ランキングモデルのパフォーマンスを分析します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-09">
        <br><font color="black">2020-08-09</font>
      </time>
    </span>
</section>
<!-- paper0: Spectrum and Prosody Conversion for Cross-lingual Voice Conversion with
  CycleGAN -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_4.html">
      <font color="black">Spectrum and Prosody Conversion for Cross-lingual Voice Conversion with
  CycleGAN</font>
    </a>
  </h2>
  <font color="black">F0モデリングに連続ウェーブレット変換（CWT）分解を使用することを提案します。クロスリンガル音声変換に関するこれまでの研究は、主にF0転送の線形変換によるスペクトル変換に焦点を当てています。CWTは、信号を異なる時間に分解する方法を提供します異なる時間分解能で韻律を説明するスケール。 
[ABSTRACT]たとえば、2つのスピーカーのパラレルデータの必要性を排除します。これは、クロスリンガル音声変換における韻律の最初の研究です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Transformer with Bidirectional Decoder for Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_5.html">
      <font color="black">Transformer with Bidirectional Decoder for Speech Recognition</font>
    </a>
  </h2>
  <font color="black">推論段階では、左から右の候補を生成できるだけでなく、右から左の候補も生成できる導入された双方向ビーム検索方法を使用し、スコアによって最良の仮説を決定します。双方向デコーダー（STBD）を使用して、AISHELL-1データセットで大規模な実験を行います。具体的には、提案するトランスフォーマーの出力には、左から右へのターゲットと右から左へのターゲットが含まれます。 
[ABSTRACT]私たちは双方向デコーダー（stbd）を使用して、提案された音声変換器を示しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal
  Transformer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_6.html">
      <font color="black">A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal
  Transformer</font>
    </a>
  </h2>
  <font color="black">パフォーマンスは、私たちのモデルが優れたパフォーマンスを達成する挑戦的なActivityNetキャプションデータセットで実証されます。高密度のビデオキャプションタスクでオーディオとビジュアルのモダリティを使用して提案されたモデルの有効性を示しますが、モジュールは2つのモダリティをシーケンスからシーケンスへのタスク。また、バイモーダルトランスフォーマーの一部として事前トレーニングされたバイモーダルエンコーダーが、単純な提案生成モジュールの特徴抽出器として使用できることを示します。 
[ABSTRACT]バイモーダルトランスフォーマーモデルは、シンプルなプロポーザル生成モジュールの特徴エクストラクターとして使用できます。事前トレーニング済みのバイモーダルエンコーダーは、バイモーダルモデルの一部です。フィーチャーの作成にも使用できます。提案生成のためのキャプター</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-17">
        <br><font color="black">2020-05-17</font>
      </time>
    </span>
</section>
<!-- paper0: SpeechBERT: An Audio-and-text Jointly Learned Language Model for
  End-to-end Spoken Question Answering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_7.html">
      <font color="black">SpeechBERT: An Audio-and-text Jointly Learned Language Model for
  End-to-end Spoken Question Answering</font>
    </a>
  </h2>
  <font color="black">エンドツーエンドのSQAの可能性に加えて、SpeechBERTは、他の多くの音声言語理解タスクでも、多くのテキスト処理タスクのBERTと同様に考慮することができます。さまざまなテキスト処理タスクで非常に成功したBERTモデルから学ぶ音声とテキストを組み合わせて学習したSpeechBERTモデルを提案しました。このモデルは、エンドツーエンドモデルが原因で、応答スパンにASRエラーが含まれるデータセットに対して、次のテキスト質問応答（TQA）モデルを使用してASRをカスケードする従来のアプローチよりも優れています。 ASRがエラーを生成する前に、オーディオデータから情報を抽出できることが示されました。 
[要約]成功したbertモデルの一部として、音声と質問のスピーチベルトモデルを提案します。提案されたエンドツーエンドを組み立てる一方で、十分に準備された効果もありました。成功したモデルからの学習に加えて、これらは成功した</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-25">
        <br><font color="black">2019-10-25</font>
      </time>
    </span>
</section>
<!-- paper0: Hybrid Ranking Network for Text-to-SQL -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_8.html">
      <font color="black">Hybrid Ranking Network for Text-to-SQL</font>
    </a>
  </h2>
  <font color="black">WikiSQLデータセットでの実験は、提案されたアプローチが非常に効果的で、リーダーボードのトップを達成することを示しています。問題を列ごとのランキングとデコードに分解し、最終的に組み立てるハイブリッドランキングネットワーク（HydraNet）と呼ばれるきちんとしたアプローチを提案します。単純なルールによって列ごとの出力をSQLクエリに変換します。このホワイトペーパーでは、Text-to-SQLで事前にトレーニングされた言語モデルを活用する方法を検討します。 
[ABSTRACT]このアプローチでは、エンコーダーにnlの質問と1つの列が与えられます。従来のアプローチで必要だったアドホックプーリングや追加のgitmoレイヤーを回避します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Revisiting Low Resource Status of Indian Languages in Machine
  Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_9.html">
      <font color="black">Revisiting Low Resource Status of Indian Languages in Machine
  Translation</font>
    </a>
  </h2>
  <font color="black">私たちの作業を通じて、ピボット言語の選択やコーパスサイズの反復的な漸増の影響など、設計の選択も評価します。自動化フレームワークを提供することに加えて、私たちの作業は、既存のコーパスと比較して比較的大きなコーパスを生成する結果にもなります。インドの言語で利用可能なコーパス..このコーパスは、公開されているWAT評価ベンチマークおよびその他の標準評価ベンチマークで大幅に改善された結果を取得するのに役立ちます。 
[要約]この取り組みの主な貢献は、上記のパイプラインを使用してコーパスのサイズを改善するインクリメンタルメソッドを取得することです。また、インドの言語で利用できる既存のコーパスと比較して、比較的大きなコーパスを生成することにもなります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Semantic-based End-to-End Learning for Typhoon Intensity Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_10.html">
      <font color="black">Semantic-based End-to-End Learning for Typhoon Intensity Prediction</font>
    </a>
  </h2>
  <font color="black">これらの制限を緩和するために、私たちは、意味的に豊富な単語埋め込みモデルと、tweet内のエンティティを、traditionalword2vecで計算されたセマンティック表現で表現することの組み合わせを提案します。ソーシャルメディアは、歴史的な環境データに加えて、補足的な情報源と見なします。 、ソーシャルメディアの投稿と台風のマグニチュード（強度とも呼ばれます）の間の相関関係を、ツイートの量と感情の観点から調べます。 
[要約]これは、嵐を予測するために使用された一連の主張の最新のものです。ただし、ソーシャルメディアの投稿は非公式で、ツイートが限られています。これは、ツイートやツイートなどの異常気象や嵐の使用が原因です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-22">
        <br><font color="black">2020-03-22</font>
      </time>
    </span>
</section>
<!-- paper0: KR-BERT: A Small-Scale Korean-Specific Language Model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_11.html">
      <font color="black">KR-BERT: A Small-Scale Korean-Specific Language Model</font>
    </a>
  </h2>
  <font color="black">これらの調整により、KR-BERTモデルは、サイズの約1/10のコーパスを使用して、他の既存の事前トレーニング済みモデルよりも同等に、さらに優れた性能を発揮しました。 、多言語BERTモデルが見逃した言語固有の言語現象をキャプチャすることも重要です。BERTの登場以来、XLNetやRoBERTaを含む最近の作品は、大規模なコーパスと多数のパラメーターによって事前トレーニングされた文埋め込みモデルを利用しています。 
[ABSTRACT]たとえば、韓国語固有のモデルkr-bertをトレーニングしました。モデルは小さな語彙とデータセットを使用しています。サブ文字番号などの多くの要素を使用して、より良い語彙を作成しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-10">
        <br><font color="black">2020-08-10</font>
      </time>
    </span>
</section>
<!-- paper0: Neural PLDA Modeling for End-to-End Speaker Verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_12.html">
      <font color="black">Neural PLDA Modeling for End-to-End Speaker Verification</font>
    </a>
  </h2>
  <font color="black">最近、ニューラルPLDA（NPLDA）と呼ばれるスピーカー検証におけるバックエンドモデリングのためのニューラルネットワークアプローチを提案しました。このPLDA（NPLDA）は、生成PLDAモデルの尤度比スコアが判別類似度関数として提起され、スコア関数の学習可能なパラメーターが次のように最適化されます。検証コスト。ディープラーニングモデルは教師あり分類問題で大幅な進歩を遂げましたが、話者認識などの外れの検証タスクへのこれらのモデルの適用は、特徴埋め込みの導出に限定されていました。アートx-ベクトルPLDAベースの話者検証システムは、検証スコアの計算に確率的線形判別分析（PLDA）に基づく生成モデルを使用します。 
[要約] npldaネットワークを使用した提案されたe2eモデルは、モデルよりも大幅に改善されています。提案されたモデルは、他のさまざまなモデルとリンクされています。新しいモデルは、テスト用のモデルとして使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Topic Adaptation and Prototype Encoding for Few-Shot Visual Storytelling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_13.html">
      <font color="black">Topic Adaptation and Prototype Encoding for Few-Shot Visual Storytelling</font>
    </a>
  </h2>
  <font color="black">実験結果は、トピックの適応とプロトタイプエンコーディング構造が、BLEUおよびMETEORメトリックの少数ショットモデルに相互にメリットをもたらすことを示しています。実際には、マルチモーダルseq2seqモデルに勾配ベースのメタ学習アルゴリズムを適用して、モデルにトピック間ですばやく適応する機能。具体的には、推論時に生成をガイドするためのリファレンスとして機能するいくつかのトレーニングストーリーテキストをエンコードして復元します。 
[ABSTRACT]これまでの研究は、大量の人間の注釈付きデータに依存する複雑なモデルの設計に重点を置いていました。実際には、勾配ベースのメタ学習アルゴリズムを適用して、トピックからトピックにすばやく適応する能力をモデルに与えますトピック</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: The Chess Transformer: Mastering Play using Generative Language Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_14.html">
      <font color="black">The Chess Transformer: Mastering Play using Generative Language Models</font>
    </a>
  </h2>
  <font color="black">今後の作業は、このトランスフォーマーの約束に基づいて、特に他の戦略ゲームで機能が構築されると予想します。この機能では、基本的な複雑なルール構文をシンプルだが表現力豊かなプレーヤーアノテーションからキャプチャできます。 、英語やスラブ交換など。自然言語スキルの学習に加えて、抽象的なトランスフォーマーアーキテクチャは、チェス盤上で意味のある動きを生成できます。 
[要旨]トランスフォーマーアーキテクチャは、チェス盤で数百万を生成できます。30,000のトレーニングステップの後、トランスは7億7,400万のパラメーターの重みを最適化します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-02">
        <br><font color="black">2020-08-02</font>
      </time>
    </span>
</section>
<!-- paper0: On Learning Language-Invariant Representations for Universal Machine
  Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_15.html">
      <font color="black">On Learning Language-Invariant Representations for Universal Machine
  Translation</font>
    </a>
  </h2>
  <font color="black">後者の場合、コーパス内のペアになっているドキュメントが自然な\ emph {encoder-decoder}生成プロセスに従う場合、「一般化」の自然な概念を期待できることを示します：二次式ではなく線形の言語ペアの数、適切な表現を学習するのに十分です。ユニバーサル機械翻訳の目的は、すべての言語のペアの\ emph {小さなサブセット}のペアの翻訳ドキュメントのコーパスが与えられれば、任意の言語のペアの間で翻訳することを学ぶことです。私たちの理論的洞察と含意は、ユニバーサル機械翻訳の将来のアルゴリズム設計に貢献しています。 
[ABSTRACT]これらのユニバーサル機械翻訳モデルによる翻訳エラーの理論的分析はまだ初期段階です。ただし、前者の場合は、多対多の翻訳設定で翻訳エラーの下限を作成します。たとえば、共有文を学習します言語と言語のペアから学ぶための複数の言語ペア間の表現</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: DensE: An Enhanced Non-Abelian Group Representation for Knowledge Graph
  Embedding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_16.html">
      <font color="black">DensE: An Enhanced Non-Abelian Group Representation for Knowledge Graph
  Embedding</font>
    </a>
  </h2>
  <font color="black">この問題に取り組むために、DensEという新しい知識グラフの埋め込み方法を開発し、複雑な合成パターンに十分なモデリング能力を提供しました。特に、この方法では、各関係をSO（3）グループベースの回転演算子と3次元（3-D）ユークリッド空間でのスケーリング演算子..この方法の利点は2つあります。（1）複合関係の場合、対応する対角関係行列は非可換であり、エンティティの埋め込みに関連しています。 （2）RotatEの概念をモデルの複雑さを抑えたより表現力豊かな設定に拡張し、直接の幾何学的解釈を維持します。これにより、明確なパターンとの関係（つまり、対称性/反対称性、反転、構成）がモデル化されます。 
[ABSTRACT] rotateは、構成パターンに対していくつかの過度に単純化された仮定を作成します。これにより、関係は可換性、エンティティーから独立し、スケールが固定された</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: A parallel evaluation data set of software documentation with document
  structure annotation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_17.html">
      <font color="black">A parallel evaluation data set of software documentation with document
  structure annotation</font>
    </a>
  </h2>
  <font color="black">データセットは、英語からヒンディー語、インドネシア語、マレー語、タイ語までの言語ペアで構成されているため、多くの低リソース言語ペアのテストカバレッジも増加します。その起源と作成、特殊性、特性の洞察を提供します。データセット..このペーパーには、機械翻訳用のソフトウェアドキュメントデータセット（SAPヘルプポータルから発信されたデータの並列評価データセット）が付属しており、研究目的で機械翻訳コミュニティに公開しています。 
[ABSTRACT]機械翻訳システムを調整および評価する可能性を提供します。幅広い評価シナリオの可用性を高めます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: The Sockeye 2 Neural Machine Translation Toolkit at AMTA 2020 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_18.html">
      <font color="black">The Sockeye 2 Neural Machine Translation Toolkit at AMTA 2020</font>
    </a>
  </h2>
  <font color="black">Sockeye 2は、Sockeyeのニューラル機械翻訳（NMT）ツールキットの最新版で合理化されたバージョンです。これらの改善により、トレーニングと推論が速くなり、自動メトリックスコアが高くなり、研究から製造までの経路が短くなります。新機能には、 MXNetのGluon APIを使用した簡素化されたコードベース、最先端のモデルアーキテクチャ、分散型混合精度トレーニング、および8ビット量子化による効率的なCPUデコードに焦点を当てています。 
[ABSTRACT]新機能には、簡略化されたコードベースと最先端のモデルアーキテクチャへの焦点が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: An Empirical Study on Robustness to Spurious Correlations using
  Pre-trained Language Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_19.html">
      <font color="black">An Empirical Study on Robustness to Spurious Correlations using
  Pre-trained Language Models</font>
    </a>
  </h2>
  <font color="black">自然言語の推論と言い換えの識別に関する実験では、適切な補助タスクを使用したMTLは、分布内のパフォーマンスを損なうことなく、挑戦的な例のパフォーマンスを大幅に向上させることを示しています。さらに、MTLからの利益は主にマイノリティの例からの一般化の向上から得られることを示しています..極端なマイノリティの場合、一般化を改善するためにマルチタスク学習（MTL）を使用することを提案します。 
[ABSTRACT]成功の鍵は、偽の相関関係が保持されない少量の反例からの一般化です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-14">
        <br><font color="black">2020-07-14</font>
      </time>
    </span>
</section>
<!-- paper0: Investigation of End-To-End Speaker-Attributed ASR for Continuous
  Multi-Talker Recordings -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_20.html">
      <font color="black">Investigation of End-To-End Speaker-Attributed ASR for Continuous
  Multi-Talker Recordings</font>
    </a>
  </h2>
  <font color="black">具体的には、E2E SA-ASRモデルの内部スピーカー表現を使用してスピーカーカウントとクラスタリングを実行し、プロファイルがスピーカーインベントリから欠落しているスピーカーの発話を二値化します。元のE2E SA-ASRの包括的な調査を行います。モノラルLibriCSSデータセットに対する提案された方法。また、継続的なマルチトーカー録音の処理に役立つE2E SA-ASRトレーニングの参照ラベルに簡単な変更を提案します。 
[要約]提案された方法は、事前の話者の知識なしに近いスタートを達成しました。e2esa-asrトレーニング-マルチトーカー録音を処理できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Real-Time Sign Language Detection using Human Pose Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_21.html">
      <font color="black">Real-Time Sign Language Detection using Human Pose Estimation</font>
    </a>
  </h2>
  <font color="black">ビデオ会議でこのようなケースの必要性を特定するため、軽量のリアルタイム手話検出モデルを提案します。人間の姿勢推定に基づいてオプティカルフローの特徴を抽出し、線形分類子を使用して、これらの特徴が正確で意味があることを示しますDGS Corpusで評価された80％です。入力でリカレントモデルを直接使用すると、4ms未満で動作しながら、最大91％の精度の改善が見られます。 
[ABSTRACT]人間の姿勢の推定に基づいてオプティカルフローの特徴を抽出します。これらの特徴は80％の精度で有意であり、DGSコーパスで評価されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: LTIatCMU at SemEval-2020 Task 11: Incorporating Multi-Level Features for
  Multi-Granular Propaganda Span Identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/cs.CL/paper_22.html">
      <font color="black">LTIatCMU at SemEval-2020 Task 11: Incorporating Multi-Level Features for
  Multi-Granular Propaganda Span Identification</font>
    </a>
  </h2>
  <font color="black">より適切な表現学習を促進するために、1万のニュース記事のコーパスを収集し、それを使用してモデルを微調整します。文内のどのトークンスパンが指標であるかを識別するBERT-BiLSTMベースのスパンレベルプロパガンダ分類モデルを導入します最終的なモデルは、組み込まれた知識のさまざまなサブセットを活用することによってさまざまな宣伝クラスの境界を学習する多数決投票アンサンブルです。\ footnote {私たちの最終的なアンサンブルは、テストリーダーボードで$ 4 ^ {th} $の位置を獲得します。 
[要旨] bert-bilstmベースのスパン-レベルのプロパガンダ分類モデルを導入します。これは、文のさまざまなレベルがプロパガンダを示すことを識別します。モデルには、1万のニュース記事のコーパスが含まれています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: TinySpeech: Attention Condensers for Deep Speech Recognition Neural
  Networks on Edge Devices -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_0.html">
      <font color="black">TinySpeech: Attention Condensers for Deep Speech Recognition Neural
  Networks on Edge Devices</font>
    </a>
  </h2>
  <font color="black">この研究では、エッジでのデバイス上の音声認識用に、設置面積が小さく、高効率のディープニューラルネットワークを構築するための注意コンデンサーの概念を紹介します。限られた語彙の音声認識用のGoogle音声コマンドベンチマークデータセットの実験結果は、 TinySpeechネットワークは、研究文献の以前のディープニューラルネットワークと比較して、大幅に低いアーキテクチャの複雑さ（$ 207 \ times $少ないパラメーター）および低い計算の複雑さ（$ 21 \ times $少ない乗算-加算演算）を達成したことを示しています。その有効性を示すために、機械駆動の設計探査戦略を使用して、デバイス上の音声認識用に調整された注意コンデンサーで主に構成される低精度のディープニューラルネットワークであるTinySpeechを紹介します。 
[ABSTRACT]オンのディープニューラルネットワーク-デバイスの音声認識は依然として課題です。これは、ディープニューラル接続が広く使用されているためです。これらには、圧縮された埋め込みを学習して生成するアテンションデバイスが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-10">
        <br><font color="black">2020-08-10</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Speech Decomposition via Triple Information Bottleneck -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_1.html">
      <font color="black">Unsupervised Speech Decomposition via Triple Information Bottleneck</font>
    </a>
  </h2>
  <font color="black">ただし、これらのシステムは音色のもつれをほぐすことができるだけで、ピッチ、リズム、内容に関する情報は依然として混合されています。残りの音声コンポーネントのもつれをさらに解くのは、各コンポーネントの明示的な注釈がない場合、未確定の問題であり、困難で費用がかかります。この論文では、注意深く設計された3つの情報ボトルネックを導入することにより、音声を盲目的に4つのコンポーネントに分解できるSpeechSplitを提案します。 
[要旨]これらのシステムは音色をほぐすことができるだけですが、ピッチ、リズム、コンテンツに関する情報は依然依存しています。システムはgithubで公開されています。 com / auspicious3000 / speechsplit。ラベルは公開されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-23">
        <br><font color="black">2020-04-23</font>
      </time>
    </span>
</section>
<!-- paper0: Transfer Learning for Improving Singing-voice Detection in Polyphonic
  Instrumental Music -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_2.html">
      <font color="black">Transfer Learning for Improving Singing-voice Detection in Polyphonic
  Instrumental Music</font>
    </a>
  </h2>
  <font color="black">この調査では、音声アクティビティエンドポイントと個別のインストゥルメンタルミュージッククリップを含むクリーンなスピーチクリップを人工的に追加して、ポリフォニックボーカルをシミュレートし、ボーカル/非ボーカルディテクターをトレーニングします。歌声検出（S-VD）に利用できる適切にラベル付けされたデータセットはほとんどありません。話すことと歌うことのアーティキュレーションと発音が異なるため、人工データセットでトレーニングされた音声検出器は、伴奏と一緒にボーカルを歌う。 
[要約]転移学習による提案されたデータ拡張方法は、s-vdパフォーマンスを改善し、f-スコアを89.5％から93. 2％に改善できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Data Efficient Voice Cloning from Noisy Samples with Domain Adversarial
  Training -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_3.html">
      <font color="black">Data Efficient Voice Cloning from Noisy Samples with Domain Adversarial
  Training</font>
    </a>
  </h2>
  <font color="black">前者は少数のターゲットスピーカーデータを使用して、直接モデル更新を通じてマルチスピーカーモデルをターゲットスピーカーの音声に転送しますが、後者では、数秒だけのターゲットスピーカーの音声が、モデルを更新せずにターゲットスピーカーの音声を合成するマルチスピーカーモデル。実験により、スピーカーの適応とエンコーディングの両方について、提案されたアプローチはノイズの多いスピーカーサンプルから一貫してクリーンな音声を合成でき、明らかに最先端の音声を採用する方法よりも優れている拡張モジュール..それにもかかわらず、2つの方法は、クリーンなターゲットスピーカーデータを必要とします。 
[ABSTRACT]ターゲット音声のスピーカー適応は、複数のスピーカーからトレーニングされたモデルの一種です。2つの方法には、スピーカーデータと話すデータが必要です。ただし、両方のスピーカーデータが必要です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-10">
        <br><font color="black">2020-08-10</font>
      </time>
    </span>
</section>
<!-- paper0: PoCoNet: Better Speech Enhancement with Frequency-Positional Embeddings,
  Semi-Supervised Conversational Data, and Biased Loss -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_4.html">
      <font color="black">PoCoNet: Better Speech Enhancement with Frequency-Positional Embeddings,
  Semi-Supervised Conversational Data, and Biased Loss</font>
    </a>
  </h2>
  <font color="black">新しいPoCoNetアーキテクチャは畳み込みニューラルネットワークであり、周波数ポジショナルエンベディングを使用して、初期レイヤーで周波数依存の機能をより効率的に構築できます。音声品質を維持するようにバイアスされた新しい損失関数により、最適化がより適切に一致します。音声品質に関する人間の知覚的意見。音声強化のためのより大きな大規模ニューラルネットワークにつながるいくつかの革新を紹介します。 
[要約]複数の革新により、音声強調のためのより大きな大規模ニューラルネットワークが実現します。半教師付き方法は、ノイズの多いデータセットを事前に強化することにより会話トレーニングデータの量を増やし、実際の録音のパフォーマンスを向上させます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Learning For Sequence-to-sequence Text-to-speech For
  Low-resource Languages -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_5.html">
      <font color="black">Unsupervised Learning For Sequence-to-sequence Text-to-speech For
  Low-resource Languages</font>
    </a>
  </h2>
  <font color="black">具体的には、まずベクトル量子化変分オートエンコーダー（VQ-VAE）を使用して、監視されていない言語単位を、大規模で公に発見された、書き起こされていない音声から抽出します。さらに、提案された方法を仮定された低リソースに拡張します。言語と客観的評価を使用してメソッドの有効性を検証します。最近、注意を払ってシーケンスからシーケンスへのモデルは、テキストからスピーチ（TTS）にうまく適用されています。 
[ABSTRACT]これらのモデルは、人間に近い音声を正確に大きく生成できます-書き起こされたテキストコーパス。現在、新しい教師なし事前トレーニングメカニズムの準備をしています。次に、シーケンス-教師付き言語ユニットを事前トレーニングします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Spectrum and Prosody Conversion for Cross-lingual Voice Conversion with
  CycleGAN -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_6.html">
      <font color="black">Spectrum and Prosody Conversion for Cross-lingual Voice Conversion with
  CycleGAN</font>
    </a>
  </h2>
  <font color="black">クロスリンガル音声変換に関するこれまでの研究は、主にF0転送の線形変換によるスペクトル変換に焦点を当てていました。クロスリンガル音声変換は、ソーススピーカーとターゲットスピーカーが異なる言語を話すときに、ソーススピーカーの音声をターゲットスピーカーの音声に変えることを目的としています。また、スペクトルマッピングと韻律マッピングの2つのCycleGANパイプラインをトレーニングすることも提案しています。 
[ABSTRACT]たとえば、2つのスピーカーのパラレルデータの必要性を排除します。これは、クロスリンガル音声変換における韻律の最初の研究です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Transformer with Bidirectional Decoder for Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_7.html">
      <font color="black">Transformer with Bidirectional Decoder for Speech Recognition</font>
    </a>
  </h2>
  <font color="black">推論段階では、左から右への候補を生成できるだけでなく、右から左への候補も生成できる導入された双方向ビーム検索方法を使用し、スコアによって最良の仮説を決定します。具体的には、提案されたトランスフォーマーには、左から右のターゲットと右から左のターゲットが含まれます。双方向デコーダー（STBD）を使用して提案された音声トランスフォーマーを示すために、AISHELL-1データセットで大規模な実験を行います。 
[ABSTRACT]私たちは双方向デコーダー（stbd）を使用して、提案された音声変換器を示しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Exploring Optimal DNN Architecture for End-to-End Beamformers Based on
  Time-frequency References -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_8.html">
      <font color="black">Exploring Optimal DNN Architecture for End-to-End Beamformers Based on
  Time-frequency References</font>
    </a>
  </h2>
  <font color="black">結果のモデルはW-Netビームフォーマーと呼ばれ、2つのコンポーネントが含まれています。 1つ目は、2つ目がビーム形成フィルターを推定するために使用する時間周波数参照を計算します。両方のアプローチが効果的です。ただし、それらの一般化可能性には盲点があります。したがって、これら2つの方法を組み合わせて、両方の最良の機能を利用しようとする単一のフレームワークにする新しいアプローチを提案します。 
[ABSTRACT]現在、最良の方法は、ディープニューラルネットワーク（dnn）およびdnnベースのフィルター-ビームフォーミングフィルターを直接計算するために使用される推定方法です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-23">
        <br><font color="black">2020-05-23</font>
      </time>
    </span>
</section>
<!-- paper0: Coswara -- A Database of Breathing, Cough, and Voice Sounds for COVID-19
  Diagnosis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_9.html">
      <font color="black">Coswara -- A Database of Breathing, Cough, and Voice Sounds for COVID-19
  Diagnosis</font>
    </a>
  </h2>
  <font color="black">これに向けて、この論文では、Coswaraと呼ばれる呼吸音、つまり咳、呼吸、声のデータベースを作成（および分析）する際の初期の取り組みについて説明します。COVID-19の顕著な症状には、咳や呼吸困難があります。サウンドサンプルは、ウェブサイトアプリケーションを使用した世界中のクラウドソーシングを通じて収集されます。 
[要約] covid、呼吸音の現在の標準的な方法は、有用な洞察を提供し、診断ツールの設計を可能にします。サウンドサンプルは、Webサイトアプリケーションを使用して世界中のクラウドソーシングで収集されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-21">
        <br><font color="black">2020-05-21</font>
      </time>
    </span>
</section>
<!-- paper0: A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal
  Transformer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_10.html">
      <font color="black">A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal
  Transformer</font>
    </a>
  </h2>
  <font color="black">既存の方法は、主に視覚機能のみを活用してこのタスクに取り組みますが、オーディオトラックは完全に無視します。高密度のビデオキャプションタスクでのオーディオと視覚のモダリティを持つ提案モデルの有効性を示しますが、モジュールは2つのモダリティを消化できます。また、バイモーダルトランスフォーマーの一部としての事前トレーニング済みバイモーダルエンコーダーを、単純な提案生成モジュールの特徴抽出器として使用できることも示しています。 
[ABSTRACT]バイモーダルトランスフォーマーモデルは、シンプルなプロポーザル生成モジュールの特徴エクストラクターとして使用できます。事前トレーニング済みのバイモーダルエンコーダーは、バイモーダルモデルの一部です。フィーチャーの作成にも使用できます。提案生成のためのキャプター</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-17">
        <br><font color="black">2020-05-17</font>
      </time>
    </span>
</section>
<!-- paper0: Alzheimer's Dementia Detection from Audio and Text Modalities -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_11.html">
      <font color="black">Alzheimer's Dementia Detection from Audio and Text Modalities</font>
    </a>
  </h2>
  <font color="black">x-ベクトル、i-ベクトル、および統計的な音声ベースの機能機能が評価されます。いくつかのモダリティ内およびモダリティ間スコア融合戦略が調査されます。6つの異なるシステムが構築され、比較されています。ベースで、他の2つのシステムはテキストベースです。 
[ABSTRACT]提出されたシステムは、患者の音声とメッセージの両方からアルツハイマー病のパターンを検出することを目的としています。これらは、音声とテキストシステムからそれらを識別することを目的としています。ソフトウェアは、アルツハイマー病のパターンを識別するために使用されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Meta-Learning for Short Utterance Speaker Recognition with Imbalance
  Length Pairs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_12.html">
      <font color="black">Meta-Learning for Short Utterance Speaker Recognition with Imbalance
  Length Pairs</font>
    </a>
  </h2>
  <font color="black">また、目に見えない話者を特定するための提案モデルを検証します。これにより、既存のアプローチよりも大幅にパフォーマンスが向上します。さらに、特定のエピソードのクラスのみを最適化することは、目に見えないクラスの識別的な埋め込みを学習するには不十分な場合があるため、さらにトレーニングセット内のクラスのセット全体に対してサポートとクエリセットの両方を分類するようにモデルを強制します。実際の設定では、話者認識システムは、短い発話を与えられた話者を識別する必要がありますが、登録発話は比較的長い場合があります。 
[ABSTRACT]既存の話者認識モデルは短い発話では不十分に機能します。既存のモデル認識モデルを使用してスキルをテストできます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-06">
        <br><font color="black">2020-04-06</font>
      </time>
    </span>
</section>
<!-- paper0: SpeechBERT: An Audio-and-text Jointly Learned Language Model for
  End-to-end Spoken Question Answering -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_13.html">
      <font color="black">SpeechBERT: An Audio-and-text Jointly Learned Language Model for
  End-to-end Spoken Question Answering</font>
    </a>
  </h2>
  <font color="black">エンドツーエンドのSQAの可能性に加えて、SpeechBERTは、多くのテキスト処理タスクのBERTと同じように、他の多くの音声言語理解タスクでも検討できます。エンドツーエンドモデルは、ASRがエラーを生成する前にオーディオデータから情報を抽出できることが示されたため、応答スパンのASRエラーを含むデータセットのアンサー（TQA）モデル。提案されたエンドツーエンドモデルを拡張する場合カスケードアーキテクチャにより、さらに優れたパフォーマンスが達成されました。 
[要約]成功したbertモデルの一部として、音声と質問のスピーチベルトモデルを提案します。提案されたエンドツーエンドを組み立てる一方で、十分に準備された効果もありました。成功したモデルからの学習に加えて、これらは成功した</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-25">
        <br><font color="black">2019-10-25</font>
      </time>
    </span>
</section>
<!-- paper0: Exploring Aligned Lyrics-Informed Singing Voice Separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_14.html">
      <font color="black">Exploring Aligned Lyrics-Informed Singing Voice Separation</font>
    </a>
  </h2>
  <font color="black">実験結果は、モデルが音声アクティビティ情報だけでなく、整列された歌詞の音声コンテンツも使用できることを示しています。このために、誤った歌詞が与えられたときのパフォーマンスの変化を観察することにより、多面的な方法でパフォーマンスの増加の原因を調査しましたこの論文では、歌声分離の性能を改善するための追加情報として整列歌詞を利用する方法を提案します。 
[ABSTRACT]正しい歌詞でトレーニングされたモデルは、実際には通知されなかったモデルよりも優れたパフォーマンスをもたらします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Exploring the Best Loss Function for DNN-Based Low-latency Speech
  Enhancement with Temporal Convolutional Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_15.html">
      <font color="black">Exploring the Best Loss Function for DNN-Based Low-latency Speech
  Enhancement with Temporal Convolutional Networks</font>
    </a>
  </h2>
  <font color="black">最も適切な方法は、データセットの規模とタスクのタイプによって異なります。私たちの提案する方法は、ボイスバンク+ DEMANDデータセットで効果的であり、他の最先端の方法と比較して有利です。STFT-より小さなデータセットの主観的な品質を改善するために、問題にとらわれない音声エンコーダ（PASE）機能を使用した、ベースの方法と損失関数。 
[ABSTRACT]時間-周波数マスキングはdnn音声強調に広く使用されていますが、tasnetやtasnetなどの時間領域法も提案されています。この方法は、音声バンクの需要データセットに効果的です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-23">
        <br><font color="black">2020-05-23</font>
      </time>
    </span>
</section>
<!-- paper0: Surgical Mask Detection with Convolutional Neural Networks and Data
  Augmentations on Spectrograms -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_16.html">
      <font color="black">Surgical Mask Detection with Convolutional Neural Networks and Data
  Augmentations on Spectrograms</font>
    </a>
  </h2>
  <font color="black">結果は、ComParEによって与えられたベースラインのほとんどが性能を上回っていることを示しています。人間の声のサンプルでのサージカルマスク検出のバイナリ分類タスクへのデータ拡張の影響を示しています（ComParEチャレンジ2020）。ここでの考え方は、モデルを-記述不足の小さなトレーニングデータセットの特徴分布に適合。 
[ABSTRACT]これは、データ拡張がトレーニングデータの欠如を克服することを約束する場所です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: S-vectors: Speaker Embeddings based on Transformer's Encoder for
  Text-Independent Speaker Verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_17.html">
      <font color="black">S-vectors: Speaker Embeddings based on Transformer's Encoder for
  Text-Independent Speaker Verification</font>
    </a>
  </h2>
  <font color="black">この構造からの話者の埋め込みをs-vectorsと名付けました。この論文では、発声レベルの特徴を得るために適切な統計プーリングの後で、訓練されたトランスフォーマーエンコーダー構造の出力から話者の埋め込みを導出することを提案しました。基本アーキテクチャとして自己注意に基づいて構築され、話者分類タスクを実行するようにトレーニングしたトランスフォーマーのエンコーダー構造。 
[ABSTRACT] x-erzliesは、時間遅延ニューラルネットワーク（tdnn）を使用して取得されます。これらは、自己注意に基づいて構築されたトランスフォーマーのエンコーダー構造を基本アーキテクチャとして使用して構築されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Neural PLDA Modeling for End-to-End Speaker Verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_18.html">
      <font color="black">Neural PLDA Modeling for End-to-End Speaker Verification</font>
    </a>
  </h2>
  <font color="black">ディープラーニングモデルは教師付き分類問題で大きな進歩を遂げましたが、これらのモデルの話者認識などのアウトオブセット検証タスクへの適用は、特徴埋め込みの導出に限定されています。ニューラルPLDA（NPLDA）と呼ばれる話者検証では、生成PLDAモデルの尤度比スコアが判別類似度関数として提起され、スコア関数の学習可能なパラメーターが検証コストを使用して最適化されます。アートx-ベクトルPLDAベースの話者検証システムは、検証スコアの計算に確率的線形判別分析（PLDA）に基づく生成モデルを使用します。 
[要約] npldaネットワークを使用した提案されたe2eモデルは、モデルよりも大幅に改善されています。提案されたモデルは、他のさまざまなモデルとリンクされています。新しいモデルは、テスト用のモデルとして使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Neuro-Steered Hearing Devices: Decoding Auditory Attention From the
  Brain -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_19.html">
      <font color="black">Neuro-Steered Hearing Devices: Decoding Auditory Attention From the
  Brain</font>
    </a>
  </h2>
  <font color="black">この論文では、この分野における主な信号処理の課題に取り組み、最新のAADアルゴリズムのレビューと比較研究を提供します。聴覚障害に苦しむ人々は、いわゆる「カクテル」での会話に参加することがしばしば困難です。複数の人が同時に話しているパーティのシナリオ。最近の神経科学の進歩により、脳波（EEG）などの非侵襲的な神経記録技術から聴覚注意の焦点を決定できることが示されています。 
[要旨]聴覚装置には、ユーザーが実際に聴きたいスピーカーの情報が必要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: PlugSonic: a web- and mobile-based platform for binaural audio and sonic
  narratives -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_20.html">
      <font color="black">PlugSonic: a web- and mobile-based platform for binaural audio and sonic
  narratives</font>
    </a>
  </h2>
  <font color="black">PlugSonicの主な目的は技術の民主化です。 PlugSonicユーザー-機関であれ市民であれ-はすべて、3Dサウンドスケープとソニックナラティブの作成、処理、体験に必要な機器を与えられています。特定のデバイス、外部ツール（ソフトウェアおよび/またはハードウェア）、専門知識、またはカスタム開発を必要としません。PlugSonic内のオーディオ処理は、Web Audio APIと3D Tune-In Toolkitに基づいています。物理的なスペースは、AppleのARKitを使用して取得されます。PlugSonicは、バイノーラルインタラクティブサウンドスケープとソニックナラティブのキュレーションとエクスペリエンスのためのWebベースおよびモバイルベースのアプリケーションスイートです。 
[ABSTRACT] plugsonicサウンドスケープは、pluggy eu project.itの一部として作成されました。2つの主要なアプリケーションで構成されています：プラグインサンプル、オーディオエフェクトを編集および適用する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Why Did the x-Vector System Miss a Target Speaker? Impact of Acoustic
  Mismatch Upon Target Score on VoxCeleb Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_21.html">
      <font color="black">Why Did the x-Vector System Miss a Target Speaker? Impact of Acoustic
  Mismatch Upon Target Score on VoxCeleb Data</font>
    </a>
  </h2>
  <font color="black">標準のカルディxベクトルシステムがASVブラックボックスを形成する一方で、線形混合効果モデルの予測子として、選択した音響機能の1次および2次統計の距離を使用します。VoxCelebデータの結果から、最も顕著な不一致係数はF0の平均であり、その後にフォルマント周波数に関連する不一致があります。最新の自動話者検証（ASV）は、ディープニューラルネットワークを通じて実装された機械学習に大きく依存しています。 
[要約]ターゲットスピーカーのミスを予測する不一致要因を特定することを目的としています。これらの要因を使用して、潜在的なスピーカーミスを検出します（誤った拒否）</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Investigation of End-To-End Speaker-Attributed ASR for Continuous
  Multi-Talker Recordings -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_22.html">
      <font color="black">Investigation of End-To-End Speaker-Attributed ASR for Continuous
  Multi-Talker Recordings</font>
    </a>
  </h2>
  <font color="black">これは、さまざまな数の話者で構成されるシミュレートされた音声混合で有望な結果を示しました。関連する話者プロファイルを持つ元のE2E SA-ASRと比較して、提案された方法は、事前の話者知識なしで近いパフォーマンスを実現します。また、簡単な変更を提案します。 E2E SA-ASRトレーニングの参照ラベルは、継続的なマルチトーカー録音の処理に役立ちます。 
[要約]提案された方法は、事前の話者の知識なしに近いスタートを達成しました。e2esa-asrトレーニング-マルチトーカー録音を処理できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Acoustic effects of medical, cloth, and transparent face masks on speech
  signals -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_23.html">
      <font color="black">Acoustic effects of medical, cloth, and transparent face masks on speech
  signals</font>
    </a>
  </h2>
  <font color="black">ほとんどのマスクはラペルマイクにほとんど影響を与えません。これは、既存のサウンド補強および支援リスニングシステムがマスクとの口頭によるコミュニケーションに効果的である可能性があることを示唆しています。周波数が1 kHzを超える場合、その減衰は送話者の前で最大になり、マスクの種類、特に異なる素材や織りの布マスクの間にはかなりのばらつきがあります。 
[ABSTRACT]この調査では、さまざまなフェイスマスクによって引き起こされる音響減衰を調べます。これには、ヘッド型のスピーカーとライブの人間の話し手が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Efficient Integration of Multi-channel Information for
  Speaker-independent Speech Separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_24.html">
      <font color="black">Efficient Integration of Multi-channel Information for
  Speaker-independent Speech Separation</font>
    </a>
  </h2>
  <font color="black">また、低チャネルネットワーク用にトレーニングされたパラメーターを高チャネルネットワークの初期値として適用する転移学習フレームワークであるチャネル順次転移学習も提案します。公平な比較のために、オープンソースのwsj0-2mixデータセットの空間化されたバージョン。提案された方法は、マルチチャネルのディープクラスタリングよりも優れており、マイクの数に比例してパフォーマンスを向上できることがわかりました。 
[要約]時間領域の音声分離ネットワークに基づいてマルチチャネル情報を統合する2つの方法を提案します。オープンベースのwsj0-2mixデータセットの空間バージョンを使用して方法を評価しました。後期のパフォーマンス-フュージョン方式は、スピーカー間の角度の違いに関係なく、シングルチャネル方式よりも常に高い</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-23">
        <br><font color="black">2020-05-23</font>
      </time>
    </span>
</section>
<!-- paper0: Bunched LPCNet : Vocoder for Low-cost Neural Text-To-Speech Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-12/eess.AS/paper_25.html">
      <font color="black">Bunched LPCNet : Vocoder for Low-cost Neural Text-To-Speech Systems</font>
    </a>
  </h2>
  <font color="black">提案されたバンチング手法を使用すると、LPCNetは、Deep Convolutional TTS（DCTTS）音響モデルと組み合わせて、モバイルデバイスで実行したときにベースラインランタイムより2.19倍向上し、TTS平均オピニオンスコアの減少は0.1未満である（MOS）。これらの手法は次のとおりです。1）LPCNetが推論ごとに複数のオーディオサンプルを生成できるようにするサンプルバンチング。および2）ビットバンチング。LPCNetの最終層での計算を削減します。LPCNetは、線形予測とディープニューラルネットワークモジュールを組み合わせて計算の複雑さを低く抑える効率的なボコーダーです。 
[ABSTRACT]低コストのlpcnetボコーダーベースのニューラルテキストからオーディオへのシステム。新しい調査では、ベースラインネットワークに対して2.19倍の改善が見られました。0未満です。tts平均オピニオンスコア（mos）の減少</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
