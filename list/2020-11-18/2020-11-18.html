<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-11-18の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Accent and Speaker Disentanglement in Many-to-many Voice Conversion -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.SD/paper_0.html">
      <font color="black">Accent and Speaker Disentanglement in Many-to-many Voice Conversion</font>
    </a>
  </h2>
  <font color="black">提案されたトリックはアクセントを改善するのに非常に効果的であり、オーディオ品質と話者の類似性はよく維持されます。これは変換モデルトレーニングのためのBN機能の言語情報以外の他の要因を一掃することを目的としています。この論文は興味深い音声とアクセントのジョイントを提案します。任意のソーススピーカーの音声を非ネイティブアクセントのターゲットスピーカーに変換できる変換アプローチ。 
[概要]変換モデルのトレーニングでアクセントと話者の情報を解きほぐし、変換段階でそれらを再結合する必要があります。敵対的なトレーニングを使用して、エンコーダーデコーダベースの変換モデルで話者とアクセントの情報を解きほぐすことを提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Ultra-Lightweight Speech Separation via Group Communication -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.SD/paper_1.html">
      <font color="black">Ultra-Lightweight Speech Separation via Group Communication</font>
    </a>
  </h2>
  <font color="black">サブバンド出力が連結されている標準のF-LSTMモデルとは異なり、超小型モジュールがすべてのグループに並列に適用されるため、モデルサイズを大幅に縮小できます。このペーパーでは、単純なモデル設計を提供します。パフォーマンスを犠牲にすることなく超軽量モデルを明示的に設計するパラダイム..サブバンド周波数-LSTM（F-LSTM）アーキテクチャに動機付けられて、特徴ベクトルがより小さなグループに分割されるグループ通信（GroupComm）を導入します。小さな処理ブロックは、グループ間通信を実行するために使用されます。 
[概要]グループ通信（groupcomm）は、35.6倍少ないパラメータと2.3倍少ない操作で同等のパフォーマンスを達成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Gender domain adaptation for automatic speech recognition task -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.SD/paper_2.html">
      <font color="black">Gender domain adaptation for automatic speech recognition task</font>
    </a>
  </h2>
  <font color="black">さらに、アクセント付き音声の完全なL2 Arcticデータセットに基づいて基本モデルを適合させ、特定の話者と男性と女性の性別ごとに微調整しました。最後に、事前にトレーニングされたxベクトル音声埋め込みとからの埋め込みの連結をテストしました。従来のエンコーダーですが、精度の向上は重要ではありません。一般に、このアプローチによる微調整技術では、本質的なWERの削減は得られません。 
[概要]性別サブセットでトレーニングされたモデルは、l2北極データセット全体で調整されたモデルと比較して1〜2％高い精度を取得しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-08">
        <br><font color="black">2020-10-08</font>
      </time>
    </span>
</section>
<!-- paper0: Hierarchical Prosody Modeling for Non-Autoregressive Speech Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.SD/paper_3.html">
      <font color="black">Hierarchical Prosody Modeling for Non-Autoregressive Speech Synthesis</font>
    </a>
  </h2>
  <font color="black">提案された方法は、客観的および主観的な評価において、オーディオ品質と韻律の自然さの点で他の競合他社よりも優れています。韻律モデリングは、現代のテキスト読み上げ（TTS）フレームワークに不可欠なコンポーネントです。TTSモデルに韻律機能を明示的に提供することによってしたがって、合成された発話のスタイルを制御することができます。 
[概要]この作品では、さまざまな韻律の下での非自己回帰モデルの動作を分析しました-モデリング設定。韻律機能を提供することに加えて、平坦化された音声のスタイルを制御できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-12">
        <br><font color="black">2020-11-12</font>
      </time>
    </span>
</section>
<!-- paper0: Optimizing voice conversion network with cycle consistency loss of
  speaker identity -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.SD/paper_4.html">
      <font color="black">Optimizing voice conversion network with cycle consistency loss of
  speaker identity</font>
    </a>
  </h2>
  <font color="black">CMU-ARCTICおよびCSTR-VCTKコーパスで実施された実験により、提案された方法が話者の類似性の点でベースライン方法よりも優れていることが確認されました。提案されたトレーニングスキームはすべての音声変換ネットワークに適用できますが、平均モデル音声変換フレームワークの下で研究を定式化します。この論文では..変換された音声を制約して、発声レベルで参照音声と同じ話者のアイデンティティを維持するサイクル一貫性の損失を紹介します。 
[ABSTRACT]提案されたトレーニングスキームは音声損失を減らすだけでなく、音声アイデンティティの損失も減らします。提案されたスキームは音声変換ネットワークを可能にします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: An Overview of Voice Conversion and its Challenges: From Statistical
  Modeling to Deep Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.SD/paper_5.html">
      <font color="black">An Overview of Voice Conversion and its Challenges: From Statistical
  Modeling to Deep Learning</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、最先端の音声変換手法と、統計的アプローチからディープラーニングまでのパフォーマンス評価方法の包括的な概要を示し、その約束と制限について説明します。スピーカーのアイデンティティは重要なものの1つです。人間の音声の特性..また、最近の音声変換の課題（VCC）、現在の技術のパフォーマンスを報告し、音声変換の研究に利用可能なリソースの概要を提供します。 
[概要]音声変換では、言語コンテンツを変更せずに、話者のアイデンティティを変更します。話者の類似性が高い、人間のような音声品質を生成できるようになりました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-09">
        <br><font color="black">2020-08-09</font>
      </time>
    </span>
</section>
<!-- paper0: FoolHD: Fooling speaker identification by Highly imperceptible
  adversarial Disturbances -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.SD/paper_6.html">
      <font color="black">FoolHD: Fooling speaker identification by Highly imperceptible
  adversarial Disturbances</font>
    </a>
  </h2>
  <font color="black">話者識別モデルは、誤分類を引き起こす入力信号の慎重に設計された敵対的摂動に対して脆弱です。話者識別のパフォーマンスを妨げることに加えて、この多目的損失は、から抽出されたMFCC特徴ベクトル間のフレームごとの余弦類似性を通じて人間の知覚を説明します。元の音声ファイルと敵対的な音声ファイル..私たちのアプローチであるFoolHDは、元の音声ファイル内の敵対的な摂動を生成して隠すために、DCTドメインで動作し、多目的損失関数でトレーニングされたゲート付き畳み込み自動エンコーダーを使用します。 
[ABSTRACT] trickhdは、ホワイトボックスステガノグラフィに触発された敵対的攻撃を使用します。これは、知覚できない敵対的生成ネットワークの欠如に基づいています。話者識別パフォーマンスを妨げることに加えて、この多目的損失は、フレームを介した人間の知覚を説明します。 xfcsとmfccの間のコサイン類似性</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: s-Transformer: Segment-Transformer for Robust Neural Speech Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.SD/paper_7.html">
      <font color="black">s-Transformer: Segment-Transformer for Robust Neural Speech Synthesis</font>
    </a>
  </h2>
  <font color="black">エンコーダとデコーダの両方のキャッシュメモリを介して繰り返しが再利用されるセグメントレベルで音声をモデル化する新しいセグメントトランスフォーマ（s-Transformer）を提案します。長距離コンテキストは拡張メモリによってキャプチャできますが、エンコーダは-処理がはるかに簡単なセグメントへのデコーダーの注意..音声発声を特徴付けるトランスフォーマーは、音声合成の大幅な改善を達成しました。 
[要約]短い文では、どちらも同じmosスコア4.29を達成します。これは、長い文では4.2に非常に近い値です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Refining Automatic Speech Recognition System for older adults -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.SD/paper_8.html">
      <font color="black">Refining Automatic Speech Recognition System for older adults</font>
    </a>
  </h2>
  <font color="black">TLの基本的な考え方、モデルパラメータの調整に基づいて、注意メカニズムを活用してモデルの中間情報を活用することで、システムをさらに改善します。12時間のトレーニングデータを使用して、社会的に孤立した高齢者向けのASRシステムの開発を試みます（80 +歳）認知障害の可能性あり..私たちのアプローチは、TLモデルに対して1.58％の絶対的な改善を達成します。 
[概要]自動システムシステムシステムは、成人からの十分なデータに基づいてトレーニングされています。成人と高齢者の音響の不一致により、高齢者のスピーチの影響を受けやすくなっています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Implicit Filter-and-sum Network for Multi-channel Speech Separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.SD/paper_9.html">
      <font color="black">Implicit Filter-and-sum Network for Multi-channel Speech Separation</font>
    </a>
  </h2>
  <font color="black">アドホックマイクアレイジオメトリと固定マイクアレイジオメトリの両方での実験結果は、iFaSNetと呼ばれるFaSNetの提案された変更が、モデルの複雑さを超えて、すべての条件でベンチマークFaSNetを大幅に上回ることができることを示しています。抽出の観点から、サンプルレベルの正規化相互相関（NCC）機能の計算を機能レベルのNCC（fNCC）機能に変更します。問題の定式化の観点から、以下を含む明示的な時間領域フィルターと合計の操作を変更します。すべてのマイクロフォンを、参照マイクロフォンのみの潜在空間での暗黙的なフィルターアンドサム操作に変換します。 
[ABSTRACT] filter-および--sum network（fasnet）は、アドホックおよび固定マイクアレイジオメトリで効果的であることが示されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Cascade RNN-Transducer: Syllable Based Streaming On-device Mandarin
  Speech Recognition with a Syllable-to-Character Converter -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.SD/paper_10.html">
      <font color="black">Cascade RNN-Transducer: Syllable Based Streaming On-device Mandarin
  Speech Recognition with a Syllable-to-Character Converter</font>
    </a>
  </h2>
  <font color="black">いくつかの重要なトリックを導入することにより、カスケードRNN-Tアプローチは、文字ベースのRNN-Tを大幅に上回り、いくつかの北京語テストセットで、はるかに高い認識品質と同様の遅延を実現します。文字ベースの言語であり、各文字は音の音節として発音されます。このペーパーでは、RNN-Tの言語モデリング機能を向上させるための新しいカスケードRNN-Tアプローチを提案します。私たちのアプローチでは、最初にRNN-Tを使用して音響機能を音節シーケンスに変換し、次にRNN-Tベースの音節から文字へのコンバーターを介して音節シーケンスを文字シーケンスに変換します。したがって、リッチテキストリポジトリを簡単に使用できます。言語モデル能力を強化します。 
[概要]言語認識システムは、言語モデルの能力を強化するために使用できます。新しい言語モデルを開発するために簡単に使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Rethinking the Separation Layers in Speech Separation Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.SD/paper_11.html">
      <font color="black">Rethinking the Separation Layers in Speech Separation Networks</font>
    </a>
  </h2>
  <font color="black">標準のSIMOのみの設計と比較して、同じモデルサイズの混合SIMO-SISO設計は、特に低オーバーラップ条件下で分離性能を向上させることができることを示します。分離モデルの大部分にはSIMOアーキテクチャのみが含まれていますが、また、エンハンスメント後のSISOモジュールと統合された特定の2ステージ分離システムにより、分離品質を向上できることも示されています。SIMOモジュールは入力よりも多くの出力を生成し、SISOモジュールは入力と出力の数を同じに保ちます。 
[ABSTRACT] simoモジュールは出力よりも多くの出力を生成し、sisoモジュールは出力の数を同じに保ちます。ほとんどのモデルは、パフォーマンスを犠牲にすることなく分離を実行できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Fine-grained Emotion Strength Transfer, Control and Prediction for
  Emotional Speech Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.SD/paper_12.html">
      <font color="black">Fine-grained Emotion Strength Transfer, Control and Prediction for
  Emotional Speech Synthesis</font>
    </a>
  </h2>
  <font color="black">感情のグローバルレンダリングとローカル記述子を使用すると、参照オーディオから感情記述子を介して（転送用）、または音素レベルの手動ラベルから直接（制御用）、きめ細かい感情表現を取得できます。きめ細かい感情表現を実行します。生成では、学習したランキング機能を介して音素レベルの感情の強さの表現を導入し、ローカルの感情の詳細を記述します。文レベルの感情カテゴリを採用して、合成されたスピーチのグローバルな感情をレンダリングします。このような粗いラベルでは、スピーチの詳細を制御できません。感情。多くの場合、平均的な感情表現の配信になります。また、推論中に適切な参照オーディオを選択することも困難です。 
[要約]提案されたモデルは、テキストから音素レベルの感情表現を予測することもできます。これには、参照オーディオや手動ラベルは必要ありません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Controllable Emotion Transfer For End-to-End Speech Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.SD/paper_13.html">
      <font color="black">Controllable Emotion Transfer For End-to-End Speech Synthesis</font>
    </a>
  </h2>
  <font color="black">合成音声の感情の強さは、感情の埋め込みの値を調整することで制御できます。感情の埋め込みは、メルスペクトルの特徴マップとして表示できるためです。最初に、2つの感情分類子を接続します。1つは参照エンコーダーの後にあります。 、デコーダー出力の次の1つ-感情の埋め込みと予測されるメルスペクトルの感情識別能力を強化します。さらに、適切な参照を選択して、目的の感情の強さを提供することは困難です。 
[要約]合成音声は、感情の混乱では十分に正確で表現力がありません。感情カテゴリの混乱でもより正確です。提案された方法はタコトロンに基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Simplified Self-Attention for Transformer-based End-to-End Speech
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.SD/paper_14.html">
      <font color="black">Simplified Self-Attention for Transformer-based End-to-End Speech
  Recognition</font>
    </a>
  </h2>
  <font color="black">印象的な20％のパラメーター削減により、私たちのモデルは20,000時間の大規模タスクで認識パフォーマンスの損失を示しません。ただし、このような改善は通常、非常に大規模なニューラルネットワークを使用することで得られます。トランスフォーマーモデルには主に2つのサブモジュールが含まれます-位置ごとのフィードフォワード層と自己注意（SAN）層。 
[ABSTRACT]トランスフォーマーモデルは、aishellで実際に20％以上の削減を達成できます-1つのタスク</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-21">
        <br><font color="black">2020-05-21</font>
      </time>
    </span>
</section>
<!-- paper0: Learn2Sing: Target Speaker Singing Voice Synthesis by learning from a
  Singing Teacher -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.SD/paper_15.html">
      <font color="black">Learn2Sing: Target Speaker Singing Voice Synthesis by learning from a
  Singing Teacher</font>
    </a>
  </h2>
  <font color="black">一方、ターゲットスピーカーには音楽関連の文字起こしがないため、統一された入力表現を構築するための音響モデルの入力として、対数スケールの基本周波数（LF0）を補助機能として使用します。複数のターゲットスピーカーからの歌唱コーパスとスピーチは、フレームレベルの自動回帰音響モデルでトレーニングされます。このモデルでは、歌声とスピーキングが共通のスピーカー埋め込みとスタイルタグ埋め込みを共有します。スピーチ合成の急速な発展に伴い、歌声合成が注目を集めています。範囲。 
[概要] learning2singは、歌の先生を使用して、歌声データなしでターゲットスピーカーの歌声を生成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: DARF: A data-reduced FADE version for simulations of speech recognition
  thresholds with real hearing aids -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.SD/paper_16.html">
      <font color="black">DARF: A data-reduced FADE version for simulations of speech recognition
  thresholds with real hearing aids</font>
    </a>
  </h2>
  <font color="black">8 dB）.. DARFは、（ドイツ語）マトリックスセンテンステストの約30分の記録および処理された信号で1つの音声認識しきい値（SRT）をシミュレートします。 。 
[ABSTRACT] darfは、補聴器の開発またはモデルベースのフィッティングだけでなく、改善にも使用できます。以前、聴覚弁別実験のフレームワークは、二乗平均平方根予測誤差が3db未満の補聴器アルゴリズムの利点を正確にシミュレートしました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-10">
        <br><font color="black">2020-07-10</font>
      </time>
    </span>
</section>
<!-- paper0: Universal Adversarial Audio Perturbations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.SD/paper_17.html">
      <font color="black">Universal Adversarial Audio Perturbations</font>
    </a>
  </h2>
  <font color="black">欲張りアプローチとは異なり、ペナルティ法は、サンプルのバッチで適切な目的関数を最小化します。さまざまな1D CNNアーキテクチャを攻撃した実験結果では、ターゲット攻撃と非ターゲット攻撃でそれぞれ85.0％と83.1％を超える攻撃成功率が示されています。提案されたペナルティ法..さらに、提案されたペナルティ法が、普遍的な敵対的摂動に対応する解に理論的に収束するという証拠を提供します。 
[ABSTRACT]ユニバーサル敵対的摂動はユニバーサル敵対的で見つかります。ただし、トレーニングサンプルの数が限られている場合は、攻撃がより成功します。ターゲットデータセットからサンプルが1つしかない場合は、この方法を使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-08">
        <br><font color="black">2019-08-08</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-band MelGAN: Faster Waveform Generation for High-Quality
  Text-to-Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.SD/paper_18.html">
      <font color="black">Multi-band MelGAN: Faster Waveform Generation for High-Quality
  Text-to-Speech</font>
    </a>
  </h2>
  <font color="black">間もなくオープンリソースとなるPytorchの実装では、ハードウェア固有の最適化を行わなくても、CPUでリアルタイム係数0.03を達成できます。提案されたマルチバンドMelGANは、波形生成とTTSでそれぞれ4.34と4.22の高いMOSを達成しました。 ..わずか191万のパラメーターで、私たちのモデルは、元のMelGANの計算の複雑さの合計を5.85から0.95GFLOPSに効果的に削減します。 
[概要]ジェネレータは、入力としてメルスペクトログラムを受け取り、サブバンド信号を生成します。サブバンド信号は、その後、ディスクリミネータ入力としてフルバンド信号に合計されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-11">
        <br><font color="black">2020-05-11</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Training for Multi-domain Speaker Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.SD/paper_19.html">
      <font color="black">Adversarial Training for Multi-domain Speaker Recognition</font>
    </a>
  </h2>
  <font color="black">提案手法を採用することで、話者認識のためのマルチドメイン不変音声表現と話者識別音声表現の両方を取得することができます。ただし、通常、トレーニングデータと評価データの両方を複数のサブセットで構成できます。話者認識におけるドメインの不一致を排除するために正常に使用されました。 
[要約]提案された方法は、不一致の問題を解決するのに効果的ですが、比較された教師なしドメイン適応方法よりも優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Uncertainty Modelling in Deep Neural Networks for Image Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.IV/paper_0.html">
      <font color="black">Uncertainty Modelling in Deep Neural Networks for Image Data</font>
    </a>
  </h2>
  <font color="black">最後に、分類のための一般的な画像データセットでの方法の効率を示します。ディープニューラルネットワークは、広範囲のタスクで印象的なパフォーマンスを最近達成した強力なブラックボックス予測子です。モデルの予測の不確実性を定量化することは、それが可能にするため重要です。たとえば、情報に基づいた方法でモデルの出力に作用することにより、AIシステムの安全性が向上します。 
[概要]システムにいくつかのツールを装備して、予測が不確かな場合に通知することができます。これは、自動運転車の制御、医療画像分析、財務見積もりなど、エラーのコストが高いアプリケーションにとって重要です。または法的分野</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning Based HPV Status Prediction for Oropharyngeal Cancer
  Patients -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.IV/paper_1.html">
      <font color="black">Deep Learning Based HPV Status Prediction for Oropharyngeal Cancer
  Patients</font>
    </a>
  </h2>
  <font color="black">ゼロからトレーニングされた3D畳み込みニューラルネットワーク（CNN）およびImageNetで事前トレーニングされた2Dアーキテクチャと比較して、ビデオ事前トレーニングモデルが最高のパフォーマンスを発揮しました。スポーツビデオクリップで事前トレーニングされた3D畳み込みネットワークは、完全になるように微調整されました。 CT画像の3D情報を活用できます。イメージングベースのHPVステータス検出のための深層学習モデルの能力を調査しました。 
[概要]ビデオの事前トレーニング済みモデルは、hpvを区別することができました-外部テストセットの受信者動作特性曲線の下の領域が0.81の陽性と陰性のケース</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Assistive Diagnostic Tool for Brain Tumor Detection using Computer
  Vision -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.IV/paper_2.html">
      <font color="black">Assistive Diagnostic Tool for Brain Tumor Detection using Computer
  Vision</font>
    </a>
  </h2>
  <font color="black">モデルは20エポックでトレーニングされ、後でテストされました。これは、モデルが高レベルで実行できたことを示しています。MaskR畳み込みニューラルネットワーク（Mask R CNN）などのコンピュータービジョン技術を使用して、セグメント脳腫瘍は、予測精度を高めながら、ヒューマンエラーの可能性を軽減できます。 
[概要]脳腫瘍の決定的かつ早期の診断は、一部の人にとっては生と死の違いである可能性があります。マスクr畳み込みニューラルネットワークなどのコンピュータービジョン技術を使用すると、予測精度を高めながら、ヒューマンエラーの可能性を軽減できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Temporally Coherent Embeddings for Self-Supervised Video Representation
  Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.IV/paper_3.html">
      <font color="black">Temporally Coherent Embeddings for Self-Supervised Video Representation
  Learning</font>
    </a>
  </h2>
  <font color="black">世界の高レベルの視覚情報がスムーズに変化するのと同じように、学習された表現の近くのフレームは、同様のプロパティを示すことでメリットが得られると考えています。この仮定を使用して、隣接するフレームが近くに存在するようにビデオをエンコードするようにTCEモデルをトレーニングします。このペーパーでは、TCE：自己監視型ビデオ表現学習のための時間的にコヒーレントな埋め込みについて説明します。 
[概要]提案された方法は、ラベルのないビデオデータを使用して、埋め込みスペースで時間的コヒーレンシを明示的に適用します。さらに、隣接するフレームが互いに近くに存在し、ビデオが互いに分離されるようにビデオをエンコードするようにtceモデルをトレーニングします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-21">
        <br><font color="black">2020-03-21</font>
      </time>
    </span>
</section>
<!-- paper0: Decision and Feature Level Fusion of Deep Features Extracted from Public
  COVID-19 Data-sets -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.IV/paper_4.html">
      <font color="black">Decision and Feature Level Fusion of Deep Features Extracted from Public
  COVID-19 Data-sets</font>
    </a>
  </h2>
  <font color="black">実験結果は、提案されたアプローチが、既存の研究と同等の精度と優れた適合率/再現率値によって証明された高いCOVID-19検出性能を達成したことを示しています。既存のCOVID-19検出よりも優れた提案された方法の側面研究が議論され、提案されたアプローチの融合性能は、クラスアクティベーションマッピング技術を使用して視覚的に検証されました。CNNを使用して抽出された深い特徴セットは、特徴レベルの融合のために連結され、目的を持って決定レベルの融合のアイデアの観点から複数の分類器に供給されましたCOVID-19、肺炎、および発見されていないクラスを区別すること。 
[要約] covid-19は伝染性と伝染性が高いです。伝染性と伝染性の伝染性に関連しています。しかし、rt-pcrは、特に初期段階では感度が低いという問題があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: A Digital Image Processing Approach for Hepatic Diseases Staging based
  on the Glisson's Capsule -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.IV/paper_5.html">
      <font color="black">A Digital Image Processing Approach for Hepatic Diseases Staging based
  on the Glisson's Capsule</font>
    </a>
  </h2>
  <font color="black">超音波画像では、グリソン鞘は、文献の古典的な方法で抽出できる線の形で表示されます。この目的のために、さまざまなタイプのデータ、つまり超音波画像、バイナリ画像を処理するいくつかの分類器が考慮されます。グリソンの線と元の画像から抽出された特徴ベクトルを描いています。これは、弾性超音波検査の結果に基づいて遡及的に実施された予備調査です。 
[概要]グリソン鞘が見える臓器の特定の領域に焦点を当てて、肝臓の超音波画像を研究します。これは、弾性超音波検査の結果に基づいて遡及的に実施された予備研究です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Noise adaptive beamforming for linear array photoacoustic imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.IV/paper_6.html">
      <font color="black">Noise adaptive beamforming for linear array photoacoustic imaging</font>
    </a>
  </h2>
  <font color="black">残念ながら、これらの既存の適応ベースのアルゴリズムは、処理ハードウェアの計算能力の向上を要求するため、計算コストが高くなります。この記事では、ノイズレベルの変動を考慮した変動コヒーレンス係数（VCF）という名前の新しい適応重み係数を紹介します。これらの問題に対処するために、画質を改善するためにいくつかの適応重み付けベースのアルゴリズムが導入されました。 
[ABSTRACT] dasメソッドは、適切な遅延と合計を使用してビーム形成信号を計算することに依存しています。これらのアルゴリズムは、画質を向上させるために導入されました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Sentinel-2 Spectral Dynamics for Long-Run Predictions Using
  Residual Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.IV/paper_7.html">
      <font color="black">Learning Sentinel-2 Spectral Dynamics for Long-Run Predictions Using
  Residual Neural Networks</font>
    </a>
  </h2>
  <font color="black">この論文では、ニューラルネットワークアーキテクチャを適応させて、シミュレートされた時系列と実際のマルチスペクトル時系列の両方の周期的ダイナミクスを学習します。周期的ダイナミクスをキャプチャするために適切な状態変数を選択する必要性を強調し、モデルが平均的な季節的ダイナミクスを再現できることを示します。たった1年間のトレーニングデータを使用して植生を分析します。時間ダイナミクスをキャプチャできると、混合解除や分類などのタスクの結果が大幅に向上し、統一された原理的なプロセスでフィルタリング、ノイズ除去、補間の問題に取り組むことができます。時系列での処理ダイナミクスは古典的にダイナミクスモデルと観測モデルの知識を必要とします。前者は正しくないか、計算上扱いにくいため、データから直接ダイナミクスを学習することを目的としたデータ駆動型戦略を動機付けます。 
[ABSTRACT]時系列ダイナミクスをキャプチャできることで、混合解除や分類などのタスクの結果を大幅に改善できます。これにより、統一された原理的なプロセスでフィルタリング、ノイズ除去、または補間の問題に取り組むことができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: PerceptNet: A Human Visual System Inspired Neural Network for Estimating
  Perceptual Distance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.IV/paper_8.html">
      <font color="black">PerceptNet: A Human Visual System Inspired Neural Network for Estimating
  Perceptual Distance</font>
    </a>
  </h2>
  <font color="black">同様の深層学習方法と比較すると、パフォーマンスは似ていますが、ネットワークには数桁少ないパラメーターがいくつかあります。また、古典的な深層ニューラルネットワークアーキテクチャに人間の視覚系に触発された非線形性を含めると増加する可能性があることも示しています。知覚の類似性を判断する能力..最近の研究では、人間の知覚の質を予測するように訓練されたディープニューラルネットワークが提示されていますが、人間の視覚系から直感を借りる人はほとんどいません。 
[概要]人間の視覚システムは、画質を判断する能力をどの程度決定するかを複製するために、さまざまな摂動を処理します。これらのアーキテクチャは、知覚の類似性を決定する能力を高めることができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-28">
        <br><font color="black">2019-10-28</font>
      </time>
    </span>
</section>
<!-- paper0: AI slipping on tiles: data leakage in digital pathology -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.IV/paper_9.html">
      <font color="black">AI slipping on tiles: data leakage in digital pathology</font>
    </a>
  </h2>
  <font color="black">「タイル」）が考慮されます。適切に設計されたDAP（10x5の繰り返し交差検証）を使用しても、同じ主題のタイルをトレーニングセットと検証セットの両方で使用すると、予測スコアが最大41％膨らむ可能性があることを証明します。ディープラーニングモデル..2つの公開データセット（TCGAとGTEx）でソリューションを検証します。 
[概要] maqcコンソーシアムは、データ分析では見落とされがちであると主張しています。適切に設計されたdapを使用しても、同じ主題のタイルを使用すると、予測スコアが最大41％膨らむ可能性があるとのことです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-14">
        <br><font color="black">2019-09-14</font>
      </time>
    </span>
</section>
<!-- paper0: Exposure Interpolation by Combining Model-driven and Data-driven Methods -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.IV/paper_10.html">
      <font color="black">Exposure Interpolation by Combining Model-driven and Data-driven Methods</font>
    </a>
  </h2>
  <font color="black">実験結果は、深層学習法を使用して従来の方法で補間画像を洗練することにより、中程度の露出画像の品質が大幅に向上することを示しています。従来の方法を採用して、深層学習法の収束速度を改善し、低減することができます。深層学習法に必要なサンプル数..従来の深層学習法と深層学習法を融合するフレームワークを導入して、2つの大露光画像に対して中露光画像を生成します。 
[概要]従来の深層学習法を融合するフレームワークを導入して、2つの大きな露出比画像の中程度の露出画像を生成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-05-09">
        <br><font color="black">2019-05-09</font>
      </time>
    </span>
</section>
<!-- paper0: Enforcing Perceptual Consistency on Generative Adversarial Networks by
  Using the Normalised Laplacian Pyramid Distance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.IV/paper_11.html">
      <font color="black">Enforcing Perceptual Consistency on Generative Adversarial Networks by
  Using the Normalised Laplacian Pyramid Distance</font>
    </a>
  </h2>
  <font color="black">NLPDをレギュラライザーとして使用すると、生成された画像の画像セグメンテーションの精度が向上するだけでなく、2つの非参照画像品質メトリックも向上することがわかりました。NLPDは、平均エネルギーのローカル推定値に関して係数の値を正規化する原理に基づいています。さまざまなスケールで、人間の知覚を含むさまざまな実験ですでに正常にテストされています。自動的に（目視検査を回避して）行うには、正規化ラプラシアンピラミッド距離（NLPD）を使用して、生成された画像と元の画像の知覚の類似性を測定します。 。 
[概要]正規化されたラプラシアンピラミッドネットワークを使用して、このレギュラライザーを最初に提案されたl1距離と比較します。segmentpdを使用すると、生成された画像にはローカルコントラストとグローバルコントラストの両方のより現実的な値が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-09">
        <br><font color="black">2019-08-09</font>
      </time>
    </span>
</section>
<!-- paper0: Time-Resolved Focused Ion Beam Microscopy: Modeling, Estimation Methods,
  and Analyses -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.IV/paper_12.html">
      <font color="black">Time-Resolved Focused Ion Beam Microscopy: Modeling, Estimation Methods,
  and Analyses</font>
    </a>
  </h2>
  <font color="black">シミュレートされたFIB顕微鏡の結果は理論的分析と一致しており、時間分解測定によって従来のFIB顕微鏡画像形成に対する大幅な改善が可能であることを示しています。連続時間測定で使用する新しい推定器が導入および分析され、離散で使用する推定器-時間測定値が分析され、時間分解能が増加するにつれて、連続時間の対応物に近づくことが示されます。入射粒子の数のランダム性は、基礎となる粒子とサンプルの相互作用の変動を超えて、顕微鏡写真に過剰な変動を引き起こします。 
[概要]新しい論文は、fib顕微鏡法からのデータを分析することによって作成されました。fibと呼ばれ、結果は、従来のfib顕微鏡法の画像形成に対する大幅な改善が時間分解測定によって可能になることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Progressive Multi-Scale Residual Network for Single Image
  Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.IV/paper_13.html">
      <font color="black">Progressive Multi-Scale Residual Network for Single Image
  Super-Resolution</font>
    </a>
  </h2>
  <font color="black">さらに、チャネルおよびピクセル単位の注意メカニズム（CPA）は、画像の特徴と重みおよびバイアス係数との固有の相関関係を見つけるために設計されており、高周波情報に重点を置いています。具体的には、プログレッシブマルチスケール残差ブロックを考案します。 （PMRB）大きなフィルターを小さなフィルターの組み合わせに置き換え、階層情報を徐々に探索します。自己アンサンブルを備えた拡張モデルPMRN $ ^ + $は、パラメーターがはるかに少なく、計算の複雑さが少ない大規模ネットワークよりも競争力のある、または優れた結果を実現します。 
[ABSTRACT]巨大ネットワークは通常、さまざまなサイズのフィルターで分類を構築することを目的としています。これらには、大きなフィルターを小さなフィルターの組み合わせに置き換えるためのプログレッシブマルチスケール残差ブロックが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-19">
        <br><font color="black">2020-07-19</font>
      </time>
    </span>
</section>
<!-- paper0: A Multi-Task Deep Learning Framework to Localize the Eloquent Cortex in
  Brain Tumor Patients Using Dynamic Functional Connectivity -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.IV/paper_14.html">
      <font color="black">A Multi-Task Deep Learning Framework to Localize the Eloquent Cortex in
  Brain Tumor Patients Using Dynamic Functional Connectivity</font>
    </a>
  </h2>
  <font color="black">タスクfMRIアクティベーションをトレーニングとテストの代理グラウンドトゥルースラベルとして使用しながら、56人の脳腫瘍患者からの安静時fMRIデータでメソッドを評価します。モデルの最終段階では、マルチタスク学習を使用してさまざまな雄弁なサブシステムを識別します。私たちのユニークなトレーニング戦略は、関心のある認知ネットワーク間で共有された表現を見つけます。これにより、欠落している患者データを処理できます。 
[ABSTRACT]深層学習法では、畳み込み層を使用してモデルから特徴を抽出し、長短期記憶（lstm）注意ネットワークを使用します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Uncertainty Modelling in Deep Neural Networks for Image Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_0.html">
      <font color="black">Uncertainty Modelling in Deep Neural Networks for Image Data</font>
    </a>
  </h2>
  <font color="black">このフレームワークには、モデルの不確実性のためのCNNのアンサンブル、分布の不確実性をキャプチャするための教師あり再構成オートエンコーダ、およびデータの不確実性をキャプチャするためのネットワークの最終層での活性化関数の出力の使用が含まれます。モデルの予測における不確実性の定量化は重要です。たとえば、情報に基づいた方法でモデルの出力に作用することにより、AIシステムの安全性を高めることができます。最後に、分類のための一般的な画像データセットでの方法の効率を示します。 
[概要]システムにいくつかのツールを装備して、予測が不確かな場合に通知することができます。これは、自動運転車の制御、医療画像分析、財務見積もりなど、エラーのコストが高いアプリケーションにとって重要です。または法的分野</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Improved Techniques for Training Single-Image GANs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_1.html">
      <font color="black">Improved Techniques for Training Single-Image GANs</font>
    </a>
  </h2>
  <font color="black">最近の最先端のベースラインと比較して、私たちのモデルはトレーニングが最大6倍速く、パラメーターが少なく、画像のグローバル構造をより適切にキャプチャできます。この作業では、課題を理解するためにいくつかの実験を行います。これらの方法をトレーニングし、この分野での以前の作業よりも改善された結果を生成できることがわかったいくつかのベストプラクティスを提案します。重要な点の1つは、以前の単一画像生成方法とは異なり、複数のステージを順次多段階で同時にトレーニングすることです。 、画像解像度を上げる段階が少ないモデルを学習できます。 
[概要]このタスクは、大規模なデータセットの収集が不可能な領域で寛大なモデルを使用できることを意味するため、実用的に重要です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-25">
        <br><font color="black">2020-03-25</font>
      </time>
    </span>
</section>
<!-- paper0: RAIST: Learning Risk Aware Traffic Interactions via Spatio-Temporal
  Graph Convolutional Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_2.html">
      <font color="black">RAIST: Learning Risk Aware Traffic Interactions via Spatio-Temporal
  Graph Convolutional Networks</font>
    </a>
  </h2>
  <font color="black">戦術的行動予測とともに、提案されたフレームワークのリスク評価能力を評価することが重要です。グラフのエッジをトレーニングするために時空間グラフ畳み込みネットワーク（ST-GCN）を活用します。フレームワークはリスク認識表現を学習すると主張します。特に歩行者やサイクリストのような脆弱な相互作用を持つオブジェクトを識別する際に、リスクオブジェクトの識別のタスクを改善することによって。 
[概要]私たちのフレームワークは、特に歩行者や自転車のような脆弱な相互作用を持つオブジェクトを識別する際に、リスクオブジェクトの識別のタスクを改善することによってリスク認識表現を学習すると主張します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Multi Receptive Field Network for Semantic Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_3.html">
      <font color="black">Multi Receptive Field Network for Semantic Segmentation</font>
    </a>
  </h2>
  <font color="black">最初の問題に対処するために、マルチスケール機能を明示的に考慮した新しいマルチ受容野モジュール（MRFM）を提案します。2番目の問題では、オブジェクトの境界を区別するのに効果的なエッジ認識損失を設計します。 / stuff ..セマンティックセグメンテーションは、コンピュータビジョンの重要なタスクの1つであり、画像の各ピクセルにカテゴリラベルを割り当てることです。 
[概要]ほとんどの既存の方法には、まだ2つの課題があります。画像内のオブジェクトやもののサイズは非常に多様である可能性があります。2番目の問題では、オブジェクト/ものの境界を区別するのに効果的なエッジ認識損失を設計します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Digging Deeper into CRNN Model in Chinese Text Images Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_4.html">
      <font color="black">Digging Deeper into CRNN Model in Chinese Text Images Recognition</font>
    </a>
  </h2>
  <font color="black">ボックスラインを含むExcelのような画像を認識するために、ボックスラインを復元するためのLine-Deep Denoising Convolutional AutoEncoder（Line-DDeCAE）を提案します。このペーパーでは、最初に単一行の画像を認識し、次に同じアーキテクチャを拡張する1つの代替方法を示します提案された複数の方法で複数行の画像を認識するために..しかし、CRNNは、複数行の画像やExcelのような画像の検出に失敗することで有名です。 
[概要]この論文では、最初に単一行の画像を認識するための1つの代替案を提示します。次に、同じアーキテクチャを拡張して、提案された複数の方法で複数行の画像を認識します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Anatomy Prior Based U-net for Pathology Segmentation with Attention -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_5.html">
      <font color="black">Anatomy Prior Based U-net for Pathology Segmentation with Attention</font>
    </a>
  </h2>
  <font color="black">この近隣ペナルティ戦略は、包括的関係を持つ任意の2つのラベル（梗塞全体や心筋など）に適用できます。隣接する損失を形成するために..提案されたフレームワークは、EMIDECデータセットで評価されます。 
[ABSTRACT]病理学的分割は常に困難な作業でした。心筋と心筋の間の包含関係を測定するための近隣ペナルティ戦略を提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Relative Drone-Ground Vehicle Localization using LiDAR and Fisheye
  Cameras through Direct and Indirect Observations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_6.html">
      <font color="black">Relative Drone-Ground Vehicle Localization using LiDAR and Fisheye
  Cameras through Direct and Indirect Observations</font>
    </a>
  </h2>
  <font color="black">私たちの方法は完全に自動化されています。LiDARを使用したドローンの検出と追跡のための動的適応カーネルベースの方法を提案します。この作業では、ドローンと地上車両の間のLiDARカメラベースの相対姿勢推定方法を提示します。 LiDARセンサーと車両の屋根にあるフィッシュアイカメラ、およびドローンの下に取り付けられた別のフィッシュアイカメラを使用します。 
[概要]ライダーセンサーがドローンを直接観測し、その位置を測定します。2台のカメラが周囲の物体を間接的に観測して物体を推定します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-13">
        <br><font color="black">2020-11-13</font>
      </time>
    </span>
</section>
<!-- paper0: Non-Local Robust Quaternion Matrix Completion for Large-Scale Color
  Images and Videos Inpainting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_7.html">
      <font color="black">Non-Local Robust Quaternion Matrix Completion for Large-Scale Color
  Images and Videos Inpainting</font>
    </a>
  </h2>
  <font color="black">カラービデオ修復問題ベースのクォータニオンテンソル表現を解決するために、新しいジョイントNSSベースのQMCメソッドも提示されます。大規模なカラー画像とビデオの数値実験は、現状に対するNSSベースのQMCの利点を示しています。 -アートメソッド..以前の画像の非局所的自己類似性（NSS）は、局所パッチが画像全体に多くの非局所的類似パッチを持っていることが多いという事実を指します。 
[要約]ロバストなクォータニオン行列補完（qmc）法を強化し、修復性能を大幅に向上させる前に、nssを適用した新しい論文の結果</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Bridging the Performance Gap Between Pose Estimation Networks Trained on
  Real And Synthetic Data Using Domain Randomization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_8.html">
      <font color="black">Bridging the Performance Gap Between Pose Estimation Networks Trained on
  Real And Synthetic Data Using Domain Randomization</font>
    </a>
  </h2>
  <font color="black">これにより、3Dでのドメインのランダム化と、推論中に隣接する幾何学的情報の使用の両方が可能になります。このホワイトペーパーでは、実際のネットワークと合成的にトレーニングされたネットワークの間のギャップを埋める新しい方法を紹介します。現在、実際のネットワークと合成トレーニングされたネットワークの間には大きなパフォーマンスギャップがあります。合成データ。 
[概要]現在、実際のデータと合成データでトレーニングされたメソッドの間には大きなパフォーマンスのギャップがあります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-frame Feature Aggregation for Real-time Instrument Segmentation in
  Endoscopic Video -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_9.html">
      <font color="black">Multi-frame Feature Aggregation for Real-time Instrument Segmentation in
  Endoscopic Video</font>
    </a>
  </h2>
  <font color="black">深い特徴抽出の計算ワークロードをシーケンス内の各フレームに分散することで、軽量エンコーダーを使用して計算コストを削減できます。通常、隣接するフレームの外観は似ているため、フレームシーケンス全体の特徴集約を反復的な特徴集約と見なします。手順..フレーム間の空間的不整合の影響を低減しながら、セグメント化のために隣接するフレームの情報を活用する新しいマルチフレーム特徴集約（MFFA）モジュールを提案します。 
[ABSTRACT]ディープモデルはオンライン手術ビデオ分析を使用できる可能性があります。mffaモジュールを使用して、フレーム間の空間的不整合の影響を軽減できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Zero-Shot Action Recognition in Videos: A Survey -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_10.html">
      <font color="black">Zero-Shot Action Recognition in Videos: A Survey</font>
    </a>
  </h2>
  <font color="black">文献には多くの方法がありますが、どの技術を最先端と見なすことができるかを分類することは困難です。また、データセット、実験、プロトコルの完全な説明を提供し、未解決の問題と方向性を示します。コンピュータビジョン研究分野の発展に不可欠な将来の研究。静止画像および実験プロトコルにおけるゼロショットアクション認識に関するいくつかの調査が存在するにもかかわらず、ビデオに焦点を当てた研究はありません。 
[概要]モデルのトレーニングに存在しないクラスからインスタンスを分類できるメソッドが求められています。データセット、実験、プロトコルの完全な説明もあり、未解決の問題と方向性を示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-13">
        <br><font color="black">2019-09-13</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Learning Based HPV Status Prediction for Oropharyngeal Cancer
  Patients -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_11.html">
      <font color="black">Deep Learning Based HPV Status Prediction for Oropharyngeal Cancer
  Patients</font>
    </a>
  </h2>
  <font color="black">スポーツビデオクリップで事前トレーニングされた3D畳み込みネットワークは、CT画像の完全な3D情報を活用できるように微調整されました。ゼロからトレーニングされた3D畳み込みニューラルネットワーク（CNN）および事前トレーニングされた2Dアーキテクチャと比較してImageNetは、ビデオの事前トレーニング済みモデルが最高のパフォーマンスを発揮しました。小さな医療データセットの問題を克服するために、転移学習アプローチを使用しました。 
[概要]ビデオの事前トレーニング済みモデルは、hpvを区別することができました-外部テストセットの受信者動作特性曲線の下の領域が0.81の陽性と陰性のケース</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Empowering Things with Intelligence: A Survey of the Progress,
  Challenges, and Opportunities in Artificial Intelligence of Things -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_12.html">
      <font color="black">Empowering Things with Intelligence: A Survey of the Progress,
  Challenges, and Opportunities in Artificial Intelligence of Things</font>
    </a>
  </h2>
  <font color="black">最後に、AIoTが直面している課題といくつかの潜在的な研究機会に焦点を当てます。具体的には、クラウドコンピューティング、フォグコンピューティング、エッジコンピューティングのコンテキストでAIoTアーキテクチャを簡単に紹介します。次に、IoTのAI研究の進捗状況を4つから紹介します。視点：知覚、学習、推論、および行動。 
[ABSTRACT] aiは、モノの人工知能の時代をIoTに紹介します。クラウドコンピューティング、フォグ、エッジコンピューティングのコンテキストで、AIOTアーキテクチャを明らかにします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Structural and Functional Decomposition for Personality Image Captioning
  in a Communication Game -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_13.html">
      <font color="black">Structural and Functional Decomposition for Personality Image Captioning
  in a Communication Game</font>
    </a>
  </h2>
  <font color="black">私たちの実験は、提案されたモデルがPICの最先端のパフォーマンスを達成することを示しています。パーソナリティ画像キャプション（PIC）は、パーソナリティ特性が与えられた自然言語キャプションで画像を記述することを目的としています。 GPT2の言語エンコーディング機能の恩恵を受けます。 
[ABSTRACT]キャプションを改善して、画像を自然に表現し、特徴を表現することができます。これにより、話し手と聞き手はgpt2の言語能力を活用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Transducer Adaptive Ultrasound Volume Reconstruction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_14.html">
      <font color="black">Transducer Adaptive Ultrasound Volume Reconstruction</font>
    </a>
  </h2>
  <font color="black">我々の結果は、提案されたドメイン適応法が、ユニバーサルフリーハンド超音波ボリューム再構成のためのトランスデューサ固有の情報を保存しながら、異なる特徴分布をうまく整列できることを示しています。この論文では、ドメインシフト問題としてデータ取得の違いに取り組み、新しいドメインを提案します。深部学習アルゴリズムをさまざまなトランスデューサーで取得したデータに適応させる適応戦略。再構成された3D超音波ボリュームは、2Dスキャンフレームのシーケンスと比較してより多くのコンテキスト情報を提供します。これは、超音波ガイド下前立腺生検などのさまざまな臨床アプリケーションに適しています。 
[概要] 2Dスキャンからの3Dボリューム再構成は非常に困難な動作ですが、このようなアルゴリズムは特定のトランスデューサーに固有であり、トレーニングデータと相互にスキャンする軌跡があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Assistive Diagnostic Tool for Brain Tumor Detection using Computer
  Vision -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_15.html">
      <font color="black">Assistive Diagnostic Tool for Brain Tumor Detection using Computer
  Vision</font>
    </a>
  </h2>
  <font color="black">Mask R畳み込みニューラルネットワーク（Mask R CNN）などのコンピュータービジョン技術を使用して脳腫瘍を検出およびセグメント化すると、予測精度を高めながら、人為的エラーの可能性を軽減できます。予測セグメンテーションは、グラウンドトゥルースと90％一致しました。 ..転送学習はMaskR CNNで使用され、それに応じて必要なパラメーターが開始点として変更されました。 
[概要]脳腫瘍の決定的かつ早期の診断は、一部の人にとっては生と死の違いである可能性があります。マスクr畳み込みニューラルネットワークなどのコンピュータービジョン技術を使用すると、予測精度を高めながら、ヒューマンエラーの可能性を軽減できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Back to the Future: Cycle Encoding Prediction for Self-supervised
  Contrastive Video Representation Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_16.html">
      <font color="black">Back to the Future: Cycle Encoding Prediction for Self-supervised
  Contrastive Video Representation Learning</font>
    </a>
  </h2>
  <font color="black">標準データセットUCF101およびHMDB51の大幅に改善された結果を報告します。詳細なアブレーション研究は、提案されたコンポーネントの有効性をサポートします。および後方遷移。 
[ABSTRACT] cepは、ラベルのないビデオコンテンツの高レベルの空間時間構造を効果的に表すことができます。自己監視信号として、cepは、保存された機能分離だけでなく、異文化を活用します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-14">
        <br><font color="black">2020-10-14</font>
      </time>
    </span>
</section>
<!-- paper0: Dilated Convolutions with Lateral Inhibitions for Semantic Image
  Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_17.html">
      <font color="black">Dilated Convolutions with Lateral Inhibitions for Semantic Image
  Segmentation</font>
    </a>
  </h2>
  <font color="black">さらに、フィルターを拡張すると受容野を拡大できますが、サンプリングされたピクセルの総数は変化しません。これは通常、受容野の総面積のごく一部を構成します。人間の視覚システムの横方向抑制（LI）メカニズムに触発されてこれらの制限を克服するために、横方向抑制を伴う拡張畳み込み（LI-Convs）を提案します。LIメカニズムを導入すると、意味オブジェクトの境界に対する畳み込みフィルターの感度が向上します。 
[ABSTRACT]拡張畳み込みフィルターは、意味的に意味のある輪郭上のピクセルに関する位置知識を持っていませんが、オブジェクト境界のあいまいな予測につながる可能性があります。li-ディスペンサーフィルターは、不均衡に影響を受ける領域がどのように影響を受けるかを説明するのに役立ちます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-05">
        <br><font color="black">2020-06-05</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised BatchNorm Adaptation (UBNA): A Domain Adaptation Method for
  Semantic Segmentation Without Using Source Domain Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_18.html">
      <font color="black">Unsupervised BatchNorm Adaptation (UBNA): A Domain Adaptation Method for
  Semantic Segmentation Without Using Source Domain Representations</font>
    </a>
  </h2>
  <font color="black">対照的に、新しいUnsupervised BatchNorm Adaptation（UBNA）メソッドを紹介します。これは、事前トレーニングからの既存のモデルパラメータを超えて、ソースドメイン表現を使用せずに、事前トレーニングされたモデルを見えないターゲットドメインに適応させます（どちらもデータもジェネレーターも）、オンライン設定で適用したり、ターゲットドメインからのラベルのない画像を数ショットで使用したりすることもできます。標準のUDAアプローチと比較して、パフォーマンスと使用量のトレードオフを報告します。ソースドメイン表現..具体的には、指数関数的に減衰する運動量係数を使用して、正規化レイヤーの統計をターゲットドメインに部分的に適合させ、それによって両方のドメインからの統計を混合します。 
[概要]セマンティックセグメンテーションの以前のアプローチでは、ソースドメインとターゲットドメインでモデルの同時トレーニングを採用していました。代わりに、ジェネレータネットワークに依存し、適応中にソースデータをモデルに再生していました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Temporally Coherent Embeddings for Self-Supervised Video Representation
  Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_19.html">
      <font color="black">Temporally Coherent Embeddings for Self-Supervised Video Representation
  Learning</font>
    </a>
  </h2>
  <font color="black">TCEを使用して、ラベルのない大量のビデオデータから堅牢な表現を学習します。世界の高レベルの視覚情報がスムーズに変化するのと同じように、学習した表現の近くのフレームは、同様のプロパティを示すことでメリットが得られると考えています。この仮定を使用する、隣接するフレームが互いに近くに存在し、ビデオが互いに分離されるようにビデオをエンコードするようにTCEモデルをトレーニングします。 
[概要]提案された方法は、ラベルのないビデオデータを使用して、埋め込みスペースで時間的コヒーレンシを明示的に適用します。さらに、隣接するフレームが互いに近くに存在し、ビデオが互いに分離されるようにビデオをエンコードするようにtceモデルをトレーニングします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-21">
        <br><font color="black">2020-03-21</font>
      </time>
    </span>
</section>
<!-- paper0: Facial Expressions as a Vulnerability in Face Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_20.html">
      <font color="black">Facial Expressions as a Vulnerability in Face Recognition</font>
    </a>
  </h2>
  <font color="black">私たちの実験フレームワークには、4つの顔検出器、3つの顔認識モデル、および4つの異なるデータベースが含まれています。ただし、最先端の顔認識システムによって達成された優れたパフォーマンスにもかかわらず、アルゴリズムは依然として広範囲の共変量に敏感です。観察された脆弱性の軽減に焦点を当てた新しい研究ラインへの扉を開きます。 
[概要]顔認識技術は過去数十年で大きな進歩を遂げました。この作業は、観察された脆弱性の軽減に焦点を当てた新しい研究ラインへの扉を開きます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Slender Object Detection: Diagnoses and Improvements -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_21.html">
      <font color="black">Slender Object Detection: Diagnoses and Improvements</font>
    </a>
  </h2>
  <font color="black">主な調査結果は次のとおりです。1）ラベルの割り当てにおけるアンカーの重要な役割。 2）2点表現の記述能力。 3）細身物体と通常の物体の検出を改善するための重要な戦略。さらに、現在の代表的な物体検出方法に対して明確で一貫した改善を実現する機能適応戦略を提案します。したがって、細身物体検出の問題を体系的に研究します。この仕事で。 
[概要]実際のシナリオでは、細いオブジェクトが実際には非常に一般的です。ただし、これは主にcocoなどのデータセットに基づいています。これらの要因には、cocoでの18.9％のマップの単純な低下が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Efficient GANs via Differentiable Masks and co-Attention
  Distillation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_22.html">
      <font color="black">Learning Efficient GANs via Differentiable Masks and co-Attention
  Distillation</font>
    </a>
  </h2>
  <font color="black">残りの接続をプルーニングする際のチャネルの不整合を克服するために、適応型クロスブロックグループスパース性がさらに組み込まれています。生成的敵対的ネットワーク（GAN）は画像変換で広く使用されていますが、計算コストとストレージコストが高いため、モバイルデバイスへの展開が妨げられています。 ..これらを解決するために、この論文では、微分可能マスクと共注意蒸留を提案することにより、DMADと呼ばれる新しいGAN圧縮方法を紹介します。 
[ABSTRACT] cnn圧縮の主な方法は、ガンに直接適用できます。これは、複雑なジェネレータアーキテクチャと不安定な敵対的トレーニングによるものです。これらは、トレーニングに基づいています-適応型の方法</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Decision and Feature Level Fusion of Deep Features Extracted from Public
  COVID-19 Data-sets -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_23.html">
      <font color="black">Decision and Feature Level Fusion of Deep Features Extracted from Public
  COVID-19 Data-sets</font>
    </a>
  </h2>
  <font color="black">CNNを使用して抽出された深い特徴セットは、特徴レベルの融合のために連結され、COVID-19、肺炎、および発見されないクラスを区別することを目的として、意思決定レベルの融合のアイデアの観点から複数の分類器に供給されました。畳み込みニューラルネットワーク（CNN）に基づくX線画像の診断システムが提案されました。これは放射線技師がCOVID-19検出のサポートツールとして使用できます。実験結果は、提案されたアプローチが高いCOVID-を達成したことを示しています。既存の研究と同等の精度と優れた精度/リコール値によって証明された19の検出性能。 
[要約] covid-19は伝染性と伝染性が高いです。伝染性と伝染性の伝染性に関連しています。しかし、rt-pcrは、特に初期段階では感度が低いという問題があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Pixel-wise Dense Detector for Image Inpainting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_24.html">
      <font color="black">Pixel-wise Dense Detector for Image Inpainting</font>
    </a>
  </h2>
  <font color="black">本論文では、敵対的プロセスにおいてミニマックス戦略を採用する、画像修復のための新しい検出ベースの生成フレームワークを提案する。このような位置情報は、ジェネレータにアーティファクトに注意を向けさせ、それらをさらに強化する。複数のパブリックでの実験データセットは、提案されたフレームワークの優れたパフォーマンスを示しています。 
[概要]ジェネレーターはエンコーダー-デコーダーアーキテクチャに従って欠落領域を埋めます。弱い教師あり学習を使用する検出器は、アーティファクトの位置をピクセル単位でローカライズします。このようなデータは、敵対的損失と再構築損失の重みのバランスを取るために必要です。手動ではなく自動</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Discriminative Segmentation Tracking Using Dual Memory Banks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_25.html">
      <font color="black">Discriminative Segmentation Tracking Using Dual Memory Banks</font>
    </a>
  </h2>
  <font color="black">ベルやホイッスルがない、シンプルでありながら効果的なトラッキングアーキテクチャは、VOT2016、VOT2018、VOT2019、GOT-10K、TrackingNetベンチマークに新しい最先端を設定し、特にVOT2016とTrackingNetでそれぞれ0.535と0.506のEAOを達成します。 VOT2018 ..さらに、私たちのアプローチは、2つのビデオオブジェクトセグメンテーションベンチマークDAVIS16およびDAVIS17で主要なセグメンテーショントラッカーD3Sよりも優れています。これらの制限を克服するために、この作業は、デュアルメモリバンク、つまり外観メモリバンクと空間メモリバンク。 
[概要]外観メモリバンクは、空間的および地域的な非局所的類似性を使用して、外観マスクを現在のフレームに伝播します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-21">
        <br><font color="black">2020-09-21</font>
      </time>
    </span>
</section>
<!-- paper0: Vis-CRF, A Classical Receptive Field Model for VISION -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_26.html">
      <font color="black">Vis-CRF, A Classical Receptive Field Model for VISION</font>
    </a>
  </h2>
  <font color="black">Vis-CRFという名前の網膜ステージモデルの出力は、自然画像のサンプルといくつかのタイプの傾斜錯視についてここに示されています。ここでは、最終的な傾斜知覚は、ガウスの違い（DoG）と前景要素と背景要素の知覚的相互作用（Nematzadeh and Powers、2019; Nematzadeh、2018; Nematzadeh、Powers and Lewis、2017; Nematzadeh、Lewis and Powers、2015）..これらの神経生物学的発見に基づいて、以前の研究でコンピューターを提供しました幾何学的錯覚が網膜で実行されるマルチスケール視覚処理の相互作用によって部分的に説明されることを示唆するシミュレーション証拠..過去10年間、さまざまな新しい神経生理学的実験により、網膜処理の方法、時期、場所に関する新しい洞察がもたらされました。が行われ、網膜表現エンコーディングの性質がさらなる処理のために皮質に送信されます。 
[要約]神経生物学者は、幾何学的錯覚がマルチスケール視覚処理の相互作用によって部分的に説明されることを示唆する新しい証拠を提供しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Building Movie Map -- A Tool for Exploring Areas in a City -- and its
  Evaluation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_27.html">
      <font color="black">Building Movie Map -- A Tool for Exploring Areas in a City -- and its
  Evaluation</font>
    </a>
  </h2>
  <font color="black">その後、交差点での回転ビューが生成されます。ユーザーが自由に移動してランドマークを見つけることができるシナリオで、システムをGSVと比較するためのユーザー調査を実施しました。エリア内の指定された移動に続いてビデオセグメントを接続することにより、通りをよりよく見ることができます。 
[概要]システムは、取得、分析、管理、インタラクションの4つの段階で構成されています。シナリオでは、ユーザーは自由に移動して探索し、ランドマークを見つけることができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: A Review of Uncertainty Quantification in Deep Learning: Techniques,
  Applications and Challenges -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_28.html">
      <font color="black">A Review of Uncertainty Quantification in Deep Learning: Techniques,
  Applications and Challenges</font>
    </a>
  </h2>
  <font color="black">この点に関して、研究者はさまざまなUQ手法を提案し、コンピュータービジョン（例：自動運転車や物体検出）、画像処理（例：画像復元）、医療画像分析（例：医療画像の分類とセグメンテーション）、自然言語処理（テキスト分類、ソーシャルメディアテキスト、共生リスクスコアリングなど）、バイオインフォマティクスなど。ベイジアン近似とアンサンブル学習技術は、文献で最も広く使用されている2つのUQ手法です。この研究では、深層学習で使用されるUQ手法の最近の進歩をレビューします。 
[ABSTRACT] uqメソッドは、科学と工学におけるさまざまな実世界のアプリケーションを解決するために使用できます。研究者は、さまざまなuqメソッドを提案し、そのパフォーマンスを調べました。また、強化された学習に加えて、これらのメソッドのアプリケーションを調査しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-12">
        <br><font color="black">2020-11-12</font>
      </time>
    </span>
</section>
<!-- paper0: Domain Adaptation based Technique for Image Emotion Recognition using
  Pre-trained Facial Expression Recognition Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_29.html">
      <font color="black">Domain Adaptation based Technique for Image Emotion Recognition using
  Pre-trained Facial Expression Recognition Models</font>
    </a>
  </h2>
  <font color="black">実験はFlickr画像データセットで実行され、画像は「怒り」、「幸せ」、「悲しい」、「中立」の感情クラスに分類されました。提案されたシステムは、ベンチマーク結果よりも優れたパフォーマンスを示しました。画像の感情認識の精度は63.87％です。画像の視覚的特徴を検出し、それに基づいて感情の分類を実行します。 
[概要]提案されたシステムは、63。87％の精度でベンチマーク結果よりも優れたパフォーマンスを示しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Depth Descent Synchronization in $\mathrm{SO}(D)$ -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_30.html">
      <font color="black">Depth Descent Synchronization in $\mathrm{SO}(D)$</font>
    </a>
  </h2>
  <font color="black">$ D = 2 $の場合、このアルゴリズムの変形がグラウンドトゥルース回転に線形に収束することを示します。最後に、最小絶対偏差に基づくより単純な非凸エネルギー最小化フレームワークに関連してこの結果を説明します。スプリアス固定小数点..接線空間でTukeyの深さを利用する新しいアルゴリズムを提供します。これは、$ 1 /（D（D-1）+2）$の異常なパーセンテージまで基礎となる回転を正確に回復します。 
[ABSTRACT]特に、観測値の限られた割合が任意に破損する敵対的な破損設定を検討します。この結果を、偽の固定小数点を示す最小絶対偏差に基づく単純な非凸エネルギー最小化フレームワークに関連して説明します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-13">
        <br><font color="black">2020-02-13</font>
      </time>
    </span>
</section>
<!-- paper0: Learning color space adaptation from synthetic to real images of cirrus
  clouds -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_31.html">
      <font color="black">Learning color space adaptation from synthetic to real images of cirrus
  clouds</font>
    </a>
  </h2>
  <font color="black">セマンティックセグメンテーションをトレーニングするために適応された合成データを使用すると、実際の画像に適用した場合、代替方法よりも優れた6：59％の改善を達成します。また、合成から現実へのサーラスクラウドデータセットSynCloudを構築し、適応効果を示します。サーラス雲のセマンティックセグメンテーションタスクについて..一般的な畳み込みカーネルによって画像を変換する代わりに、ラベルを保持しながら色空間調整を行うための一連の閉じた形式の操作を採用します。 
[概要]合成画像と実画像の間のドメインギャップは、トレーニングされたモデルのパフォーマンスを大幅に低下させます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-10-24">
        <br><font color="black">2018-10-24</font>
      </time>
    </span>
</section>
<!-- paper0: Dense-Resolution Network for Point Cloud Classification and Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_32.html">
      <font color="black">Dense-Resolution Network for Point Cloud Classification and Segmentation</font>
    </a>
  </h2>
  <font color="black">他の最先端の方法と比較して、私たちのネットワークはModelNet40、ShapeNet合成、およびScanObjectNN実点群データセットで優位性を示しています。ローカルポイントグループをより効果的に学習するために、ローカル近傍検索の新しいグループ化方法を提示します。ローカル特徴をキャプチャするためのエラー最小化モジュール。この記事では、点群分析用のDense-Resolution Network（DRNet）という名前の新しいネットワークを提案します。 
[概要]当社のdrnetは、さまざまな解像度でネットワークからローカルポイント機能を学習するように設計されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-14">
        <br><font color="black">2020-05-14</font>
      </time>
    </span>
</section>
<!-- paper0: Modality-Buffet for Real-Time Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_33.html">
      <font color="black">Modality-Buffet for Real-Time Object Detection</font>
    </a>
  </h2>
  <font color="black">このタスクを順次意思決定問題として定式化し、強化学習（RL）を使用して、RGB入力から、さまざまなオブジェクト検出器のポートフォリオから次の予測に使用する検出器を決定するポリシーを生成します。さまざまなモダリティを使用する検出器と計算の複雑さが異なると、さまざまなトレードオフが発生します。RLエージェントの目的は、画像ごとの予測の精度を最大化することです。 
[概要]多くのオペレーターは、さまざまなモダリティを使用し、さまざまな計算の複雑さを持っています。しかし、より複雑であるがより正確なモデルを持ち、処理時に入ってくるフレームの以前の予測から推定する方が良いかもしれません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Few-Shot Unsupervised Continual Learning through Meta-Examples -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_34.html">
      <font color="black">Few-Shot Unsupervised Continual Learning through Meta-Examples</font>
    </a>
  </h2>
  <font color="black">このギャップを狭めるために、この作業では、不均衡なタスクを伴う教師なしメタ連続学習を含む、斬新で複雑な設定を紹介します。実際のアプリケーションでは、データは通常、ニューラルネットワークのトレーニングに一般的に使用されるものを反映していません。 、ラベルなしで、ストリームとして利用できます。したがって、多くの既存の深層学習ソリューションは、特に時間とともに進化するオンラインストリーミングデータの場合、限られた範囲のアプリケーションに悩まされます。 
[概要]多くの既存の深層学習ソリューションは、限られた範囲のアプリケーションに悩まされています。これらには、集約された表現を利用する単一の内部ループが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-17">
        <br><font color="black">2020-09-17</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Active Surface Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_35.html">
      <font color="black">Deep Active Surface Models</font>
    </a>
  </h2>
  <font color="black">グラフ畳み込みネットワークにシームレスに統合できるレイヤーを実装して、許容可能な計算コストで高度な滑らかさの事前分布を適用できます。アクティブサーフェスモデルは、複雑な3Dサーフェスのモデル化に役立つ長い歴史がありますが、で使用されているのはアクティブコンターのみです。ディープネットワークと組み合わせて、データ項とそれらを制御するメタパラメータマップを生成するだけです。結果として得られるディープアクティブサーフェスモデルは、従来の正規化損失項を使用して3Dサーフェスに滑らかさの優先順位を課す同等のアーキテクチャよりも優れていることを示します。 2D画像からの再構成および3Dボリュームセグメンテーション用。 
[概要]結果として得られるディープアクティブサーフェスモデルは、アーキテクチャよりも優れています。結果は、結果がモデルよりも優れていることを示します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Sub-clusters of Normal Data for Anomaly Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_36.html">
      <font color="black">Sub-clusters of Normal Data for Anomaly Detection</font>
    </a>
  </h2>
  <font color="black">与えられた通常のデータのサブクラスターを意味的に分離する適度に優れた特徴空間が存在する場合、目に見えない異常も通常のデータからその空間で十分に区別できると仮定します。ただし、既存の異常検出方法は、次のような高次元データでは限られたパフォーマンスを示します。 ImageNetとして..私たちの観察では、一般に、異常データは、通常のデータのセマンティックサブクラスターの定義にも使用できるセマンティックに説明可能な機能によって定義されます。 
[ABSTRACT]既存の研究では、mnistやcifarなどの高品質でクリーンで十分に分離されたデータセットでのパフォーマンスが評価されています-10。与えられた通常のデータでセマンティッククラスタリングを実行し、分類器をトレーニングして高データ検出を学習することを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Can Semantic Labels Assist Self-Supervised Visual Representation
  Learning? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_37.html">
      <font color="black">Can Semantic Labels Assist Self-Supervised Visual Representation
  Learning?</font>
    </a>
  </h2>
  <font color="black">さらに重要なことに、私たちの研究は、セマンティックラベルが自己監視メソッドを支援し、コミュニティに新しい方向性を開くのに役立つことを明らかにしています。この論文では、セマンティックラベルの有用性を擁護しますが、完全監視および自己監視メソッドを指摘します。さまざまな種類の機能を追求しています。最近、対照学習は、教師なし視覚表現学習の進歩を大幅に進歩させました。 
[概要]近隣での教師あり対照調整と呼ばれる新しいアルゴリズムは、セマンティックガイダンスが外観特徴の埋め込みに損傷を与えるのを防ぎます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: ACSC: Automatic Calibration for Non-repetitive Scanning Solid-State
  LiDAR and Camera Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_38.html">
      <font color="black">ACSC: Automatic Calibration for Non-repetitive Scanning Solid-State
  LiDAR and Camera Systems</font>
    </a>
  </h2>
  <font color="black">コードはhttps://github.com/HViktorTsoi/ACSC.gitで入手できます。上記に基づいて、ターゲットベースの外部キャリブレーション方法が最終的に提案されます。まず、時間空間ベースの幾何学的特徴の改良方法は次のとおりです。 SSLポイントクラウドから効果的な機能を抽出するために提示されました。次に、ポイントの反射率分布を使用して、キャリブレーションターゲット（印刷されたチェッカーボード）の3Dコーナーが推定されます。 
[概要]キャリブレーションターゲットの3Dコーナーは、ポイントの反射率分布を使用して推定されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Mutual Information Based Method for Unsupervised Disentanglement of
  Video Representation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_39.html">
      <font color="black">Mutual Information Based Method for Unsupervised Disentanglement of
  Video Representation</font>
    </a>
  </h2>
  <font color="black">MIGスコアは、MIPAEによって予測されたフレームの視覚的優位性を裏付けます。私たちのアプローチは、ビデオの潜在的な生成要因の時間的構造と新しい相互情報量の損失を活用して、解きほぐされたビデオ表現を学習します。相互情報量に基づくメトリックも提案します。 DSpritesおよびMPI3D-realデータセットでの解きほぐしの有効性に定量的にアクセスするためのギャップ（MIG）。 
[概要]ビデオ予測モデルは、操縦計画、ヘルスケア、自律ナビゲーション、シミュレーションでのアプリケーションを発見しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Exploring Self-Attention for Visual Odometry -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_40.html">
      <font color="black">Exploring Self-Attention for Visual Odometry</font>
    </a>
  </h2>
  <font color="black">この論文では、視覚的オドメトリにおける自己注意の有効性を探求します。視覚的オドメトリネットワークは、連続するフレーム間のエゴモーションを導出するために、通常、事前トレーニング済みオプティカルフローネットワークを使用します。SOTA法に対する定性的および定量的結果を報告します。 
[概要]これらのネットワークによって抽出されたプロパティは、フレーム間のすべてのピクセルの動きを表します。ただし、構造に注意メカニズムがない画像とは関係ありません。オドメトリを使用すると、より良い特徴を抽出できると同時に、そのような構造を欠いているネットワークと比較してより良い走行距離測定性能</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Recognition and standardization of cardiac MRI orientation via
  multi-tasking learning and deep neural networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_41.html">
      <font color="black">Recognition and standardization of cardiac MRI orientation via
  multi-tasking learning and deep neural networks</font>
    </a>
  </h2>
  <font color="black">ソースコード、ニューラルネットワークモデル、ツールがリリースされ、https：//zmiclab.github.io/projects.htmlから公開されています。この論文では、心臓MRIにおける画像の向きの問題を研究し、ディープニューラルネットワークを介した認識と標準化の方向を分類します。MRIの複数のシーケンスとモダリティについて、提案されたモデルを単一のモダリティから複数のモダリティに適応させる転送学習戦略を提案します。 
[概要]心臓mri方向調整ツール、つまりcmradjustnetに方向認識ネットワークを組み込みます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: PaDiM: a Patch Distribution Modeling Framework for Anomaly Detection and
  Localization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_42.html">
      <font color="black">PaDiM: a Patch Distribution Modeling Framework for Anomaly Detection and
  Localization</font>
    </a>
  </h2>
  <font color="black">また、CNNのさまざまなセマンティックレベル間の相関関係を利用して、異常をより適切にローカライズします。PaDiMの最先端のパフォーマンスと低い複雑さにより、多くの産業用アプリケーションに適しています。PaDiMは現在の最先端のパフォーマンスを上回っています。 -MVTecADおよびSTCデータセットでの異常検出とローカリゼーションの両方のためのアートアプローチ。 
[ABSTRACT] padimは、mvtec adおよびstcデータセットでの異常検出とローカリゼーションの両方について、現在の最先端のアプローチよりも優れています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Spatiotemporal Attacks for Embodied Agents -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_43.html">
      <font color="black">Spatiotemporal Attacks for Embodied Agents</font>
    </a>
  </h2>
  <font color="black">ホワイトボックス設定とブラックボックス設定の両方でいくつかの具体化されたタスクのEQA-v1データセットで広範な実験が行われ、摂動が強力な攻撃能力と一般化能力を持っていることを示しています。エージェントは予測に基づいて予測を行うため、時間的次元に関して歴史的観察では、シーンビューの寄与を探索するための軌道注意モジュールを開発します。これは、最も高い刺激で出現した3Dオブジェクトのローカライズにさらに役立ちます。時間次元からの手がかりと空間次元に沿って調整することにより、物理的特性を敵対的に混乱させます（たとえば、最も重要なシーンビューに表示されたコンテキストオブジェクトのテクスチャや3D形状）。 
[ABSTRACT]敵対的攻撃に関する既存の作業は主に静的シーンに焦点を当てていますが、これらの攻撃がアディダスに対して有効であるかどうかは不明です。特に、時空間摂動を生成して3D敵対的例を形成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-19">
        <br><font color="black">2020-05-19</font>
      </time>
    </span>
</section>
<!-- paper0: Ego2Hands: A Dataset for Egocentric Two-hand Segmentation and Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_44.html">
      <font color="black">Ego2Hands: A Dataset for Egocentric Two-hand Segmentation and Detection</font>
    </a>
  </h2>
  <font color="black">定量分析のために、量、多様性、注釈の精度において既存のベンチマークを大幅に超える評価セットに手動で注釈を付けました。データセットとトレーニング手法により、ドメインを適応させることなく、目に見えない環境に一般化するモデルを生成できることを示します。方法は、制約された環境、一貫した肌の色、照明など、多くの基本的な仮定によって制限されます。 
[概要]私たちのデータセットとトレーニング手法は、ドメイン適応なしで見えない環境に一般化するモデルを作成できます。私たちのデータセットは、流域適応なしで目に見えない環境に一般化するモデルを生成できることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-14">
        <br><font color="black">2020-11-14</font>
      </time>
    </span>
</section>
<!-- paper0: Abstracting Deep Neural Networks into Concept Graphs for Concept Level
  Interpretability -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_45.html">
      <font color="black">Abstracting Deep Neural Networks into Concept Graphs for Concept Level
  Interpretability</font>
    </a>
  </h2>
  <font color="black">抽象的でより高い概念レベルでモデルの動作のそのようなグラフィック表現を抽出すると、これらのモデルの学習が解明され、予測のためにモデルが実行する手順を評価するのに役立ちます。フレームワークはhttps：// githubで入手できます。 com / koriavinash1 / BioExp。これらの証跡を理解することで、モデルが従う意思決定プロセスの階層を理解できます。 
[要約]ほとんどの説明可能性手法は、人間が従う概念ベースの推論をキャプチャしません。代わりに、これらのモデルの学習に代わるものを提供します。これは、予測のためにモデルが実行する手順を評価するのに役立ちます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-14">
        <br><font color="black">2020-08-14</font>
      </time>
    </span>
</section>
<!-- paper0: InstanceMotSeg: Real-time Instance Motion Segmentation for Autonomous
  Driving -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_46.html">
      <font color="black">InstanceMotSeg: Real-time Instance Motion Segmentation for Autonomous
  Driving</font>
    </a>
  </h2>
  <font color="black">移動オブジェクトのセグメンテーションは、モーションキューに基づいてクラスにとらわれない方法でオブジェクトをセグメント化するために使用できるため、自動運転車にとって重要なタスクです。さらに、クラスにとらわれないインスタンスのセグメンテーションをヘッドスルーで学習する効率的なマルチタスク学習アプローチを提案します。プロトタイプ生成ネットワークをセマンティックヘッドと共有します。リアルタイムのパフォーマンスを得るために、さまざまな効率的なエンコーダーを研究し、MobileNetV2を使用してTitan XpGPUで39fpsを取得し、ベースラインと比較して10％mAPを改善します。 
[ABSTRACT]動きに基づいて未知の物体を検出できるようにします。注釈キットを含むデータセットによって研究されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-16">
        <br><font color="black">2020-08-16</font>
      </time>
    </span>
</section>
<!-- paper0: A Digital Image Processing Approach for Hepatic Diseases Staging based
  on the Glisson's Capsule -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_47.html">
      <font color="black">A Digital Image Processing Approach for Hepatic Diseases Staging based
  on the Glisson's Capsule</font>
    </a>
  </h2>
  <font color="black">超音波画像では、グリソン鞘は、文献の古典的な方法で抽出できる線の形で表示されます。標準的な画像処理技術と畳み込みニューラルネットワークアプローチの組み合わせを利用することにより、この作業の範囲は次のようになります。大きな有益な可能性がグリソン鞘表面の滑らかさに依存しているという考えの証拠..過去に肝疾患に使用された診断用腹腔鏡検査からインスピレーションを得て、この論文では肝臓の超音波画像を研究し、グリソン鞘が見える器官。 
[概要]グリソン鞘が見える臓器の特定の領域に焦点を当てて、肝臓の超音波画像を研究します。これは、弾性超音波検査の結果に基づいて遡及的に実施された予備研究です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: AID: Pushing the Performance Boundary of Human Pose Estimation with
  Information Dropping Augmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_48.html">
      <font color="black">AID: Pushing the Performance Boundary of Human Pose Estimation with
  Information Dropping Augmentation</font>
    </a>
  </h2>
  <font color="black">より挑戦的なCrowdPoseデータセットでは、改善は1.5 AP以上です。人気のあるCOCO人間ポーズ推定テストセットでは、AIDは、トップダウンパラダイムで約0.6 AP、ボトムアップで最大1.5APまでさまざまな構成のパフォーマンスを一貫して向上させます。パラダイム..AIDが人間の姿勢推定問題のパフォーマンス境界をかなりのマージンで押し上げ、新しい最先端を設定するので、AIDが人間の姿勢推定者をトレーニングするための通常の構成になることを願っています。 
[概要]トレーニングプロセスにおける損失とパフォーマンスのパターンを分析することによって設計された、カスタマイズされたトレーニングスケジュールを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-17">
        <br><font color="black">2020-08-17</font>
      </time>
    </span>
</section>
<!-- paper0: Semi-Supervised Few-Shot Atomic Action Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_49.html">
      <font color="black">Semi-Supervised Few-Shot Atomic Action Recognition</font>
    </a>
  </h2>
  <font color="black">実験により、私たちのモデルは、完全な監視設定でそれぞれの最先端の分類精度を上回る代表的な原子作用データセットで高精度を達成できることが示されています。上記の問題に対処するために、原子作用に焦点を当て、半教師ありの新しいモデルを提案します。 -監視された数ショットのアトミックアクション認識..優れた進歩があったにもかかわらず、アクション認識のパフォーマンスは依然として特定のデータセットに大きく依存しており、労働集約的なラベリングのために新しいアクションクラスを拡張することは困難です。 
[概要]私たちのモデルは、教師なしで対照的なビデオ埋め込みを特徴としています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Understanding Image Captioning Models beyond Visualizing Attention -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_50.html">
      <font color="black">Understanding Image Captioning Models beyond Visualizing Attention</font>
    </a>
  </h2>
  <font color="black">加法注意で計算された適応注意メカニズムとスケーリングされたドット積で計算されたマルチヘッド注意の2つの広く使用されている注意メカニズムで実験を行います。この論文は、注意自体を視覚化することを超えた注意メカニズムで画像キャプションモデルの予測を解釈します。 ..観察された説明の特性を使用して、画像キャプションモデルのオブジェクトの幻覚を軽減し、同時に文の流暢さを維持できるLRP推論微調整戦略をさらに設計します。 
[要約]説明のパターンは、コレによると、画像キャプションモデルのオブジェクトの幻覚を軽減することができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-04">
        <br><font color="black">2020-01-04</font>
      </time>
    </span>
</section>
<!-- paper0: A Divide et Impera Approach for 3D Shape Reconstruction from Multiple
  Views -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_51.html">
      <font color="black">A Divide et Impera Approach for 3D Shape Reconstruction from Multiple
  Views</font>
    </a>
  </h2>
  <font color="black">オブジェクトのまばらなビューから始めて、最初にすべてのペア間の相対ポーズを推定することにより、それらを共通の座標系に位置合わせします。提案された方法を検証するために、相対ポーズの観点からShapeNet参照ベンチマークの包括的な評価を実行します。推定と3D形状再構成..私たちのアプローチは3つのステップに分かれています。 
[要約]この論文は、与えられたビューからの可視情報をマージすることにより、視点バリアントの再構築に依存することを提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Residual Learning for Effective joint Demosaicing-Denoising -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_52.html">
      <font color="black">Residual Learning for Effective joint Demosaicing-Denoising</font>
    </a>
  </h2>
  <font color="black">さらに、非常に高レベルのノイズを処理できますが、20を超えるノイズに対する他のCNNベースの方法のパフォーマンスはかなり制限されます。これは、統計的仮定に強く依存する従来のノイズ除去モデルにとって非常に困難です。この論文では、この厄介な問題に取り組むために。 
[概要]新しい方法は、従来の方法と畳み込みニューラルネットワークを組み合わせて、ノイズを無視してフルカラー画像を再構築します。この段階では、楽しい最終結果を得るための重要なポイントであるすべての既知の情報が保持されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-14">
        <br><font color="black">2020-09-14</font>
      </time>
    </span>
</section>
<!-- paper0: CG-Net: Conditional GIS-aware Network for Individual Building
  Segmentation in VHR SAR Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_53.html">
      <font color="black">CG-Net: Conditional GIS-aware Network for Individual Building
  Segmentation in VHR SAR Images</font>
    </a>
  </h2>
  <font color="black">ベルリンで収集された高解像度スポットライトTerraSAR-X画像を使用して、この方法を検証します。さらに、大規模な数値生成に使用できる正確な数値標高モデル（DEM）から建物のグラウンドトゥルース生成のアプローチを提案します。 SAR画像データセット..セグメンテーションの結果を適用して、詳細レベル（LoD）1で3D建物モデルを再構築できます。これは、実験で実証されています。 
[概要]提案されたモデルは、マルチレベルの視覚的特徴を学習し、建物のフットプリントを使用して、sar画像の建物のマスクを予測するための特徴を正規化します。新しいモデルは、バリアントバックボーンで効果的に改善をもたらします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Canonical Transformations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_54.html">
      <font color="black">Learning Canonical Transformations</font>
    </a>
  </h2>
  <font color="black">具体的には、高いトレーニングセットの多様性は、目に見えない形状やスケールへの変換の外挿に十分であり、反復トレーニングスキームは、時間内の回転の有意な外挿を実現します。ニューラルネットワークモデルがこれらの変換を学習するのに役立つ誘導バイアスを調査します。ピクセル空間で、ドメイン外を一般化できる方法で..人間は、特定のオブジェクトに拘束されないことで一般化をサポートする一連の標準的な幾何学的変換（平行移動や回転など）を理解します。 
[要約]人間が人間に誘導バイアスを見たのは初めてです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Exposure Interpolation by Combining Model-driven and Data-driven Methods -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_55.html">
      <font color="black">Exposure Interpolation by Combining Model-driven and Data-driven Methods</font>
    </a>
  </h2>
  <font color="black">実験結果は、深層学習法を使用して従来の方法で補間画像を洗練することにより、中程度の露出画像の品質が大幅に向上することを示しています。従来の方法を採用して、深層学習法の収束速度を改善し、低減することができます。深層学習法に必要なサンプル数..この論文では、露出補間を例としてこの質問に答え、答えは「はい」です。 
[概要]従来の深層学習法を融合するフレームワークを導入して、2つの大きな露出比画像の中程度の露出画像を生成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-05-09">
        <br><font color="black">2019-05-09</font>
      </time>
    </span>
</section>
<!-- paper0: AdCo: Adversarial Contrast for Efficient Learning of Unsupervised
  Representations from Self-Trained Negative Adversaries -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_56.html">
      <font color="black">AdCo: Adversarial Contrast for Efficient Learning of Unsupervised
  Representations from Self-Trained Negative Adversaries</font>
    </a>
  </h2>
  <font color="black">対照的な学習は、表現が自己訓練されている場合、肯定的なクエリを区別するのが十分に難しい否定的な例のコレクションを構築することに依存しています。あるいは、自己訓練された表現と対戦する一連の否定的な敵を直接学習するために提示します。キュー全体を更新することによって、反復にわたって学習された表現の変化を厳密に追跡したり、過去のミニバッチから有用な情報を破棄したりすることはできませんでした。 
[概要]ネガティブが更新されないためのミニバッチの以前の例。代わりに、ネガティブな敵のセットを直接学習するために提示します。ネガティブな敵は、ポジティブクエリの加重組み合わせに向けて更新されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: The Vulnerability of the Neural Networks Against Adversarial Examples in
  Deep Learning Algorithms -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_57.html">
      <font color="black">The Vulnerability of the Neural Networks Against Adversarial Examples in
  Deep Learning Algorithms</font>
    </a>
  </h2>
  <font color="black">この論文では、一般的なホワイトボックス攻撃方法を詳細に紹介し、さらにブラックボックスとホワイトボックスの攻撃の類似点と相違点を比較します。これに対応して、著者は防御方法も紹介し、これらの方法の黒に対するパフォーマンスを分析します。ディープラーニングが直面している現在のセキュリティの脅威に基づいて、このホワイトペーパーでは、ディープラーニングにおける敵対的な例の問題を紹介し、ブラックボックスとホワイトボックスの既存の攻撃と防御の方法を分類して分類します。 
[概要]深層学習アルゴリズムは、データの本質的な特性を効果的に説明できます。これにより、悪意のある入力に直面しても、アルゴリズムは正しい結果を出すことができなくなります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-02">
        <br><font color="black">2020-11-02</font>
      </time>
    </span>
</section>
<!-- paper0: Asymmetric Loss For Multi-Label Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_58.html">
      <font color="black">Asymmetric Loss For Multi-Label Classification</font>
    </a>
  </h2>
  <font color="black">実装はhttps://github.com/Alibaba-MIIL/ASLで入手できます。この正負の不均衡が最適化プロセスを支配し、トレーニング中に正のラベルからの勾配を強調しすぎて、精度が低下する可能性があります。 ASLを使用すると、MS-COCO、Pascal-VOC、NUS-WIDE、OpenImagesなどの複数の人気のあるマルチラベルデータセットで最先端の結果を得ることができます。 
[要約]損失により、簡単なネガティブサンプルの重みとハードしきい値を動的に下げることができます。また、誤ってラベル付けされた可能性のあるサンプルを破棄することもできます。aslは効果的で、実装が斬新であり、トレーニング時間や複雑さを増加させません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-29">
        <br><font color="black">2020-09-29</font>
      </time>
    </span>
</section>
<!-- paper0: ABC-Net: Semi-Supervised Multimodal GAN-based Engagement Detection using
  an Affective, Behavioral and Cognitive Model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_59.html">
      <font color="black">ABC-Net: Semi-Supervised Multimodal GAN-based Engagement Detection using
  an Affective, Behavioral and Cognitive Model</font>
    </a>
  </h2>
  <font color="black">私たちの方法を評価するために、RECOLAでのパフォーマンスを分析および比較し、ベースライン方法よりも5％以上の相対的なパフォーマンスの向上を報告します。RECOLAデータベースでの実験を通じてネットワークの効率を実証します。知識、私たちのアプローチは、マルチモーダル半教師ありネットワークに基づいてエンゲージメントを分類する最初の方法です。 
[概要] 3つのタイプを使用して、エンゲージメントレベルを効果的にキャプチャできるさまざまな機能を抽出します。recolaデータベースでの実験を通じて、ネットワークの効率を実証しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Global Road Damage Detection: State-of-the-art Solutions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_60.html">
      <font color="black">Global Road Damage Detection: State-of-the-art Solutions</font>
    </a>
  </h2>
  <font color="black">このペーパーは、これらのチームによって提案された上位12のソリューションを要約しています。このペーパーは、提示された課題でうまく機能したファセットと、将来の課題で改善できるファセットのレビューで締めくくります。合計で、いくつかの国から121チームが登録しました。この競争。 
[概要]ビッグデータカップの課題には、リリースされたデータセットと明確な評価指標を備えた明確な問題が含まれます。提示されたケースでは、データはインド、日本、チェコ共和国から収集された26336の道路画像を構成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene
  Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_61.html">
      <font color="black">Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene
  Understanding</font>
    </a>
  </h2>
  <font color="black">データセットの生成に使用したすべてのコードはオンラインで入手できます。データセットは次のとおりです。（1）公開されている3Dアセットのみに依存しています。 （2）すべてのシーンの完全なシーンジオメトリ、マテリアル情報、および照明情報が含まれます。 （3）すべての画像のピクセルごとの密なセマンティックインスタンスセグメンテーションが含まれます。 （4）すべての画像を拡散反射、拡散照明、およびビューに依存する照明効果をキャプチャする非拡散残差項に因数分解します。驚くべきことに、データセット全体を最初から生成できることがわかりました。最先端の自然言語処理モデルをトレーニングするコスト。 
[概要]私たちは、全体的な屋内シーンを理解するための合成データセットであるハイパーアリムを作成します。ハイパーアリティは、オンラインで利用可能なデータセットを作成するためにハイパーアリティを必要とします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-04">
        <br><font color="black">2020-11-04</font>
      </time>
    </span>
</section>
<!-- paper0: Extreme Value Preserving Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_62.html">
      <font color="black">Extreme Value Preserving Networks</font>
    </a>
  </h2>
  <font color="black">SIFTからスケール空間の極値のアイデアを借りて、極値保存ネットワーク（EVPNets）を提案します。このペーパーは、SIFTの優れた特性を活用して、CNNアーキテクチャを改良し、精度と堅牢性を向上させることを目的としています。または、従来のCNNよりも精度が高く、敵対的なトレーニングがなくても、一連の敵対的な攻撃（FGSM、PGDなど）に対してはるかに優れた堅牢性を実現します。 
[要約]調査によると、従来のcnnsと同等またはそれ以上の精度を達成できることが示されています。実験によると、evpnetは従来のcnnsと同等またはそれ以上の価値を達成できると同時に、はるかに優れた価値を達成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Pyramid Point: A Multi-Level Focusing Network for Revisiting Feature
  Layers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_63.html">
      <font color="black">Pyramid Point: A Multi-Level Focusing Network for Revisiting Feature
  Layers</font>
    </a>
  </h2>
  <font color="black">このFKP変換により、機能の品質が向上し、カーネル出力を動的に重み付けできます。これらのFKP変換は、エンコーダのバックボーンを構成するRecurrent FKPボトルネックブロックの中心部分です。また、アブレーション調査を実行して、 FKPコンバージョンの各要素のプラスの効果。 
[概要]従来の「u」形状の代わりに、密なピラミッド構造を使用するピラミッドポイントネットワークを提案します。また、アブレーション研究を実行して、fkpconvの各要素の正の効果を示します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: A Method to Generate High Precision Mesh Model and RGB-D Datasetfor 6D
  Pose Estimation Task -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_64.html">
      <font color="black">A Method to Generate High Precision Mesh Model and RGB-D Datasetfor 6D
  Pose Estimation Task</font>
    </a>
  </h2>
  <font color="black">高品質のデータセットは深層学習法にとって重要です。私たちの方法を使用して、より優れたより正確な注釈付きの大規模なデータセットを作成できます。機器と技術は向上しましたが、新しくより優れたデータセットを収集しようとした人は誰もいませんでした。 
[概要]これらのデータセットの作成に使用された深度センサーが古くなっているため、解像度と精度がより高い需要基準を完全に満たすことができませんでした</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: P1AC: Revisiting Absolute Pose From a Single Affine Correspondence -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_65.html">
      <font color="black">P1AC: Revisiting Absolute Pose From a Single Affine Correspondence</font>
    </a>
  </h2>
  <font color="black">合成データの評価は、私たちのアプローチが数値的に安定しており、P3Pよりも点観測ノイズに対してロバストであることを示しています。また、大規模な画像ベースのローカリゼーションへのアプローチの適用を評価し、反復回数の実際的な削減を示します。画像をロバストにローカライズするために必要な計算時間。私たちの方法は、前の作業で行われた制限的な仮定を取り除き、大規模な画像ベースのローカリゼーションに適用できる一般的なソリューションを提供します。 
[ABSTRACT]アフィン対応は、従来、広いベースラインでの特徴マッチングを改善するために使用されてきました。しかし、これまでの研究では、絶対カメラポーズ計算にこのような対応を使用することを検討していませんでした。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Normalized Weighting Schemes for Image Interpolation Algorithms -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_66.html">
      <font color="black">Normalized Weighting Schemes for Image Interpolation Algorithms</font>
    </a>
  </h2>
  <font color="black">最初のスキームは、直径が四角形の最小辺に等しい円の正規化された面積に基づいています。2番目のスキームは、半径が斜辺に等しい円の正規化された面積に基づいています。画像補間アルゴリズムの4つの重み付けスキームを提示および評価します。 
[概要]最初のスキームは、直径が四角形の最小辺に等しい円の正規化された面積に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Enforcing Perceptual Consistency on Generative Adversarial Networks by
  Using the Normalised Laplacian Pyramid Distance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_67.html">
      <font color="black">Enforcing Perceptual Consistency on Generative Adversarial Networks by
  Using the Normalised Laplacian Pyramid Distance</font>
    </a>
  </h2>
  <font color="black">NLPDをレギュラライザーとして使用すると、生成された画像の画像セグメンテーションの精度が向上し、2つの非参照画像品質メトリックが向上することがわかりました。NLPDは、平均エネルギーのローカル推定値に関して係数の値を正規化する原理に基づいています。さまざまなスケールで、人間の知覚を含むさまざまな実験ですでに正常にテストされています。この論文では、条件付き生成敵対ネットワーク（cGAN）を使用した画像から画像への変換のための代替知覚レギュラライザーを提案します。 
[概要]正規化されたラプラシアンピラミッドネットワークを使用して、このレギュラライザーを最初に提案されたl1距離と比較します。segmentpdを使用すると、生成された画像にはローカルコントラストとグローバルコントラストの両方のより現実的な値が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-09">
        <br><font color="black">2019-08-09</font>
      </time>
    </span>
</section>
<!-- paper0: Spatio-Temporal Analysis of Facial Actions using Lifecycle-Aware Capsule
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_68.html">
      <font color="black">Spatio-Temporal Analysis of Facial Actions using Lifecycle-Aware Capsule
  Networks</font>
    </a>
  </h2>
  <font color="black">フレームレベルでは、AULA-CapsのカプセルレイヤーはAUアクティベーションを決定するために空間機能プリミティブを学習しますが、シーケンスレベルでは、シーケンス内の関連する時空間セグメントに焦点を当てることにより、隣接するフレーム間の時間依存性を学習します。モデルは、一般的に使用されるBP4DおよびGFTベンチマークデータセットで評価され、両方のデータセットで最先端の結果が得られます。学習された機能カプセルは、モデルが空間情報または時空間情報により選択的に焦点を当てることを学習するように一緒にルーティングされます。 AUのライフサイクルによって異なります。 
[概要]アクションユニットのライフサイクルを意識したカプセルネットワーク（aula-caps）は、フレームレベルとシーケンスレベルの両方の機能を使用してau検出を実行します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: NeurIPS 2019 Disentanglement Challenge: Improved Disentanglement through
  Learned Aggregation of Convolutional Feature Maps -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_69.html">
      <font color="black">NeurIPS 2019 Disentanglement Challenge: Improved Disentanglement through
  Learned Aggregation of Convolutional Feature Maps</font>
    </a>
  </h2>
  <font color="black">NeurIPS 2019解きほぐしチャレンジへのステージ2提出へのこのレポートは、解きほぐされた潜在因子を学習するための簡単な画像前処理方法を提示します。コードはhttps://github.com/mseitzer/neurips2019-disentanglement-challengeで入手できます。チャレンジのステージ2で2位。 
[ABSTRACT]オートエンコーダーを使用して、解きほぐすために地域的に集約された機能で単純なオートエンコーダーをトレーニングできます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-27">
        <br><font color="black">2020-02-27</font>
      </time>
    </span>
</section>
<!-- paper0: Show and Speak: Directly Synthesize Spoken Description of Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_70.html">
      <font color="black">Show and Speak: Directly Synthesize Spoken Description of Images</font>
    </a>
  </h2>
  <font color="black">最終的な音声音声は、WaveNetを介して予測されたスペクトログラムから取得されます。SASの基本構造は、入力として画像を受け取り、この画像を説明する音声のスペクトログラムを予測するエンコーダ-デコーダアーキテクチャです。このペーパーでは、新しいモデルを提案します。ショーアンドスピーチ（SAS）モデルと呼ばれ、テキストや電話番号の必要性を回避して、画像の音声による説明を初めて直接合成できるようになりました。 
[ABSTRACT] sasは、画像を入力として受け取り、この画像を説明する音声のスペクトログラムを予測するエンコーダーデコーダーアーキテクチャです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-23">
        <br><font color="black">2020-10-23</font>
      </time>
    </span>
</section>
<!-- paper0: Progressive Multi-Scale Residual Network for Single Image
  Super-Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_71.html">
      <font color="black">Progressive Multi-Scale Residual Network for Single Image
  Super-Resolution</font>
    </a>
  </h2>
  <font color="black">自己アンサンブルを備えた拡張モデルPMRN $ ^ + $は、パラメーターがはるかに少なく、計算の複雑さが少ない大規模ネットワークよりも競争力のある、または優れた結果を実現します。さらに、チャネルおよびピクセル単位の注意メカニズム（CPA）は、固有の相関関係を見つけるために設計されています。高周波情報に重点を置く重みとバイアス係数を持つ画像特徴の中で..具体的には、プログレッシブマルチスケール残差ブロック（PMRB）を考案して、大きなフィルターを小さなフィルターの組み合わせに置き換え、階層情報を徐々に調査します。 
[ABSTRACT]巨大ネットワークは通常、さまざまなサイズのフィルターで分類を構築することを目的としています。これらには、大きなフィルターを小さなフィルターの組み合わせに置き換えるためのプログレッシブマルチスケール残差ブロックが含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-19">
        <br><font color="black">2020-07-19</font>
      </time>
    </span>
</section>
<!-- paper0: IF-Defense: 3D Adversarial Point Cloud Defense via Implicit Function
  based Restoration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_72.html">
      <font color="black">IF-Defense: 3D Adversarial Point Cloud Defense via Implicit Function
  based Restoration</font>
    </a>
  </h2>
  <font color="black">以前の方法と比較すると、IF-Defenseは、顕著な点落下攻撃に対して20.02％、PointNetに対するLG-GAN攻撃に対して16.29％の分類精度の向上を示します。これは、2つのステップで構成されます。表面回復モジュールによる形状、および2）ジオメトリおよび分布を意識した制約の下で、攻撃された点群と予測された陰関数との差を最小限に抑えることにより、クリーンで完全な点群を復元します。実験結果は、IF-DefenseがPointNet、PointNet ++、DGCNN、およびPointConvに対する既存のすべての敵対的攻撃に対する最先端の防御パフォーマンス。 
[概要]最近の作品は、3D点群の処理に大きな成功を収めています。これらには、3D点群などの高度な高度な技術が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-11">
        <br><font color="black">2020-10-11</font>
      </time>
    </span>
</section>
<!-- paper0: Exploring Intermediate Representation for Monocular Vehicle Pose
  Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_73.html">
      <font color="black">Exploring Intermediate Representation for Monocular Vehicle Pose
  Estimation</font>
    </a>
  </h2>
  <font color="black">知覚された強度をIGRに変換するディープモデルを設計します。IGRは、カメラ座標系でオブジェクト指向をエンコードする3D表現にマッピングされます。プリミティブ3D注釈から容易に派生する補間された立方体表現を設計することにより、前者の質問に答えます。目標を達成するには、使用するIGRと、それらをより効果的に学習する方法を指定する必要があります。 
[要約]新しい作品は、3D表現のための意味のある中間の幾何学的表現（igrs）を抽出することにより、進歩的なアプローチを探求することを可能にします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: 3D CNNs with Adaptive Temporal Feature Resolutions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_74.html">
      <font color="black">3D CNNs with Adaptive Temporal Feature Resolutions</font>
    </a>
  </h2>
  <font color="black">その結果、時間的特徴の解像度は静的ではなくなりましたが、入力ビデオクリップごとに異なります。Kinetics-600、Kineticsなどのさまざまなデータセットの複数の最先端の3D CNNにモジュールを追加して、モジュールを評価します。 -400、ミニキネティクス、Something-Something V2、UCF101、およびHMDB51。 3D CNNのGFLOPは、ネットワーク内の時間的特徴の解像度を下げることで減らすことができますが、すべての入力クリップに最適な設定はありません。 
[概要] 3d cnnはgflopsに縮小できますが、すべての入力クリップに最適な設定はありません。cnnsは、時間的特徴の類似性を学習し、類似した特徴をグループ化することにより、3dcnnを強化します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: SeekNet: Improved Human Instance Segmentation via Reinforcement Learning
  Based Optimized Robot Relocation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_75.html">
      <font color="black">SeekNet: Improved Human Instance Segmentation via Reinforcement Learning
  Based Optimized Robot Relocation</font>
    </a>
  </h2>
  <font color="black">具現化された視覚認識によるアモーダル認識の改善された最適化手法であるSeekNetを提案します。また、効率的な具現化された認識システムを使用して、COVID-19症状の事前スクリーニングアルゴリズムの信頼性を向上させるためにSeekNetを実験します。検出と追跡を行い、他のベースラインに対するアルゴリズムの優位性を示します。 
[概要]ほとんどの最先端の視覚認識システムは、アモーダル認識を実行する機能を欠いています。これらのアプローチは、動的オブジェクトなどの実世界のアプリケーションでの課題に悩まされています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: A Review of Generalized Zero-Shot Learning Methods -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_76.html">
      <font color="black">A Review of Generalized Zero-Shot Learning Methods</font>
    </a>
  </h2>
  <font color="black">さらに、将来の研究のためのいくつかの研究の方向性について議論します。導入以来、多くのGZSLモデルが策定されています。最初に、問題と挑戦的な問題を含むGZSLの概要を説明します。 
[概要] gzslはソーシャル情報を使用して、表示されているクラスと表示されていないクラスの間のギャップを埋めます。gzslの包括的なレビューを提示します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Sketch-Based Modeling: Tips and Tricks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_77.html">
      <font color="black">Deep Sketch-Based Modeling: Tips and Tricks</font>
    </a>
  </h2>
  <font color="black">そうすることで、いくつかの重要な洞察を導き出します。（i）スパース性は通常、前景と背景の誤った予測をもたらします。（ii）人間のスタイルの多様性は、考慮されない場合、非常に貧弱な一般化特性につながる可能性があります。最後に（iii）専用のスケッチインターフェイスを使用しない限り、スケッチが固定視点の視点に一致することは期待できません。近年、深い画像ベースのモデリングが大きな注目を集めていますが、スケッチベースのモデリングの並行問題には多くの場合、潜在的なアプリケーションとして、簡単に研究されただけです。これらの違いのそれぞれが問題を引き起こす可能性がある理由について説明し、特定のクラスの画像ベースの方法を適用できないようにすることさえあります。 
[概要]新しいプロジェクトは、スケッチと画像入力の主な違いを特定するために使用されます。問題に対処するための一連の異なる方法を作成するために使用できます。プロジェクトは、代替モデルとしても使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-12">
        <br><font color="black">2020-11-12</font>
      </time>
    </span>
</section>
<!-- paper0: DeepSeqSLAM: A Trainable CNN+RNN for Joint Global Description and
  Sequence-based Place Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_78.html">
      <font color="black">DeepSeqSLAM: A Trainable CNN+RNN for Joint Global Description and
  Sequence-based Place Recognition</font>
    </a>
  </h2>
  <font color="black">フレームワークのコードとビデオはhttps://mchancan.github.io/deepseqslamで入手できます。その結果、マルチフレームの場所認識の実行は、自動運転車への展開や大きなデータセットでの評価では非常に遅くなる可能性があり、2フレームのシーケンス長などの比較的短いパラメータ値を使用すると失敗する可能性があります。2つの大きなデータセットでのアプローチを示します。ベンチマークデータセットであるNordlandとOxfordRobotCarは、それぞれ728kmと10kmを超えるルートを、複数の季節、天候、照明条件で1年間に記録しました。 
[ABSTRACT] deepseqslamは、ルートの単一の単眼画像シーケンスから視覚的および位置的表現を学習するためのトレーニング可能なcnnrnnアーキテクチャです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: EvoPose2D: Pushing the Boundaries of 2D Human Pose Estimation using
  Neuroevolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_79.html">
      <font color="black">EvoPose2D: Pushing the Boundaries of 2D Human Pose Estimation using
  Neuroevolution</font>
    </a>
  </h2>
  <font color="black">私たちの最大のネットワークであるEvoPose2D-Lは、Microsoft COCO Keypointsベンチマークで新しい最先端の精度を実現し、最も近い競合他社の2.0分の1の操作と4.3分の1のパラメーターを使用します。ニューラルアーキテクチャ検索が2D人間の姿勢推定では、機能を維持する変異を緩和し、柔軟な方法で神経進化を加速できる新しい体重移動スキームを提案します。神経進化を使用して設計されたベースラインネットワーク（EvoPose2D-Sと呼びます）は、 SimpleBaselineは、使用する浮動小数点演算が4.9倍少なく、パラメーターが13.5倍少なくなっています。 
[ABSTRACT]私たちの方法は、最先端の手作業で設計されたネットワークよりも効率的で正確な2D人間ポーズネットワーク設計を生成します。ニューロエボリューション用に設計されたベースラインネットワークは、evopose2dと呼ばれ、 4.9倍少ない浮動小数点演算と13.5倍少ないパラメータ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Beyond Static Features for Temporally Consistent 3D Human Pose and Shape
  from a Video -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_80.html">
      <font color="black">Beyond Static Features for Temporally Consistent 3D Human Pose and Shape
  from a Video</font>
    </a>
  </h2>
  <font color="black">私たちのTCMRは、フレームごとの3Dポーズと形状の精度が向上し、時間的整合性において以前のビデオベースの方法を大幅に上回ります。現在の静的機能に支配されることなく、過去と未来のフレームの時間情報に効果的に焦点を合わせます。時間的に一貫したメッシュ回復システム（TCMR）を提示します。 
[要約]ビデオベースの方法が提案されていますが、問題を解決できません。現在の静的機能に支配されることなく、過去と未来のフレームに焦点を当てています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: RIFE: Real-Time Intermediate Flow Estimation for Video Frame
  Interpolation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_81.html">
      <font color="black">RIFE: Real-Time Intermediate Flow Estimation for Video Frame
  Interpolation</font>
    </a>
  </h2>
  <font color="black">コードはhttps://github.com/hzwer/arXiv2020-RIFEで入手できます。RIFEは、画像から中間フローを直接推定できるIFNetという名前のニューラルネットワークを使用します。ほとんどの既存の方法では、最初に双方向オプティカルフローを推定します。次に、それらを線形に組み合わせて中間フローを近似し、モーション境界にアーティファクトをもたらします。 
[概要]私たちの方法は、既存のvfi方法よりも大幅に高速です。補間品質を向上させ、速度を大幅に向上させることができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-12">
        <br><font color="black">2020-11-12</font>
      </time>
    </span>
</section>
<!-- paper0: SRF-GAN: Super-Resolved Feature GAN for Multi-Scale Representation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_82.html">
      <font color="black">SRF-GAN: Super-Resolved Feature GAN for Multi-Scale Representation</font>
    </a>
  </h2>
  <font color="black">ただし、単純な内挿法（たとえば、この論文では、畳み込みオブジェクト検出器の超解像特徴のための新しいジェネレータを提案します。これを実現するために、まず、検出からなる超解像特徴GAN（SRF-GAN）を設計します。ベースのジェネレーターと機能パッチディスクリミネーター。
[ABSTRACT]新しいシステムは、検出ベースのジェネレーターと機能パッチディスクリミネーターを使用します。従来の補間方法に取って代わり、他の従来の検出器と一緒に簡単に微調整できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Generalized Continual Zero-Shot Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_83.html">
      <font color="black">Generalized Continual Zero-Shot Learning</font>
    </a>
  </h2>
  <font color="black">壊滅的な忘却と非妥協を避けるために、知識の蒸留を使用し、小さなエピソード記憶を使用して前のタスクからのいくつかのサンプルを保存および再生します。ベースラインを開発し、継続学習の2つの異なる設定について5つのZSLベンチマークデータセットで一般化されたCZSLを評価します。クラスインクリメンタルなし..実験結果は、シングルヘッドCZSLがより一般化可能であり、実際のアプリケーションに適していることを明確に示しています。 
[概要] zslは継続的なzsl（czsl）を使用して、過去の経験を活用して変化する環境から学習します。壊滅的な忘却と非定常を回避するために、知識の蒸留を使用し、小さなエピソード記憶を使用して前のタスクからのいくつかのサンプルを保存および再生します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Quantifying Sources of Uncertainty in Deep Learning-Based Image
  Reconstruction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_84.html">
      <font color="black">Quantifying Sources of Uncertainty in Deep Learning-Based Image
  Reconstruction</font>
    </a>
  </h2>
  <font color="black">推定された不確実性は、制限された測定モデルと、制限された角度ジオメトリによる情報の欠落によって引き起こされる再構成の変動をキャプチャします。ディープニューラルネットワークに基づく画像再構成方法は、状態と同等またはそれを超える優れたパフォーマンスを示しています。 -従来のアプローチの最先端の結果ですが、再構成に関する不確実性情報を提供しないことがよくあります。私たちの方法は、スパースビューと制限された角度データの両方を使用したコンピューター断層撮影の従来のベンチマークに対して競争力のあるパフォーマンスを示すことを示します。 
[概要]反復画像再構成の学習における偶然性および認識論的不確実性を定量化するためのスケーラブルで効率的なフレームワークを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Effect of the sEMG electrode (re)placement and feature set size on the
  hand movement recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_85.html">
      <font color="black">Effect of the sEMG electrode (re)placement and feature set size on the
  hand movement recognition</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、アレイ電極を再配置したときに分類精度に有意差がないことを示し、分類の再トレーニングと最適な機能セットの選択が成功したことを示しています。この研究は、分類精度と分類器の選択に影響を与える要因の相互作用をテストすることの重要性を強調しています。手の動き認識システムの設計の指針を確立するために、独立して影響を与えました。最大のデータセット（9つの手の動き）では、LDAとQDAはANNを上回りましたが、3つの把持動作ではANNは有望な結果を示しました。 
[要約]研究は、電極アレイの位置と機能セットのサイズの間の相互作用が統計的に有意ではないことを示しています。結果はまた、主成分の数が許容可能な分類精度に重要な役割を果たすことを期待して示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-05">
        <br><font color="black">2020-05-05</font>
      </time>
    </span>
</section>
<!-- paper0: DOTS: Decoupling Operation and Topology in Differentiable Architecture
  Search -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CV/paper_86.html">
      <font color="black">DOTS: Decoupling Operation and Topology in Differentiable Architecture
  Search</font>
    </a>
  </h2>
  <font color="black">CIFAR10 / 100とImageNetでの実験は、DOTSが微分可能なNASの効果的なソリューションであることを示しています。既存の勾配ベースのNASメソッドをDOTSに組み込んで、トポロジ検索によってさらに改善することができます。トポロジに影響を与える可能性がある場合は、グループ戦略を使用した操作検索を採用して、トポロジ関連の操作を保持し、トポロジ検索を改善します。 
[概要]ダーツは主に操作検索に焦点を当てています。操作の重みからセルメッシュを導出します。提案された検索スペースは、検索されたセル内の柔軟な数のエッジをサポートするために簡単に拡張できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-02">
        <br><font color="black">2020-10-02</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Visually Grounded Continual Learning of Compositional Phrases -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_0.html">
      <font color="black">Visually Grounded Continual Learning of Compositional Phrases</font>
    </a>
  </h2>
  <font color="black">VisCOLLの研究を容易にするために、COCOシフトとFlickrシフトの2つのデータセットを構築し、異なる継続的な学習方法を使用してそれらをベンチマークします。将来の作業を導くために、さらにアブレーションと分析を行います。人間は、はるかに制限されたアクセスで継続的に言語を習得します。現代のNLPシステムと比較して、一度にデータサンプルに。 
[ABSTRACT] viscollは、視覚的に根拠のある言語学習タスクであり、ストリーミングビジュアルシーンからのオブジェクトフレーズの継続的な取得をシミュレートします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-02">
        <br><font color="black">2020-05-02</font>
      </time>
    </span>
</section>
<!-- paper0: Compressing Language Models using Doped Kronecker Products -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_1.html">
      <font color="black">Compressing Language Models using Doped Kronecker Products</font>
    </a>
  </h2>
  <font color="black">これらのモデルをトレーニングするために、コマトリックスドロップアウト正則化（CMR）と呼ばれる新しい正則化スキームを使用するコマトリックス適応（CMA）の現象に対する新しいソリューションを提示します。25倍の圧縮で、同等のプルーニングされたネットワークは7.9％になります。 HMDとLMFは、それぞれ15％と27％のパープレキシティスコアの損失につながりますが、パープレキシティスコアの損失は、この圧縮方法をドープされたクローネッカー製品の圧縮と呼びます。 
[ABSTRACT] kpは、大規模な自然言語処理タスクに適用されます。ただし、存在する場合、精度が大幅に低下します。これは、co行列ドロップアウト正則化と呼ばれる新しい方法によるものです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-24">
        <br><font color="black">2020-01-24</font>
      </time>
    </span>
</section>
<!-- paper0: Analyzing Sustainability Reports Using Natural Language Processing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_2.html">
      <font color="black">Analyzing Sustainability Reports Using Natural Language Processing</font>
    </a>
  </h2>
  <font color="black">このツールと、それを開発するために使用した方法論を今回の記事で紹介します。近年、企業は環境への影響を軽減し、変化する気候の状況に適応することをますます目指しています。私たちは自然言語の最近の進歩を活用しました。質問回答アプローチに基づいて気候関連セクションを特定するために財務レポートの分析を可能にするカスタムモデルClimateQAを作成するための処理（NLP）。 
[概要]多くの企業が、環境への影響を軽減し、変化する気候の状況に適応しようとしています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-03">
        <br><font color="black">2020-11-03</font>
      </time>
    </span>
</section>
<!-- paper0: Structural and Functional Decomposition for Personality Image Captioning
  in a Communication Game -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_3.html">
      <font color="black">Structural and Functional Decomposition for Personality Image Captioning
  in a Communication Game</font>
    </a>
  </h2>
  <font color="black">私たちの実験は、提案されたモデルがPICの最先端のパフォーマンスを達成することを示しています。これにより、話者と聴取者はGPT2の言語エンコーディング能力の恩恵を受けることができます。さらに、言語モデルGPT2をPICのキャプション生成を実行します。 
[ABSTRACT]キャプションを改善して、画像を自然に表現し、特徴を表現することができます。これにより、話し手と聞き手はgpt2の言語能力を活用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: KddRES: A Multi-level Knowledge-driven Dialogue Dataset for Restaurant
  Towards Customized Dialogue System -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_4.html">
      <font color="black">KddRES: A Multi-level Knowledge-driven Dialogue Dataset for Restaurant
  Towards Customized Dialogue System</font>
    </a>
  </h2>
  <font color="black">私たちのコーパスには、さまざまな地域のさまざまなスタイルの10のレストランから派生した0.8kの会話が含まれています。ベンチマーク実験とデータ統計分析は、データセットの多様性と豊富な注釈を示しています。それに加えて、きめ細かいスロットとインテントを設計しました。セマンティック情報をより適切にキャプチャします。 
[概要]香港のレストラン（kddres）向けの最初の広東語の知識主導型対話データセットを公開します。これは、特定の1つのレストランへの複数回の会話の情報に基づいています。kddresのリリースは、現在の対話データセットの必要な補足になる可能性があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Neural Semi-supervised Learning for Text Classification Under
  Large-Scale Pretraining -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_5.html">
      <font color="black">Neural Semi-supervised Learning for Text Classification Under
  Large-Scale Pretraining</font>
    </a>
  </h2>
  <font color="black">私たちの仕事は、大規模な事前トレーニングのコンテキストでの半教師あり学習モデルの動作を理解するための最初のステップです。半教師あり学習戦略を使用すると、わずか50のトレーニングデータで約93.8％の精度のパフォーマンスを達成できます。 IMDBデータセットのポイント、および完全なIMDBデータセットとの96.6％の競争力のあるパフォーマンス..さまざまな半教師あり戦略がさまざまなサイズのD、さまざまなサイズのUなどに関するパフォーマンスにどのように影響するか。
[要約]私たちの作業は最初のステップを示します大規模な事前トレーニングのコンテキストでの半教師あり学習モデルの動作を理解する上で</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Toward Understanding Clinical Context of Medication Change Events in
  Clinical Narratives -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_6.html">
      <font color="black">Toward Understanding Clinical Context of Medication Change Events in
  Clinical Narratives</font>
    </a>
  </h2>
  <font color="black">アクション、否定、時間性、確実性、およびアクター）は、注釈プロセスと発生した課題を説明し、予備実験の結果を報告します。このペーパーでは、関連するコンテキストをキャプチャするためのデータセットであるContextualized Medication Event Dataset（CMED）を示します。臨床イベントのコンテキストをさまざまな直交次元に編成する新しい概念フレームワークを使用して開発された、臨床ノートに文書化された投薬変更の一覧。結果のデータセットCMEDは、500を超える臨床ノートに注釈が付けられた9,013の投薬言及で構成されます。 
[要約]結果のデータセットcmedは、500を超える臨床ノートに注釈が付けられた9、013の薬の言及で構成されています。薬の変更のより正確なコンテキストの側面を作成するために、研究者は薬の変更イベントに関連するコンテキストの側面を定義しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Generative Data Augmentation for Commonsense Reasoning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_7.html">
      <font color="black">Generative Data Augmentation for Commonsense Reasoning</font>
    </a>
  </h2>
  <font color="black">私たちの分析は、G-DAUG ^ Cが流暢なトレーニング例の多様なセットを生成し、その選択とトレーニングアプローチがパフォーマンスにとって重要であることを示しています。複数の常識的な推論ベンチマークを使用した実験では、G-DAUG ^ Cは一貫して既存のデータ拡張方法を上回っています。逆翻訳に基づいて、WinoGrande、CODAH、およびCommonsenseQAに関する新しい最先端技術を確立します。G-DAUG^ Cを調査します。これは、より正確で堅牢な学習を実現することを目的とした新しい生成データ拡張方法です。低リソース設定。 
[概要]私たちのアプローチは、事前にトレーニングされた言語モデルを使用して合成例を生成します。g-daug* c-拡張トレーニングも強化されます-分布の一般化</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-24">
        <br><font color="black">2020-04-24</font>
      </time>
    </span>
</section>
<!-- paper0: Curriculum CycleGAN for Textual Sentiment Domain Adaptation with
  Multiple Sources -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_8.html">
      <font color="black">Curriculum CycleGAN for Textual Sentiment Domain Adaptation with
  Multiple Sources</font>
    </a>
  </h2>
  <font color="black">3つのベンチマークデータセットで広範な実験を実施し、最先端のアプローチを大幅に上回ります。このホワイトペーパーでは、カリキュラムサイクル整合性のある敵対的生成ネットワーク（C）という名前のインスタンスレベルのマルチソースドメイン適応フレームワークを提案します。 -CycleGAN）..大規模な注釈を軽減するために、ドメイン適応（DA）は、別のラベル付きソースドメインから転送可能なモデルを学習することにより、代替ソリューションを提供します。 
[ABSTRACT]ドメイン適応（da）は、別のラベル付きソースドメインから転送可能なモデルを学習することにより、代替ソリューションを提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Refining Automatic Speech Recognition System for older adults -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_9.html">
      <font color="black">Refining Automatic Speech Recognition System for older adults</font>
    </a>
  </h2>
  <font color="black">12時間のトレーニングデータを使用して、認知障害の可能性がある社会的に孤立した高齢者（80歳以上）向けのASRシステムの開発を試みます。TLの基本的な考え方に基づいて、モデルパラメータを調整し、モデルの中間情報を利用するための注意メカニズム..成人集団のASRがターゲット集団で不十分に機能し、転移学習（TL）がシステムのパフォーマンスを向上させる可能性があることを実験的に特定します。 
[概要]自動システムシステムシステムは、成人からの十分なデータに基づいてトレーニングされています。成人と高齢者の音響の不一致により、高齢者のスピーチの影響を受けやすくなっています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Cascade RNN-Transducer: Syllable Based Streaming On-device Mandarin
  Speech Recognition with a Syllable-to-Character Converter -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_10.html">
      <font color="black">Cascade RNN-Transducer: Syllable Based Streaming On-device Mandarin
  Speech Recognition with a Syllable-to-Character Converter</font>
    </a>
  </h2>
  <font color="black">RNN-Tは、言語情報を強化するために予測ネットワークを採用していますが、トレーニングにはペアの音声テキストデータが必要であるため、言語モデリング機能は制限されています。マンダリン中国語は文字ベースの言語であり、各文字は音色の音節として発音されるこの論文は、RNN-Tの言語モデリング能力を改善するための新しいカスケードRNN-Tアプローチを提案します。私たちのアプローチでは、最初にRNN-Tを使用して音響機能を音節シーケンスに変換し、次にRNN-Tベースの音節から文字へのコンバーターを介して音節シーケンスを文字シーケンスに変換します。追加のテキストデータを通じて言語モデリング機能をさらに強化します。 、外部言語モデルとの浅い融合など、パフォーマンスの向上はわずかです。 
[概要]言語認識システムは、言語モデルの能力を強化するために使用できます。新しい言語モデルを開発するために簡単に使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Self-supervised Document Clustering Based on BERT with Data Augment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_11.html">
      <font color="black">Self-supervised Document Clustering Based on BERT with Data Augment</font>
    </a>
  </h2>
  <font color="black">どちらの方法も、最近提案された教師なしクラスタリング手法と比較して、クラスタリングの精度で最先端の結果を達成します。対照的な学習は、複雑にならずに十分に研究された深いモデルの利点と経験を継承できる、識別可能な教師なし学習を追求するための良い方法です。新しいモデル設計..この論文では、文書クラスタリングのための2つの学習方法を提案します。1つは教師なしデータ拡張による部分的対照学習であり、もう1つは自己監視対照学習です。 
[概要]新しい研究では、ドキュメントクラスタリングの2つの学習方法が提案されています。この方法は、教師なしデータ拡張を使用した部分対照学習であり、もう1つは自己教師あり対照学習方法です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Machine Reading Comprehension with Single-choice Decision and
  Transfer Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_12.html">
      <font color="black">Improving Machine Reading Comprehension with Single-choice Decision and
  Transfer Learning</font>
    </a>
  </h2>
  <font color="black">実験結果は、単一選択が複数選択よりも優れていることを示しています。多肢選択式機械読解（MMRC）は、与えられたパッセージと質問に基づいて一連のオプションから正しい答えを選択することを目的としています。より良いパラメータを調整するためのAutoML戦略。 
[ABSTRACT] mmrcは、mmrcに固有のタスクに基づいています。他のmrcタスクから知識を転送することは簡単ではありません。これにより、最高の信頼スコアでオプションを修正できます。モデルは、次のような新しい最先端の結果を実現します。シングル設定とアンサンブル設定の両方</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-06">
        <br><font color="black">2020-11-06</font>
      </time>
    </span>
</section>
<!-- paper0: Understanding Image Captioning Models beyond Visualizing Attention -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_13.html">
      <font color="black">Understanding Image Captioning Models beyond Visualizing Attention</font>
    </a>
  </h2>
  <font color="black">この論文は、注意自体を視覚化するだけでなく、注意メカニズムを備えた画像キャプションモデルの予測を解釈します。私たちは、2つの広く使用されている注意メカニズムで実験を行います。 ..説明方法は、予測されたキャプションの各単語について、ピクセル単位の画像説明（入力画像のサポートおよび反対のピクセル）と言語的説明（前のシーケンスのサポートおよび反対の単語）を同時に提供することを示します。 
[要約]説明のパターンは、コレによると、画像キャプションモデルのオブジェクトの幻覚を軽減することができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-04">
        <br><font color="black">2020-01-04</font>
      </time>
    </span>
</section>
<!-- paper0: Is Multihop QA in DiRe Condition? Measuring and Reducing Disconnected
  Reasoning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_14.html">
      <font color="black">Is Multihop QA in DiRe Condition? Measuring and Reducing Disconnected
  Reasoning</font>
    </a>
  </h2>
  <font color="black">まず、サポートファクトのサブセット間で切断された推論などの望ましくない動作を形式化します。次に、\ emph {対照的なサポートの十分性}の概念を使用して、切断された推論の量を減らす既存のデータセットの自動変換を導入します。この制限真の進捗状況を測定し、マルチホップQAデータセットを構築するという目的を打ち破る私たちの能力。 
[概要]データセットの大まかな量に3つの貢献をします。これにより、モデルの開発が可能になります。これにより、切断された推論を通じてモデルが不正を行うことができる量を測定するための敏捷性プローブが可能になります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-02">
        <br><font color="black">2020-05-02</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Augmentation Policy Search for Domain and Cross-Lingual
  Generalization in Reading Comprehension -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_15.html">
      <font color="black">Adversarial Augmentation Policy Search for Domain and Cross-Lingual
  Generalization in Reading Comprehension</font>
    </a>
  </h2>
  <font color="black">これらの学習したポリシーを使用して、敵対者のトレーニングがドメイン内、ドメイン外、および言語間（ドイツ語、ロシア語、トルコ語）の一般化に大幅な改善をもたらす可能性があることを示します。この作業では、いくつかの効果的な敵対者と読解力モデルを敵対的評価に対してより堅牢にするだけでなく、ソースドメインおよび新しいドメインと言語への一般化を改善することを目的とした自動データ拡張ポリシー検索方法..読解力モデルは、トレーニングデータセットのニュアンスに適合しすぎて、失敗することがよくあります。敵対的評価。 
[ABSTRACT]敵対的に拡張されたデータセットを使用したトレーニングは、パフォーマンスによって改善されましたが、モデルの一般化に悪影響を及ぼしました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-13">
        <br><font color="black">2020-04-13</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Ignore: Long Document Coreference with Bounded Memory Neural
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_16.html">
      <font color="black">Learning to Ignore: Long Document Coreference with Bounded Memory Neural
  Networks</font>
    </a>
  </h2>
  <font color="black">すべてのエンティティをメモリに保持する必要はないと主張し、一度に少数の制限されたエンティティのみを追跡するメモリ拡張ニューラルネットワークを提案します。これにより、ドキュメントの長さの線形ランタイムが保証されます。増分共参照解決を行う最近の作業エンティティのグローバル表現のみを使用することは実用的な利点を示しますが、すべてのエンティティをメモリに保持する必要があり、長いドキュメントには実用的ではない可能性があります。 （b）モデルは、ルールベースの戦略を簡単に上回る効率的なメモリ管理戦略を学習します。 
[要約]エンティティのグローバル表現のみを使用して行われた調査は、実用的な利点を示していますが、すべてのエンティティをメモリに保持する必要があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-06">
        <br><font color="black">2020-10-06</font>
      </time>
    </span>
</section>
<!-- paper0: MVP-BERT: Redesigning Vocabularies for Chinese BERT and Multi-Vocab
  Pretraining -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_17.html">
      <font color="black">MVP-BERT: Redesigning Vocabularies for Chinese BERT and Multi-Vocab
  Pretraining</font>
    </a>
  </h2>
  <font color="black">この作業では、最初に、中国語の単語セグメンテーション（CWS）とサブ単語のトークン化を使用して、中国語のBERTの語彙を形成するための新しい方法\ emph {seg \ _tok}を提案します。実験によると：（a）比較文字ベースの語彙を使用すると、\ emph {seg \ _tok}は、文レベルのタスクでの中国語PLMのパフォーマンスを向上させるだけでなく、効率も向上させることができます。 （b）MVPは、PLMのダウンストリームパフォーマンスを向上させます。特に、シーケンスラベリングタスクでの\ emph {seg \ _tok}のパフォーマンスを向上させることができます。次に、モデルの表現力を向上させるために、3つのバージョンのマルチボキャブラリー事前トレーニング（MVP）を提案します。 
[概要]プレズニックバージョンのプラウディットは、モデルのパフォーマンスを向上させるために使用できます。また、単一の語彙を持つレスポンダーの能力を向上させることができ、パフォーマンスが制限されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Simplified Self-Attention for Transformer-based End-to-End Speech
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_18.html">
      <font color="black">Simplified Self-Attention for Transformer-based End-to-End Speech
  Recognition</font>
    </a>
  </h2>
  <font color="black">印象的な20％のパラメーター削減により、私たちのモデルは20,000時間の大規模タスクで認識パフォーマンスの損失を示しません。公共のAISHELL-1、内部1000時間でSSANベースおよび従来のSANベースの変圧器を評価します20,000時間の大規模な北京語タスク..トランスフォーマーモデルは、長期的な依存関係のモデリングに優れているため、さまざまなタスクで最先端のパフォーマンスを備えたエンドツーエンドの音声認識に導入されています。 
[ABSTRACT]トランスフォーマーモデルは、aishellで実際に20％以上の削減を達成できます-1つのタスク</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-21">
        <br><font color="black">2020-05-21</font>
      </time>
    </span>
</section>
<!-- paper0: Blank Language Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_19.html">
      <font color="black">Blank Language Models</font>
    </a>
  </h2>
  <font color="black">スタイルの転送と破損した古代のテキストの復元に関する実験は、このフレームワークが幅広いアプリケーションに対応できる可能性を示しています。BLMは、限界データの可能性の下限を使用して効率的にトレーニングできます。空白に配置する単語を繰り返し決定し、新しいブランクを挿入するかどうか、および埋めるブランクがなくなると生成を停止します。 
[概要]空白は、シーケンスのどの部分を展開するかを制御します。blmを使用して、テキストの編集および書き換えタスクを作成できます。欠落しているテキストスニペットを埋めるタスクでは、blmは、精度と流暢さの両方の点で他のすべてのベースラインよりも優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-08">
        <br><font color="black">2020-02-08</font>
      </time>
    </span>
</section>
<!-- paper0: Generating universal language adversarial examples by understanding and
  enhancing the transferability across neural models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_20.html">
      <font color="black">Generating universal language adversarial examples by understanding and
  enhancing the transferability across neural models</font>
    </a>
  </h2>
  <font color="black">特に、ネットワークアーキテクチャ、入力形式、単語の埋め込み、モデル容量などのさまざまな要因が敵対的攻撃の転送可能性にどのように影響するかを調査するために、広範な実験を実施します。ディープニューラルネットワークモデルは敵対的攻撃に対して脆弱です。これらに基づいて次に、研究では、敵対的な例を誘発してほとんどすべての既存のモデルを攻撃できるユニバーサルブラックボックス攻撃アルゴリズムを提案します。 
[概要]この論文では、テキスト分類モデルに対する敵対的攻撃の転送可能性を調査します。この論文では、そのような要因を体系的に調査します。これらの調査では、敵対的例を誘発して既存のほぼすべてのモデルを攻撃できるユニバーサルブラックワード置換アルゴリズムを提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: An Unsupervised Joint System for Text Generation from Knowledge Graphs
  and Semantic Parsing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_21.html">
      <font color="black">An Unsupervised Joint System for Text Generation from Knowledge Graphs
  and Semantic Parsing</font>
    </a>
  </h2>
  <font color="black">追加の実験では、さまざまな教師なし目的を使用した場合の影響を調査します。この状況では、（1）大量の注釈付きデータを必要としないため、（2）ドメイン適応手法に依存する必要がないというアプローチが必要です。異なるドメインで..この目的のために、KGからの教師なしテキスト生成への最初のアプローチを提示し、教師なしセマンティック解析にどのように使用できるかを同時に示します。 
[概要] kgからの教師なしテキスト生成への最初のアプローチが提示されています。教師なしセマンティック解析にどのように使用できるかを同時に示します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-20">
        <br><font color="black">2019-04-20</font>
      </time>
    </span>
</section>
<!-- paper0: Natural language processing for achieving sustainable development: the
  case of neural labelling to enhance community profiling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_22.html">
      <font color="black">Natural language processing for achieving sustainable development: the
  case of neural labelling to enhance community profiling</font>
    </a>
  </h2>
  <font color="black">専門家が注釈を付けたデータセットであるStories2Insightsをリリースし、詳細なコーパス分析を提供し、タスクに対処するための強力なニューラルベースラインを多数実装します。特に、発展途上国でのコミュニティプロファイリングのケースに焦点を当てます。発展途上国では、顕著なデータギャップが存在します。実験結果は、問題が困難であり、NLPとSDの交差点で将来の研究のための十分な余地を残していることを示しています。 
[概要]自動アドレスの新しいタスク、極端な複数人のデータ分類の問題。データ認識の使用に関心がありますが、このアドレスでは番号が使用されていません。ただし、新しいシステムは使用できます。データ数を増やす</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-27">
        <br><font color="black">2020-04-27</font>
      </time>
    </span>
</section>
<!-- paper0: Measuring the Novelty of Natural Language Text Using the Conjunctive
  Clauses of a Tsetlin Machine Text Classifier -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_23.html">
      <font color="black">Measuring the Novelty of Natural Language Text Using the Conjunctive
  Clauses of a Tsetlin Machine Text Classifier</font>
    </a>
  </h2>
  <font color="black">このメカニズムは、TMの接続句を使用して、テキストがトレーニングデータの対象となるクラスにどの程度一致するかを測定します。この目的のために、最近導入されたTsetlinマシン（TM）をノベルティスコアリングメカニズムで拡張します。条項は、既知のトピックの簡潔で解釈可能な説明を提供し、スコアリングメカニズムにより、既知のトピックから新しいトピックを識別することができます。 
[要約]訓練された分類器は、操作中に新しいクラスを処理する必要があります。このメカニズムは、tmの接続句を使用します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Show and Speak: Directly Synthesize Spoken Description of Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/cs.CL/paper_24.html">
      <font color="black">Show and Speak: Directly Synthesize Spoken Description of Images</font>
    </a>
  </h2>
  <font color="black">公開ベンチマークデータベースFlickr8kでの広範な実験は、提案されたSASが画像の自然な音声記述を合成できることを示し、テキストと音素をバイパスしながら画像の音声記述を合成することが可能であることを示しています。最終的な音声音声は、予測されたスペクトログラムから取得されます。 WaveNet ..このペーパーでは、テキストや音素の必要性を回避して、画像の音声による説明を初めて直接合成できる、ショーアンドスピーク（SAS）モデルと呼ばれる新しいモデルを提案します。 
[ABSTRACT] sasは、画像を入力として受け取り、この画像を説明する音声のスペクトログラムを予測するエンコーダーデコーダーアーキテクチャです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-23">
        <br><font color="black">2020-10-23</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: Accent and Speaker Disentanglement in Many-to-many Voice Conversion -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.AS/paper_0.html">
      <font color="black">Accent and Speaker Disentanglement in Many-to-many Voice Conversion</font>
    </a>
  </h2>
  <font color="black">提案されたトリックはアクセントを改善するのに非常に効果的であり、オーディオ品質とスピーカーの類似性はよく維持されます。次に、敵対的なトレーニングを使用して、エンコーダーデコーダーベースの変換モデルでスピーカーとアクセント情報をよりよく解きほぐすことを提案します。変換モデルトレーニング用のBN機能の言語情報以外の他の要因を排除します。 
[概要]変換モデルのトレーニングでアクセントと話者の情報を解きほぐし、変換段階でそれらを再結合する必要があります。敵対的なトレーニングを使用して、エンコーダーデコーダベースの変換モデルで話者とアクセントの情報を解きほぐすことを提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Ultra-Lightweight Speech Separation via Group Communication -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.AS/paper_1.html">
      <font color="black">Ultra-Lightweight Speech Separation via Group Communication</font>
    </a>
  </h2>
  <font color="black">サブバンド出力が連結されている標準のF-LSTMモデルとは異なり、超小型モジュールがすべてのグループに並列に適用されるため、モデルサイズを大幅に縮小できます。サブバンド周波数-LSTM（ F-LSTM）アーキテクチャでは、特徴ベクトルを小さなグループに分割し、小さな処理ブロックを使用してグループ間通信を実行するグループ通信（GroupComm）を紹介します。実験結果は、強力なベースラインモデルと比較してGroupCommはすでに軽量であり、35.6分の1のパラメーターと2.3分の1の操作で同等のパフォーマンスを実現できます。 
[概要]グループ通信（groupcomm）は、35.6倍少ないパラメータと2.3倍少ない操作で同等のパフォーマンスを達成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Gender domain adaptation for automatic speech recognition task -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.AS/paper_2.html">
      <font color="black">Gender domain adaptation for automatic speech recognition task</font>
    </a>
  </h2>
  <font color="black">さらに、アクセント付き音声の完全なL2 Arcticデータセットに基づいてベースモデルを適合させ、特定の話者と男性と女性の性別ごとに微調整しました。性別サブセットでトレーニングされたモデルは、 L2 Arcticデータセット全体で調整されたモデル。最後に、事前にトレーニングされたxベクトル音声埋め込みと従来のエンコーダーからの埋め込みの連結をテストしましたが、精度の向上は重要ではありません。 
[概要]性別サブセットでトレーニングされたモデルは、l2北極データセット全体で調整されたモデルと比較して1〜2％高い精度を取得しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-08">
        <br><font color="black">2020-10-08</font>
      </time>
    </span>
</section>
<!-- paper0: Hierarchical Prosody Modeling for Non-Autoregressive Speech Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.AS/paper_3.html">
      <font color="black">Hierarchical Prosody Modeling for Non-Autoregressive Speech Synthesis</font>
    </a>
  </h2>
  <font color="black">TTSモデルに韻律機能を明示的に提供することにより、合成発話のスタイルを制御できます。提案された方法は、客観的および主観的な評価において、音声品質と韻律の自然さの点で他の競合他社よりも優れています。ただし、自然で合理的な韻律を予測する推論時に挑戦的です。 
[概要]この作品では、さまざまな韻律の下での非自己回帰モデルの動作を分析しました-モデリング設定。韻律機能を提供することに加えて、平坦化された音声のスタイルを制御できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-12">
        <br><font color="black">2020-11-12</font>
      </time>
    </span>
</section>
<!-- paper0: Optimizing voice conversion network with cycle consistency loss of
  speaker identity -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.AS/paper_4.html">
      <font color="black">Optimizing voice conversion network with cycle consistency loss of
  speaker identity</font>
    </a>
  </h2>
  <font color="black">CMU-ARCTICおよびCSTR-VCTKコーパスで実施された実験により、提案された方法が話者の類似性の点でベースライン方法よりも優れていることが確認されました。話者のアイデンティティ損失関数を使用して音声変換ネットワークを最適化する新しいトレーニングスキームを提案します。フレームレベルのスペクトル損失を最小限に抑えるだけでなく、話者のアイデンティティの損失も最小限に抑えます。 
[ABSTRACT]提案されたトレーニングスキームは音声損失を減らすだけでなく、音声アイデンティティの損失も減らします。提案されたスキームは音声変換ネットワークを可能にします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: An Overview of Voice Conversion and its Challenges: From Statistical
  Modeling to Deep Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.AS/paper_5.html">
      <font color="black">An Overview of Voice Conversion and its Challenges: From Statistical
  Modeling to Deep Learning</font>
    </a>
  </h2>
  <font color="black">また、最近の音声変換の課題（VCC）、現在の技術のパフォーマンスを報告し、音声変換の研究に利用できるリソースの概要を提供します。理論と実践の最近の進歩により、次のことができるようになりました。話者の類似性が高く、人間のような音声品質を生成します。この論文では、統計的アプローチから深層学習まで、最先端の音声変換技術とその性能評価方法の包括的な概要を示し、それらについて説明します。約束と制限。 
[概要]音声変換では、言語コンテンツを変更せずに、話者のアイデンティティを変更します。話者の類似性が高い、人間のような音声品質を生成できるようになりました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-09">
        <br><font color="black">2020-08-09</font>
      </time>
    </span>
</section>
<!-- paper0: FoolHD: Fooling speaker identification by Highly imperceptible
  adversarial Disturbances -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.AS/paper_6.html">
      <font color="black">FoolHD: Fooling speaker identification by Highly imperceptible
  adversarial Disturbances</font>
    </a>
  </h2>
  <font color="black">私たちのアプローチであるFoolHDは、DCTドメインで動作し、多目的損失関数でトレーニングされたゲート畳み込みオートエンコーダーを使用して、元のオーディオファイル内の敵対的な摂動を生成および隠蔽します。スピーカーの識別パフォーマンスを妨げることに加えて、この多目的損失は、元のオーディオファイルと敵対的なオーディオファイルから抽出されたMFCC特徴ベクトル間のフレームごとの余弦の類似性を通じて人間の知覚を説明します。スピーカー識別モデルは、誤分類を引き起こす入力信号の慎重に設計された敵対的な摂動に対して脆弱です。 
[ABSTRACT] trickhdは、ホワイトボックスステガノグラフィに触発された敵対的攻撃を使用します。これは、知覚できない敵対的生成ネットワークの欠如に基づいています。話者識別パフォーマンスを妨げることに加えて、この多目的損失は、フレームを介した人間の知覚を説明します。 xfcsとmfccの間のコサイン類似性</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: s-Transformer: Segment-Transformer for Robust Neural Speech Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.AS/paper_7.html">
      <font color="black">s-Transformer: Segment-Transformer for Robust Neural Speech Synthesis</font>
    </a>
  </h2>
  <font color="black">エンコーダとデコーダの両方のキャッシュメモリを介して繰り返しが再利用されるセグメントレベルで音声をモデル化する新しいセグメントトランスフォーマ（s-Transformer）を提案します。長距離コンテキストは拡張メモリによってキャプチャできますが、エンコーダは-処理がはるかに簡単なセグメントへのデコーダーの注意..音声発声を特徴付けるトランスフォーマーは、音声合成の大幅な改善を達成しました。 
[要約]短い文では、どちらも同じmosスコア4.29を達成します。これは、長い文では4.2に非常に近い値です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Refining Automatic Speech Recognition System for older adults -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.AS/paper_8.html">
      <font color="black">Refining Automatic Speech Recognition System for older adults</font>
    </a>
  </h2>
  <font color="black">TLの基本的な考え方に基づいて、モデルパラメータを調整し、注意メカニズムを活用してモデルの中間情報を利用することにより、システムをさらに改善します。私たちのアプローチは、TLモデルに比べて1.58％の絶対的な改善を達成します。高品質の自動音声認識を構築します。トレーニングデータが限られている（ASR）システムは、特に狭いターゲット人口にとって困難な作業でした。 
[概要]自動システムシステムシステムは、成人からの十分なデータに基づいてトレーニングされています。成人と高齢者の音響の不一致により、高齢者のスピーチの影響を受けやすくなっています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Implicit Filter-and-sum Network for Multi-channel Speech Separation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.AS/paper_9.html">
      <font color="black">Implicit Filter-and-sum Network for Multi-channel Speech Separation</font>
    </a>
  </h2>
  <font color="black">アドホックマイクアレイジオメトリと固定マイクアレイジオメトリの両方での実験結果は、FaSNetに提案された変更（iFaSNetと呼びます）が、モデルの複雑さが同等であるすべての条件でベンチマークFaSNetを大幅に上回ることができることを示しています。公式化の観点から、すべてのマイクを含む明示的な時間領域のフィルターと合計の操作を、参照マイクのみの潜在空間での暗黙的なフィルターと合計の操作に変更します。これにより、問題の公式化をより適切に一致させることができます。エンドツーエンドの分離の目的。 
[ABSTRACT] filter-および--sum network（fasnet）は、アドホックおよび固定マイクアレイジオメトリで効果的であることが示されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Cascade RNN-Transducer: Syllable Based Streaming On-device Mandarin
  Speech Recognition with a Syllable-to-Character Converter -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.AS/paper_10.html">
      <font color="black">Cascade RNN-Transducer: Syllable Based Streaming On-device Mandarin
  Speech Recognition with a Syllable-to-Character Converter</font>
    </a>
  </h2>
  <font color="black">RNN-Tは、言語情報を強化するために予測ネットワークを採用していますが、トレーニングにはペアの音声テキストデータが必要であるため、言語モデリング機能は制限されています。したがって、リッチテキストリポジトリを使用して言語モデル機能を強化できます。いくつかの重要なトリックを導入すると、カスケードRNN-Tアプローチは、いくつかのマンダリンテストセットで文字ベースのRNN-Tを大幅に上回り、認識品質がはるかに高く、待ち時間も同様です。 
[概要]言語認識システムは、言語モデルの能力を強化するために使用できます。新しい言語モデルを開発するために簡単に使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Rethinking the Separation Layers in Speech Separation Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.AS/paper_11.html">
      <font color="black">Rethinking the Separation Layers in Speech Separation Networks</font>
    </a>
  </h2>
  <font color="black">分離モデルの大部分にはSIMOアーキテクチャのみが含まれていますが、拡張後のSISOモジュールと統合された特定の2段階分離システムが分離品質を向上させることも示されています。標準のSIMOのみの設計と比較して、同じモデルサイズの混合SIMO-SISO設計は、特に低オーバーラップ条件下での分離性能を向上させることができます。観察により、モデル設計パラダイムを再考し、分離がどのように実行されるかについて異なる見解を示すことができます。 
[ABSTRACT] simoモジュールは出力よりも多くの出力を生成し、sisoモジュールは出力の数を同じに保ちます。ほとんどのモデルは、パフォーマンスを犠牲にすることなく分離を実行できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Fine-grained Emotion Strength Transfer, Control and Prediction for
  Emotional Speech Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.AS/paper_12.html">
      <font color="black">Fine-grained Emotion Strength Transfer, Control and Prediction for
  Emotional Speech Synthesis</font>
    </a>
  </h2>
  <font color="black">感情のグローバルレンダリングとローカル記述子を使用すると、参照オーディオから感情記述子（転送用）を介して、または音素レベルの手動ラベル（制御用）から直接、きめ細かい感情表現を取得できます。従来の感情的な音声合成には、多くの場合手動が必要です。合成音声の感情表現を決定するためのラベルまたは参照オーディオ..このペーパーでは、シーケンス間ベースのきめ細かい感情音声合成の感情転送、制御、および予測を実行するための統合モデルを提案します。 
[要約]提案されたモデルは、テキストから音素レベルの感情表現を予測することもできます。これには、参照オーディオや手動ラベルは必要ありません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Controllable Emotion Transfer For End-to-End Speech Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.AS/paper_13.html">
      <font color="black">Controllable Emotion Transfer For End-to-End Speech Synthesis</font>
    </a>
  </h2>
  <font color="black">さらに、適切な参照を選択して目的の感情の強さを提供することは困難です。感情の埋め込みはメルの特徴マップとして表示できるため、合成音声の感情の強さは、感情の埋め込みの値を調整することで制御できます。スペクトル..最初に、2つの感情分類子（1つは参照エンコーダーの後、もう1つはデコーダー出力の後）をプラグインして、感情埋め込みと予測されるメルスペクトルの感情識別能力を強化します。 
[要約]合成音声は、感情の混乱では十分に正確で表現力がありません。感情カテゴリの混乱でもより正確です。提案された方法はタコトロンに基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Simplified Self-Attention for Transformer-based End-to-End Speech
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.AS/paper_14.html">
      <font color="black">Simplified Self-Attention for Transformer-based End-to-End Speech
  Recognition</font>
    </a>
  </h2>
  <font color="black">印象的な20％のパラメーター削減により、私たちのモデルは20,000時間の大規模タスクで認識パフォーマンスの損失を示しません。公共のAISHELL-1、内部1000時間でSSANベースおよび従来のSANベースの変圧器を評価します。結果は、提案されたSSANベースのトランスフォーマーモデルが、AISHELL-1タスクでモデルパラメーターの相対的な削減を20％以上、CERの相対的な削減を6.7％達成できることを示しています。 
[ABSTRACT]トランスフォーマーモデルは、aishellで実際に20％以上の削減を達成できます-1つのタスク</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-21">
        <br><font color="black">2020-05-21</font>
      </time>
    </span>
</section>
<!-- paper0: Learn2Sing: Target Speaker Singing Voice Synthesis by learning from a
  Singing Teacher -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.AS/paper_15.html">
      <font color="black">Learn2Sing: Target Speaker Singing Voice Synthesis by learning from a
  Singing Teacher</font>
    </a>
  </h2>
  <font color="black">私たちのアプローチでは、教師の歌唱コーパスと複数のターゲットスピーカーからのスピーチは、フレームレベルの自動回帰音響モデルでトレーニングされます。このモデルでは、歌と話し方が共通のスピーカー埋め込みとスタイルタグ埋め込みを共有します。一方、音楽関連がないためターゲットスピーカーの転写では、統一された入力表現を構築するための音響モデルの入力として、補助機能としてログスケール基本周波数（LF0）を使用します。特に、音響モデルではドメイン敵対トレーニング（DAT）を使用します。これは、歌と話しのデータの音響的特徴からスタイルを解きほぐすことによって、ターゲットスピーカーの歌唱パフォーマンスを向上させることを目的としています。 
[概要] learning2singは、歌の先生を使用して、歌声データなしでターゲットスピーカーの歌声を生成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: DARF: A data-reduced FADE version for simulations of speech recognition
  thresholds with real hearing aids -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.AS/paper_16.html">
      <font color="black">DARF: A data-reduced FADE version for simulations of speech recognition
  thresholds with real hearing aids</font>
    </a>
  </h2>
  <font color="black">8 dB）.. DARFは、（ドイツ語）マトリックスセンテンステストの約30分の記録および処理された信号で1つの音声認識しきい値（SRT）をシミュレートします。 。 
[ABSTRACT] darfは、補聴器の開発またはモデルベースのフィッティングだけでなく、改善にも使用できます。以前、聴覚弁別実験のフレームワークは、二乗平均平方根予測誤差が3db未満の補聴器アルゴリズムの利点を正確にシミュレートしました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-10">
        <br><font color="black">2020-07-10</font>
      </time>
    </span>
</section>
<!-- paper0: Universal Adversarial Audio Perturbations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.AS/paper_17.html">
      <font color="black">Universal Adversarial Audio Perturbations</font>
    </a>
  </h2>
  <font color="black">さまざまな1DCNNアーキテクチャへの攻撃に関する実験結果では、提案されたペナルティ方法を使用して、ターゲット攻撃と非ターゲット攻撃でそれぞれ85.0％と83.1％を超える攻撃成功率が示されています。これは、普遍的な敵対的摂動に対応します。ターゲットと非ターゲットの両方の攻撃シナリオで、オーディオ分類アーキテクチャのファミリをだますことができる普遍的な敵対的摂動の存在を示します。 
[ABSTRACT]ユニバーサル敵対的摂動はユニバーサル敵対的で見つかります。ただし、トレーニングサンプルの数が限られている場合は、攻撃がより成功します。ターゲットデータセットからサンプルが1つしかない場合は、この方法を使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-08">
        <br><font color="black">2019-08-08</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-band MelGAN: Faster Waveform Generation for High-Quality
  Text-to-Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.AS/paper_18.html">
      <font color="black">Multi-band MelGAN: Faster Waveform Generation for High-Quality
  Text-to-Speech</font>
    </a>
  </h2>
  <font color="black">提案されたマルチバンドMelGANは、波形生成とTTSでそれぞれ4.34と4.22の高いMOSを達成しました。まず、音声生成に有益であることが証明されているジェネレータの受容野を増やします。具体的には、以下の側面によるオリジナルのMelGAN。 
[概要]ジェネレータは、入力としてメルスペクトログラムを受け取り、サブバンド信号を生成します。サブバンド信号は、その後、ディスクリミネータ入力としてフルバンド信号に合計されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-11">
        <br><font color="black">2020-05-11</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Training for Multi-domain Speaker Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-18/eess.AS/paper_19.html">
      <font color="black">Adversarial Training for Multi-domain Speaker Recognition</font>
    </a>
  </h2>
  <font color="black">ただし、通常、トレーニングデータと評価データ自体の両方を複数のサブセットで構成できます。提案された方法を採用することにより、話者認識のためにマルチドメイン不変と話者識別の両方の音声表現を取得できます。それぞれのこれらの内部分散データセットは、異なるドメインと見なすこともできます。 
[要約]提案された方法は、不一致の問題を解決するのに効果的ですが、比較された教師なしドメイン適応方法よりも優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
