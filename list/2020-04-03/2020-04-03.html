<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-04-03の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: The RWTH ASR System for TED-LIUM Release 2: Improving Hybrid HMM with
  SpecAugment -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.SD/paper_0.html">
      The RWTH ASR System for TED-LIUM Release 2: Improving Hybrid HMM with
  SpecAugment
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの最高のシステムは、テストセットで5.6％のWERを達成します。これは、以前の最新技術よりも27％優れています。さまざまなマスキングの影響を調査することにより、モデルを増やすことなく、ハイブリッドHMMモデルでSpecAugmentからの改善を実現します。サイズとトレーニング時間.. TED-LIUMコーパスの2回目のリリースで、最先端のハイブリッドHMMベースのASRシステムを構築するための完全なトレーニングパイプラインを提示します。 
[ABSTRACT] specaugmentを使用したデータ拡張は、最高のsatモデルに加えてパフォーマンスを改善するために正常に適用されます。dataaugmentationは、データ拡張を使用した成功したデータ拡張に基づいています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: iMetricGAN: Intelligibility Enhancement for Speech-in-Noise using
  Generative Adversarial Network-based Metric Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.SD/paper_1.html">
      iMetricGAN: Intelligibility Enhancement for Speech-in-Noise using
  Generative Adversarial Network-based Metric Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、正式なリスニングテストにより、ノイズと残響の両方が存在する場合に明瞭度が大幅に向上することがわかります。実験結果は、提案されたiMetricGANが客観的測定、つまりビット単位の音声明瞭度（SIIB）に関して従来の最先端のアルゴリズムよりも優れていることを示しています。カフェテリアノイズ条件下での拡張短時間客観的了解度（ESTOI）。具体的には、iMetricGANアプローチを使用して、生成的敵対的ネットワーク（GAN）で音声了解度メトリックを最適化します。 
[要約]提案された了解度法は、音声の損失を補償するように設計されています。これは、修正の前後に音声信号の二乗平均平方根（rms）レベルと持続時間が維持されるためです
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Full-Sum Decoding for Hybrid HMM based Speech Recognition using LSTM
  Language Model -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.SD/paper_2.html">
      Full-Sum Decoding for Hybrid HMM based Speech Recognition using LSTM
  Language Model
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      提案されたフルサムデコードは、スイッチボードコーパスとLibrispeechコーパスの両方で評価されます。さらに、一般的なベイズ決定ルールの近似バリアントとしてのMAPとコンフュージョンネットワークの両方のデコードが評価されます。強力なベースラインに対する一貫した改善は、ほとんどすべてのケースで追加コストなしで達成されます。 。 
[ABSTRACT]研究は、再結合をデコードに適用してはならないことを示しています。フルビビビビデコードは、提案された検索フレームワークに基づいています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving auditory attention decoding performance of linear and
  non-linear methods using state-space model -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.SD/paper_3.html">
      Improving auditory attention decoding performance of linear and
  non-linear methods using state-space model
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      AADメソッドは、線形最小二乗コスト関数または非線形ニューラルネットワークに基づいて、EEG記録から有人スピーチエンベロープを再構築し、再構築されたエンベロープをスピーカーのスピーチエンベロープと直接比較して、ピアソン相関係数を使用して有人スピーカーを識別します。 。このホワイトペーパーでは、線形および非線形AAD法の復号化パフォーマンスを向上させるために、小さな相関ウィンドウで取得した相関係数を使用して状態空間モデルを調査します。これらの相関係数は変動が大きいため、信頼性の高い復号化大きな相関ウィンドウが使用されるため、処理の遅延が大きくなります。 
[要約]高度な先進技術の新しいシステムは、聴覚注意デコード（aad）メソッドを使用してターゲットスピーカーを識別できることを示しています。状態空間モデルは、デコードパフォーマンスを大幅に向上させます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Environment Sound Classification using Multiple Feature Channels and
  Attention based Deep Convolutional Neural Network -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.SD/paper_4.html">
      Environment Sound Classification using Multiple Feature Channels and
  Attention based Deep Convolutional Neural Network
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちのモデルは、3つのベンチマーク環境音分類データセットすべてで最先端のパフォーマンスを実現できます。同時に、チャンネルと空間の注意を一緒に実行する注意モジュールを使用します。このような複数の機能は、信号またはオーディオ処理にこれまで使用されたことはありません。 
[要旨]複数の機能チャネルには、メル周波数ケプストラム度（mfcc）、ガンマトーン周波数ケプストラムエントリ（gfcc）、定数q-変換（cqt）、クロマグラムが含まれます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-28">
        <br>2019-08-28
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multi-Modal Video Forensic Platform for Investigating Post-Terrorist
  Attack Scenarios -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.SD/paper_5.html">
      Multi-Modal Video Forensic Platform for Investigating Post-Terrorist
  Attack Scenarios
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      オーディオ類似性検索は、さまざまな視点から記録された類似のビデオシーケンスを識別するために利用されます。テロリスト攻撃の法医学的調査は、数千時間のビデオ映像を表示しなければならないことが多いため、捜査当局に大きな課題を提起します。ビジュアルオブジェクトの検出と追跡関連する概念に従ってコンテンツにインデックスを付けるために使用されます。 
[ABSTRACT]大規模なビデオ分析プラットフォーム（vawitnesses）は、法執行機関（lea）が容疑者を特定し、証拠を保護するのを支援します。ビデオ分析プラットフォームは、監視カメラからの情報と目撃者からのビデオアップロードを融合します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: NUBES: A Corpus of Negation and Uncertainty in Spanish Clinical Texts -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.CL/paper_0.html">
      NUBES: A Corpus of Negation and Uncertainty in Spanish Clinical Texts
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちが知る限り、NUBesはスペイン語での否定のための公的に利用可能な最大のコーパスであり、推測キュー、スコープ、およびイベントの注釈を組み込んだ最初のものです。さらに、我々は、深層学習アルゴリズムを使用して予備実験を行い、注釈付きデータセットを検証します。 
[要約]コーパスは進行中の研究の一部です。現在、匿名の健康記録からの29,682文で構成されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: How Furiously Can Colourless Green Ideas Sleep? Sentence Acceptability
  in Context -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.CL/paper_1.html">
      How Furiously Can Colourless Green Ideas Sleep? Sentence Acceptability
  in Context
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      2組の実験は、テキスト処理と談話の計算モデリングにおける文処理の認知的側面と中心的な問題への洞察を提供します。次に、単一性と双方向性の言語モデルを受け入れ可能性評価を予測する能力でテストします。最初に受け入れ性を比較します関連性のある文脈と無関係な文脈で孤立して判断された文の評価。 
[ABSTRACT]最高のモデルは、教師なしの受け入れ可能性予測のための最新の技術を実現しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Zero-Shot Cross-Lingual Transfer with Meta Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.CL/paper_2.html">
      Zero-Shot Cross-Lingual Transfer with Meta Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このホワイトペーパーでは、英語のトレーニングデータがあり、言語内データがほとんどまたはまったくない場合に、同時に複数の異なる言語でのトレーニングモデルの設定を検討します。標準の教師ありゼロショットクロスリンガルを使用して実験します。知識の戦略的共有が示されているように、最近、タスク間で何を共有するかを学ぶことは非常に重要なトピックとなっています。ダウンストリームタスクのパフォーマンスを向上させます。 
[ABSTRACT]ソース言語モデルのトレーニングに加えて、別のモデルは、どのトレーニングインスタンスが最も有益かを選択することを学習します。実験的な設定は、合計16言語でメタ学習の一貫した有効性を実証します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-05">
        <br>2020-03-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Analysing the Extent of Misinformation in Cancer Related Tweets -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.CL/paper_3.html">
      Analysing the Extent of Misinformation in Cancer Related Tweets
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      次に、誤った情報と真実に対応するテキストの言語変動の比較分析を行います。癌について具体的に話しているツイートに関するデータセットを収集して提示し、誤解を自動検出するための注意に基づくディープラーニングモデルを提案します。広がります。しかし、そのような主張の有効性を議論する適切な分析は行われていません。 
[ABSTRACT]これは、がんのさまざまな原因、治療法、予防方法についての認識を広めるのに役立ちます。また、ツイートは、そのようなプラットフォームで広まった誤報に取り組むことを目的としています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-30">
        <br>2020-03-30
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Learning to cooperate: Emergent communication in multi-agent navigation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.CL/paper_4.html">
      Learning to cooperate: Emergent communication in multi-agent navigation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      エージェントのポリシーを分析すると、緊急信号が状態空間を空間的にクラスター化し、「左」、「上」、「左上の部屋」などの特定の場所と空間方向を参照する信号があることがわかります。エージェントの母集団を使用して、出現プロトコルは基本的な構成構造を持ち、自然言語のコアプロパティを示すことを示しています。人工エージェントでの創発的コミュニケーションは、言語の進化を理解し、人間とのコミュニケーションを学ぶ人工システムを開発するために研究されています。 
[要約]特定のタスクを実行するエージェントが解釈可能な通信プロトコルを学習することを示します。これにより、エージェントはタスクに応答し、多くの場合、最適にタスクを解決できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving the Utility of Knowledge Graph Embeddings with Calibration -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.CL/paper_5.html">
      Improving the Utility of Knowledge Graph Embeddings with Calibration
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      また、評価タスクから2つのリソースを解放します。FB15Kベンチマークの強化バージョンとWikidataから抽出された新しいナレッジグラフデータセットです。このペーパーでは、ナレッジグラフのエンティティと関係を埋め込んで、見えないトリプルを予測するという目標に向けた機械学習モデルを扱います。ほとんどのナレッジグラフは本質的に不完全であるため、は重要なタスクです。クラウドソーシングの実験では、キャリブレーションされた信頼スコアにより、ナレッジグラフの埋め込みタスクの実行者とデータアノテーターにとって、ナレッジグラフの埋め込みがより有用になることを示します。 
[ABSTRACT]埋め込みモデルは、予測を受け入れるか信頼するかが明確でないため、現実世界の知識グラフの完了タスクで実用性が制限されています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Causal Inference of Script Knowledge -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.CL/paper_6.html">
      Causal Inference of Script Knowledge
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      一連のイベントが日常のシナリオを定義するのはいつですか。また、この知識をテキストからどのように誘導できるでしょうか。純粋に相関ベースのアプローチでは不十分であるという概念的および実用的な意味の両方から議論し、代わりに、介入によって正式に定義された、イベント間の因果関係に基づくスクリプト誘導へのアプローチを提案します。そのようなスクリプトを誘導する以前の研究は何らかの形で、コーパス内のイベントのインスタンス間の相関の測定。 
[ABSTRACT]そのようなスクリプトを誘導する前の作業は、コーパス内のイベントのインスタンス間の相関の測定に依存してきました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Revisiting the linearity in cross-lingual embedding mappings: from a
  perspective of word analogies -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.CL/paper_7.html">
      Revisiting the linearity in cross-lingual embedding mappings: from a
  perspective of word analogies
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ほとんどのクロスリンガル埋め込みマッピングアルゴリズムは、最適化された変換関数が線形であることを前提としています。アナロジー完了ベンチマークとBLIタスクに基づく実験結果は、マッピングがアナロジー情報をキャプチャして線形であるかどうかの強い相関関係を示しています。この主張。 
[要約]場合によっては、線形性の学習が機能せず、一般的に使用される仮定が失敗する可能性があることを示唆している
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Fake News Detection by means of Uncertainty Weighted Causal Graphs -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.CL/paper_8.html">
      Fake News Detection by means of Uncertainty Weighted Causal Graphs
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      したがって、情報を偽物として検出および分類し、情報源を信頼に値するかどうかに分類できるシステムを設計することが望ましい。社会は、ソーシャルネットワークなどの新しい情報チャネルが人々に必ずしも信頼できるとは限らないニュースを共有します。時々、これらの情報源は意図的に疑わしい目的で偽のニュースを生成し、その情報の消費者は情報が正確であると考えて他のユーザーと共有します。 
[要約]情報を偽物として検出および分類できるシステムを設計することが望ましい。このシステムは、重み付き因果関係グラフに基づく分類子を使用して取得できます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-04">
        <br>2020-02-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Mapping Languages and Demographics with Georeferenced Corpora -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.CL/paper_9.html">
      Mapping Languages and Demographics with Georeferenced Corpora
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このペーパーでは、2つのデータセットが非常に異なる母集団を表し、r = 0.60（ソーシャルメディア）とr = 0.49（ウェブでクロールされた）の値を持つ実際の母集団と相関していることがわかりました。さらに、Twitterデータは、各国で使用されている言語..このペーパーでは、Webでクロールされたソースとソーシャルメディアのソースの両方から取得された大規模な地理参照コーパスを、グラウンドトゥルースの人口と言語調査のデータセットに対して評価します。 
[ABSTRACT]目標は、人口統計に最も適したソーシャルデータセットを決定することです。データセットは、データデータデータセット、紙のデータ、その他のデータセットに基づいています。twitterデータは、各国で使用されている言語を予測しています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal
  Transformers -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.CL/paper_10.html">
      Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal
  Transformers
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ピクセルおよびテキストレベルでセマンティック接続を調整する当社のPixel-BERTは、視覚および言語タスクのタスク固有の視覚的表現の制限を解決します。 Visual GenomeデータセットとMS-COCOデータセットからの画像と文のペアを含むモデルを終了します。領域ベースの画像機能を使用する代わりに、画像と文のペアから直接、画像ピクセルと言語セマンティクス間のより正確で完全な接続を構築することを目指しています。最新のビジョンと言語のタスク。 
[要旨]最新のビジョンと言語のタスクとして、領域ベースの画像機能を使用する代わりに、画像と文のペアから直接、画像のピクセルと言語の意味の間のより詳細な接続を構築することを目指しています。また、境界ボックスの注釈のコストを軽減し、視覚課題の意味ラベルと言語の意味の不均衡を克服する
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Mapping Languages: The Corpus of Global Language Use -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.CL/paper_11.html">
      Mapping Languages: The Corpus of Global Language Use
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      言語マッピングに重点を置いたこのペーパーでは、このデジタル言語データが実際の母集団をどの程度適切に表しているかを分析します。これは、（i）コーパスを人口統計のグラウンドトゥルースデータと体系的に比較し、（ii）コーパスを別のTwitterベースのデータセットで三角測量します。合計で、コーパスには148の言語（各言語から100万語を超える）と158か国（これも各国から100万語を超える）を表す4230億語が含まれており、すべてCommon Crawl Webデータから抽出されています。代替の既製のモデルよりもサンプルサイズが小さく、より多くのローカル言語をサポートする言語識別モデル。 
[要約]コーパスには、148の言語と158の国を表す4230億語が含まれています。データは、さまざまなタイプの言語のデータからのデータに基づいています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Give your Text Representation Models some Love: the Case for Basque -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.CL/paper_12.html">
      Give your Text Representation Models some Love: the Case for Basque
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このホワイトペーパーでは、より大きなバスクコーパスでトレーニングされた多数の単一言語モデル（FastText単語埋め込み、FLAIRおよびBERT言語モデル）が、トピック分類、感情分類、PoSタグ付け、 NER ..せいぜい、これらの言語のモデルは多言語バージョンに含まれており、各言語は残りの言語と部分文字列とパラメーターの割り当てを共有しています。バスク。 
[ABSTRACT]単言語の事前トレーニング済み言語が常に利用できるとは限りません。これらには、英語以外の言語の単言語の事前計画バージョンが含まれます。これは、バスク語などの小さな言語に特に当てはまります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-31">
        <br>2020-03-31
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Audio Summarization with Audio Features and Probability Distribution
  Divergence -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.CL/paper_13.html">
      Audio Summarization with Audio Features and Probability Distribution
  Divergence
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      マルチメディアソースの自動要約は、関連情報を維持しながらソースを要約することにより、個人の理解を促進する重要なタスクです。これは、セグメントの長さ、位置、および有益性の値を考慮に入れます。各セグメントの有益性は、メル周波数ケプストラム係数とそれに対応するジェンセンシャノン発散スコアから発行されたオーディオ機能のセット。 
[要約]このホワイトペーパーでは、音声の特徴と分布の相違の確率に基づく音声の要約に焦点を当てます。この方法では、セグメントの長さ、位置、情報量の値を考慮します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-20">
        <br>2020-01-20
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Integrating Crowdsourcing and Active Learning for Classification of
  Work-Life Events from Tweets -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.CL/paper_14.html">
      Integrating Crowdsourcing and Active Learning for Classification of
  Work-Life Events from Tweets
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      個々のツイートから失業イベントを特定するケーススタディを通じてその効果を実証しました。手動による注釈の負担を軽減し、その信頼性を維持するために、アクティブな学習戦略と組み合わせたクラウドソーシングパイプラインを考案しました。ソーシャルメディア、特にTwitter、予測分析による研究にますます使用されています。 
[ABSTRACT]手動注釈は、最も多くのリソースと時間を要するプロセスです。複数の専門家評価者がすべての項目について合意に達する必要があります。個々のツイートから失業イベントを保証するケーススタディを通じて、その効果を実証しました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-26">
        <br>2020-03-26
      </time>
    </span>
  </h3>
</article>
<!-- paper0: MCEN: Bridging Cross-Modal Gap between Cooking Recipes and Dish Images
  with Latent Variable Model -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/cs.CL/paper_15.html">
      MCEN: Bridging Cross-Modal Gap between Cooking Recipes and Dish Images
  with Latent Variable Model
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      モダリティ間の潜在的な配置をキャプチャするには、確率的潜在変数を組み込んで、テキストと視覚的機能間の相互作用を明示的に利用します。広範な実験結果は、提案されたMCENがベンチマークRecipe1Mデータセットの既存のすべてのアプローチよりも優れており、必要な計算コストが少ないことを明確に示しています。 
[ABSTRACT]モダリティ-モダリティを学習する一貫性のある埋め込みネットワーク（mcen）-画像とテキストを同じ埋め込みスペースに投影することで壊れやすい表現を提示します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: The RWTH ASR System for TED-LIUM Release 2: Improving Hybrid HMM with
  SpecAugment -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/eess.AS/paper_0.html">
      The RWTH ASR System for TED-LIUM Release 2: Improving Hybrid HMM with
  SpecAugment
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さまざまなマスキングの効果を調査することにより、モデルサイズとトレーニング時間を増やすことなく、ハイブリッドHMMモデルのSpecAugmentからの改善を実現します。その後のsMBRトレーニングを適用して、最終的な音響モデルを微調整し、LSTMとトランスフォーマー言語モデルの両方をトレーニングします。私たちの最高のシステムは、テストセットで5.6％のWERを達成します。これは、以前の最新技術よりも27％優れています。 
[ABSTRACT] specaugmentを使用したデータ拡張は、最高のsatモデルに加えてパフォーマンスを改善するために正常に適用されます。dataaugmentationは、データ拡張を使用した成功したデータ拡張に基づいています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: iMetricGAN: Intelligibility Enhancement for Speech-in-Noise using
  Generative Adversarial Network-based Metric Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/eess.AS/paper_1.html">
      iMetricGAN: Intelligibility Enhancement for Speech-in-Noise using
  Generative Adversarial Network-based Metric Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、正式なリスニングテストにより、ノイズと残響の両方が存在する場合に明瞭度が大幅に向上することがわかります。実験結果は、提案されたiMetricGANが客観的測定、つまりビット単位の音声明瞭度（SIIB）に関して従来の最先端のアルゴリズムよりも優れていることを示しています。カフェテリアノイズ条件下での拡張短時間客観的了解度（ESTOI）。具体的には、iMetricGANアプローチを使用して、生成的敵対的ネットワーク（GAN）で音声了解度メトリックを最適化します。 
[要約]提案された了解度法は、音声の損失を補償するように設計されています。これは、修正の前後に音声信号の二乗平均平方根（rms）レベルと持続時間が維持されるためです
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Full-Sum Decoding for Hybrid HMM based Speech Recognition using LSTM
  Language Model -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/eess.AS/paper_2.html">
      Full-Sum Decoding for Hybrid HMM based Speech Recognition using LSTM
  Language Model
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これは、デコードでビタビ近似ではなく、HMM状態シーケンス全体の総和を再検討する動機になります。さらに、一般的なベイズ決定ルールの近似バリアントとしてのMAPとコンフュージョンネットワークの両方のデコードが評価されます。提案されたフルサムデコードは両方で評価されますスイッチボードとLibrispeechコーパス。 
[ABSTRACT]研究は、再結合をデコードに適用してはならないことを示しています。フルビビビビデコードは、提案された検索フレームワークに基づいています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving auditory attention decoding performance of linear and
  non-linear methods using state-space model -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/eess.AS/paper_3.html">
      Improving auditory attention decoding performance of linear and
  non-linear methods using state-space model
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このペーパーでは、線形および非線形AADメソッドのデコードパフォーマンスを改善するために、小さな相関ウィンドウで取得した相関係数を使用して状態空間モデルを調査します。線形最小二乗コスト関数または非線形ニューラルネットワーク。次に、再構成されたエンベロープをスピーカーの音声エンベロープと直接比較して、ピアソン相関係数を使用して、出席したスピーカーを識別します。これらの相関係数は変動が大きいため、信頼性の高いデコード大きな相関ウィンドウが使用されるため、処理の遅延が大きくなります。 
[要約]高度な先進技術の新しいシステムは、聴覚注意デコード（aad）メソッドを使用してターゲットスピーカーを識別できることを示しています。状態空間モデルは、デコードパフォーマンスを大幅に向上させます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Environment Sound Classification using Multiple Feature Channels and
  Attention based Deep Convolutional Neural Network -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/eess.AS/paper_4.html">
      Environment Sound Classification using Multiple Feature Channels and
  Attention based Deep Convolutional Neural Network
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このような複数の機能は、これまで信号またはオーディオ処理に使用されたことはありません。同時に、チャネルと空間の注意を一緒に実行する注意モジュールを使用します。ESC-10およびESC-50データセットの場合、提案されたモデルによって達成される精度は人間を超えています精度はそれぞれ95.7％と81.3％です。 
[要旨]複数の機能チャネルには、メル周波数ケプストラム度（mfcc）、ガンマトーン周波数ケプストラムエントリ（gfcc）、定数q-変換（cqt）、クロマグラムが含まれます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-28">
        <br>2019-08-28
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multi-Modal Video Forensic Platform for Investigating Post-Terrorist
  Attack Scenarios -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/eess.AS/paper_5.html">
      Multi-Modal Video Forensic Platform for Investigating Post-Terrorist
  Attack Scenarios
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      オーディオ類似検索は、異なる視点から記録された類似のビデオシーケンスを識別するために利用されます。視覚オブジェクトの検出と追跡は、関連する概念に従ってコンテンツにインデックスを付けるために使用されます。ビデオは、音響コンテンツと視覚コンテンツに従って分析されます。 
[ABSTRACT]大規模なビデオ分析プラットフォーム（vawitnesses）は、法執行機関（lea）が容疑者を特定し、証拠を保護するのを支援します。ビデオ分析プラットフォームは、監視カメラからの情報と目撃者からのビデオアップロードを融合します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-02">
        <br>2020-04-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Learning to Have an Ear for Face Super-Resolution -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-04-03/eess.AS/paper_6.html">
      Learning to Have an Ear for Face Super-Resolution
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      オーディオは性別、年齢、アイデンティティなどの属性の回復を支援し、高解像度画像再構成プロセスの正確さを向上させることができることを実験的に示します。聴覚と視覚のモダリティを組み合わせるために、最初に方法を提案します単一のオーディオトラックから、次に単一の低解像度画像から顔の潜在的な表現を構築します。オーディオと低解像度画像の両方を使用して、極端な顔の超解像（16倍の増加）を実行する新しい方法を提案します。入力サイズ）。 
[ABSTRACT]入力画像の解像度が非常に低い場合、情報の損失は非常に悲惨なので、元のアイデンティティの重要な詳細が失われています。さらに、オーディオは、性別、年齢、性別などの人間の属性を回復するのに役立ちます身元
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-27">
        <br>2019-09-27
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="">
      
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本日更新された論文はありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="">
        <br>
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
