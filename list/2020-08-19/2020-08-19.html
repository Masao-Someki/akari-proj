<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-08-19の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Lite Audio-Visual Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.SD/paper_0.html">
      <font color="black">Lite Audio-Visual Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">さらに、実験結果は、視覚データ圧縮のための2つの手法の有効性を確認します。この研究では、これらの問題に対処するLite AVSE（LAVSE）システムを提案します。実験結果は、提案されたLAVSEシステムが特に類似した数のモデルパラメータを持つオーディオのみのSEシステムよりも優れたパフォーマンス。 
[要約]システムには2つの視覚的データ圧縮技術が含まれています。結果は2つの技術の有効性を確認します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-24">
        <br><font color="black">2020-05-24</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Attack and Defense Strategies for Deep Speaker Recognition
  Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.SD/paper_1.html">
      <font color="black">Adversarial Attack and Defense Strategies for Deep Speaker Recognition
  Systems</font>
    </a>
  </h2>
  <font color="black">敵対的攻撃は最近復活したドメインであり、ディープニューラルネットワークベースの分類子を破るのに効果的であることが示されています。このホワイトペーパーで紹介するベースラインは、話者認識システムの敵対的堅牢性をさらに研究することに関心を持つ研究コミュニティに役立つ可能性のあるベースラインを提供します。実験により、話者認識システムは敵対的攻撃に対して脆弱であり、最も強い攻撃は94％から0％までのシステム。 
[ABSTRACT]敵対的攻撃は、最近復活したドメインであり、ディープニューラルネットワークの破壊に効果的であることが示されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Separating Varying Numbers of Sources with Auxiliary Autoencoding Loss -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.SD/paper_2.html">
      <font color="black">Separating Varying Numbers of Sources with Auxiliary Autoencoding Loss</font>
    </a>
  </h2>
  <font color="black">最近の多くの音源分離システムは、一定数の音源を混合物から分離するように設計されています。このホワイトペーパーでは、2つの問題を軽減するために、簡単なトレーニング方法である補助自動エンコード順列不変トレーニング（A2PIT）を提案します。A2PITは固定数の出力を使用し、補助自動エンコード損失を使用して無効な出力を強制的に入力混合のコピーにし、推論フェーズ中に完全に監視されていない方法で無効な出力を検出します。 
[ABSTRACT]多くの自動エンコーディングの個別のシステムは、2つの問題を軽減するように設計されています。多くの自動エンコーディングは、システムの必要性を軽減するのに役立ちます。テストを使用して、さまざまな数のスピーカーで分離パフォーマンスを改善できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-27">
        <br><font color="black">2020-03-27</font>
      </time>
    </span>
</section>
<!-- paper0: A Real-time Robot-based Auxiliary System for Risk Evaluation of COVID-19
  Infection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.SD/paper_3.html">
      <font color="black">A Real-time Robot-based Auxiliary System for Risk Evaluation of COVID-19
  Infection</font>
    </a>
  </h2>
  <font color="black">モデルの構造は、リアルタイムアプリケーションに実装するために簡潔に維持されています。このシステムは、ビジネスルールエンジン内でさらに活用できるため、リアルタイムの監視および支援アプリケーションの基盤として機能します。 COVID-19感染では、提案されたシステムの咳検出と分類のためのエンドツーエンドの方法を提案します。 
[ABSTRACT]プロジェクトは、リアルタイム音声認識、温度測定、キーワード検出、咳検出などの機能を組み合わせています。人間のロボットからの実際の会話データに基づいており、音声信号を処理して咳を検出し、検出された場合は分類します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: PopMAG: Pop Music Accompaniment Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.SD/paper_4.html">
      <font color="black">PopMAG: Pop Music Accompaniment Generation</font>
    </a>
  </h2>
  <font color="black">この課題に対処するために、2つの新しい手法をさらに紹介します。1）MuMIDIシーケンスの長さを短くできる複数のステップではなく、1つのステップで音符の複数の音符属性（ピッチ、持続時間、速度など）をモデル化します。ハーモニーを改善するために、このペーパーでは、単一のシーケンスで同時マルチトラック生成を可能にし、異なるトラックからのノートの依存性を明示的にモデル化する新しいMUltiトラックMIDI表現（MuMIDI）を提案します。残念ながら、それはシーケンスの長さを拡大し、長期的な音楽モデリングの新たな挑戦をもたらします。 
[ABSTRACT]ピッチの真実に加えて、music.popmagの長期的な依存関係をキャプチャするためのメモリとして、余分なロングコンテキストを導入し、複数のデータセット（lmd、freemidi、cpmd、チャイニーズポップソングのプライベートデータセット）の調和を勝ち取ります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: SpEx+: A Complete Time Domain Speaker Extraction Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.SD/paper_5.html">
      <font color="black">SpEx+: A Complete Time Domain Speaker Extraction Network</font>
    </a>
  </h2>
  <font color="black">具体的には、2つの同一の音声エンコーダネットワークの重みを結合します。1つはエンコーダ-抽出器-デコーダパイプライン用で、もう1つはスピーカーエンコーダの一部として結合します。スピーカー抽出は、ターゲットが指定されたマルチトーカー環境からターゲット音声信号を抽出することを目的としています。スピーカーの参照音声..このような不一致は、システムのパフォーマンスに悪影響を及ぼします。 
[ABSTRACT]周波数領域アプローチでの位相推定を回避する時間またはソリューションspexを最近提案しました。時間領域の分析ウィンドウのサイズと周波数ゾーンのスピーカーのサイズも異なります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-10">
        <br><font color="black">2020-05-10</font>
      </time>
    </span>
</section>
<!-- paper0: DCCRN: Deep Complex Convolution Recurrent Network for Phase-Aware Speech
  Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.SD/paper_6.html">
      <font color="black">DCCRN: Deep Complex Convolution Recurrent Network for Phase-Aware Speech
  Enhancement</font>
    </a>
  </h2>
  <font color="black">最近のいくつかの研究では、複素数値スペクトログラムをトレーニングターゲットとして使用していますが、実数値ネットワークでトレーニングを行って、マグニチュードとフェーズコンポーネント、または実部と虚部をそれぞれ予測しています。特に、畳み込みリカレントネットワーク（CRN）は、畳み込みエンコーダーデコーダー（CED）構造と長い短期記憶（LSTM）は、複雑なターゲットに役立つことが証明されています。複雑なターゲットをより効果的にトレーニングするために、このホワイトペーパーでは、複雑なDeep Complex Convolution Recurrent Network（DCCRN）と呼ばれる重要な演算。CNN構造とRNN構造の両方が複雑な値の演算を処理できます。 
[要旨]たたみ込み実ネットワークは、たたみ込みニューラルネットワーク（cnn）を使用します。これらのたたみ込み実項には、たたみ込み部分と長い短期記憶（lstm）が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-01">
        <br><font color="black">2020-08-01</font>
      </time>
    </span>
</section>
<!-- paper0: WG-WaveNet: Real-Time High-Fidelity Speech Synthesis without GPU -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.SD/paper_7.html">
      <font color="black">WG-WaveNet: Real-Time High-Fidelity Speech Synthesis without GPU</font>
    </a>
  </h2>
  <font color="black">実験により、生成されたオーディオの品質は他の方法と同等であることも示されています。PyTorchの実装は、8 GB未満のGPUメモリを使用してトレーニングでき、NVIDIA 1080Ti GPUで960 kHzを超えるレートでオーディオサンプルを生成します。さらに、CPUで合成した場合でも、提案手法はリアルタイムの1.2倍の速さで44.1 kHzの音声波形を生成できることを示しています。 
[要約]提案されたモデルは、コンパクトなフローベースのモデルとポストフィルターに基づいています。他の波形生成モデルよりも必要なリソースが少なくなります。提案された方法は、44。1センチメートルの音声波形1. 2倍の速度で生成できます。リアルタイム</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-15">
        <br><font color="black">2020-05-15</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Fully automated deep learning based segmentation of normal, infarcted
  and edema regions from multiple cardiac MRI sequences -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.IV/paper_0.html">
      <font color="black">Fully automated deep learning based segmentation of normal, infarcted
  and edema regions from multiple cardiac MRI sequences</font>
    </a>
  </h2>
  <font color="black">ネットワークへの入力は、3つのCMRシーケンス、つまり後期ガドリニウム増強（LGE）、T2、および平衡定常状態自由歳差運動（bSSFP）で構成されます。提案されたアプローチは、MICCAI 2020が主催するMyoPSチャレンジによって提供されるデータと組み合わせて利用しました。 STACOM ..提案されたアプローチは、20のケースを含むテストセットでチャレンジオーガナイザーによって評価され、LV MSの場合は46.8 \％$、LV ME + MSの場合は55.7 \％$の平均サイコロスコアを達成します。深い畳み込みニューラルネットワーク.Miccai 2020がホストするmyopsチャレンジのデータを使用します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Contact Area Detector using Cross View Projection Consistency for
  COVID-19 Projects -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.IV/paper_1.html">
      <font color="black">Contact Area Detector using Cross View Projection Consistency for
  COVID-19 Projects</font>
    </a>
  </h2>
  <font color="black">具体的には、オブジェクトと静的サーフェス間の接触は、2つの異なる視点からオブジェクトを静的サーフェスに投影し、それらの2D交差を分析することで識別できることを示しています。この問題に対する別のアプローチは、人が触れたかどうかを識別することです定義されたオブジェクト..この作業では、この問題の解決策が簡単にできることを示します。 
[ABSTRACT]人がビジュアルデータ、画像、またはビデオを使用してオブジェクトに触れたかどうかを判断することは難しい問題です。これは、投影エラーのためにアプリケーションの正確さの要件を満たしません。この戦略では、大量のトレーニングデータが必要になりますスケールと視点の変化を一般化する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Disentangled Expression Representations from Facial Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.IV/paper_2.html">
      <font color="black">Learning Disentangled Expression Representations from Facial Images</font>
    </a>
  </h2>
  <font color="black">この論文では、敵対損失の定式化を使用して、顔画像のもつれのない表現を学習します。使用されたモデルは、シングルタスクデータセットの学習を容易にし、60.53の精度で最先端の式認識を改善します。追加のデータを使用せずにAffectNetdatasetで％。顔の画像は、特に制約のない野生のシナリオでは、多くの異なる変動要因の影響を受けます。 
[ABSTRACT]使用されたモデルは、単一タスクのデータセットの学習に役立ちます。これにより、表現認識の最先端が60の精度で向上します。追加データを使用せずに、afectnetdatasetで53％</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-16">
        <br><font color="black">2020-08-16</font>
      </time>
    </span>
</section>
<!-- paper0: STEm-Seg: Spatio-temporal Embeddings for Instance Segmentation in Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.IV/paper_3.html">
      <font color="black">STEm-Seg: Spatio-temporal Embeddings for Instance Segmentation in Videos</font>
    </a>
  </h2>
  <font color="black">特に、ビデオクリップを単一の3D時空間ボリュームとしてモデル化し、単一のステージで空間と時間にわたってインスタンスをセグメント化および追跡する新しいアプローチを提案します。ネットワークは、時空間埋め込みを学習するようにエンドツーエンドでトレーニングされています。これらの埋め込みをクラスタ化するために必要なパラメータだけでなく、推論を簡素化します。私たちの方法は、複数のデータセットとタスクにわたって最先端の結果を達成します。 
[ABSTRACT]複数のネットワーク-個々のフレームでオブジェクトを検出するために使用されます。これらは、時間の経過とともにこれらの検出に関連付けられます。ネットワークはトレーニング済みです-埋め込みのメモリを強化します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-18">
        <br><font color="black">2020-03-18</font>
      </time>
    </span>
</section>
<!-- paper0: UDC 2020 Challenge on Image Restoration of Under-Display Camera: Methods
  and Results -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.IV/paper_4.html">
      <font color="black">UDC 2020 Challenge on Image Restoration of Under-Display Camera: Methods
  and Results</font>
    </a>
  </h2>
  <font color="black">論文の結果は、アンダーディスプレイカメラレストレーションの最先端の復元性能です。チャレンジトラックは、4k透明OLED（T-OLED）と電話用ペンティルOLED（P- OLED）..約150のチームがチャレンジを登録したほか、8と9のチームが各トラックのテスト段階で結果を提出しました。 
[要約]課題は、新しく収集されたアンダーディスプレイカメラのデータベースに基づいています。8と9つのチームがテスト段階で結果を提出しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Domain Generalizer: A Few-shot Meta Learning Framework for Domain
  Generalization in Medical Imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.IV/paper_5.html">
      <font color="black">Domain Generalizer: A Few-shot Meta Learning Framework for Domain
  Generalization in Medical Imaging</font>
    </a>
  </h2>
  <font color="black">次に、少数ショット学習を採用します。この作業では、モデルにとらわれないメタ学習フレームワークに基づくドメイン一般化手法を生物医学的イメージングに適合させます。この手法は、基盤となるモデルアーキテクチャに依存しないため、あらゆるイメージングタスクに使用できます。 
[ABSTRACT]ターゲットドメインとソースドメインのデータに違いがあると、一般化が妨げられる可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Parallel MRI Reconstruction Network Without Coil Sensitivities -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.IV/paper_6.html">
      <font color="black">Deep Parallel MRI Reconstruction Network Without Coil Sensitivities</font>
    </a>
  </h2>
  <font color="black">既存のほとんどの深い画像再構成ネットワークとは異なり、私たちのネットワークは、正確に推定するのが難しい感度マップの知識を必要とせず、実際のpMRIアプリケーションにおける画像再構成の主要なボトルネックとなっています。実験結果は有望であることを示しています。さまざまなpMRIイメージングデータセットに対する本手法のパフォーマンス。データから学習した正則化関数を使用した並列MRI（pMRI）での高速画像再構成のための堅牢な近位勾配スキームをマッピングすることにより、新しいディープニューラルネットワークアーキテクチャを提案します。 
[要約]提案されたネットワークは、不完全なpmriデータからの画像を単一の画像に適応的に結合することを学習します。ネットワークは、ネットワークのネットワークの画像のスローダウンに応答することを学習します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-04">
        <br><font color="black">2020-08-04</font>
      </time>
    </span>
</section>
<!-- paper0: Grading Loss: A Fracture Grade-based Metric Loss for Vertebral Fracture
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.IV/paper_7.html">
      <font color="black">Grading Loss: A Fracture Grade-based Metric Loss for Vertebral Fracture
  Detection</font>
    </a>
  </h2>
  <font color="black">最先端のメトリック損失に基づいて、Genantの骨折グレーディングスキーマを尊重する表現を学習するための新規のGrading Lossを提示します。公開されている脊椎データセットで、提案された損失関数は、81.5％の骨折検出F1スコア、10単純な分類ベースラインを超える％の増加。これに対処するために、骨折検出に効率的な潜在表現の学習を目的とした、自動化された椎骨骨折検出のための表現学習にヒントを得たアプローチを提案します。 
[要約]提案された方法は、脊椎骨折検出システムを開発するために使用できます。モデルは、81.5％の骨折検出で診断できます。これは、単純な分類ベースラインよりも10％増加しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: An Unsupervised Deep Learning Method for Multi-coil Cine MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.IV/paper_8.html">
      <font color="black">An Unsupervised Deep Learning Method for Multi-coil Cine MRI</font>
    </a>
  </h2>
  <font color="black">次に、これらの完全にエンコードされたデータを使用して、各コイルの画像を個別に再構成するための並列ネットワークをトレーニングできます。この論文では、時間インターリーブサンプリング戦略によるマルチコイルシネMRIの教師なし深層学習法を提案します。時間インターリーブ収集スキームを利用して、隣接する時間フレームのk空間データを直接マージすることにより、完全にエンコードされた参照データのセットを構築します。 
[要約]ディープラーニング手法は、反復的な手法と比較して、再構築に長い時間を必要とせずに、再構築の質を向上させることができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-20">
        <br><font color="black">2019-12-20</font>
      </time>
    </span>
</section>
<!-- paper0: Automated Detection of Congenital Heart Disease in Fetal Ultrasound
  Screening -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.IV/paper_9.html">
      <font color="black">Automated Detection of Congenital Heart Disease in Fetal Ultrasound
  Screening</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、胎児の超音波検査における先天性心疾患（CHD）の検出を支援するディープラーニング手法の可能性について説明します。トレーニングと推論の両方で、関連する心臓構造に向かって機能にバイアスをかける補助ビュー分類タスクを利用します。このバイアスは、健常クラスとCHDクラスのF1スコアをそれぞれ0.72と0.77から0.87と0.85に改善するのに役立ちます。 
[ABSTRACT]自動化されたデータのキュレーションと分類のためのパイプラインを提案します。autopspsingとデータのキュレーションが含まれます。このバイアスは、f1の改善に役立ちます-健全とchdの0. 72と0. 77から0. 87と0. 85のスコアクラス</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-16">
        <br><font color="black">2020-08-16</font>
      </time>
    </span>
</section>
<!-- paper0: Elimination of Central Artefacts of L-SPECT with Modular Partial Ring
  Detectors by Shifting Center of Scanning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.IV/paper_10.html">
      <font color="black">Elimination of Central Artefacts of L-SPECT with Modular Partial Ring
  Detectors by Shifting Center of Scanning</font>
    </a>
  </h2>
  <font color="black">さらに、視覚的な比較も検討されます。ライトフィールド単一光子放出計算機トモグラフィー（L-SPECT）システムは、プレノプティックイメージングのアイデアを適用することにより、従来のSPECTのいくつかの欠点を克服するために開発されました。 MPRD L-SPECTは、改良されたFWHMで中央のアーチファクトの問題を克服します。 
[ABSTRACT] l-スペクトラムシステムの視野は、小さな検出器モジュールをタイリングすることにより、検出器ヘッドをリング型に再形成することによって強化されます。システムは、スペクトラムシステムと比較して、情報損失とスキャン時間を削減するという点でパフォーマンスが向上従来のコリメータがあります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: RevPHiSeg: A Memory-Efficient Neural Network for Uncertainty
  Quantification in Medical Image Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.IV/paper_11.html">
      <font color="black">RevPHiSeg: A Memory-Efficient Neural Network for Uncertainty
  Quantification in Medical Image Segmentation</font>
    </a>
  </h2>
  <font color="black">リバーシブルアーキテクチャは、各層のアクティベーションを保存する代わりに、バックプロパゲーション中に後続のレイヤーの出力からのアクティベーションを正確に計算することでメモリ節約を実現します。画像セグメンテーション..リバーシブルアーキテクチャのRevPHiSegを使用すると、ニューラルネットワークをトレーニングして、メモリが限られているGPUのセグメンテーションの不確実性を定量化し、より大きなFOVを処理できます。 
[ABSTRACT]不確実性の定量化方法の使用は、小さな問題にうまく適用されています。主な構造は、メモリを構築するためのリバーシブルです。リバーシブルブロックは、phisegと呼ばれる最近提案されたアーキテクチャに組み込まれています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-16">
        <br><font color="black">2020-08-16</font>
      </time>
    </span>
</section>
<!-- paper0: MaskedFace-Net -- A Dataset of Correctly/Incorrectly Masked Face Images
  in the Context of COVID-19 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.IV/paper_12.html">
      <font color="black">MaskedFace-Net -- A Dataset of Correctly/Incorrectly Masked Face Images
  in the Context of COVID-19</font>
    </a>
  </h2>
  <font color="black">これらの理由により、いくつかのマスク装着キャンペーンは、この問題と優れた実践について人々を敏感にすることを目的としています。マスクされた顔画像（137,016画像）のデータセットは、https：//github.com/cabani/MaskedFace-Net。で入手できます。多くの人々は、個人（例：子供、老人）の悪い習慣、悪い行動、または脆弱性のために正しくマスクを着用していません。 
[ABSTRACT]これらの作品は、マスクされた顔のデータセットに基づいています。しかし、多くの人々は、悪い習慣、悪い行動、または個人の脆弱性のため、正しくマスクを着用していません。これらには、正しくマスクされた顔のデータセット（cmfd）、誤ってマスクされた顔のデータセットが含まれます（imfd）およびグローバルなマスクされた顔検出のためのそれらの組み合わせ（maskedface-net）</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Self-supervised Denoising via Diffeomorphic Template Estimation:
  Application to Optical Coherence Tomography -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.IV/paper_13.html">
      <font color="black">Self-supervised Denoising via Diffeomorphic Template Estimation:
  Application to Optical Coherence Tomography</font>
    </a>
  </h2>
  <font color="black">現在のOCTデノイザーは、ノイズ分布の仮定を活用するか、繰り返し取得の平均化によって深い監視デノイザーをトレーニングするためのターゲットを生成します。自己監視メソッドの明確な利点にもかかわらず、OCTが同じ連続スキャン間でも強力な構造変形を示すため、それらの使用は除外されます。不随意な眼球運動に起因する主題。この論文では、それらのノイズ実現を経験的に登録せずに、動き変形反復取得のための自己監視ノイズ除去の使用を可能にする合同微分同相テンプレート推定およびノイズ除去フレームワークを提案します。 
[要約] di di dispell画像はノイズによって強く破壊され、その解釈が制限されます。最近の自己管理型の進歩により、グラウンドトゥルースとしてクリーンなターゲットなしで繰り返し取得のみを使用して、深いノイズ除去ネットワークのトレーニングが可能になります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: A Two-step-training Deep Learning Framework for Real-time Computational
  Imaging without Physics Priors -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.IV/paper_14.html">
      <font color="black">A Two-step-training Deep Learning Framework for Real-time Computational
  Imaging without Physics Priors</font>
    </a>
  </h2>
  <font color="black">ただし、顕著な課題の1つは、実際のイメージングモデルが想定モデルからどの程度逸脱しているかです。この概念を、画像の非自己相関を適用した非線形モデルにさらに拡張します。線形の単一ピクセルを使用してこのフレームワークを示しますカメラ画像モデル。 
[ABSTRACT]一般的な戦略は、最適化された画像を実現するために、ニューラルネットワークの入力として予備画像を再構築することです。実際のイメージングモデルは想定モデルから逸脱しています。モデルモデルは、データに直接関連しない特徴をデータから抽出できます。イメージングモデル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-10">
        <br><font color="black">2020-01-10</font>
      </time>
    </span>
</section>
<!-- paper0: Detection of COVID-19 from Chest X-rays using Deep Learning: Comparing
  COGNEX VisionPro Deep Learning 1.0 Software with Open Source Convolutional
  Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.IV/paper_15.html">
      <font color="black">Detection of COVID-19 from Chest X-rays using Deep Learning: Comparing
  COGNEX VisionPro Deep Learning 1.0 Software with Open Source Convolutional
  Neural Networks</font>
    </a>
  </h2>
  <font color="black">この研究では、COGNEXのディープラーニングソフトウェアであるVisionProのディープラーニングを使用して、COVIDxデータセットからこれらの胸部X線を分類します。この問題は、最初に画像全体を選択することで、画像全体を関心領域ROIとして選択します。次に、最初のステップで肺をセグメント化し、次に、画像全体を使用する代わりに、セグメント化された肺のみで分類ステップを実行します。ディープラーニングツールは、モデルが特定のクラスに画像を分類する方法または理由を人間が解釈できないため、ブラックボックスとして。 
[要約]ウォータールー大学は、ダーウィンaiとともに、その深層学習モデルcovid-netを設計し、covidxと呼ばれるデータセットを作成しました。結果は、オープンソースコミュニティの深層学習モデルの結果と比較されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
<!-- paper0: Comparison of Convolutional neural network training parameters for
  detecting Alzheimers disease and effect on visualization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.IV/paper_16.html">
      <font color="black">Comparison of Convolutional neural network training parameters for
  detecting Alzheimers disease and effect on visualization</font>
    </a>
  </h2>
  <font color="black">2.現在、これらのアルゴリズムの実用的な有用性と機能の概要を提供する視覚化アルゴリズムの比較に対する大きな需要があります。モデルの精度に対するCNNハイパーパラメータの影響を体系的に評価すること。 
[ABSTRACT] cnnの新しいツールボックスの調査が利用可能になりました。ツールボックスの調査は最新の方法を使用しています。現在、ツールボックスの調査が利用可能です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Offloading Optimization in Edge Computing for Deep Learning Enabled
  Target Tracking by Internet-of-UAVs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.IV/paper_17.html">
      <font color="black">Offloading Optimization in Edge Computing for Deep Learning Enabled
  Target Tracking by Internet-of-UAVs</font>
    </a>
  </h2>
  <font color="black">具体的には、UAVが事前トレーニング済みのCNNモデルの下位層に埋め込まれている新しい階層型DLタスク配布フレームワークを提案します。一方、豊富なコンピューティングリソースを備えたMECサーバーはCNNモデルの上位層を処理します。提案されているオフロードフレームワークの有効性。エンパワーメント無人航空機（UAV）は、ターゲット追跡などのインテリジェンスを提供するために広く使用されています。 
[要約]事前トレーニング済みの畳み込みニューラルネットワークがuavに展開され、キャプチャされたビデオフレームからターゲット（車両）を識別します。これにより、このタイプのディープラーニング（dl）タスクをモバイルエッジコンピューティングサーバーにオフロードすることを検討します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Graph Density-Aware Losses for Novel Compositions in Scene Graph
  Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_0.html">
      <font color="black">Graph Density-Aware Losses for Novel Compositions in Scene Graph
  Generation</font>
    </a>
  </h2>
  <font color="black">その結果、一部の最先端のモデルは、このバイアスを利用して結果を改善します。次に、関係の頻度により、このタスクに強いバイアスが生じ、最も頻繁な関係を予測するブラインドモデルが良好なパフォーマンスを達成できるようになります。一般化に重要な多様な少数ショットの例が含まれている場合でも、トレーニング中に大きなスパースグラフの個々のエッジが無視されます。 
[要約]このホワイトペーパーでは、このような一般化を制限する2つの重要な問題を特定します。これらには、一般化に適していると見なすことができる標準ショット関数が含まれます。これにより、トレーニング中に大きなスパースグラフの個々のエッジが無視されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-17">
        <br><font color="black">2020-05-17</font>
      </time>
    </span>
</section>
<!-- paper0: Linguistically-aware Attention for Reducing the Semantic-Gap in
  Vision-Language Tasks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_1.html">
      <font color="black">Linguistically-aware Attention for Reducing the Semantic-Gap in
  Vision-Language Tasks</font>
    </a>
  </h2>
  <font color="black">Counting-VQA、VQA、およびImage captioningの3つのVLタスクでLATの有効性を適用して示します。LATは、共通の言語的に豊富な空間で視覚的およびテキスト的なモダリティを表し、注意プロセスに言語認識を提供します。VQAでキャプションでは、LATをさまざまなベースラインに適合させ、そのパフォーマンスを一貫して改善することにより、LATの一般的な性質と有効性を示しています。 
[ABSTRACT]このホワイトペーパーでは、オブジェクトの属性と事前学習済みの言語モデルを活用して、このセマンティックギャップを減らす注意メカニズム-言語学的に注意（lat）を提案します。さらに、視覚的注意（lats）の効果を適用して示します3つのv-lタスク：カウント-vqa、vqa、および画像のcaptioning.in vqaおよびcaptioningで、latのさまざまなベースラインに適合させ、パフォーマンスを一貫して改善することにより、latの一般的な性質と効果を示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Equivalent Classification Mapping for Weakly Supervised Temporal Action
  Localization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_2.html">
      <font color="black">Equivalent Classification Mapping for Weakly Supervised Temporal Action
  Localization</font>
    </a>
  </h2>
  <font color="black">弱く監視された時間アクションのローカリゼーションは、近年新たに出現し、広く研究されているトピックです。これら2つのパイプラインの分類子はさまざまな方法で使用されますが、それらが果たす役割はまったく同じです---特定の機能を分類して対応するアクションカテゴリ。このために、理想的な分類子は両方のパイプラインを機能させることができます。 
[ABSTRACT]既存の方法は、分類前のパイプラインと分類後のパイプラインを含む、分類による2つのローカリゼーションに分類できます。これにより、統合されたフレームワークでこれら2つのパイプラインを同時に学習して、効果的な分類器を取得できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Gradients as a Measure of Uncertainty in Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_3.html">
      <font color="black">Gradients as a Measure of Uncertainty in Neural Networks</font>
    </a>
  </h2>
  <font color="black">グラデーションは、与えられた入力をモデルが適切に表すために必要な変化量を示します。したがって、入力がどの程度親しみやすく、特定のモデルであるかについての貴重な洞察を提供します。モデルは、馴染みのない条件の入力に遭遇します。特に、訓練されたモデルの不確実性を定量化するために、逆伝播勾配を利用することを提案します。 
[ABSTRACT]「グラハム」モデルモデルモデルシステムの有効性を実証しました。逆伝播問題の使用を実証する方法を実証しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Depth Completion with RGB Prior -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_4.html">
      <font color="black">Depth Completion with RGB Prior</font>
    </a>
  </h2>
  <font color="black">データはローエンドデプスカメラで収集され、グラウンドトゥルースデプスはマルチビューフュージョンによって生成されました。モデルをトレーニングするために、私たちは今公開している新しい産業用データセットを作成しました。デプスカメラは有名な認識です特に自然の非構造化環境で動作する場合のロボット工学のためのシステム。 
[ABSTRACT]モデルは詳細な産業用データセットを作成するために作成されました。データセットは低深度カメラによって作成されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Fully automated deep learning based segmentation of normal, infarcted
  and edema regions from multiple cardiac MRI sequences -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_5.html">
      <font color="black">Fully automated deep learning based segmentation of normal, infarcted
  and edema regions from multiple cardiac MRI sequences</font>
    </a>
  </h2>
  <font color="black">ネットワークへの入力は、3つのCMRシーケンス、つまり後期ガドリニウム増強（LGE）、T2、および平衡定常状態自由歳差運動（bSSFP）で構成されます。提案されたアプローチは、20のケースを含むテストセットでチャレンジオーガナイザーによって評価され、達成します。 LV MSの平均サイコロスコアは$ 46.8 \％$、LV ME + MSの平均は$ 55.7 \％$です。提案されたアプローチは、STACOMと組み合わせてMICCAI 2020が主催するMyoPSチャレンジによって提供されるデータを利用しました。 
[要約]提案されたアプローチは、深い畳み込みニューラルネットワークを使用します。Miccai2020がホストするmyopsチャレンジのデータを使用します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Multiple View Generation and Classification of Mid-wave Infrared Images
  using Deep Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_6.html">
      <font color="black">Multiple View Generation and Classification of Mid-wave Infrared Images
  using Deep Learning</font>
    </a>
  </h2>
  <font color="black">新しい画像内のピクセルの位置を予測するための幾何学的変換は学習せず、むしろ多様体を学習します。非線形特徴部分空間で赤外線画像の見えない任意の視点を生成する新しい研究を提案します。非線形特徴部分空間であり、私たちのネットワークはユークリッド部分空間ではなくリーマン部分空間で動作していると結論付けています。 
[ABSTRACT]現在の方法では合成画像を使用しており、ビューがぼやけることがよくあります。非線形の特徴solaceをさらに調査します。これは、ネットワークが241歳の介護者では動作しないと結論付けています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Anomaly Detection with Convolutional Autoencoders for Fingerprint
  Presentation Attack Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_7.html">
      <font color="black">Anomaly Detection with Convolutional Autoencoders for Fingerprint
  Presentation Attack Detection</font>
    </a>
  </h2>
  <font color="black">報告された良好な検出率にもかかわらず、これらの方法は未知の材料からPAIを検出するのに依然として困難に直面しています。このコンテキストでは、ほとんどの作業は、PADを2クラスの分類問題として解決することに専念しています。 PAサンプル..したがって、プレゼンテーション攻撃検出（PAD）メソッドは、サンプルが実在の被験者（本物）に由来するのか、プレゼンテーション攻撃手段（PAI）に由来するのかを判断するために使用されます。 
[ABSTRACT]プレゼンテーション攻撃検出（pad）メソッドは、サンプルがライブの被験者に由来するのか、プレゼンテーションの問題（pai）に由来するのかを決定するために使用されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Visibility-aware Multi-view Stereo Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_8.html">
      <font color="black">Visibility-aware Multi-view Stereo Network</font>
    </a>
  </h2>
  <font color="black">提案されたフレームワークの有効性を正当化するために、DTU、BlendMVS、Tanks and Templesデータセットに対して広範囲な実験が行われます。このように、コストフュージョンでは、オクルージョンされたピクセルの悪影響が抑制されます。 
[ABSTRACT]提案されたフレームワークvis-mvsnetは、重度のオクルージョンを伴うシーンの深度精度を大幅に改善します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: ReLMoGen: Leveraging Motion Generation in Reinforcement Learning for
  Mobile Manipulation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_9.html">
      <font color="black">ReLMoGen: Leveraging Motion Generation in Reinforcement Learning for
  Mobile Manipulation</font>
    </a>
  </h2>
  <font color="black">メソッドを検証するために、ReLMoGenを2つのタイプのタスクに適用します。1）インタラクティブなナビゲーションタスク、目的地に到達するために環境との相互作用が必要なナビゲーションの問題、および2）モバイル操作タスク、ロボットベースの移動が必要な操作タスク。 。多くの強化学習（RL）アプローチは、連続制御タスクのアクションスペースとしてジョイント制御信号（位置、速度、トルク）を使用します。アクションスペースを持ち上げ、サンプリングベースのモーションプランナーを活用することで、効率的に使用できると主張します。 RLは、元のアクションスペースで既存のRLメソッドでは解決できなかった複雑で長期的なタスクを解決します。 
[ABSTRACT]アクションスペースをサブゴールの形でより高いレベルに上げることを提案します。これらには、モーションジェネレーター（モーションプランナーと軌道エクゼキューターの組み合わせ）のサブゴールが含まれます。relmogen-サブゴールを予測するために学習したポリシーを組み合わせるフレームワーク</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: SoDA: Multi-Object Tracking with Soft Data Association -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_10.html">
      <font color="black">SoDA: Multi-Object Tracking with Soft Data Association</font>
    </a>
  </h2>
  <font color="black">代わりに、私たちのモデルは、ソフトデータアソシエーションを介してすべてのオブジェクト検出からの情報を集約します。堅牢なマルチオブジェクトトラッキング（MOT）は、自動運転車の安全な配備の前提条件です。注意を使用してトラックを計算するMOTへの新しいアプローチを提案します観測されたオブジェクト間の時空間依存性をエンコードする埋め込み。 
[ABSTRACT]私たちのモデルは、ハードデータの関連付けを緩和できます。これにより、回復不可能なエラーが発生する可能性があります。その結果、潜在空間表現により、モデルはオクルージョンの理由を学習できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Mesh Guided One-shot Face Reenactment using Graph Convolutional Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_11.html">
      <font color="black">Mesh Guided One-shot Face Reenactment using Graph Convolutional Networks</font>
    </a>
  </h2>
  <font color="black">顔の再現は、ソースの顔の画像を、駆動画像によって提供される別のポーズと表現にアニメーション化することを目的としています。このように、ネットワークは、駆動する顔の形状の干渉なしに、ソースの顔の動き推定に焦点を当てることができます。非対称オートエンコーダである顔の動きを学習するためのモーションネット。 
[ABSTRACT]再構築されたドライビングメッシュでドライビングフェースのID情報を明示的に除外します。システムは、3D密なメッシュからオプティカルフローを直接学習できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Motion Capture from Internet Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_12.html">
      <font color="black">Motion Capture from Internet Videos</font>
    </a>
  </h2>
  <font color="black">したがって、単一のビデオを個別に使用するのではなく、これらのインターネットビデオを一緒に分析して人間の動きをキャプチャすることを提案します。これらのビデオが異なる時間に記録されたとしても、人の同じモーション特性をエンコードします。単一のビューにおけるあいまいさと自己閉塞は、マルチビュー再構成と同じくらい高品質のモーションの回復を妨げます。 
[ABSTRACT]これは、人間のビデオが異なる時間インスタンスで記録された初めての例です。これらは、人間の同じモーション特性をエンコードします。これらのビデオは同期されておらず、カメラの視点は不明であり、背景のシーンは異なり、人間の動き動画間でまったく同じではありません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Contact Area Detector using Cross View Projection Consistency for
  COVID-19 Projects -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_13.html">
      <font color="black">Contact Area Detector using Cross View Projection Consistency for
  COVID-19 Projects</font>
    </a>
  </h2>
  <font color="black">コンピュータビジョンの3D再構成では、オブジェクトと人体を2D画像領域から3Dに投影し、3D空間の交差を直接実行します。具体的には、オブジェクトを静的面に投影することで、オブジェクトと静的面の間の接触を特定できることを示します2つの異なる視点からサーフェスを作成し、2D交差を分析します。ただし、このソリューションは、投影エラーのため、アプリケーションの精度要件を満たしません。 
[ABSTRACT]人がビジュアルデータ、画像、またはビデオを使用してオブジェクトに触れたかどうかを判断することは難しい問題です。これは、投影エラーのためにアプリケーションの正確さの要件を満たしません。この戦略では、大量のトレーニングデータが必要になりますスケールと視点の変化を一般化する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: EASTER: Efficient and Scalable Text Recognizer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_14.html">
      <font color="black">EASTER: Efficient and Scalable Text Recognizer</font>
    </a>
  </h2>
  <font color="black">アーキテクチャの複数のバリエーションを試してみました。最小のバリアント（深さとパラメーターごとの数）の1つは、RNNベースの複雑な選択肢と同等に機能します。また、オフライン手書きテキスト認識タスクの現在の最良の結果に対する改善も示しています。手書きのテキストと機械で印刷されたテキストの両方の合成データセットを生成するための拡張セットアップを備えたデータ生成パイプラインを提示します。 
[ABSTRACT]研究は、リカレントネットワークと複雑なゲートレイヤーに関連しています。これにより、ソリューション全体が複雑になり、スケーリングが困難になります。当社のモデルでは、1-d畳み込みレイヤーを使用しており、特性はありません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Disentangled Expression Representations from Facial Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_15.html">
      <font color="black">Learning Disentangled Expression Representations from Facial Images</font>
    </a>
  </h2>
  <font color="black">この論文では、敵対損失の定式化を使用して、顔画像のもつれのない表現を学習します。使用されたモデルは、シングルタスクデータセットの学習を容易にし、60.53の精度で最先端の式認識を改善します。追加のデータを使用せずに、AffectNetdatasetで％。このような画像を含むほとんどのタスク、たとえば
[ABSTRACT]使用されたモデルは、単一タスクのデータセットでの学習に役立ちます。これにより、表現認識の最先端が精度60で向上します。追加データを使用せずに、afectnetdatasetで53％</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-16">
        <br><font color="black">2020-08-16</font>
      </time>
    </span>
</section>
<!-- paper0: STEm-Seg: Spatio-temporal Embeddings for Instance Segmentation in Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_16.html">
      <font color="black">STEm-Seg: Spatio-temporal Embeddings for Instance Segmentation in Videos</font>
    </a>
  </h2>
  <font color="black">特に、ビデオクリップを単一の3D時空間ボリュームとしてモデル化し、単一のステージで空間と時間にわたってインスタンスをセグメント化して追跡する新しいアプローチを提案します。コードとモデルはhttps://github.com/sabarim/で入手できますSTEm-Seg ..私たちのネットワークは、これらの埋め込みをクラスター化するために必要なパラメーターと同様に、時空間埋め込みを学習するためにエンドツーエンドでトレーニングされており、推論を簡素化します。 
[ABSTRACT]複数のネットワーク-個々のフレームでオブジェクトを検出するために使用されます。これらは、時間の経過とともにこれらの検出に関連付けられます。ネットワークはトレーニング済みです-埋め込みのメモリを強化します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-18">
        <br><font color="black">2020-03-18</font>
      </time>
    </span>
</section>
<!-- paper0: Knowledge Transfer via Dense Cross-Layer Mutual-Distillation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_17.html">
      <font color="black">Knowledge Transfer via Dense Cross-Layer Mutual-Distillation</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、教師と生徒のネットワークを最初から協調的にトレーニングする、改良された双方向KT法である高密度クロスレイヤー相互蒸留（DCM）を提案します。KTパフォーマンスを向上させるために、分類子が追加されたレイヤー..トレーニング後、すべての補助分類子が破棄されるため、最終的なモデルに追加のパラメーターが導入されることはありません。 
[ABSTRACT] kt戦略は、学生ネットワークが教師ネットワークの改善に役立つことを示しています。ktは、学生ネットワークも役立つことを示しています。ktの作業には、ktトレーニングが含まれますが、学生ネットワークも役立ちます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Retargetable AR: Context-aware Augmented Reality in Indoor Scenes based
  on 3D Scene Graph -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_18.html">
      <font color="black">Retargetable AR: Context-aware Augmented Reality in Indoor Scenes based
  on 3D Scene Graph</font>
    </a>
  </h2>
  <font color="black">シーングラフ.. ARコンテンツによって想定されるコンテキストと、ユーザーがARを体験する実際の環境によって形成されるコンテキストは、抽象的なグラフ表現として表されます。このホワイトペーパーでは、さまざまな現実の環境で設定されたシーンコンテキストを認識し、仮想世界と現実世界の間の自然な相互作用を実現するARエクスペリエンスを生み出す新しいARフレームワークであるRetargetable ARを紹介します。 
[ABSTRACT]構築されたグラフとarの内容のコンテキストを示すarシーンのグラフの間の引数は、意味的に登録された環境を提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: UDC 2020 Challenge on Image Restoration of Under-Display Camera: Methods
  and Results -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_19.html">
      <font color="black">UDC 2020 Challenge on Image Restoration of Under-Display Camera: Methods
  and Results</font>
    </a>
  </h2>
  <font color="black">このペーパーの結果は、アンダーディスプレイカメラレストレーションの最先端の復元パフォーマンスです。このペーパーは、ECCV 2020のRLQワークショップと連携した、最初のアンダーディスプレイカメラ（UDC）画像復元の課題のレポートです。 ..約150のチームがチャレンジを登録したことに加えて、8と9のチームが各トラックのテスト段階で結果を提出しました。 
[要約]課題は、新しく収集されたアンダーディスプレイカメラのデータベースに基づいています。8と9つのチームがテスト段階で結果を提出しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Augmented Skeleton Based Contrastive Action Learning with Momentum LSTM
  for Unsupervised Action Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_20.html">
      <font color="black">Augmented Skeleton Based Contrastive Action Learning with Momentum LSTM
  for Unsupervised Action Recognition</font>
    </a>
  </h2>
  <font color="black">最後に、クエリエンコーダーによって学習された非表示のアクションの状態を時間的に平均化することにより、コントラストアクションエンコーディング（CAE）という名前の新しい表現が提案され、人間のアクションを効果的に表現します。3Dスケルトンデータを介したアクションの認識は、近年重要なトピックとして浮上しています。この論文では、AS-CALという名前の対照的なアクション学習パラダイムを初めて提案します。AS-CALは、ラベルなしのスケルトンデータのさまざまな拡張を利用して、教師なしの方法でアクション表現を学習できます。 
[ABSTRACT]ほとんどの既存の手作りの記述子は、アクション表現を学習するために使用されます。これらには、平均的な被験者の被験者の対面データによるアクション表現が含まれます。このメソッドは、同等またはさらに優れたパフォーマンスを達成できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-01">
        <br><font color="black">2020-08-01</font>
      </time>
    </span>
</section>
<!-- paper0: Attention-based Fully Gated CNN-BGRU for Russian Handwritten Text -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_21.html">
      <font color="black">Attention-based Fully Gated CNN-BGRU for Russian Handwritten Text</font>
    </a>
  </h2>
  <font color="black">また、Tahnの複数の出力機能と入力機能を利用して完全にゲート化されたレイヤーを提案します。この提案された作業はより良い結果を達成し、手書きカザフとロシア語データベース（HKR）でモデルを実験しました。この研究は、カザフ語とロシア語でトレーニングされたアテンションエンコーダー/デコーダーネットワークを使用した手書きテキストのタスク。私たちの研究はHKRデータセットでの最初の作業であり、他のほとんどの既存のモデルに対して最先端の結果を示しています。 
[要約]私たちは、完全にゲートされたcnnに基づく新しいディープニューラルネットワークモデルを開発します。複数の双方向GRUと機能を操作する注意メカニズムによってサポートされています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-12">
        <br><font color="black">2020-08-12</font>
      </time>
    </span>
</section>
<!-- paper0: Dataset Bias in Few-shot Image Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_22.html">
      <font color="black">Dataset Bias in Few-shot Image Recognition</font>
    </a>
  </h2>
  <font color="black">さらに、ほとんどの少数ショット学習法は異なるデータセットに偏っています。これは、詳細に調査する必要がある重要な問題でもあります。このホワイトペーパーでは、まず、ベースカテゴリから学習した転送可能な機能の影響を調査します。具体的には、基本カテゴリと小説カテゴリの関係を説明する関連性と、基本カテゴリの分布を表すインスタンスの多様性とカテゴリの多様性を紹介します。 
[ABSTRACT]最新の研究では、伝達可能な知識を使用して新規カテゴリを特定できると想定しています。しかし、ほとんどの少数ショット学習法は異なるデータセットに偏っています。これもまた、詳細に調査する必要がある重要な問題です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: TapLab: A Fast Framework for Semantic Video Segmentation Tapping into
  Compressed-Domain Knowledge -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_23.html">
      <font color="black">TapLab: A Fast Framework for Semantic Video Segmentation Tapping into
  Compressed-Domain Knowledge</font>
    </a>
  </h2>
  <font color="black">高速バージョンは160+ FPSの速度に達します。TapLabは、制御可能な精度の低下で3〜10倍速く実行される、最先端の高速セマンティックイメージセグメンテーションモデルの冗長な計算を大幅に削減します。動きベクトルによって導入されるノイズについては、残差を使用して、残差ガイド補正モジュールと残差ガイドフレーム選択モジュールを設計します。 
[ABSTRACT] taplabは70. 6％miouのモデルデータセット、99.8 fps、1024x2048ビデオ用の単一のgpuカードで達成</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-30">
        <br><font color="black">2020-03-30</font>
      </time>
    </span>
</section>
<!-- paper0: Domain Generalizer: A Few-shot Meta Learning Framework for Domain
  Generalization in Medical Imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_24.html">
      <font color="black">Domain Generalizer: A Few-shot Meta Learning Framework for Domain
  Generalization in Medical Imaging</font>
    </a>
  </h2>
  <font color="black">次に、少数ショット学習を採用します。この方法は、基盤となるモデルアーキテクチャに依存しないため、あらゆるイメージングタスクに使用できます。この作業では、モデルにとらわれないメタ学習フレームワークに基づくドメイン汎化法を生物医学的イメージングに適合させます。 
[ABSTRACT]ターゲットドメインとソースドメインのデータに違いがあると、一般化が妨げられる可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Self-supervised Sparse to Dense Motion Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_25.html">
      <font color="black">Self-supervised Sparse to Dense Motion Segmentation</font>
    </a>
  </h2>
  <font color="black">これは、シーケンス固有の方法でトレーニングして、スパースでノイズの多い入力から高品質の高密度セグメンテーションを生成できます。このホワイトペーパーでは、単一のビデオフレームからスパースモーションセグメンテーションの高密度化を学習するための自己管理手法を提案します。モーションセグメンテーションは、大規模なサロゲートデータセットの事前トレーニングに基づいて構築され、ピクセル単位のセグメンテーションの重要な手がかりとして密なモーション情報を使用します。モデルは事前トレーニングを必要とせず、単一フレームのテスト時に動作します。 
[ABSTRACT]モーションセグメンテーションは、通常、モーション情報を長くてまばらなポイントの軌跡に集約することで対処されます。これには、フレームごとの高密度モーション情報の生成が含まれます。モデルは事前トレーニングを必要とせず、単一フレームのテスト時に動作します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Hierarchical HMM for Eye Movement Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_26.html">
      <font color="black">Hierarchical HMM for Eye Movement Classification</font>
    </a>
  </h2>
  <font color="black">いくつかの定義済みのしきい値によって眼球運動を検出する既存の方法とは異なり、凝視、サッカード、スムーズな追跡を検出するための階層的隠れマルコフモデル（HMM）統計アルゴリズムを提案します。実験結果は、提案された方法の有効性と堅牢性を示しています。最先端の方法と比較して競争力のある、またはより優れたパフォーマンスを達成することによって。これらの異なるタイプの眼球運動の効率的な分類は、視線追跡データのより良い分析と利用に役立ちます。 
[要約]計画は高度な分類システムを使用することを提案しています。システムはシステムの開発に使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Tripping through time: Efficient Localization of Activities in Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_27.html">
      <font color="black">Tripping through time: Efficient Localization of Activities in Videos</font>
    </a>
  </h2>
  <font color="black">さらに、TripNetは強化学習を使用して、ビデオの周りをインテリジェントにスキップする方法を学習することにより、長いビデオの関連するアクティビティクリップを効率的にローカライズします。Charades-STA、ActivityNetキャプション、およびTACoSデータセットに対する評価では、TripNetが高い精度とビデオ全体の32〜41％だけを見ることで処理時間を節約します。ビデオ監視など、このアプローチの実際のアプリケーションでは、効率が重要なシステム要件です。 
[要約]論文では、ゲーテッドアテンションアーキテクチャを使用して、きめ細かい文学的および視覚的表現をモデル化するエンドツーエンドシステムであるtripnetを紹介しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-04-22">
        <br><font color="black">2019-04-22</font>
      </time>
    </span>
</section>
<!-- paper0: Actor-Action Video Classification CSC 249/449 Spring 2020 Challenge
  Report -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_28.html">
      <font color="black">Actor-Action Video Classification CSC 249/449 Spring 2020 Challenge
  Report</font>
    </a>
  </h2>
  <font color="black">このテクニカルレポートは提出を要約し、ロスター大学のCSC 249/449マシンビジョンコース（2020年春）の最終プロジェクトとして開催された俳優とアクションのビデオ分類の課題からのコンパイルを要約します。ロチェスター大学で行われた分類チャレンジ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-01">
        <br><font color="black">2020-08-01</font>
      </time>
    </span>
</section>
<!-- paper0: Understanding Crowd Flow Movements Using Active-Langevin Model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_29.html">
      <font color="black">Understanding Crowd Flow Movements Using Active-Langevin Model</font>
    </a>
  </h2>
  <font color="black">ただし、これらのフローを記述する群集モデルの開発は困難な作業です。提案された方法は、既存の最新の方法と比較して、オプティカルフローエラーが少なく、精度が高いフローをセグメント化できます。評価の評価ランジュバン方程式に基づくアクティブな群集セグメンテーションは、一般に公開されている群集のビデオと私たち自身のビデオで行われました。 
[要約]この論文では、密集した群集の動きを記述するために物理学ベースのモデルが提案されています。提案された方法は、既存の状態と比較して、オプティカルフローエラーが少なく、精度が高いフローをセグメント化できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-12">
        <br><font color="black">2020-03-12</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Parallel MRI Reconstruction Network Without Coil Sensitivities -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_30.html">
      <font color="black">Deep Parallel MRI Reconstruction Network Without Coil Sensitivities</font>
    </a>
  </h2>
  <font color="black">既存のほとんどのディープイメージ再構成ネットワークとは異なり、私たちのネットワークは、正確な推定が困難な感度マップの知識を必要とせず、実際のpMRIアプリケーションにおけるイメージ再構成の主要なボトルネックとなっています。データからトレーニングされた正則化機能を備えた並列MRI（pMRI）での高速画像再構成のための堅牢な近位勾配スキームをマッピングすることによるネットワークアーキテクチャ。 
[要約]提案されたネットワークは、不完全なpmriデータからの画像を単一の画像に適応的に結合することを学習します。ネットワークは、ネットワークのネットワークの画像のスローダウンに応答することを学習します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-04">
        <br><font color="black">2020-08-04</font>
      </time>
    </span>
</section>
<!-- paper0: UnrealText: Synthesizing Realistic Scene Text Images from the Unreal
  World -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_31.html">
      <font color="black">UnrealText: Synthesizing Realistic Scene Text Images from the Unreal
  World</font>
    </a>
  </h2>
  <font color="black">通常のメッシュとオブジェクトメッシュです。ただし、シーンテキスト検出器は、手動で注釈を付けた大量の現実世界の画像に依然依存しています。これはコストがかかります。包括的な実験により、シーンテキストの検出と認識の両方でその効果が確認されています。 。 
[ABSTRACT]合成単語画像は、シーンのテキスト認識機能のトレーニングにおいて、実際の画像の代替として成功していることが証明されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-24">
        <br><font color="black">2020-03-24</font>
      </time>
    </span>
</section>
<!-- paper0: Communicative Reinforcement Learning Agents for Landmark Detection in
  Brain Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_32.html">
      <font color="black">Communicative Reinforcement Learning Agents for Landmark Detection in
  Brain Images</font>
    </a>
  </h2>
  <font color="black">C-MARLを使用すると、エージェントは明示的な通信チャネルを学習できるだけでなく、すべてのエージェント間でアーキテクチャの特定の重みを共有することで暗黙的な通信信号を学習できます。単一のエージェント。.解剖学的ランドマークの正確な検出は、いくつかの医療画像処理タスクの重要なステップです。 
[要約]提案されたアプローチは2つの脳学習データセットで評価されます。2つのmriと胎児の超音波スキャンに基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Grading Loss: A Fracture Grade-based Metric Loss for Vertebral Fracture
  Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_33.html">
      <font color="black">Grading Loss: A Fracture Grade-based Metric Loss for Vertebral Fracture
  Detection</font>
    </a>
  </h2>
  <font color="black">最先端のメトリック損失に基づいて、Genantの骨折グレーディングスキーマを尊重する表現を学習するための新規のGrading Lossを提示します。公開されている脊椎データセットで、提案された損失関数は、81.5％の骨折検出F1スコア、10単純な分類ベースラインを超える％の増加。これに対処するために、骨折検出に効率的な潜在表現の学習を目的とした、自動化された椎骨骨折検出のための表現学習にヒントを得たアプローチを提案します。 
[要約]提案された方法は、脊椎骨折検出システムを開発するために使用できます。モデルは、81.5％の骨折検出で診断できます。これは、単純な分類ベースラインよりも10％増加しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Multilanguage Number Plate Detection using Convolutional Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_34.html">
      <font color="black">Multilanguage Number Plate Detection using Convolutional Neural Networks</font>
    </a>
  </h2>
  <font color="black">ResNet属性抽出ハートを備えたYOLOv2センサーがNP検出用に提案され、新しい畳み込みニューラルネットワークアーキテクチャがNPを分類するために提案されています。結果は以前の研究の大部分を上回り、領域を国際的なNP検出と認識に向けて前進させることができます。このペーパーでは、NPを検出し、NPの国、言語、レイアウトを理解するための新しい戦略を提案します。 
[ABSTRACT] npを検出し、npsの国、言語、レイアウトを理解するための新しい戦略を提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: An Unsupervised Deep Learning Method for Multi-coil Cine MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_35.html">
      <font color="black">An Unsupervised Deep Learning Method for Multi-coil Cine MRI</font>
    </a>
  </h2>
  <font color="black">次に、これらの完全にエンコードされたデータを使用して、各コイルの画像を個別に再構築するための並列ネットワークをトレーニングできます。具体的には、時間インターリーブ取得スキームを使用して、隣接する時間フレーム..この論文では、時間インターリーブサンプリング戦略によるマルチコイルシネMRIの教師なし深層学習法を提案します。 
[要約]ディープラーニング手法は、反復的な手法と比較して、再構築に長い時間を必要とせずに、再構築の質を向上させることができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-20">
        <br><font color="black">2019-12-20</font>
      </time>
    </span>
</section>
<!-- paper0: Person image generation with semantic attention network for person
  re-identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_36.html">
      <font color="black">Person image generation with semantic attention network for person
  re-identification</font>
    </a>
  </h2>
  <font color="black">実験結果は、Market-1501とDeepFashionの定量的結果と定性的結果の両方に関して、私たちのアプローチが競争力があることを示しています。他の方法と比較して、私たちのネットワークは、より良い体型を特徴付け、同時に服の属性を維持できます。元の画像に関連する外観と形状の一貫性。 【要約】セマンティックアテンションネットワークと呼ばれる新しい人物姿勢誘導画像生成手法を提案</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: TextSnake: A Flexible Representation for Detecting Text of Arbitrary
  Shapes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_37.html">
      <font color="black">TextSnake: A Flexible Representation for Detecting Text of Arbitrary
  Shapes</font>
    </a>
  </h2>
  <font color="black">このようなジオメトリ属性は、完全畳み込みネットワーク（FCN）モデルを介して推定されます。実験では、TextSnakeに基づくテキスト検出器は、Total-TextとSCUT-CTW1500（新しく公開された2つのベンチマーク）で最先端または同等のパフォーマンスを実現します。自然画像の湾曲したテキスト、および広く使用されているデータセットICDAR 2015とMSRA-TD500に特に重点を置いています。TextSnakeでは、テキストインスタンスは、対称軸を中心とする順序付けられた重複するディスクのシーケンスとして記述されます。半径と方向が変化する可能性があります。 
[ABSTRACT] textsnakeでは、テキストインスタンスは、シーンを中心とする大きなオーバーラップセットのシーケンスとして記述されます。これらは、湾曲したテキストなどのさまざまなフォームにリンクされます。これらの例は、現実のシナリオで非常に一般的です。これらには、湾曲したテキストが含まれます。これは非常に一般的と見なすことができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-07-04">
        <br><font color="black">2018-07-04</font>
      </time>
    </span>
</section>
<!-- paper0: TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_38.html">
      <font color="black">TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval</font>
    </a>
  </h2>
  <font color="black">提案されたXMLモデルは、新しいConvolutional Start-End Detector（ConvSE）を備えたレイトフュージョンデザインを使用しており、ベースラインを大幅に上回り、効率が高く、将来の作業の強力な出発点となります。両方のデータセットが公開されています。また、TVRの注釈付きの各瞬間の追加の説明を収集して、262Kキャプションを含む新しいマルチモーダルキャプションデータセットを形成し、TVショーキャプション（TVC）という名前を付けました。 
[ABSTRACT] tvrは、システムがビデオとそれに関連するサブタイトル（ダイアログ）テキストの両方を理解することを必要とします。クエリには、ビデオまたはサブタイトル、あるいはその両方との関連性が高いかどうかを示す質問タイプのラベルも付けられます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-24">
        <br><font color="black">2020-01-24</font>
      </time>
    </span>
</section>
<!-- paper0: Mastering Large Scale Multi-label Image Recognition with high efficiency
  overCamera trap images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_39.html">
      <font color="black">Mastering Large Scale Multi-label Image Recognition with high efficiency
  overCamera trap images</font>
    </a>
  </h2>
  <font color="black">カメラトラップは生物多様性の動機付けされた研究において重要ですが、これらのデータセットに注釈を付けながら多数の画像を処理することは、退屈で時間のかかる作業です。トレーニングセット（6.7Mの画像と6TB）にもかかわらず、単一のGPU（1080Ti）を備えた非常に限られたハードウェアでトップスコアリングシステムをトレーニングするテクニックと、比較的大きなデータセットを考えると、増強はほとんど、またはまったくない、一度だけのイメージ。 
[ABSTRACT]機械学習アプローチは、このプロセスをスピードアップするための合理的な資産です。当社のシステムは、97％の精度を達成し、人間レベルのパフォーマンスを上回りました。機械学習は、シンプルでありながら効果的なベースラインシステムを組み合わせています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Tackling the Unannotated: Scene Graph Generation with Bias-Reduced
  Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_40.html">
      <font color="black">Tackling the Unannotated: Scene Graph Generation with Bias-Reduced
  Models</font>
    </a>
  </h2>
  <font color="black">RとmRの結果の不一致は、高いRを追求することから、まだ競争力のあるRを備えた高いmRに焦点を移すことを促します。ペアは、まったく注釈が付けられていないか、複数のものが有効である可能性がある場合に単一の関係のみでアノテーションが付けられます。2つの関係分類子が含まれ、1つは、もう1つがベースとするバイアス設定が少なくなっています。ただし、最新の状態アートの結果は、まだ満足のいくものではありません。たとえば、
[ABSTRACT]これらのモデルの多くは、同僚とcolleagueの間の最も頻繁な相互作用をキャプチャできました。mr@ 100は、ビジュアルゲノム（vg）の約8％にすぎません。標準ではrequireには「シンプルな」トレーニングスキームが必要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Beyond Monolingual Vision-Language Navigation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_41.html">
      <font color="black">Beyond Monolingual Vision-Language Navigation</font>
    </a>
  </h2>
  <font color="black">ターゲット言語のトレーニングデータがない場合、モデルは、ターゲット言語のトレーニングデータに完全にアクセスできるモデルと比較しても競争力のある結果を示します。さらに、一定量のターゲット言語のトレーニングデータが与えられたときのモデルの転送能力を調査します。 ..この新しく導入されたデータセットに基づいて、既存の英語の指示でエージェントをトレーニングしながら、ゼロショット学習シナリオの下で別の言語で効果的にナビゲートする方法を研究します。 
[ABSTRACT]視覚言語ナビゲーション（vln）に関するこれまでの研究によると、主要言語は英語です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-24">
        <br><font color="black">2019-10-24</font>
      </time>
    </span>
</section>
<!-- paper0: Feature Products Yield Efficient Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_42.html">
      <font color="black">Feature Products Yield Efficient Networks</font>
    </a>
  </h2>
  <font color="black">このようにして、最先端のネットワークに基づいていくつかの新しいFP-netを作成し、Cifar-10およびImageNetの課題でそれらを評価します。畳み込みニューラルネットワークは、従来のネットワークを置き換えることにより、パラメータ効率の高いFP-netに変換できます。このようなFPブロックは、皮質領域V1、特にV2で一般的なエンドストップニューロンのモデルから発想を得ています。 
[要約] fp-ブロックを使用すると、汎化機能を評価せずにパラメーターの数が大幅に減少します。これまでのところ、fps-ブロックブロックを使用すると、汎化機能を低下させることなくパラメーターの量が減少することがわかります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Structured Domain Randomization: Bridging the Reality Gap by
  Context-Aware Synthetic Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_43.html">
      <font color="black">Structured Domain Randomization: Bridging the Reality Gap by
  Context-Aware Synthetic Data</font>
    </a>
  </h2>
  <font color="black">KITTIの簡単、中程度、そしてハードなタスクでは、SDRが他のアプローチで合成データ（VKITTI、Sim 200k、またはDR）を生成するほか、別のドメインで収集された実際のデータ（BDD100K）を生成することを示しています。 、SDRによって生成された画像により、ニューラルネットワークは検出中にオブジェクトの周囲のコンテキストを考慮に入れることができます。2Dバウンディングボックスの車の検出の問題に対してSDRの威力を発揮し、合成データのみでトレーニングした後、実際のデータで競争力のある結果を達成します。 
[ABSTRACT] sdrは、目前の特定の問題から生じるパターンに従って、オブジェクトとディストラクタをランダムに配置します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-10-23">
        <br><font color="black">2018-10-23</font>
      </time>
    </span>
</section>
<!-- paper0: AssembleNet++: Assembling Modality Representations via Attention
  Connections -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_44.html">
      <font color="black">AssembleNet++: Assembling Modality Representations via Attention
  Connections</font>
    </a>
  </h2>
  <font color="black">別のブロックまたは入力モダリティを使用して注意の重みを動的に学習する、peer-attentionという名前の新しいネットワークコンポーネントが導入されました。事前トレーニングを行わなくても、私たちのモデルは、連続動画を使用した標準の公共活動認識データセットに関する以前の作業よりも優れており、新しい状態を確立します-of-the-art ..また、オブジェクトモダリティからのニューラル接続とピアアテンションの使用に関する調査結果は、既存のさまざまなアーキテクチャに一般的に適用でき、それらのパフォーマンスが向上することも確認しています。 
[ABSTRACT] peerという名前の新しいネットワークコンポーネント-Attentionperが導入されました。別のブロックまたは入力モダリティを使用して注意の重みを学習します。コードは年の初めに利用可能になります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Automated Detection of Congenital Heart Disease in Fetal Ultrasound
  Screening -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_45.html">
      <font color="black">Automated Detection of Congenital Heart Disease in Fetal Ultrasound
  Screening</font>
    </a>
  </h2>
  <font color="black">トレーニングと推論の両方で、補助ビュー分類タスクを利用して、関連する心臓構造に向かって機能をバイアスします。このバイアスは、健常クラスとCHDクラスのF1スコアをそれぞれ0.72と0.77から0.87と0.85に改善するのに役立ちます。胎児超音波検査における先天性心疾患（CHD）の検出を支援するディープラーニング技術の可能性について説明します。 
[ABSTRACT]自動化されたデータのキュレーションと分類のためのパイプラインを提案します。autopspsingとデータのキュレーションが含まれます。このバイアスは、f1の改善に役立ちます-健全とchdの0. 72と0. 77から0. 87と0. 85のスコアクラス</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-16">
        <br><font color="black">2020-08-16</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Gradient Fields for Shape Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_46.html">
      <font color="black">Learning Gradient Fields for Shape Generation</font>
    </a>
  </h2>
  <font color="black">私たちの方法は、点群の自動エンコードと生成の最先端のパフォーマンスに到達できると同時に、高品質の陰面の抽出も可能であることを示しています。このモデルは、対数密度フィールドの勾配を直接予測し、スコアベースの生成モデルから採用された単純な目的でトレーニングできます。点群は、形状の表面付近に密度が集中している3Dポイントの分布からのサンプルとして表示できます。 
[要旨]点群は、形状の表面近くに密度が集中する3d点の分布からのサンプルとして表示できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-14">
        <br><font color="black">2020-08-14</font>
      </time>
    </span>
</section>
<!-- paper0: Image Pre-processing on NumtaDB for Bengali Handwritten Digit
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_47.html">
      <font color="black">Image Pre-processing on NumtaDB for Bengali Handwritten Digit
  Recognition</font>
    </a>
  </h2>
  <font color="black">その理由は、ベンガル語の数字認識がMNISTの英語の数字のように機能するために利用可能な前処理済みのデータがないためです。NumtaDBは、ベンガル語の手書き数字のデータセットのコレクションとしては群を抜いています。しかし、この多様性により、処理が非常に難しいデータセット。 
[要約]このペーパーの目的は、前処理された画像のベンチマークを見つけることです。これは、85000を超える画像を含む多様なデータセットです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Describing Unseen Videos via Multi-ModalCooperative Dialog Agents -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_48.html">
      <font color="black">Describing Unseen Videos via Multi-ModalCooperative Dialog Agents</font>
    </a>
  </h2>
  <font color="black">動的ダイアログ履歴更新学習メカニズムを備えたQA協調ネットワークを提案して、知識をA-BOTからQ-BOTに転送し、Q-BOTがビデオをよりよく説明できるようにします。具体的には、インテリジェントエージェントの1つ-Q-BOT -ビデオの最初と最後から2つの静的フレームと、目に見えないビデオを説明する前に関連する自然言語の質問をする有限数の機会が与えられます。A-BOT、すでに全体を見た他のエージェントビデオ、これらの質問への回答を提供することにより、Q-BOTが目標を達成するのを支援します。 
[ABSTRACT]新しいタスクは、2つのマルチモーダル協調ダイアログエージェントによるビデオの説明と呼ばれます。目的は、1つのチャットエージェントが、ダイアログと2つの静的フレームに基づいて見えないビデオを説明することです。ビデオは、ビデオベースのチームの発案によるものですビデオ全体を見たことがある人</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: RevPHiSeg: A Memory-Efficient Neural Network for Uncertainty
  Quantification in Medical Image Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_49.html">
      <font color="black">RevPHiSeg: A Memory-Efficient Neural Network for Uncertainty
  Quantification in Medical Image Segmentation</font>
    </a>
  </h2>
  <font color="black">リバーシブルアーキテクチャは、各層のアクティベーションを保存する代わりに、バックプロパゲーション中に後続のレイヤーの出力からのアクティベーションを正確に計算することにより、メモリの節約を実現します。画像の分割..最近、ニューラルネットワークに基づく不確実性の定量化手法がさまざまな問題にうまく適用されています。 
[ABSTRACT]不確実性の定量化方法の使用は、小さな問題にうまく適用されています。主な構造は、メモリを構築するためのリバーシブルです。リバーシブルブロックは、phisegと呼ばれる最近提案されたアーキテクチャに組み込まれています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-16">
        <br><font color="black">2020-08-16</font>
      </time>
    </span>
</section>
<!-- paper0: MaskedFace-Net -- A Dataset of Correctly/Incorrectly Masked Face Images
  in the Context of COVID-19 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_50.html">
      <font color="black">MaskedFace-Net -- A Dataset of Correctly/Incorrectly Masked Face Images
  in the Context of COVID-19</font>
    </a>
  </h2>
  <font color="black">これらの理由により、いくつかのマスク装着キャンペーンは、この問題と優れた実践について人々を敏感にすることを目的としています。私たちの知る限り、マスクされた顔の大規模なデータセットは、マスク装着分析を許可するような細かい分類を提供していません。さらに、この作業特に特定のマスクを使用して、他のマスクされた顔画像の生成を可能にするために、適用されたマスクから顔に変形可能なモデルをグローバルに提示します。 
[ABSTRACT]これらの作品は、マスクされた顔のデータセットに基づいています。しかし、多くの人々は、悪い習慣、悪い行動、または個人の脆弱性のため、正しくマスクを着用していません。これらには、正しくマスクされた顔のデータセット（cmfd）、誤ってマスクされた顔のデータセットが含まれます（imfd）およびグローバルなマスクされた顔検出のためのそれらの組み合わせ（maskedface-net）</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: AB3DMOT: A Baseline for 3D Multi-Object Tracking and New Evaluation
  Metrics -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_51.html">
      <font color="black">AB3DMOT: A Baseline for 3D Multi-Object Tracking and New Evaluation
  Metrics</font>
    </a>
  </h2>
  <font color="black">私たちのコードはhttp://www.xinshuoweng.com/projects/AB3DMOTで公開されています。3DMOTメソッドを包括的に評価するために、3つの新しいメトリックとともに新しい3D MOT評価ツールを提案します。 KITTIでの強力な3D MOTパフォーマンス。KITTIデータセットで$ 207.4 $ FPSのレートで実行され、最新の3D MOTシステムの中で最速のスピードを実現します。 
[要約]提案された方法は、kittiで強力な3Dモットパフォーマンスを実現し、207ドルのレートで実行されることを示しています。 kittiデータセットで4 $ fps、最新の3d motシステムの中で最速を達成</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Self-supervised Denoising via Diffeomorphic Template Estimation:
  Application to Optical Coherence Tomography -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_52.html">
      <font color="black">Self-supervised Denoising via Diffeomorphic Template Estimation:
  Application to Optical Coherence Tomography</font>
    </a>
  </h2>
  <font color="black">ただし、OCT画像はノイズによって強く破壊され、その解釈が制限されます。さらに、リピートの直接非線形アラインメントは、画像間のノイズの相関を引き起こします。光コヒーレンストモグラフィー（OCT）は、眼科の研究と臨床の両方に広まっています。 
[要約] di di dispell画像はノイズによって強く破壊され、その解釈が制限されます。最近の自己管理型の進歩により、グラウンドトゥルースとしてクリーンなターゲットなしで繰り返し取得のみを使用して、深いノイズ除去ネットワークのトレーニングが可能になります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Reinforcement Learning for Improving Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_53.html">
      <font color="black">Reinforcement Learning for Improving Object Detection</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、ObjectRLと呼ばれるアルゴリズムを導入して、事前トレーニング済みネットワークのオブジェクト検出パフォーマンスを向上させるために適用する特定の前処理の量を選択します。通常、画像はニューラルネットワークに送る前に前処理されます。画像データセットに関するドメイン知識を使用して、前処理手法を選択します。トレーニング済みオブジェクト検出ニューラルネットワークのパフォーマンスは、画像品質に大きく依存します。 
[ABSTRACT] objectrlの主な動機は、人間の目には美しく見える画像が、事前に訓練されたオブジェクト検出器がオブジェクトを検出するのに必ずしも完璧ではないことです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Fatigue Assessment using ECG and Actigraphy Sensors -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_54.html">
      <font color="black">Fatigue Assessment using ECG and Actigraphy Sensors</font>
    </a>
  </h2>
  <font color="black">具体的には、解釈可能なソリューションについて、システムの意思決定プロセスをよりよく理解するために、相関性が低く情報量の多い機能を選択できる機能選択アプローチを提案しました。広範な実験が行われ、非常に有望な結果が得られました。ディープラーニングソリューションについては、最先端の自己注意モデルを使用しました。これに基づいて、疲労評価のための一貫性自己注意（CSA）メカニズムをさらに提案しました。 
[要約]私たちは、ウェアラブルセンシングと機械学習技術を使用して、客観的な疲労評価のための自動システムを開発しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-06">
        <br><font color="black">2020-08-06</font>
      </time>
    </span>
</section>
<!-- paper0: Conditional Flow Variational Autoencoders for Structured Sequence
  Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_55.html">
      <font color="black">Conditional Flow Variational Autoencoders for Structured Sequence
  Prediction</font>
    </a>
  </h2>
  <font color="black">さらに、2つの新しい正則化スキームを提案します。これは、トレーニングを安定させ、安定したトレーニングのために後方崩壊を処理し、ターゲットデータ分布によりよく適合します。3つのマルチモーダル構造化シーケンス予測データセット（MNISTシーケンス、スタンフォードドローン、HighD）での実験- -提案された方法が、さまざまな評価指標にわたって最新の結果を取得することを示します。潜在変数モデルに基づく構造化シーケンス予測の先行研究は、潜在変数に単一モード標準ガウス事前分布を課しています。 
[ABSTRACT]潜在変数モデルに基づく構造化シーケンス予測の先行作業は、ユニを課します-潜在変数に完全にガウス分布を取り込む</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-24">
        <br><font color="black">2019-08-24</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Semantic Segmentation via Decoupled Body and Edge Supervision -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_56.html">
      <font color="black">Improving Semantic Segmentation via Decoupled Body and Edge Supervision</font>
    </a>
  </h2>
  <font color="black">結果のボディフィーチャと残余エッジフィーチャは、異なるパーツ（ボディまたはエッジ）ピクセルを明示的にサンプリングすることにより、分離された監視の下でさらに最適化されます。セマンティックセグメンテーションの魅力的なパフォーマンスには、\ textit {explicitly}オブジェクトのモデリング\ textit {bodyが必要であることがわかります}と\ textit {edge}は、画像の高周波数と低周波数に対応しています。さまざまなベースラインまたはバックボーンネットワークを備えた提案されたフレームワークが、オブジェクトの内部整合性とオブジェクト境界の改善につながることを示しています。 
[要約]このホワイトペーパーでは、セマンティックセグメントの新しいベンチが提案されています。4年後に提案されます。提案されたフレームワークは、オブジェクトの整合性とオブジェクトの境界の改善につながります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-20">
        <br><font color="black">2020-07-20</font>
      </time>
    </span>
</section>
<!-- paper0: Pix2Surf: Learning Parametric 3D Surface Models of Objects from Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_57.html">
      <font color="black">Pix2Surf: Learning Parametric 3D Surface Models of Objects from Images</font>
    </a>
  </h2>
  <font color="black">私たちの方法は、一般的なオブジェクトカテゴリの形状のパブリックデータセットで監視およびトレーニングされています。さらに、生成された3Dサーフェスは、正確な画像ピクセルと3Dサーフェスポイントの対応を保持するため、テクスチャ情報を持ち上げて、豊富なジオメトリと外観を持つ形状を再構築できます。定量的な結果は、私たちの方法が以前の研究を大幅に上回っていることを示していますが、定性的な結果は、再構成の質が高いことを示しています。 
[要約]複数のビューから形状再構成を学習する以前の作業では、点群やボクセルなどの離散表現を使用しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: ConvGRU in Fine-grained Pitching Action Recognition for Action Outcome
  Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_58.html">
      <font color="black">ConvGRU in Fine-grained Pitching Action Recognition for Action Outcome
  Prediction</font>
    </a>
  </h2>
  <font color="black">また、さまざまなネットワーク実装を比較し、さまざまな画像サンプリング手法、さまざまな融合手法、事前トレーニングなどの影響を示しました。さまざまな結果がアクションの微妙な違いに密接に関連していることを考えると、きめの細かいアクション認識は実用的ですアクションの結果を予測する方法..このホワイトペーパーでは、細かいアクション認識タスクでのたたみ込みゲートリカレントユニット（ConvGRU）メソッドのパフォーマンスを調査し、ボールピッチングの結果を予測します。 
[ABSTRACT]きめの細かいアクション認識は、アクションの結果を予測するための実用的な方法です。提案されたアプローチは、79。17％のパフォーマンスを達成しました。これは、現在の状態を超えています-最先端の結果</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: MVLidarNet: Real-Time Multi-Class Scene Understanding for Autonomous
  Driving Using Multiple Views -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_59.html">
      <font color="black">MVLidarNet: Real-Time Multi-Class Scene Understanding for Autonomous
  Driving Using Multiple Views</font>
    </a>
  </h2>
  <font color="black">両方のステージでエンコーダーデコーダーアーキテクチャを使用します。システムは、自動運転車用に設計された組み込みGPUで150 fpsで効率的に動作します。これには、時間をかけてIDを維持する後処理ステップが含まれます。内部データセット。したがって、1桁ずつスケーリングするメソッドの能力を示します。 
[ABSTRACT]マルチビューLIDARNET（mvlidarnet）は、単一のLIDARポイントクラウドの複数のビューを使用して、マルチクラスオブジェクト検出と駆動可能スペースセグメンテーションのための2段階のディープニューラルネットワークです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-09">
        <br><font color="black">2020-06-09</font>
      </time>
    </span>
</section>
<!-- paper0: CoDiNet: Path Distribution Modeling with Consistency and Diversity for
  Dynamic Routing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_60.html">
      <font color="black">CoDiNet: Path Distribution Modeling with Consistency and Diversity for
  Dynamic Routing</font>
    </a>
  </h2>
  <font color="black">具体的には、類似したセマンティクスのサンプルはルーティングスペースの同じ領域にマッピングする必要がありますが、異なるセマンティクスのサンプルは異なる領域にマッピングする必要があります。ResNetモデルにデプロイすると、このメソッドはより高いパフォーマンスを実現し、4つの平均計算コストを効果的に削減します。使用されたデータセット。スペースマッピングの観点から、ダイナミックルーティングの一般的な方法では、ルーティングスペースで推論パスがどのように分散されるかを考慮していませんでした。 
[要約]このペーパーでは、サンプルスペースとルーティングスペースの関係をモデル化する新しい方法を提案します。この方法は、codinetと呼ばれ、特定の特定のルート上のサンプルスペースからのマッピングとして使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-29">
        <br><font color="black">2020-05-29</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Deep Cross-modality Spectral Hashing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_61.html">
      <font color="black">Unsupervised Deep Cross-modality Spectral Hashing</font>
    </a>
  </h2>
  <font color="black">2番目のステップでは、有益なデータ入力（画像と単語の埋め込み）から最初のステップで取得したバイナリコードへのマッピング関数を学習するために、画像の強力なCNNを活用し、CNNベースのディープアーキテクチャを提案してテキストモダリティを学習します。 3つの標準的なベンチマークデータセットの評価は、提案されたDCSHメソッドが他の最先端のメソッドよりも常に優れていることを示しています。前者は各モダリティのローカル構造を十分に維持できますが、後者はすべてのモダリティの隠れたパターンを示します。 
[要約] dcshシステムは2段階のハッシュアプローチです。2段階のハッシュ方式に拡張を分離します。2番目は、すべてのモダリティから隠されたパターンを明らかにします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-01">
        <br><font color="black">2020-08-01</font>
      </time>
    </span>
</section>
<!-- paper0: Comparison of Convolutional neural network training parameters for
  detecting Alzheimers disease and effect on visualization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_62.html">
      <font color="black">Comparison of Convolutional neural network training parameters for
  detecting Alzheimers disease and effect on visualization</font>
    </a>
  </h2>
  <font color="black">モデルの精度に対するCNNハイパーパラメータの影響を体系的に評価するために.. 2 ..最近、ツールボックスiNNvestigateが利用可能になり、ディープラーニングの視覚化のためのさまざまな最先端の方法が実装されました。 
[ABSTRACT] cnnの新しいツールボックスの調査が利用可能になりました。ツールボックスの調査は最新の方法を使用しています。現在、ツールボックスの調査が利用可能です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: One-pixel Signature: Characterizing CNN Models for Backdoor Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_63.html">
      <font color="black">One-pixel Signature: Characterizing CNN Models for Backdoor Detection</font>
    </a>
  </h2>
  <font color="black">1ピクセル署名は、バックドア検出を超えてCNNモデルを特徴付けるために使用できる一般的な表現です。1ピクセル署名と呼ばれる新しい表現を提案することで、畳み込みニューラルネットワーク（CNN）バックドア検出の問題に取り組みます。シグネチャは、バックドアCNN検出/分類のための既存の競合する方法よりも大幅な改善（絶対検出精度が約30％向上）を示しています。 
[要旨]私たちのタスクは、未知のトリガーを使用するかどうかによってcnnモデルを検出することです。1ピクセルのシグネチャは、cnnアーキテクチャの設計値と、それらがどのようにトレーニングされたかに対応しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Two-hand Global 3D Pose Estimation Using Monocular RGB -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_64.html">
      <font color="black">Two-hand Global 3D Pose Estimation Using Monocular RGB</font>
    </a>
  </h2>
  <font color="black">私たちのシステムは、RGBのみの情報を使用して、3Dの標準的な手姿勢推定ベンチマークデータセットに関する以前の作業よりも優れていることを示しています。さらに、RGBのみの入力を使用して両手で正確なグローバル3D手の追跡を実現し、広範な定量的および定性評価..この新しいタスクのCNNをトレーニングするために、大規模な合成3D手のポーズデータセットを導入します。 
[要旨]私たちは、正確に手をセグメント化して位置を特定する、新しい多段畳み込み畳み込みニューラルネットワークベースのパイプラインを提案します。rgbを使用して、両手で正確なグローバル3Dハンドトラッキングを実現する最初の作業を紹介します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-01">
        <br><font color="black">2020-06-01</font>
      </time>
    </span>
</section>
<!-- paper0: Offloading Optimization in Edge Computing for Deep Learning Enabled
  Target Tracking by Internet-of-UAVs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CV/paper_65.html">
      <font color="black">Offloading Optimization in Edge Computing for Deep Learning Enabled
  Target Tracking by Internet-of-UAVs</font>
    </a>
  </h2>
  <font color="black">具体的には、UAVに事前トレーニング済みのCNNモデルの下位層が埋め込まれている新しい階層型DLタスク分散フレームワークを提案し、豊富なコンピューティングリソースを持つMECサーバーがCNNモデルの上位層を処理します。分析結果は次のとおりです。提案されたフレームワークにおける加重合計コストと推論エラー率の間のトレードオフを理解するために得られた洞察が提供されます。数値結果は、提案されたオフロードフレームワークの有効性を示しています。 
[要約]事前トレーニング済みの畳み込みニューラルネットワークがuavに展開され、キャプチャされたビデオフレームからターゲット（車両）を識別します。これにより、このタイプのディープラーニング（dl）タスクをモバイルエッジコンピューティングサーバーにオフロードすることを検討します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Lite Audio-Visual Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CL/paper_0.html">
      <font color="black">Lite Audio-Visual Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">システムには2つの視覚データ圧縮技術が含まれており、視覚特徴抽出ネットワークをトレーニングモデルから削除して、オンライン計算効率を向上させています。さらに、実験結果は、視覚データ圧縮のための2つの技術の有効性を確認しています。提案されたLAVSEシステムは、同様の数のモデルパラメータを持つオーディオのみのSEシステムよりも著しく優れたパフォーマンスを提供できることを示しています。 
[要約]システムには2つの視覚的データ圧縮技術が含まれています。結果は2つの技術の有効性を確認します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-24">
        <br><font color="black">2020-05-24</font>
      </time>
    </span>
</section>
<!-- paper0: Just another quantum assembly language (Jaqal) -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CL/paper_1.html">
      <font color="black">Just another quantum assembly language (Jaqal)</font>
    </a>
  </h2>
  <font color="black">Jaqal用に開発したメタプログラミングPythonパッケージであるQSCOUT、Jaqal、またはJaqalPaqの詳細については、https：//qscout.sandia.gov、https：//gitlab.com/jaqalにアクセスするか、qscoutに電子メールを送信してください@ sandia.gov .. JaqalはQSCOUT以外にも便利です---ゲート名とそのパルスシーケンス定義を外部ファイルにオフロードするため、複数のハードウェアターゲットをサポートできます。Jaqal言語の機能と、設計におけるアプローチについて説明します、およびその作成の理由。 
[ABSTRACT]量子アセンブリ言語であるjaqalは、qscoutで実行されるプログラムを指定するために作成されました。jaqalを含む言語は、qscout.qscoutによって開発されました。これはjaqalと呼ばれ、便利なツールです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: An Enhanced Text Classification to Explore Health based Indian
  Government Policy Tweets -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CL/paper_2.html">
      <font color="black">An Enhanced Text Classification to Explore Health based Indian
  Government Policy Tweets</font>
    </a>
  </h2>
  <font color="black">ただし、これらのLRモデルは、十分な注釈付きデータが不足しているため、リアルタイムでの適用性が低くなります。これを処理するために、新しいGloVe単語の埋め込みとクラス固有の感情ベースのテキスト拡張アプローチ（Mod-EDAと呼ばれる）を提案します。ラベル付きデータのサイズを増やすことによるテキスト分類タスクのパフォーマンス。提案されたフレームワークは、言語表現モデル（LRモデル）BERT、ELMO、およびUSEを活用します。 
[ABSTRACT]政府はソーシャルネットワークを使用して、新しいソーシャルメディアのテキスト拡張アプローチを開発しています。新しいシステムを使用して、ラベル付きデータのサイズを増やすことにより、テキスト分類タスクのパフォーマンスを向上させることができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-13">
        <br><font color="black">2020-07-13</font>
      </time>
    </span>
</section>
<!-- paper0: Are Neural Open-Domain Dialog Systems Robust to Speech Recognition
  Errors in the Dialog History? An Empirical Study -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CL/paper_3.html">
      <font color="black">Are Neural Open-Domain Dialog Systems Robust to Speech Recognition
  Errors in the Dialog History? An Empirical Study</font>
    </a>
  </h2>
  <font color="black">大規模なエンドツーエンドのニューラルオープンドメインチャットボットは、ますます人気が高まっています。ベースラインの緩和戦略として、トレーニング中にダイアログ履歴に合成ASR仮説を導入し、限界改善を観察します。エンドツーエンドのオープンドメインチャットボットは完全に音声耐性があります。最新のTransferTransfoのダイアログ履歴にあるさまざまなタイプの合成および実際のASR仮説の影響を経験的に調査することにより、この重要な質問に注目することを目指しています。 -art Generative Pre-trained Transformer（GPT）ベースのニューラルオープンドメインダイアログシステム。NeurIPSConvAI2チャレンジから。 
[ABSTRACT]そのようなチャットボットの構築に関する研究では、通常、ユーザー入力は自然な形で書かれていると想定されていました。これらのチャットボットが人間の相互作用のレベルにシームレスに疑問を投げかけるかどうかは不明です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Working Memory Graphs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CL/paper_4.html">
      <font color="black">Working Memory Graphs</font>
    </a>
  </h2>
  <font color="black">WMGは、Transformerベースのモデルが、観測を因数分解できるRL環境でサンプル効率を劇的に向上させる方法を示しています。WMGのTransformerベースのアーキテクチャと因数分解観測スペースの組み合わせにより、全体的なベースラインアーキテクチャと比較して、学習効率が大幅に向上することがわかります。すべてのタスク..観察および再発状態を表す動的なベクトルのセットを推論するためにマルチヘッドの自己注意を採用するエージェントであるワーキングメモリグラフ（WMG）を提示します。 
[ABSTRACT]変圧器-ベースのモデルは視覚的な理由のパフォーマンスを向上させることができます。変圧器の例を評価します-ベースのモデルはそのパフォーマンスを向上させることができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-17">
        <br><font color="black">2019-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: COVID-SEE: Scientific Evidence Explorer for COVID-19 Related Research -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CL/paper_5.html">
      <font color="black">COVID-SEE: Scientific Evidence Explorer for COVID-19 Related Research</font>
    </a>
  </h2>
  <font color="black">情報探索の概念に基づいた医学文献発見のためのシステムであるCOVID-SEEを紹介します。これは、複数の異なるテキスト分析と自然言語処理方法に基づいて出版物内の情報を構造化および整理し、探索をサポートする視覚的な概要を提供することで検索を強化します関心のある主要な記事を特定するためのコレクションのコレクションです。COVID-SEEはhttp://covid-see.com。で入手できます。COVID-19の文献に基づいてこのシステムを開発し、医療専門家や研究者が文献の証拠を探索し、改善できるようにしています。関連情報の見つけやすさ。 
[要旨]このシステムはcovid-19文献で開発されました。医療専門家や研究者に情報を提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: On The Evaluation of Machine Translation Systems Trained With
  Back-Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CL/paper_6.html">
      <font color="black">On The Evaluation of Machine Translation Systems Trained With
  Back-Translation</font>
    </a>
  </h2>
  <font color="black">逆翻訳は、ターゲットの単一言語データを利用する、広く使用されているデータ拡張手法です。逆翻訳はより滑らかな出力を生成するため、人間には逆翻訳が好まれるという見解を裏付ける実証的な証拠を提供します。リファレンスは翻訳であるので、BLEUは人間の好みをキャプチャできません。ソース文が自然なテキストである場合。 
[ABSTRACT]戻る-翻訳は、プロによる人間の翻訳に従って、自然に発生するテキストと翻訳者の両方の翻訳品質を向上させます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-14">
        <br><font color="black">2019-08-14</font>
      </time>
    </span>
</section>
<!-- paper0: WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words
  in Context -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CL/paper_7.html">
      <font color="black">WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words
  in Context</font>
    </a>
  </h2>
  <font color="black">WiC-TSVデータは、https：//competitions.codalab.org/competitions/23683で入手できます。最先端の言語モデルを使用して、データセットのベースラインパフォーマンスを設定します。これにより、データセットを非常に柔軟に評価できますドメイン内およびドメイン間でのモデルとシステムの多様なセットの。 
[ABSTRACT] wic-tsvは、モデルに提供される入力信号に応じて、3つの異なる評価設定を提供します。新しいシステムは、単語の意味の曖昧性解消とエンティティリンクを評価するように設計されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-30">
        <br><font color="black">2020-04-30</font>
      </time>
    </span>
</section>
<!-- paper0: PopMAG: Pop Music Accompaniment Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CL/paper_8.html">
      <font color="black">PopMAG: Pop Music Accompaniment Generation</font>
    </a>
  </h2>
  <font color="black">ポップミュージックでは、伴奏は通常、ドラム、ベース、ストリング、ギターなどの複数の楽器（トラック）で演奏され、メロディーと一緒にアレンジすることで、曲をより表現力豊かで伝染させることができます。ポップミュージックの伴奏生成システムは、 PopMAGとして。結果は、マルチトラック調和モデリングおよび長期コンテキストモデリングに対するPopMAGの有効性を示しています。 
[ABSTRACT]ピッチの真実に加えて、music.popmagの長期的な依存関係をキャプチャするためのメモリとして、余分なロングコンテキストを導入し、複数のデータセット（lmd、freemidi、cpmd、チャイニーズポップソングのプライベートデータセット）の調和を勝ち取ります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Deploying Lifelong Open-Domain Dialogue Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CL/paper_9.html">
      <font color="black">Deploying Lifelong Open-Domain Dialogue Learning</font>
    </a>
  </h2>
  <font color="black">この学習は、実際のユーザーとの会話に適用すると、クラウドソーシングされたデータよりも効率的であり、収集がはるかに安価です。この作業では、人間のプレーヤーが学習エージェントと会話するロールプレイングゲームを構築して展開します。オープンドメインのファンタジーの世界に位置しています。（2020）、クラウドソーシングされたデータには、自然性の欠如と実際のユースケースとの関連性の問題がありますが、静的データセットのパラダイムでは、モデルを使用した経験から学ぶことができません。言語（Silver et al。、2013）。 
[ABSTRACT]対照的に、人と対話することでより役立つ機械学習システムが望まれるかもしれません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CL/paper_10.html">
      <font color="black">TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval</font>
    </a>
  </h2>
  <font color="black">提案されたXMLモデルは、新しいコンボリューショナルStart-End検出器（ConvSE）を備えたレイトフュージョンデザインを使用し、ベースラインを大幅に上回り、効率が向上し、将来の作業に強力な出発点を提供します。TV番組検索（TVR） 、新しいマルチモーダル検索データセット。さらに、マルチモーダルモーメント検索タスク用のいくつかのベースラインと新しいクロスモーダルモーメントローカリゼーション（XML）ネットワークを示します。 
[ABSTRACT] tvrは、システムがビデオとそれに関連するサブタイトル（ダイアログ）テキストの両方を理解することを必要とします。クエリには、ビデオまたはサブタイトル、あるいはその両方との関連性が高いかどうかを示す質問タイプのラベルも付けられます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-24">
        <br><font color="black">2020-01-24</font>
      </time>
    </span>
</section>
<!-- paper0: Beyond Monolingual Vision-Language Navigation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CL/paper_11.html">
      <font color="black">Beyond Monolingual Vision-Language Navigation</font>
    </a>
  </h2>
  <font color="black">さらに、ターゲット言語のトレーニングデータが一定量与えられたときのモデルの転送能力を調査します。ターゲット言語のトレーニングデータがない場合、ターゲット言語のトレーニングデータに完全にアクセスできるモデルと比較しても、モデルは競争力のある結果を示します..ロボットに自然言語の指示でナビゲートするように命令することは、基本的な言語の理解とロボット工学の長期的な目標です。 
[ABSTRACT]視覚言語ナビゲーション（vln）に関するこれまでの研究によると、主要言語は英語です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-24">
        <br><font color="black">2019-10-24</font>
      </time>
    </span>
</section>
<!-- paper0: Word2vec Skip-gram Dimensionality Selection via Sequential Normalized
  Maximum Likelihood -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CL/paper_12.html">
      <font color="black">Word2vec Skip-gram Dimensionality Selection via Sequential Normalized
  Maximum Likelihood</font>
    </a>
  </h2>
  <font color="black">単語の埋め込みに関する他の評価方法と比較して、SNMLによって選択された次元は、単語の類推または単語の類似性タスクによって取得された最適な次元に非常に近くなります。したがって、対応するモデルは、真の分布に可能な限り近づけることができます。SNMLは、最小記述長に基づいてデータシーケンスを順次エンコードするために必要な合計コード長です。 
[要約]提案されたアプローチは、元のsgモデルとsgネガティブサンプリングモデルの両方に適用されます。snmlは、bicとaicの両方より優れています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: HunFlair: An Easy-to-Use Tool for State-of-the-Art Biomedical Named
  Entity Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CL/paper_13.html">
      <font color="black">HunFlair: An Easy-to-Use Tool for State-of-the-Art Biomedical Named
  Entity Recognition</font>
    </a>
  </h2>
  <font color="black">この目的のために、広く使用されているNLPフレームワークFlairに統合された複数のエンティティタイプをカバーするNERタガーであるHunFlairを提案します。可用性：HunFlairは、MITライセンスの下でFlairフレームワークを通じて無料で利用できます：https://github.com/flairNLP/フレアであり、すべての主要なオペレーティングシステムと互換性があります。連絡先：{weberple、saengema、alan.akbik} @ informatik.hu-berlin.de 
[ABSTRACT] hunflairは他の状態より優れています-最先端のスタンドアロンnerツールです。単一のコマンドで、4行のコードのみで適用されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-17">
        <br><font color="black">2020-08-17</font>
      </time>
    </span>
</section>
<!-- paper0: Glancing Transformer for Non-Autoregressive Neural Machine Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CL/paper_14.html">
      <font color="black">Glancing Transformer for Non-Autoregressive Neural Machine Translation</font>
    </a>
  </h2>
  <font color="black">ただし、現在の非自己回帰モデルは、予測精度が自己回帰モデルに遅れをとっています。精度のギャップは、非自己回帰モデルの2つの欠点によるものです。a）非常に強い条件付き独立性の仮定の下で同時生成を学習する。 b）明示的なターゲット言語モデリングがない。特に、GLATはWMT 2014ドイツ語-英語で30.91 BLEUを達成し、自己回帰モデルと非自己回帰モデルのギャップを0.5 BLEUスコア未満に狭めています。 
[ABSTRACT]非自己回帰モデルは、予測精度がまだ自己回帰モデルよりも遅れています。ただし、これらの自己回帰モデルは、自己回帰モデルの多くを実現していません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Very Deep Transformers for Neural Machine Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CL/paper_15.html">
      <font color="black">Very Deep Transformers for Neural Machine Translation</font>
    </a>
  </h2>
  <font color="black">これらのディープモデルは、ベースラインの6層モデルよりも2.5 BLEUも優れており、WMT14英語（フランス語）（43.8 BLEU）およびWMT14英語（ドイツ語）（30.1 BLEU）で新しい最先端のベンチマーク結果を達成しています。コードトレーニング済みのモデルは、https：//github.com/namisan/exdeep-nmt ..で公開されます。ニューラル機械翻訳（NMT）のための非常に深いトランスフォーマーモデルのアプリケーションについて説明します。トレーニングを安定させ、最大60のエンコーダレイヤーと12のデコーダレイヤーで標準のトランスフォーマーベースのモデルを構築することが可能であることを示します。 
[ABSTRACT]モデル用のシンプルで効果的な初期化キットが可能です。最大60のエンコーダレイヤーと12のデコーダレイヤーでモデルを構築できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: A Capsule Network-based Model for Learning Node Embeddings -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CL/paper_16.html">
      <font color="black">A Capsule Network-based Model for Learning Node Embeddings</font>
    </a>
  </h2>
  <font color="black">実験結果は、提案されたCaps2NEがノード分類タスクのベンチマークデータセットで最先端のパフォーマンスを取得することを示しています。これらの機能を2番目のカプセルレイヤーにフィードして、ターゲットノードのもっともらしい埋め込みを推測します。コードは\ url {https://github.com/daiquocnguyen/Caps2NE}で入手できます。 
[ABSTRACT] caps2neは、2つのカプセルレイヤーのネットワークの新しい埋め込みモデルです。ノード分類タスクには、ベンチマークデータセットで最先端のパフォーマンスが必要です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-12">
        <br><font color="black">2019-11-12</font>
      </time>
    </span>
</section>
<!-- paper0: Negative Training for Neural Dialogue Response Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CL/paper_17.html">
      <font color="black">Negative Training for Neural Dialogue Response Generation</font>
    </a>
  </h2>
  <font color="black">トレーニングされたモデルが与えられると、フレームワークは最初に望ましくない動作を示す生成されたサンプルを見つけ、それを使用してモデルを微調整するための負のトレーニング信号を送ります。この作業では、最小化するために「負のトレーニング」という名前のフレームワークを提案しますこのような行動..私たちの実験は、否定的なトレーニングが悪意のある応答のヒット率を大幅に低下させたり、頻繁な応答を阻止したり、応答の多様性を改善したりできることを示しています。 
[ABSTRACT]科学者は、否定的なトレーニングが悪意のある応答のヒット率を大幅に低下させる可能性があることを示しています。これらには、応答の多様性を改善するための推奨応答の奨励が含まれます。結果は実験で明らかになりました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-03-06">
        <br><font color="black">2019-03-06</font>
      </time>
    </span>
</section>
<!-- paper0: NASE: Learning Knowledge Graph Embedding for Link Prediction via Neural
  Architecture Search -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/cs.CL/paper_18.html">
      <font color="black">NASE: Learning Knowledge Graph Embedding for Link Prediction via Neural
  Architecture Search</font>
    </a>
  </h2>
  <font color="black">いくつかのベンチマークデータセットの実験結果は、いくつかの最先端のアプローチと比較した方法の有効性を示しています。最初に、入力トリプレットの埋め込みは、Representation Search Moduleによって改良されています。多様性と複雑さの性質により、実際のKGの場合、すべてのデータセットにうまく適合するモデルを設計することは本質的に困難です。 
[ABSTRACT]これまでの研究では、自動機械学習（automl）を使用してベストセットを検索しようとしましたが、それらのほとんどは、いくつかの既知のデータセットのいくつかの既知の関係パターンに基づいて設計されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: Lite Audio-Visual Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.AS/paper_0.html">
      <font color="black">Lite Audio-Visual Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">さらに、実験結果は、視覚データ圧縮のための2つの手法の有効性を確認します。この研究では、これらの問題に対処するためにLite AVSE（LAVSE）システムを提案します。システムには2つの視覚データ圧縮手法が含まれ、視覚的トレーニングモデルからの特徴抽出ネットワークにより、オンライン計算の効率が向上します。 
[要約]システムには2つの視覚的データ圧縮技術が含まれています。結果は2つの技術の有効性を確認します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-24">
        <br><font color="black">2020-05-24</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Attack and Defense Strategies for Deep Speaker Recognition
  Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.AS/paper_1.html">
      <font color="black">Adversarial Attack and Defense Strategies for Deep Speaker Recognition
  Systems</font>
    </a>
  </h2>
  <font color="black">敵対的攻撃は最近復活したドメインであり、特にニューラルネットワークベースの分類器の破壊に効果的であることが示されています。特に、入力サンプルをごくわずかに摂動させるだけで事後分布を強制的に変更することができます。実験により、話者認識システムは敵対的な攻撃に対して脆弱であり、最強の攻撃はシステムの精度を94％から0％にさえ低下させる可能性があります。このホワイトペーパーで紹介する実験が、興味のある研究コミュニティに役立つベースラインを提供することを願っています。話者認識システムの敵対的ロバスト性をさらに研究する。 
[ABSTRACT]敵対的攻撃は、最近復活したドメインであり、ディープニューラルネットワークの破壊に効果的であることが示されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Separating Varying Numbers of Sources with Auxiliary Autoencoding Loss -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.AS/paper_2.html">
      <font color="black">Separating Varying Numbers of Sources with Auxiliary Autoencoding Loss</font>
    </a>
  </h2>
  <font color="black">反復分離法は、出力数を柔軟に決定できるため、コミュニティで大きな注目を集めていますが、（1）通常、反復の停止時間を決定するために長期情報に依存するため、因果関係での操作が困難になります。設定; （2）ソースの推定数が実際の数と異なる場合、「フォールトトレランス」メカニズムが不足しています。最近の多くのソース分離システムは、混合物から一定数のソースを分離するように設計されています。実験結果では、A2PITは、さまざまな数のスピーカーで分離性能を改善し、混合されたスピーカーの数を効果的に検出できます。 
[ABSTRACT]多くの自動エンコーディングの個別のシステムは、2つの問題を軽減するように設計されています。多くの自動エンコーディングは、システムの必要性を軽減するのに役立ちます。テストを使用して、さまざまな数のスピーカーで分離パフォーマンスを改善できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-27">
        <br><font color="black">2020-03-27</font>
      </time>
    </span>
</section>
<!-- paper0: A Real-time Robot-based Auxiliary System for Risk Evaluation of COVID-19
  Infection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.AS/paper_3.html">
      <font color="black">A Real-time Robot-based Auxiliary System for Risk Evaluation of COVID-19
  Infection</font>
    </a>
  </h2>
  <font color="black">モデルの構造は、リアルタイムアプリケーションに実装するために簡潔に維持されます。これは、人間のロボットからの実際の会話データに基づいており、音声信号を処理して咳を検出し、検出された場合は分類します。この論文では、 COVID-19感染のリスク評価のためのリアルタイムのロボットベースの補助システム。 
[ABSTRACT]プロジェクトは、リアルタイム音声認識、温度測定、キーワード検出、咳検出などの機能を組み合わせています。人間のロボットからの実際の会話データに基づいており、音声信号を処理して咳を検出し、検出された場合は分類します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: PopMAG: Pop Music Accompaniment Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.AS/paper_4.html">
      <font color="black">PopMAG: Pop Music Accompaniment Generation</font>
    </a>
  </h2>
  <font color="black">調和を改善するために、この論文では、単一のシーケンスで同時マルチトラック生成を可能にし、異なるトラックからのノートの依存性を明示的にモデル化する新しいMUltiトラックMIDI表現（MuMIDI）を提案します。 \％/ 38 \％/ 40 \％の投票は、LMD、FreeMidi、およびCPMDデータセットのグラウンドトゥルースの楽曲とそれぞれ比較すると、他の最先端の音楽伴奏生成モデルおよびマルチトラックMIDI表現を大きく上回ります。主観的および客観的な測定基準..以前の作品は通常複数のトラックを個別に生成し、異なるトラックの音符は明示的に相互に依存していないため、ハーモニーモデリングに悪影響を及ぼします。 
[ABSTRACT]ピッチの真実に加えて、music.popmagの長期的な依存関係をキャプチャするためのメモリとして、余分なロングコンテキストを導入し、複数のデータセット（lmd、freemidi、cpmd、チャイニーズポップソングのプライベートデータセット）の調和を勝ち取ります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: Tdcgan: Temporal Dilated Convolutional Generative Adversarial Network
  for End-to-end Speech Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.AS/paper_5.html">
      <font color="black">Tdcgan: Temporal Dilated Convolutional Generative Adversarial Network
  for End-to-end Speech Enhancement</font>
    </a>
  </h2>
  <font color="black">実験結果は、提案された方法が最先端のエンドツーエンドのGANベースの音声強調よりも優れていることを実証しました。さらに、以前のGANベースの方法と比較して、提案されたTDCGANはパラメーターの数を大幅に減らすことができました。また、拡張音声のSNRを改善する際のジェネレーターの損失関数の正則化として、信号対雑音比（SNR）ペナルティ項目の影響も最初に調査しました。 
[ABSTRACT]初めて、深さ方向の畳み込みを含む時間拡張畳み込みネットワークをガン構造に導入しました。この結果は、ベースの方法が最先端のパフォーマンスをベースにした音声強調をさらに優れていることを示しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: CinC-GAN for Effective F0 prediction for Whisper-to-Normal Speech
  Conversion -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.AS/paper_6.html">
      <font color="black">CinC-GAN for Effective F0 prediction for Whisper-to-Normal Speech
  Conversion</font>
    </a>
  </h2>
  <font color="black">これは、MCCマッピングの精度を失うことなくF0予測の効果を高めるように特別に設計されています。これにより、予測されるF0に非線形ノイズが追加されます。このノイズを抑制するために、Cycle-in-Cycle GAN（つまり、CinC）を提案します-GAN）。 
[ABSTRACT] cycleganベースの方法は、2つの異なるモデルを使用します。これは、mccマッピングの事前トレーニング済みモデルに大きく依存します。この方法は、非並列設定で評価されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-18">
        <br><font color="black">2020-08-18</font>
      </time>
    </span>
</section>
<!-- paper0: SpEx+: A Complete Time Domain Speaker Extraction Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.AS/paper_7.html">
      <font color="black">SpEx+: A Complete Time Domain Speaker Extraction Network</font>
    </a>
  </h2>
  <font color="black">具体的には、2つの同一の音声エンコーダネットワークの重みを結合します。1つはエンコーダ-抽出器-デコーダパイプライン用で、もう1つはスピーカーエンコーダの一部として結合します。このような不一致を解消するために、完全な時間領域スピーカー抽出ソリューションを提案します。 SpEx +と呼ばれます。このような不一致は、システムパフォーマンスに悪影響を及ぼします。 
[ABSTRACT]周波数領域アプローチでの位相推定を回避する時間またはソリューションspexを最近提案しました。時間領域の分析ウィンドウのサイズと周波数ゾーンのスピーカーのサイズも異なります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-10">
        <br><font color="black">2020-05-10</font>
      </time>
    </span>
</section>
<!-- paper0: DCCRN: Deep Complex Convolution Recurrent Network for Phase-Aware Speech
  Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.AS/paper_8.html">
      <font color="black">DCCRN: Deep Complex Convolution Recurrent Network for Phase-Aware Speech
  Enhancement</font>
    </a>
  </h2>
  <font color="black">従来の時間周波数（TF）ドメイン法は、単純な畳み込みニューラルネットワーク（CNN）またはリカレントニューラルネットワーク（RNN）を介して、TFマスクまたは音声スペクトルを予測することに焦点を当てています。最近のいくつかの研究では、トレーニング値として複素数値スペクトログラムを使用していますが、マグニチュードとフェーズコンポーネント、または実数部と虚数部をそれぞれ予測して、実数値ネットワークでトレーニングします。わずか3.7Mのパラメーターで、Interspeech 2020 Deep Noise Suppression（DNS）チャレンジに提出されたDCCRNモデルは、実平均オピニオンスコア（MOS）に基づく非リアルタイムトラックのタイムトラックと秒。 
[要旨]たたみ込み実ネットワークは、たたみ込みニューラルネットワーク（cnn）を使用します。これらのたたみ込み実項には、たたみ込み部分と長い短期記憶（lstm）が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-01">
        <br><font color="black">2020-08-01</font>
      </time>
    </span>
</section>
<!-- paper0: WG-WaveNet: Real-Time High-Fidelity Speech Synthesis without GPU -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-19/eess.AS/paper_9.html">
      <font color="black">WG-WaveNet: Real-Time High-Fidelity Speech Synthesis without GPU</font>
    </a>
  </h2>
  <font color="black">PyTorchの実装は、8 GB未満のGPUメモリを使用してトレーニングでき、NVIDIA 1080Ti GPUで960 kHzを超えるレートでオーディオサンプルを生成します。オーディオサンプルはオンラインで公開されています。さらに、CPUで合成する場合でも、提案された方法は、リアルタイムより1.2倍速い44.1 kHzの音声波形を生成できることを示しています。 
[要約]提案されたモデルは、コンパクトなフローベースのモデルとポストフィルターに基づいています。他の波形生成モデルよりも必要なリソースが少なくなります。提案された方法は、44。1センチメートルの音声波形1. 2倍の速度で生成できます。リアルタイム</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-15">
        <br><font color="black">2020-05-15</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
