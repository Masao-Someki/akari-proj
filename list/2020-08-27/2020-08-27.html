<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-08-27の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: The Freesound Loop Dataset and Annotation Tool -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.SD/paper_0.html">
      <font color="black">The Freesound Loop Dataset and Annotation Tool</font>
    </a>
  </h2>
  <font color="black">自動ループ特性評価からアルゴリズム合成までのアプリケーションで、コミュニティがさらに多くのデータの用途を見つけることを期待しています。データの組み立てと注釈付けに使用される方法論について説明し、データ内のタグの分布と内部アノテーター契約。エキスパートによって注釈が付けられた音楽ループの新しい大規模データセットであるFreesound Loop Dataset（FSLD）を提示します。 
[ABSTRACT] freesoundはクリエイティブコモンズライセンスの下でリリースされたオーディオ録音のコミュニティデータベースです。fsldはfsldを使用してテンポとキーを推定し、音楽トラックを生成し、ループ分離アルゴリズムを評価します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarially Training for Audio Classifiers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.SD/paper_1.html">
      <font color="black">Adversarially Training for Audio Classifiers</font>
    </a>
  </h2>
  <font color="black">言い換えれば、敵対的攻撃はあらゆるスケールで存在しますが、敵対的でない訓練を受けたモデルと比較して、より高い敵対的な摂動が必要になる場合があります。まず、離散ウェーブレット変換の2D表現で訓練されたResNet-56モデルに、 tonnetzクロマグラムは、認識精度の点で他のモデルよりも優れています。2つのベンチマーク環境音データセットで実験を実行し、敵の予算割り当てに課せられた制限なしに、敵に訓練されたモデルの不正率が90 \％を超える可能性があることを示します。 
[ABSTRACT] 2つのベンチマーク環境音データセットで実験を行います。これらは、敵対的な敵の標的のだまし率が90％を超える可能性があることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: PyNX: high performance computing toolkit for coherent X-ray imaging
  based on operators -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_0.html">
      <font color="black">PyNX: high performance computing toolkit for coherent X-ray imaging
  based on operators</font>
    </a>
  </h2>
  <font color="black">コマンドラインスクリプトは、生のビームラインデータセットから、またはコヒーレントX線画像データフォーマットを使用して、オンラインCDIおよびPtychography分析にも使用できます
[Maia（2012）]。計算は、複数のGPUに配布することもできます。オープンソースのPyNXツールキット
[Favre-Nicolin et al（2011）arXiv：1010.2641、Mandula et al（2016）]は、コヒーレントX線画像データ分析とシミュレーションのためのツールを提供するように拡張されています。 
[ABSTRACT]量子処理ユニット（pc 2012）を使用して、高性能の計算速度を達成できます。コヒーレントなデータをコンピューターシステム（cdu）で実行できます。単純な数学演算子としてPythonでも使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Large-scale neuromorphic optoelectronic computing with a reconfigurable
  diffractive processing unit -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_1.html">
      <font color="black">Large-scale neuromorphic optoelectronic computing with a reconfigurable
  diffractive processing unit</font>
    </a>
  </h2>
  <font color="black">ほぼすべての計算操作を光学的に割り当て、光変調器と光検出器を動的にプログラミングすることにより、非常に高速なデータ変調と大規模ネットワークパラメータの更新を実現します。さまざまな回折フィードフォワードとリカレントニューラルネットワークを実装するためのDPUの再構成を実証しました。は、システムの欠陥を回避する新しい適応型トレーニングアプローチを開発しました。ここでは、さまざまなニューラルネットワークを効率的にサポートし、数百万のニューロンで高いモデル複雑度を実現できる回折処理ユニット（DPU）を構築することにより、光電子再構成可能コンピューティングパラダイムを提案します。 
[ABSTRACT]光学ニューラルネットワークアーキテクチャの最近の進歩は、さまざまな機械学習タスクを実行するために適用されています。システムは、さまざまなニューラルネットワークを効果的にサポートし、数百万のニューロンで高いモデル複雑度を実現できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Improving distribution and flexible quantization for DCT coefficients -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_2.html">
      <font color="black">Improving distribution and flexible quantization for DCT coefficients</font>
    </a>
  </h2>
  <font color="black">両方を最適化すると、ここでは量子化がほぼ均一になり、テール処理が自動化されます。$ q $を歪みだけに最適化すると、大幅に改善されますが、より均一な分布によりエントロピーが増加します。特に、このような連続分布の場合は、最適化された連続\ emph {量子化密度関数} $ q $による量子化アプローチについても説明します。これは、正則格子上の逆CDF（累積分布関数）$ Q $ $ \ {Q ^ {-1}（（i-1 / 2） / N）：i = 1 \ ldots N \} $は、最適化された（不均一な）量子化の柔軟で安価な選択を可能にする、可変サイズの$ N $の量子化ノードを提供します。 
[ABSTRACT]これは、dctカクテルの分布を予測することについても説明されています。これらは、dctブロックの既にデコグされた領域からのものです。これらの要因には、最適化された連続＆mu mu mu強調が含まれます。これらは、最適化された「不均一」量子化を含みます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-23">
        <br><font color="black">2020-07-23</font>
      </time>
    </span>
</section>
<!-- paper0: High-dimensional Fast Convolutional Framework (HICU) for Calibrationless
  MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_3.html">
      <font color="black">High-dimensional Fast Convolutional Framework (HICU) for Calibrationless
  MRI</font>
    </a>
  </h2>
  <font color="black">結論：提示された方法HICUは、効率的な計算とスケーラビリティ、および幅広いMRIアプリケーションへの拡張性を提供します。結果：静的イメージングの結果は、HICUが他のCl-と比較して1〜2桁の計算速度を提供できることを示しています。イメージング品質を犠牲にすることなく、MRIメソッド。提案された計算手順である高次元高速畳み込みフレームワーク（HICU）は、非サンプリングk空間点の高速でメモリ効率の高い回復を提供します。 
[ABSTRACT] hicuは6つの2d t2加重脳、7つの心臓シネ、1つの再構成データセットに適用されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-19">
        <br><font color="black">2020-04-19</font>
      </time>
    </span>
</section>
<!-- paper0: Parameter-Transferred Wasserstein Generative Adversarial Network
  (PT-WGAN) for Low-Dose PET Image Denoising -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_4.html">
      <font color="black">Parameter-Transferred Wasserstein Generative Adversarial Network
  (PT-WGAN) for Low-Dose PET Image Denoising</font>
    </a>
  </h2>
  <font color="black">臨床データの実験結果は、提案されたネットワークが最近公開された最新の方法よりも優れた画像忠実度を維持しながら、画像ノイズをより効果的に抑制できることを示しています。臨床現場での陽電子放出断層撮影（PET）の広範な使用により、患者へのPET関連の放射線量の潜在的なリスクを最小限に抑える必要があります。https：//github.com/90n9-yu/PT-WGANでコードを公開しています。 
[要約]結果として得られた結果の画像は、診断パフォーマンスを損なうノイズやアーティファクトの影響を受ける可能性があります。結果の画像はwww。 github.com / 90n9-yu / pt-wgan</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-13">
        <br><font color="black">2019-10-13</font>
      </time>
    </span>
</section>
<!-- paper0: Better Than Reference In Low Light Image Enhancement: Conditional
  Re-Enhancement Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_5.html">
      <font color="black">Better Than Reference In Low Light Image Enhancement: Conditional
  Re-Enhancement Networks</font>
    </a>
  </h2>
  <font color="black">トレーニングプロセス中、異なる露光時間のペアリングされた画像をトレーニングに使用でき、大幅に節約できる教師付き画像を慎重に選択する必要がないことに注意してください。次に、データ駆動型の条件付き再拡張ネットワーク（CRENetと表示）が提案されています。最初に、HSV色空間とRetinex理論の関係を分析し、Vチャネル（HSV色空間のVチャネル、RGB色空間の最大チャネルに等しい）を示します。強化された画像のコントラストは、コントラストと輝度の強化プロセスを適切に表すことができます。 
[ABSTRACT]低照度画像と強化されたvチャネルは、低照度画像のコントラストと明るさを再強化し、同時にノイズと色の歪みを低減できます。他の画像のコントラスト強調方法と組み合わせて、画質を向上させることができます強化された画像</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: High-resolution Multi-spectral Imaging with Diffractive Lenses -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_6.html">
      <font color="black">High-resolution Multi-spectral Imaging with Diffractive Lenses</font>
    </a>
  </h2>
  <font color="black">回折レンズの焦点距離は波長に依存するため、各測定は異なるぼやけたスペクトル成分の重ね合わせになります。最後に、開発された手法の有効性とパフォーマンスは、極端なさまざまな観測シナリオでの天体物理学イメージングのアプリケーションに示されています。紫外線（EUV）レジーム..これらの重ね合わせてぼやけた測定値から個々のスペクトル画像を再構成するために、乗算器の交互方向法を使用して事前分析を行う高速再構成アルゴリズムが開発されます。 
[ABSTRACT]新しいイメージングアプローチでは、重ね合わせてぼやけた測定に大きな制限があります。画像を再構成するための高速再構成アルゴリズムが開発されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: 70 years of machine learning in geoscience in review -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_7.html">
      <font color="black">70 years of machine learning in geoscience in review</font>
    </a>
  </h2>
  <font color="black">このレビューの範囲には、さまざまな浅い機械学習方法が含まれます。地球科学に関しては、レビューは地球物理学に偏っていますが、地球化学、地球統計学、地質学とのバランスを取ることを目的としていますが、これは範囲を超えるため、リモートセンシングを除外します。研究、ハードウェア、およびソフトウェア開発に関するディープラーニング。これにより、地球科学のあらゆる分野で浅いディープマシンラーニングを適切に適用できます。 
[要約]レビューでは、数学の基礎からソフトウェア開発の知識への移行を、モデル検証、応用統計、および統合された主題の専門知識のスキルに向けて検討します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-16">
        <br><font color="black">2020-06-16</font>
      </time>
    </span>
</section>
<!-- paper0: 5G Utility Pole Planner Using Google Street View and Mask R-CNN -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_8.html">
      <font color="black">5G Utility Pole Planner Using Google Street View and Mask R-CNN</font>
    </a>
  </h2>
  <font color="black">ついに、免疫アルゴリズムを使用してスマートシティに5Gポールを設定しました。第5世代（5G）セルラーネットワークテクノロジーの進歩により、スマートシティ用の5Gネットワークを構築する方法について多くの研究と研究が行われました。 。トレインエラー率は7.86％、テストエラー率は32.03％です。 
[ABSTRACT]トレインエラーレート7.86％を達成し、そのテスト方法をテストしました。ビルドするテストエラーレート32も達成しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Anime-to-Real Clothing: Cosplay Costume Generation via Image-to-Image
  Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_9.html">
      <font color="black">Anime-to-Real Clothing: Cosplay Costume Generation via Image-to-Image
  Translation</font>
    </a>
  </h2>
  <font color="black">この問題を解決するために、私たちの方法は、ウェブ画像を収集して前処理して、アニメとリアルドメインのクリーンなペアのデータセットを準備することから始まります。次に、高品質のコスプレ画像を容易にする生成敵対ネットワーク（GAN）の新しいアーキテクチャを提示します。生成..私たちのコードと事前トレーニング済みモデルは、ウェブで入手できます。 
[ABSTRACT]私たちの方法は、画像の幅広いバリエーションに基づいています。2つのドメイン間のギャップを埋め、生成された画像のグローバルな一貫性を向上させることが可能</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Fusion of Global-Local Features for Image Quality Inspection of Shipping
  Label -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_10.html">
      <font color="black">Fusion of Global-Local Features for Image Quality Inspection of Shipping
  Label</font>
    </a>
  </h2>
  <font color="black">さまざまな特徴空間でのオブジェクト検出とスケール不変特徴変換は、いくつかの独立した畳み込みニューラルネットワークからグローバルとローカルの特徴を抽出するために開発されています。実際にキャプチャおよび生成された画像に関する実験結果は、提案された方法が他の方法よりも優れたパフォーマンスを達成することを示しています。自動化された配送先住所の認識と検証の要求は、大量の荷物を処理し、誤配送に関連するコストを節約するために高まっています。 
[要約]結果は、分類された条件に基づいて異なる画像前処理ステップを適用することにより、配送先住所の認識および検証システムを改善することが期待されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Dimension Fusion Network for Light Field Spatial Super-Resolution
  using Dynamic Filters -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_11.html">
      <font color="black">Multi-Dimension Fusion Network for Light Field Spatial Super-Resolution
  using Dynamic Filters</font>
    </a>
  </h2>
  <font color="black">最後に、残差ブランチで学習されたより高周波の詳細がアップサンプリングされた画像に追加され、最終的な超解像ライトフィールドが取得されます。再構成された画像は、サブアパーチャ画像とエピポーラ平面画像の両方でシャープな詳細と明確なラインも示しています..実験結果は、提案された方法が使用するパラメーターが少ないことを示していますが、さまざまな種類のデータセットで他の最先端の方法よりも優れたパフォーマンスを実現しています。 
[ABSTRACT]ライトフィールド画像の解像度が限られているため、詳細情報の表示と抽出に多くの困難が伴います。これらには、多次元フュージョンアーキテクチャで並列に抽出および融合されたマイクロベースの画像が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Low Tensor Train- and Low Multilinear Rank Approximations for
  De-speckling and Compression of 3D Optical Coherence Tomography Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_12.html">
      <font color="black">Low Tensor Train- and Low Multilinear Rank Approximations for
  De-speckling and Compression of 3D Optical Coherence Tomography Images</font>
    </a>
  </h2>
  <font color="black">2人用<CR<60, the low S1 ML rank method compares favorably in terms of SE with image compression methods and with 2D BF and ELRpSD.. Thus, for 2<CR<60, the low S1 ML rank approximation can be considered a good choice for segmentation based diagnostics either on-site or in the remote mode of operation.. Rank constraints are implemented through the Schatten-p (Sp) norm, p e {0, 1/2, 2/3, 1}, of unfolded matrices.


[ABSTRACT]for 1 - cr - 60 , the low s1 ml rank method can be considered a good choice for segmentation based diagnostics either on- site or in the remote mode of operation</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Disentangled Representations for Domain-generalized Cardiac Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_13.html">
      <font color="black">Disentangled Representations for Domain-generalized Cardiac Segmentation</font>
    </a>
  </h2>
  <font color="black">私たちの広範な実験は、ロバストな心臓の画像セグメンテーションに対する、見られた領域と見えない領域の間の効率的な適応とモデルの汎化能力の重要性を示しています。この論文では、領域適応と汎化能力の向上に焦点を当てた2つのデータ拡張方法を提案します。最先端の心臓セグメンテーションモデル。その後、「因子ベースの増強」法は、絡み合っていない潜在空間に元のサンプルを投影し、異なるドメインから学習した解剖学的要素とモダリティ要素を組み合わせることにより、より多様なデータを生成します。 
[ABSTRACT]最近の作業は、データとスキャナー間のギャップを埋めるためのドメイン化と一般化に焦点を当てています。このメソッドは、さまざまなスキャナープロトコルにまたがる範囲内でさまざまな解像度に画像を再スケーリングすることにより、より多様なデータを生成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Stochastic Super-Resolution for Downscaling Time-Evolving Atmospheric
  Fields with a Generative Adversarial Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_14.html">
      <font color="black">Stochastic Super-Resolution for Downscaling Time-Evolving Atmospheric
  Fields with a Generative Adversarial Network</font>
    </a>
  </h2>
  <font color="black">2つのデータセットを使用してGANをテストします。1つはスイスからのレーダー測定降水量、もう1つは静止地球観測衛星16（GOES-16）から導出された雲の光学的厚さです。また、時系列よりもはるかに長い時系列を生成できます。降水量レーダーデータの3か月のデータセットにジェネレーターを適用することで示されるように、トレーニングシーケンス。GANジェネレーターは完全にたたみ込みなので、トレーニング後に、トレーニングに使用された画像よりも大きい入力画像に適用できます。 
[ABSTRACT]条件付きガンが特定の入力に対してソリューションの集合を生成する能力は、確率的ダウンスケーリングに役立ちます。しかし、ガンの確率的性質は通常、超解像アプリケーションでは考慮されません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-20">
        <br><font color="black">2020-05-20</font>
      </time>
    </span>
</section>
<!-- paper0: On the Composition and Limitations of Publicly Available COVID-19 X-Ray
  Imaging Datasets -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_15.html">
      <font color="black">On the Composition and Limitations of Publicly Available COVID-19 X-Ray
  Imaging Datasets</font>
    </a>
  </h2>
  <font color="black">データ不足、トレーニングとターゲット母集団間の不一致、グループの不均衡、およびドキュメントの欠如はバイアスの重要な原因であり、これらのモデルの実際の臨床診療への適用性を妨げています。特に、潜在的な可能性のある現在のデータセットのいくつかの主要なプロパティバイアスの原因、それらで訓練された障害モデルが指摘されています。イメージングデータからのCOVID-19の診断と進行予測のための機械学習ベースの方法は、特に深層学習モデルの使用により、ここ数か月間に大きな注目を集めています。 
[ABSTRACT]データセットはモデルの構築と評価の重要な部分です。これらの説明は、モデルの構築、モデルの目標に応じた最適なデータセットの選択、特定の制限を考慮して過剰なベンチマーク結果の報告を回避するのに役立ちます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: ARPM-net: A novel CNN-based adversarial method with Markov Random Field
  enhancement for prostate and organs at risk segmentation in pelvic CT images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_16.html">
      <font color="black">ARPM-net: A novel CNN-based adversarial method with Markov Random Field
  enhancement for prostate and organs at risk segmentation in pelvic CT images</font>
    </a>
  </h2>
  <font color="black">セグメンテーションネットワークは、新しく設計されたMRFブロックを複数の残差Uネットのバリエーションに統合します。弁別器は、元のCTと予測/グラウンドトゥルースの積を入力として受け取り、入力を偽/実数に分類します。Aセグメンテーションネットワークと弁別器ネットワークは共同でトレーニングされ、セグメンテーションネットワークのみが予測に使用されました。 
[ABSTRACT]無傷の前立腺癌患者110名の計画ctおよび構造データセットを遡及的に選択し、10倍の交差検証のために分割しました。弁別子は、元のctと等高線の予測の積を偽/実数にして、入力を分類します偽物または本物に</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Orientation-Disentangled Unsupervised Representation Learning for
  Computational Pathology -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_17.html">
      <font color="black">Orientation-Disentangled Unsupervised Representation Learning for
  Computational Pathology</font>
    </a>
  </h2>
  <font color="black">この構造化された表現を、核多形性と有糸分裂活動が専門の病理学者によって評価された組織領域で構成されるデータセットで評価しました。ここでは、回転等変畳み込みネットワークのグループ構造を活用して変分オートエンコーダフレームワークを拡張し、組織病理学画像の方向ごとに分解された生成要因を学習することを提案します。 
[ABSTRACT]そのようなモデルによって学習された表現は、その後の大きな画像データセットの分析を容易にすることができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: SNE-RoadSeg: Incorporating Surface Normal Information into Semantic
  Segmentation for Accurate Freespace Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_18.html">
      <font color="black">SNE-RoadSeg: Incorporating Surface Normal Information into Semantic
  Segmentation for Accurate Freespace Detection</font>
    </a>
  </h2>
  <font color="black">さらに、RoadSegと呼ばれるデータ融合CNNアーキテクチャを提案します。これは、RGB画像と推定された表面法線情報の両方から特徴を抽出して融合し、正確な自由空間検出を行うことができます。 、名前の付いた表面法線推定器（SNE）は、高密度の深度/視差画像から表面法線情報を高い精度と効率で推測できます。実験結果は、提案されたSNEモジュールがすべての最先端のCNNに利益をもたらすことを示しています。空き領域の検出、およびSNE-RoadSegは、異なるデータセット間で最高の全体的なパフォーマンスを実現します。 
[要約]このペーパーでは、まず、密な深さ/視差の画像から表面の法線情報を推測できる新しいモジュールを紹介します。研究目的で、準備完了（r2d）道路という名前の大規模な合成自由空間検出データセットを公開しますデータセット</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Many-shot from Low-shot: Learning to Annotate using Mixed Supervision
  for Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_19.html">
      <font color="black">Many-shot from Low-shot: Learning to Annotate using Mixed Supervision
  for Object Detection</font>
    </a>
  </h2>
  <font color="black">OAMとFast（er）R-CNNの統合により、PASCAL VOC 2007およびMS-COCOベンチマークで$ 17 \％$ mAP、$ 9 \％$ AP50のパフォーマンスが向上し、混合監視を使用する競合する方法を大幅に上回ります。完全に監視された2段階のオブジェクト検出メソッドと一緒にトレーニングでき、その場で追加のトレーニングアノテーションを提供します。ただし、これらのメソッドは、弱いトレーニング信号\ emph {しばしば}が部分的に発生するため、強力に監視された対応物と比べてパフォーマンスが大幅に低下します。または特大の検出。 
[ABSTRACT]注釈は非常に時間がかかり、費用がかかるため、ヤンの監視あり少数ショットのオブジェクト検出方法の開発に動機を与えます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-21">
        <br><font color="black">2020-08-21</font>
      </time>
    </span>
</section>
<!-- paper0: DeepSOCIAL: Social Distancing Monitoring and Infection Risk Assessment
  in COVID-19 Pandemic -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_20.html">
      <font color="black">DeepSOCIAL: Social Distancing Monitoring and Infection Risk Assessment
  in COVID-19 Pandemic</font>
    </a>
  </h2>
  <font color="black">これは、当局が公共の場所のレイアウトを再設計したり、リスクの高いゾーンを軽減するための予防措置を講じたりするのに役立つ可能性があります。提案されたモデルには、YOLOv4ベースのフレームワークと逆パースペクティブマッピングが含まれ、正確な人の検出と困難な状況での社会的距離の監視、人の閉塞、部分的な可視性、照明のバリエーションを含みます。提案された方法論の効率は、3つの最先端の方法と比較して、精度と速度の点で優れたパフォーマンスで、オックスフォードタウンセンターデータセットで評価されます。 
[ABSTRACT]大多数の政府と国の保健当局は、ショッピングセンター、学校、その他の対象地域における必須の安全対策として2メートルの物理的な距離を設定しています。提案されたモデルには、yolov4ベースのフレームワークと正確な人々のための逆透視マッピングが含まれます検出と社会的距離の監視</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: $k$-means on Positive Definite Matrices, and an Application to
  Clustering in Radar Image Sequences -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_21.html">
      <font color="black">$k$-means on Positive Definite Matrices, and an Application to
  Clustering in Radar Image Sequences</font>
    </a>
  </h2>
  <font color="black">これらのデータの自然で好ましい表現を提供する非ユークリッド空間での対称正定値（SPD）行列の$ k $平均クラスタリングの理論的特性を示します。次に、この方法の新しいアプリケーションを提供して、有限遅延自己共分散行列を介した、合成開口レーダー画像のシーケンスにおけるピクセルの時系列クラスタリング。 
[要約]次に、この方法の新しいアプリケーションを提供し、ピクセルの時系列クラスタリングを行います</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-08">
        <br><font color="black">2020-08-08</font>
      </time>
    </span>
</section>
<!-- paper0: DRR4Covid: Learning Automated COVID-19 Infection Segmentation from
  Digitally Reconstructed Radiographs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_22.html">
      <font color="black">DRR4Covid: Learning Automated COVID-19 Infection Segmentation from
  Digitally Reconstructed Radiographs</font>
    </a>
  </h2>
  <font color="black">DRR4Covidと呼ばれる新しいアプローチを提案し、デジタル再構成されたラジオグラフ（DRR）から自動化されたCOVID-19診断とCXRの感染セグメンテーションを学習します。実際のCXRとラベル付きDRRを一緒に使用します。最大平均不一致（MMD）に基づくドメイン適応モジュール、および分類ヘッダーとセグメンテーションヘッダーを持つFCNベースのネットワークを使用して、DRR4Covidのシンプルで効果的な実装を提供します。自動化された感染測定胸部X線（CXR）イメージングに基づくCOVID-19診断は、より迅速な検査のために重要です。 
[要約] drr4covidは、デジタルで再構成された放射線写真（drrs）からcxrsを使用して自動化されたcovidを学習するツールです。drr4covidは、covid-19感染の強度が調整可能なcxrsを生成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: 3D Face Anti-spoofing with Factorized Bilinear Coding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.IV/paper_23.html">
      <font color="black">3D Face Anti-spoofing with Factorized Bilinear Coding</font>
    </a>
  </h2>
  <font color="black">この作業では、これらの現実的な3D顔提示攻撃を検出する問題に取り組み、細かい分類の観点から新しいスプーフィング防止方法を提案します。画像と画像の両方を備えた大規模なワックスフィギュア顔データベース（WFFD）ビデオも超リアルな攻撃として収集され、3D顔提示攻撃検出の研究を容易にします。広範な実験結果は、提案された方法が、独自のWFFDと他の顔なりすましデータベースの両方で最先端のパフォーマンスを達成することを示しています。さまざまなデータベース内およびデータベース間のテストシナリオの下。 
[要約]顔認識システムは、実際の顔に似たマテリアルの3D特性により混乱しやすくなります。複数のカラーチャネルの因数分解バイリニアコーディングに基づく当社の方法は、実際の画像と偽の画像の微妙な違いを学習することを目的としています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-12">
        <br><font color="black">2020-05-12</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: A Prospective Study on Sequence-Driven Temporal Sampling and Ego-Motion
  Compensation for Action Recognition in the EPIC-Kitchens Dataset -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_0.html">
      <font color="black">A Prospective Study on Sequence-Driven Temporal Sampling and Ego-Motion
  Compensation for Action Recognition in the EPIC-Kitchens Dataset</font>
    </a>
  </h2>
  <font color="black">推定は、ビデオシーケンスを動き補償された一時的な\ textit {chunks}に時間的に分割するために使用され、安定したバックグラウンドでのアクションを示し、コンテンツ主導の時間的なサンプリングを可能にします。エンドツーエンドの方法でトレーニングされたCNNは、各\ textit {chunk}から時間的特徴を抽出します。これは、後で融合されます。行動認識データセットの中で、自我中心の記録されたシーケンスは、追加の課題を伴いながら重要な関連性を持つようになりました。 
[ABSTRACT] convoluitニューラルネットワーク（cnns）はそのパフォーマンスを大幅に向上させましたが、fixed-size spatio-invisible windows of analysis.proposedメソッドは、このエゴ-モーションまたはカメラモーションを推定することで対処することを目的としています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Synthetic Sample Selection via Reinforcement Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_1.html">
      <font color="black">Synthetic Sample Selection via Reinforcement Learning</font>
    </a>
  </h2>
  <font color="black">トランスフォーマーベースのコントローラーは、検証分類の精度を報酬として使用して、近接ポリシー最適化（PPO）によってトレーニングされます。選択された画像は、画像認識システムのトレーニングを改善するために元のトレーニングデータと混合されます。提案された合成サンプルの選択方法は一般的です限られた注釈が付けられたさまざまな医用画像認識システムのパフォーマンスを向上させる大きな可能性があります。 
[ABSTRACT]合成画像は現実的ではなく、実際の画像と混合するとデータの分布を歪める誤解を招く機能が含まれる場合があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Predict Context-adaptive Convolution for Semantic
  Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_2.html">
      <font color="black">Learning to Predict Context-adaptive Convolution for Semantic
  Segmentation</font>
    </a>
  </h2>
  <font color="black">CaC-Netでは、コンテキスト適応型畳み込みカーネルのセットが、グローバルなコンテキスト情報からパラメーター効率の良い方法で予測されます。意味論的機能マップとの畳み込みに使用すると、予測された畳み込みカーネルは、空間的に変化する機能の重み付けを生成できます。グローバルとローカルの両方のコンテキスト情報をキャプチャする要因。包括的な実験結果は、CaC-Netが3つのパブリックデータセット、PASCAL Context、PASCAL VOC 2012、ADE20Kで優れたセグメンテーションパフォーマンスを実現することを示しています。 
[ABSTRACT] vision-アダプティブコンボリューションネットワーク（cac-net）は、空間的に変化する特徴の重み付けコンポーネントを予測します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-17">
        <br><font color="black">2020-04-17</font>
      </time>
    </span>
</section>
<!-- paper0: How Do the Hearts of Deep Fakes Beat? Deep Fake Source Detection via
  Interpreting Residuals with Biological Signals -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_3.html">
      <font color="black">How Do the Hearts of Deep Fakes Beat? Deep Fake Source Detection via
  Interpreting Residuals with Biological Signals</font>
    </a>
  </h2>
  <font color="black">一部の純粋なディープラーニングベースのアプローチは、実際にジェネレーターの残差を学習するCNNを使用してディープフェイクを分類しようとします。この結果は、このアプローチが97.29％の精度でフェイクビデオを検出し、93.39％の精度でソースモデルを検出できることを示しています。フェイク肖像画のビデオ生成技術は、政治的宣伝、有名人の模倣、偽造された証拠、およびその他のアイデンティティ関連の操作のための写実的な深い偽物によって、社会に新たな脅威をもたらしています。 
[ABSTRACT]ディープフェイクを実際の動画から分離するアプローチを提案するだけでなく、ディープフェイクの背後にある特定のソースを発見することもお勧めします。ディープフェイクにはより多くの情報が含まれており、これらの操作のアーティファクトを生体信号ともつれを解くことで明らかにできます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: RNN-based Pedestrian Crossing Prediction using Activity and Pose-related
  Features -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_4.html">
      <font color="black">RNN-based Pedestrian Crossing Prediction using Activity and Pose-related
  Features</font>
    </a>
  </h2>
  <font color="black">提案されたモデルは、CNNベースの特徴抽出器とRNNモジュールの2つの部分で構成されています。得られた結果は、特徴抽出方法の選択、歩行者の視線方向や離散方向などの追加変数の組み込み、および選択したRNNタイプは、最終的なパフォーマンスに大きな影響を与えます。すべてのモデルは、JAADデータセットでトレーニングおよびテストされました。 
[要約]提案されたモデルはcnnベースの特徴抽出器とrnnモジュールで構成されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: PASS3D: Precise and Accelerated Semantic Segmentation for 3D Point Cloud -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_5.html">
      <font color="black">PASS3D: Precise and Accelerated Semantic Segmentation for 3D Point Cloud</font>
    </a>
  </h2>
  <font color="black">PASS3Dは、KITTIの未加工データセットで評価され、いくつかの結果で最先端の技術と比較して際立っており、自動運転システムでの3D知覚に適しています。ソースコードはオープンソースになります。 https://www.youtube.com/watch?v=cukEqDuP_Qw。 
[ABSTRACT]このシステムは、従来の幾何学的手法の効率とディープラーニング手法の堅牢性を組み合わせています。ステージ1では、加速されたクラスター提案アルゴリズムが、地面のない点群をセグメント化することにより、洗練された提案を生成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-04">
        <br><font color="black">2019-09-04</font>
      </time>
    </span>
</section>
<!-- paper0: An End-to-End Attack on Text-based CAPTCHAs Based on Cycle-Consistent
  Generative Adversarial Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_6.html">
      <font color="black">An End-to-End Attack on Text-based CAPTCHAs Based on Cycle-Consistent
  Generative Adversarial Network</font>
    </a>
  </h2>
  <font color="black">さらに、この方法には高い移植性があります。結果は、より多くの認識防止メカニズムの組み合わせがCAPTCHAのセキュリティを改善できることを示していますが、改善は制限されています。それは、いくつかの修正によってのみ、一般的なテキストベースのCAPTCHAスキームを攻撃できます。攻撃を容易にする設定パラメータ。 
[ABSTRACT]研究者は、さまざまな企業によって展開されたテキストベースのキャプチャについて攻撃調査を実施しています。しかし、これらの攻撃のほとんどには、攻撃方法の移植性の悪さ、一連のデータ前処理手順の必要性、大量のラベル付きラベルへの依存など、いくつかの欠点がありますキャプチャ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Selective Particle Attention: Visual Feature-Based Attention in Deep
  Reinforcement Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_7.html">
      <font color="black">Selective Particle Attention: Visual Feature-Based Attention in Deep
  Reinforcement Learning</font>
    </a>
  </h2>
  <font color="black">この質問への回答を助けるために、選択的粒子ベースの注意（SPA）と呼ばれる新しいアルゴリズムを提案します。これは、Deep RLエージェントに選択的な機能ベースの注意を実行する能力を吹き込みます。それにもかかわらず、脳がどのようにできるかは未解決の問題です私たちの結果は、（1）Deep RLモデルの視覚的特徴に基づく注意が学習効率とタスク構造の突然の変化に対処する能力をどのように改善できるか、および（2）パーティクルフィルターが視覚的特徴に基づく注意が脳でどのように発生するかについて、実行可能な計算アカウントを表します。 
[要旨]特徴ベースの注意と呼ばれる特定の形式の視覚的注意に焦点を当てます。空間的な場所に関係なく、現在のタスクに重要な視覚入力の特徴を特定することに焦点を当てます。彼らの下-顕著性と彼らが将来の報酬をどれだけ正確に予測するか</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Better Than Reference In Low Light Image Enhancement: Conditional
  Re-Enhancement Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_8.html">
      <font color="black">Better Than Reference In Low Light Image Enhancement: Conditional
  Re-Enhancement Networks</font>
    </a>
  </h2>
  <font color="black">次に、データ駆動型の条件付き再強化ネットワーク（CRENetと表記）が提案されます。トレーニングプロセス中に、露出時間の異なるペアリングされた画像をトレーニングに使用でき、慎重に行う必要がないことに注意してください。多くの節約になる監視画像を選択します。最初に、HSV色空間とRetinex理論の関係を分析し、Vチャネル（HSV色空間のVチャネル、RGB色空間の最大チャネルに等しい）を示します。強化された画像のコントラストは、コントラストと輝度の強化プロセスを適切に表すことができます。 
[ABSTRACT]低照度画像と強化されたvチャネルは、低照度画像のコントラストと明るさを再強化し、同時にノイズと色の歪みを低減できます。他の画像のコントラスト強調方法と組み合わせて、画質を向上させることができます強化された画像</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Buy Me That Look: An Approach for Recommending Similar Fashion Products -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_9.html">
      <font color="black">Buy Me That Look: An Approach for Recommending Similar Fashion Products</font>
    </a>
  </h2>
  <font color="black">このドメインの既存の作業の大部分は、クエリ内に存在する単一のアイテムに対応する類似の製品の取得に焦点を当てていますが、一度に複数のファッションアイテムの取得に焦点を当てています。トリプレットベースのニューラルネットワークを利用して、 embeddings .. PDPでフルショットの外観の画像を見ると、ユーザーは二次的な記事タイプの同様のアイテムを表示することに興味を持つかもしれません。 
[ABSTRACT]オンライン小売販売には、収益を高め、顧客体験とエンゲージメントを向上させることができる効率的で効果的なファッション製品の推奨システムが必要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: 70 years of machine learning in geoscience in review -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_10.html">
      <font color="black">70 years of machine learning in geoscience in review</font>
    </a>
  </h2>
  <font color="black">このレビューの範囲には、さまざまな浅い機械学習方法が含まれます。ディシジョンツリー、ランダムフォレスト、サポートベクターマシン、ガウスプロセス、およびフィードフォワードニューラルネットワーク、畳み込みニューラルネットワーク、リカレントニューラルネットワーク、生成的敵対ネットワークなどのディープニューラルネットワーク。地球科学については、レビューに地球物理学に偏っていますが、地球化学、地球統計学、地質学とのバランスを取ることを目的としていますが、これは範囲を超えるため、リモートセンシングを除外しています。 
[要約]レビューでは、数学の基礎からソフトウェア開発の知識への移行を、モデル検証、応用統計、および統合された主題の専門知識のスキルに向けて検討します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-16">
        <br><font color="black">2020-06-16</font>
      </time>
    </span>
</section>
<!-- paper0: 5G Utility Pole Planner Using Google Street View and Mask R-CNN -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_11.html">
      <font color="black">5G Utility Pole Planner Using Google Street View and Mask R-CNN</font>
    </a>
  </h2>
  <font color="black">最後に、免疫アルゴリズムを使用してスマートシティに5Gポールを設定しました。列車のエラー率は7.86％、テストのエラー率は32.03％でした。ポイントの位置を特定するために、このホワイトペーパーでは、マスクR-CNNに基づいて極を識別する新しい方法。これは、再帰的なベイジアンフィルタリングを採用して提案の伝播と再利用を実行することにより、Fast R-CNNを拡張します。 
[ABSTRACT]トレインエラーレート7.86％を達成し、そのテスト方法をテストしました。ビルドするテストエラーレート32も達成しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Vehicle Trajectory Prediction in Crowded Highway Scenarios Using Bird
  Eye View Representations and CNNs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_12.html">
      <font color="black">Vehicle Trajectory Prediction in Crowded Highway Scenarios Using Bird
  Eye View Representations and CNNs</font>
    </a>
  </h2>
  <font color="black">モデルは高速道路シナリオでテストされ、30台を超える車両が2つの反対の交通流ストリームで同時に質的および量的な結果を示しています。追加のステップが実行され、予測された表現からサブピクセル解像度で位置を抽出します。次にU-ネットモデルは、シーケンス間予測を実行するために使用されます。 
[要旨]車両は、ガウス分布を使用して鳥瞰図に表されます。これは、航空写真を使用する高データセットに基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Detection of Genuine and Posed Facial Expressions of Emotion: A Review -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_13.html">
      <font color="black">Detection of Genuine and Posed Facial Expressions of Emotion: A Review</font>
    </a>
  </h2>
  <font color="black">さらに、SVP検出方法のパフォーマンスに影響を与えるさまざまな要因について、未解決の問題や技術的な課題とともに説明します。感情の表情は、人間の社会的相互作用において重要な役割を果たします。したがって、表情の信頼性評価は、つまり、本物の（自発的な）表情とポーズ（意図的/意志的/欺瞞的）の表情を区別することは、表情を理解する上で非常に重要な課題です。 
[要約]近年、本物やポーズの表情の自動検出が急速に進歩しています。調査によると、表情は意図的な感情と必ずしも同じではないことが判明しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Anime-to-Real Clothing: Cosplay Costume Generation via Image-to-Image
  Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_14.html">
      <font color="black">Anime-to-Real Clothing: Cosplay Costume Generation via Image-to-Image
  Translation</font>
    </a>
  </h2>
  <font color="black">私たちのコードと事前トレーニング済みのモデルはウェブで入手できます。次に、生成的敵対的ネットワーク（GAN）の新しいアーキテクチャを提示して、高品質のコスプレ画像の生成を容易にします。この問題を解決するために、私たちの方法は、ウェブ画像を収集して前処理することから始まりますアニメと実ドメインのクリーンでペアになったデータセットを準備します。 
[ABSTRACT]私たちの方法は、画像の幅広いバリエーションに基づいています。2つのドメイン間のギャップを埋め、生成された画像のグローバルな一貫性を向上させることが可能</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: DeepRhythm: Exposing DeepFakes with Attentional Visual Heartbeat Rhythms -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_15.html">
      <font color="black">DeepRhythm: Exposing DeepFakes with Attentional Visual Heartbeat Rhythms</font>
    </a>
  </h2>
  <font color="black">この作業では、DeepRhythmを提案します。これは、心拍のリズムを監視することでDeepFakeを公開するDeepFake検出手法です。 {DeepRhythm}は、さまざまなDeepFakes生成手法と多種多様な挑戦的な分解によるさまざまなデータセットを対象としています。DeepRhythmは、デュアル空間時間的注意を利用して、動的に変化する顔と偽のタイプに適応します。 
[ABSTRACT] deeprhythmのディープフェイクビデオは、ディープフェイク検出のツールと見なすことができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-13">
        <br><font color="black">2020-06-13</font>
      </time>
    </span>
</section>
<!-- paper0: Semantic Graph Based Place Recognition for 3D Point Clouds -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_16.html">
      <font color="black">Semantic Graph Based Place Recognition for 3D Point Clouds</font>
    </a>
  </h2>
  <font color="black">したがって、場所の認識はグラフマッチングの問題としてモデル化されます。オクルージョンと視点の変化にロバストな効果的な記述子を生成することが難しいため、3D点群の場所の認識は未解決の問題です。KITTIデータセットの徹底的な評価は、私たちのアプローチはオクルージョンや視点の変化に対して堅牢であり、最新の方法よりもはるかに優れています。 
[要約]私たちの方法は、環境の変化に対する堅牢性の点で優れているセマンティックレベルを目指しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: A Universal Representation Transformer Layer for Few-Shot Image
  Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_17.html">
      <font color="black">A Universal Representation Transformer Layer for Few-Shot Image
  Classification</font>
    </a>
  </h2>
  <font color="black">ここでは、ユニバーサル表現トランスフォーマー（URT）レイヤーを提案します。これは、動的に再重み付けし、最も適切なドメイン固有の表現を構成することにより、少数ショット分類のユニバーサル機能を活用するためにメタ学習します。 Meta-Datasetの新しい最先端の結果。少数ショットの分類は、サンプル数が少ない場合に表示されないクラスを認識することを目的としています。 
[ABSTRACT]このマルチドメイン設定の重要な課題は、さまざまなトレーニングドメインのセットからその機能を効果的に統合することです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-21">
        <br><font color="black">2020-06-21</font>
      </time>
    </span>
</section>
<!-- paper0: Complementary Boundary Generator with Scale-Invariant Relation Modeling
  for Temporal Action Localization: Submission to ActivityNet Challenge 2020 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_18.html">
      <font color="black">Complementary Boundary Generator with Scale-Invariant Relation Modeling
  for Temporal Action Localization: Submission to ActivityNet Challenge 2020</font>
    </a>
  </h2>
  <font color="black">具体的には、高品質の提案を生成するために、ビデオフィーチャーエンコーダー、提案ジェネレーター、提案と提案の関係、スケールの不均衡、およびアンサンブル戦略を含むいくつかの要因を考慮します。提案されたスキームは、チャレンジテストセットでの\ textbf {42.26}平均mAPを使用した時間アクションローカリゼーションタスクのアートパフォーマンス。提案の生成と分類）。異なるが補完的な観点から複数のコンポーネントの影響を徹底的に調査することにより、提案の多様性を豊かにします。 
[ABSTRACT]プロポーザルプロポーザルプロポーザルでは、アクションの境界を正確に示す必要があります。生成されたプロポーザルを認識するために、最適なビデオ分類子をさらにトレーニングする必要があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-20">
        <br><font color="black">2020-07-20</font>
      </time>
    </span>
</section>
<!-- paper0: InfoMax-GAN: Improved Adversarial Image Generation via Information
  Maximization and Contrastive Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_19.html">
      <font color="black">InfoMax-GAN: Improved Adversarial Image Generation via Information
  Maximization and Contrastive Learning</font>
    </a>
  </h2>
  <font color="black">これは、GANに対照的な学習と相互情報の最大化アプローチを採用することでこれを達成し、改善の原因を理解するために広範な分析を実行します。生成的敵対的ネットワーク（GAN）は多くの生成的モデリングアプリケーションの基本ですが、多くの問題に悩まされています。アプローチは、GANトレーニングを大幅に安定させ、最新の作品に対する同じトレーニングおよび評価条件下で5つのデータセットにわたる画像合成のGANパフォーマンスを向上させます。 
[要約]ガンズの2つの基本的な問題に同時に対処するための原則的なフレームワークを提案します。ガンズの2つの基本的な課題に対処することを目的としています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-09">
        <br><font color="black">2020-07-09</font>
      </time>
    </span>
</section>
<!-- paper0: Effective Action Recognition with Embedded Key Point Shifts -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_20.html">
      <font color="black">Effective Action Recognition with Embedded Key Point Shifts</font>
    </a>
  </h2>
  <font color="black">キーポイントシフトは、複数セットの方法で線形埋め込み層を介して全体的な時間的特徴としてエンコードされます。キーポイントシフトは、分割領域で最大の特徴値を持つ特徴点として適応的に抽出され、キーポイントシフトは対応するキーの空間変位ですpoints ..この論文では、Key Point Shifts Embedding Module（$ KPSEM $）という名前の新しい時間的特徴抽出モジュールを提案し、時間的特徴抽出のためのキーポイントアノテーションなしでビデオフレーム全体のチャネル単位のキーポイントシフトを適応的に抽出します。 
[要旨]スケルトンベースのアクション認識メソッドでキーポイントが使用されています。ただし、コストのかかるキーポイントアノテーションが必要です。このメソッドは、キーポイントシフトを埋め込むことで競争力のあるパフォーマンスを実現します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Dual Graph Convolutional Network for Semantic Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_21.html">
      <font color="black">Dual Graph Convolutional Network for Semantic Segmentation</font>
    </a>
  </h2>
  <font color="black">コードとモデルは、さらなる研究を促進するために利用可能になります（\ url {https://github.com/lxtGH/GALD-DGCNet}）。これは、すべての新しい低次元空間に機能を投影することで効率的に行われますペアワイズインタラクションは、元の空間に再投影する前にモデル化できます。このシンプルな方法は、強力なベースラインを超える実質的なメリットを提供し、Cityscapes（82.0％の平均IoU）とPascalコンテキスト（53.7％の平均）の両方で最先端の結果を実現します。 IoU）データセット。 
[ABSTRACT]新しいグラフ-畳み込みネットワーク（gcn）は、この問題に対処するように設計されています。これは、ネットワークのネットワークの最初のコンポーネントであり、問題に対処します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-13">
        <br><font color="black">2019-09-13</font>
      </time>
    </span>
</section>
<!-- paper0: Interactive Knowledge Distillation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_22.html">
      <font color="black">Interactive Knowledge Distillation</font>
    </a>
  </h2>
  <font color="black">方法では、教師の強力な機能変換機能を直接利用して、生徒のパフォーマンスを大幅に向上させます。教師と生徒のネットワークの一般的な設定を使った実験では、IAKDによってトレーニングされた生徒のネットワークが、従来の知識抽出方法によってトレーニングされたものよりも優れたパフォーマンスを実現している多様な画像分類データセットで。蒸留プロセスでは、教師と生徒のネットワーク間の相互作用は、スワップイン操作によって実装されます。生徒のネットワークのブロックを教師のネットワークの対応するブロックにランダムに置き換えます。 
[要約]蒸留では、教師と生徒のネットワーク間の相互作用は、教師ネットワークのブロックを交換することによって実装されます。生徒のネットワークは、さまざまな画像分類データセットで従来の知識蒸留法によってトレーニングされたものよりも優れたパフォーマンスを実現しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-03">
        <br><font color="black">2020-07-03</font>
      </time>
    </span>
</section>
<!-- paper0: Fusion of Global-Local Features for Image Quality Inspection of Shipping
  Label -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_23.html">
      <font color="black">Fusion of Global-Local Features for Image Quality Inspection of Shipping
  Label</font>
    </a>
  </h2>
  <font color="black">実際にキャプチャおよび生成された画像に関する実験結果は、提案された方法が他の方法よりも優れたパフォーマンスを実現することを示しています。異なる特徴空間でのオブジェクト検出およびスケール不変特徴変換は、いくつかの独立した畳み込みニューラルネットワークからグローバルおよびローカル特徴を抽出するために開発されています。ラベル画像の発送の条件は、グローバルとローカルの機能が連結された完全に接続されたフュージョンレイヤーによって分類されます。 
[要約]結果は、分類された条件に基づいて異なる画像前処理ステップを適用することにより、配送先住所の認識および検証システムを改善することが期待されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Orientation-aware Vehicle Re-identification with Semantics-guided Part
  Attention Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_24.html">
      <font color="black">Orientation-aware Vehicle Re-identification with Semantics-guided Part
  Attention Network</font>
    </a>
  </h2>
  <font color="black">広範な実験により、提案された方法の有効性が検証され、フレームワークが最先端のアプローチよりも優れていることが示されています。トレーニング中に画像レベルのセマンティックラベルのみが与えられた車両のさまざまなビュー。いくつかの研究では、車両の再IDに役立つ空間注意メカニズムが組み込まれていますが、高価なキーポイントラベルが必要な場合や、高価なラベルでトレーニングされていない場合、ノイズの多い注意マスクに悩まされることがよくあります。 
[ABSTRACT]パーツアテンションネットワーク（スパン）は、トレーニング中に画像レベルのセマンティックラベルのみが指定された車両のさまざまなビューのパーツアテンションマスクを予測できます。パーツパーツ識別（cpdm）は、特徴の距離を評価するときに、共起車両パーツをより重視します。 2つの画像</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Discriminative Cross-Domain Feature Learning for Partial Domain
  Adaptation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_25.html">
      <font color="black">Discriminative Cross-Domain Feature Learning for Partial Domain
  Adaptation</font>
    </a>
  </h2>
  <font color="black">具体的には、重み付けされたクロスドメインセンター損失と重み付けされたクロスドメイングラフの伝播が提案され、ラベルのないターゲットデータを関連するソースサンプルに結合して、関係のないソースセンターが無視され、限界と条件付きの差異が緩和されます。同時にいくつかの人気のあるベンチマークの実験的評価により、ラベルなしのターゲットドメインの認識を促進するための提案されたアプローチの有効性が、それを最先端の部分ドメイン適応アプローチと比較することによって実証されています。ターゲットデータを少数のソースデータのみに揃えます。 
[ABSTRACT]新しいプラクティスでは、ターゲットの疑似ラベルを使用して、クロスドメイン分布の分散をうまく回避しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: DRG: Dual Relation Graph for Human-Object Interaction Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_26.html">
      <font color="black">DRG: Dual Relation Graph for Human-Object Interaction Detection</font>
    </a>
  </h2>
  <font color="black">私たちが提案した二重関係グラフは、シーンから識別的な手がかりを効果的にキャプチャして、ローカル予測からのあいまいさを解決します。 ..このホワイトペーパーでは、抽象的な空間的意味表現を利用して、人間とオブジェクトの各ペアを記述し、デュアル関係グラフ（1つは人間中心、もう1つはオブジェクト中心）を介してシーンのコンテキスト情報を集約します。 
[ABSTRACT]既存の方法は、各人間の相互作用を認識します-セントを孤立させるか、共同の結論を出します。提案された二重関係グラフは、二重関係情報を効果的にキャプチャ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Cross-Spectral Periocular Recognition with Conditional Adversarial
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_27.html">
      <font color="black">Cross-Spectral Periocular Recognition with Conditional Adversarial
  Networks</font>
    </a>
  </h2>
  <font color="black">これに加えて、ResNet50アーキテクチャに基づいてCNNを微調整し、EER = 1％、GAR&gt; 99％@ FAR = 1％のクロススペクトルペリオキュラーパフォーマンスを取得します。これは、 theUart with the PolyU database ..ベンチマークデータセットとしてHong Kong Polytechnic University Cross-Spectral Iris Images Database（PolyU）を使用すると、両方の画像を同じスペクトルに変換すると、クロススペクトルパフォーマンスが大幅に向上することが実験で示されています。異なるスペクトルの画像から抽出された一致する特徴との比較。認識実験は、手作りの特徴とCNN記述子の両方に基づいた、市販の周縁コンパレータを使用して行われます。 
[概要]このシステムは、眼周囲の画像を可視スペクトルと近赤外スペクトルの間で変換するために使用されます。生体認証は同じスペクトルで実行されるため、生体認証が実行されます。プロジェクトは、生体認証の新しいシステムの開発に使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Each Part Matters: Local Patterns Facilitate Cross-view Geo-localization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_28.html">
      <font color="black">Each Part Matters: Local Patterns Facilitate Cross-view Geo-localization</font>
    </a>
  </h2>
  <font color="black">追加の部品推定器を使用せずに、LPNは、画像中心までの距離に応じて注意を提供する正方形リング機能分割戦略を採用しています。具体的には、ローカルパターンネットワーク（LPN）と呼ばれるシンプルで効果的なディープニューラルネットワークを導入します。エンドツーエンドの方法でコンテキスト情報を利用します。この作業では、近隣地域を補助情報として活用して、地理的位置特定の手がかりを豊かにすることができると主張します。 
[要約]提案されたプロジェクトは、大きな画像の数を減らすために呼び出されました。現在、建築家の大学によって開発されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Gesture Recognition from Skeleton Data for Intuitive Human-Machine
  Interaction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_29.html">
      <font color="black">Gesture Recognition from Skeleton Data for Intuitive Human-Machine
  Interaction</font>
    </a>
  </h2>
  <font color="black">時系列ドメインに対するリカレントニューラルネットワークの最近の成功に勇気づけられ、我々はまた、長い時間スケールで時間関係を学習する能力を示した双方向の長期短期メモリセルに基づく同時ジェスチャセグメンテーションと分類の方法を提案します。 。最後に、認識されたジェスチャーを使用して、協調型ロボットとやり取りします。次に、さまざまな期間のウィンドウを組み合わせて、複数時間のスケールアプローチとパフォーマンスのさらなる向上を実現します。 
[ABSTRACT]最も効果的な方法は、0。75のジャカードインデックスです。これは、最先端の技術によって提示されるパフォーマンスとほぼ対になるパフォーマンスを示唆しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Dimension Fusion Network for Light Field Spatial Super-Resolution
  using Dynamic Filters -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_30.html">
      <font color="black">Multi-Dimension Fusion Network for Light Field Spatial Super-Resolution
  using Dynamic Filters</font>
    </a>
  </h2>
  <font color="black">最後に、残差分岐で学習されたより多くの高周波の詳細がアップサンプリングされた画像に追加され、最終的な超解像ライトフィールドが取得されます。実験結果は、提案された方法はより少ないパラメーターを使用するが、他の状態よりも優れたパフォーマンスを達成することを示しています。さまざまな種類のデータセットの最新のメソッド。これらの機能は、動的フィルターを生成するために使用されます。動的フィルターは、マイクロレンズ画像からサブピクセル情報を抽出し、視差情報も暗黙的に考慮します。 
[ABSTRACT]ライトフィールド画像の解像度が限られているため、詳細情報の表示と抽出に多くの困難が伴います。これらには、多次元フュージョンアーキテクチャで並列に抽出および融合されたマイクロベースの画像が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: NAS-DIP: Learning Deep Image Prior with Neural Architecture Search -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_31.html">
      <font color="black">NAS-DIP: Learning Deep Image Prior with Neural Architecture Search</font>
    </a>
  </h2>
  <font color="black">一般的なU-Netアーキテクチャを基盤として、私たちの中心的な貢献は、（1）アップサンプリングセルと（2）クロススケール残差接続のパターンのための新しいサーチスペースの設計にあります。画像の復元、曇りの除去、画像から画像への変換、行列の因数分解などのアプリケーションの組み合わせ。 
[ABSTRACT]私たちのアルゴリズムは、最先端の学習無料のアプローチに対して有利に機能します。これは、より強力な画像の事前情報をキャプチャする新しい領域を検索するために使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Disentangled Representations for Domain-generalized Cardiac Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_32.html">
      <font color="black">Disentangled Representations for Domain-generalized Cardiac Segmentation</font>
    </a>
  </h2>
  <font color="black">その後、「因子ベースの増大」法は、元のサンプルをもつれのない潜在空間に投影し、異なるドメインから学習した解剖学とモダリティの因子を組み合わせることにより、より多様なデータを生成します。この論文では、最先端の心臓セグメンテーションモデルのドメイン適応と汎化能力の向上。私たちの広範な実験は、見られたドメインと目に見えないドメインの間の効率的な適応と、モデルの汎化能力の堅牢な心臓画像セグメンテーションへの重要性を示しています。 
[ABSTRACT]最近の作業は、データとスキャナー間のギャップを埋めるためのドメイン化と一般化に焦点を当てています。このメソッドは、さまざまなスキャナープロトコルにまたがる範囲内でさまざまな解像度に画像を再スケーリングすることにより、より多様なデータを生成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: EPI-based Oriented Relation Networks for Light Field Depth Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_33.html">
      <font color="black">EPI-based Oriented Relation Networks for Light Field Depth Estimation</font>
    </a>
  </h2>
  <font color="black">ライトフィールドのユニークな2D空間角スライスであるエピポーラ平面画像（EPI）には、方向付けられたラインのパターンが含まれます。ただし、これらの方法では、隣接するピクセル間の関係を無視しながら、EPIからラインの最適な傾きを抽出することがよくあります。深度マップの予測が不正確になります。広範な実験により、学習関係の有効性が検証され、私たちのアプローチが他の最先端の方法と競合することが示されています。 
[ABSTRACT]隣接する画像の位置は視差に関連付けられています。ただし、これらの方法では、隣接するピクセル間の関係を無視しながら、エピスからラインの最適な勾配を抽出することが多いため、デプスマップの予測が不正確になります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-09">
        <br><font color="black">2020-07-09</font>
      </time>
    </span>
</section>
<!-- paper0: Performance Optimization for Federated Person Re-identification via
  Benchmark Analysis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_34.html">
      <font color="black">Performance Optimization for Federated Person Re-identification via
  Benchmark Analysis</font>
    </a>
  </h2>
  <font color="black">このベンチマークは、（1）実際に異種の状況をシミュレートするために異なるドメインから供給された異なるボリュームを持つ9つのデータセット、（2）2つの連合シナリオ、および（3）FedReIDの強化された連合アルゴリズムで構成されています。ベンチマーク分析では、クライアントがデータセットフェデレーションシナリオで表されるエッジクラウドアーキテクチャは、FedReIDのクライアントサーバーアーキテクチャよりも優れたパフォーマンスを発揮します。フェデレーションラーニングは、分散型クライアント全体で共有モデルを学習する、プライバシーを保護する機械学習手法です。 
[要約]まず、fedreidのパフォーマンスを調査するための新しいベンチマークを作成します。モデルは、クライアント、エッジクラウドアーキテクチャがfedreidのクライアント-サーバーアーキテクチャよりも優れていることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Keypoint-Aligned Embeddings for Image Retrieval and Re-identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_35.html">
      <font color="black">Keypoint-Aligned Embeddings for Image Retrieval and Re-identification</font>
    </a>
  </h2>
  <font color="black">より具体的には、KAE-Netは、このキーポイントのヒートマップ再構築の補助タスクを学習することにより、特定のキーポイントによってアクティブ化された機能マップからチャネルを抽出します。KAE-Netは、コンパクトで汎用的で概念的にシンプルです。学習に不変な埋め込みオブジェクトのポーズは、視覚的な画像の検索と再識別に不可欠です。 
[ABSTRACT]提案されたキーポイント整合埋め込みモデル（kae-net）は、キーポイントの位置から部品レベルの注文順序を学習します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Generating Handwriting via Decouple Style Descriptors -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_36.html">
      <font color="black">Generating Handwriting via Decouple Style Descriptors</font>
    </a>
  </h2>
  <font color="black">実験では、生成された結果は88％の時間で最新のベースライン方法よりも好まれ、20人の延期されたライターでのライター識別タスクでは、DSDは単一のサンプルワードから89.38％の精度を達成しました。また、柔軟性も向上します。いくつかの例を挙げれば、新しいライタースタイルで手書きを生成でき、ライタースタイル全体で新しい文字の手書きを生成できるようになりました。全体として、DSDにより、既存の手書きストローク生成アプローチよりも品質と柔軟性の両方を向上させることができます。 
[ABSTRACT]手書きを表す既存のvrnnアプローチは、これらの異なるスタイルコンポーネントを区別しないため、文字の機能が低下する可能性があります。新しいシステムにより、既存の手書きストロークスタイルアプローチよりも品質と柔軟性の両方を向上させることができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Single Versus Union: Non-parallel Support Vector Machine Frameworks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_37.html">
      <font color="black">Single Versus Union: Non-parallel Support Vector Machine Frameworks</font>
    </a>
  </h2>
  <font color="black">最適化問題を解決することで、距離マージンの大きい超平面を構築します。最初のタイプは、超平面を個別に構築します。一連の超平面を取得するために一連の小さな最適化問題を解決しますが、各サンプルの損失を測定することは困難です。 
[ABSTRACT]最初のタイプは同時に超平面を構築します。他のタイプのシリーズシリーズは小さな問題を生成します。最大-最小距離ベースの非平行サポートマシンもあります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-22">
        <br><font color="black">2019-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: A Variational-Sequential Graph Autoencoder for Neural Architecture
  Performance Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_38.html">
      <font color="black">A Variational-Sequential Graph Autoencoder for Neural Architecture
  Performance Prediction</font>
    </a>
  </h2>
  <font color="black">また、提案されたグラフエンコーダーに基づいて新しい変分順次グラフオートエンコーダー（VS-GAE）を提供します。さまざまなサンプリング方法の実験により、VS-GAEによって学習された埋め込み空間が精度予測タスクの安定性を向上させることが示されています。見られたアーキテクチャタイプ（つまり、ゼロショット予測）だけでなく、見られたアーキテクチャタイプのNASパフォーマンス予測に対する提案されたエンコーダの有効性を示します。 
[要約]このペーパーでは、グラフニューラルネットワーク上に構築されたグラフエンコーダーを提案します。提案されたグラフエンコーダーは、gnnパターンの結果として見られました。これは、新しいアーキテクチャの予測に使用できるgnnベースのシステムに基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-11">
        <br><font color="black">2019-12-11</font>
      </time>
    </span>
</section>
<!-- paper0: On the Composition and Limitations of Publicly Available COVID-19 X-Ray
  Imaging Datasets -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_39.html">
      <font color="black">On the Composition and Limitations of Publicly Available COVID-19 X-Ray
  Imaging Datasets</font>
    </a>
  </h2>
  <font color="black">特に深層学習モデルを使用することにより、画像データからCOVID-19を診断および進行予測するための機械学習ベースの方法がこの数か月間に大きな注目を集めています。バイアスについて、それらで訓練された障害のあるモデルが指摘されています。このペーパーは、現在公開されているCOVID-19胸部X線データセットの概要を示しています。 
[ABSTRACT]データセットはモデルの構築と評価の重要な部分です。これらの説明は、モデルの構築、モデルの目標に応じた最適なデータセットの選択、特定の制限を考慮して過剰なベンチマーク結果の報告を回避するのに役立ちます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: ARPM-net: A novel CNN-based adversarial method with Markov Random Field
  enhancement for prostate and organs at risk segmentation in pelvic CT images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_40.html">
      <font color="black">ARPM-net: A novel CNN-based adversarial method with Markov Random Field
  enhancement for prostate and organs at risk segmentation in pelvic CT images</font>
    </a>
  </h2>
  <font color="black">セグメンテーションネットワークは、新しく設計されたMRFブロックを多残留Uネットのバリエーションに統合します。弁別器は、元のCTと予測/グラウンドトゥルースの積を入力として受け取り、入力を偽/実数に分類します。セグメンテーションネットワークと弁別器ネットワークは、全体として一緒にトレーニングすることができます。または、弁別器は、セグメンテーションネットワークが大まかにトレーニングされた後、微調整に使用できます。 
[ABSTRACT]無傷の前立腺癌患者110名の計画ctおよび構造データセットを遡及的に選択し、10倍の交差検証のために分割しました。弁別子は、元のctと等高線の予測の積を偽/実数にして、入力を分類します偽物または本物に</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Learning Semantically Enhanced Feature for Fine-Grained Image
  Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_41.html">
      <font color="black">Learning Semantically Enhanced Feature for Fine-Grained Image
  Classification</font>
    </a>
  </h2>
  <font color="black">私たちのアプローチはパラメーターの節約であり、画像レベルの監視のみでエンドツーエンドのトレーニングのためのプラグアンドプレイモジュールとしてバックボーンモデルに簡単に統合できます。実験により、私たちのアプローチの有効性が検証され、同等のパフォーマンスが検証されました。最先端のメソッド..コードはhttps://github.com/cswluo/SEFで入手できます。
[要約]グローバル言語のサブ機能の言語を拡張することにより、細かい機能を学習するアプローチ、グループは、識別力の強いオブジェクトパーツを通じてアクティブ化されるようにガイドされます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-24">
        <br><font color="black">2020-06-24</font>
      </time>
    </span>
</section>
<!-- paper0: Orientation-Disentangled Unsupervised Representation Learning for
  Computational Pathology -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_42.html">
      <font color="black">Orientation-Disentangled Unsupervised Representation Learning for
  Computational Pathology</font>
    </a>
  </h2>
  <font color="black">ここでは、回転等変畳み込みネットワークのグループ構造を活用して変分オートエンコーダフレームワークを拡張し、組織病理画像の方向ごとに解きほぐされた生成因子を学習することを提案します。このようなモデルによって学習された表現は、その後の大きな画像の分析を容易にします。データセット..古典的なアプローチと比較して、セルのサブポピュレーションの結果として得られる集約表現は、後続のタスクでより高いパフォーマンスを生成します。 
[ABSTRACT]そのようなモデルによって学習された表現は、その後の大きな画像データセットの分析を容易にすることができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Applying Surface Normal Information in Drivable Area and Road Anomaly
  Detection for Ground Mobile Robots -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_43.html">
      <font color="black">Applying Surface Normal Information in Drivable Area and Road Anomaly
  Detection for Ground Mobile Robots</font>
    </a>
  </h2>
  <font color="black">実験結果は、私たちのNIMが運転可能エリアと道路異常検出のCNNのパフォーマンスを大幅に改善できることを示しています。 NIMの有効性と堅牢性のために、12の最先端のCNNに組み込んでいます。 
[ABSTRACT]通常の可能性モジュール（nim）は、高精度と効率で密な深度画像から表面の法線情報を生成できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: HipaccVX: Wedding of OpenVX and DSL-based Code Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_44.html">
      <font color="black">HipaccVX: Wedding of OpenVX and DSL-based Code Generation</font>
    </a>
  </h2>
  <font color="black">このペーパーでは、OpenVXビジョン関数を分析して、直交する一連の計算抽象を見つけます。このようにして、標準のコンピュータービジョン関数を使用したOpenVXグラフの実装では検出できない最適化を有効にします。最適化された異種プラットフォーム用のプログラムの作成これは、ほとんどの場合、根本的に異なるプログラミングパラダイムと言語に基づくアーキテクチャ固有の最適化を使用して、コードを低レベルで調整する必要があるため、高性能を実現するのは困難です。 
[ABSTRACT] openvxは、コンピュータービジョンアプリケーションのこの問題を、ロイヤルティ-グラフに基づく無料の業界標準-実行状態で解決することを約束します。これにより、標準に含まれていない制約付き制約付き計算が妨げられます。これは、既存のドメイン固有の言語（dsl）パフォーマンスを作成する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: SMAP: Single-Shot Multi-Person Absolute 3D Pose Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_45.html">
      <font color="black">SMAP: Single-Shot Multi-Person Absolute 3D Pose Estimation</font>
    </a>
  </h2>
  <font color="black">このようなシングルショットボトムアップスキームにより、システムは人物間の深度関係についてよりよく学習して推論でき、3Dと2Dの両方の姿勢推定が改善されます。実験により、提案されたアプローチが最先端の技術を実現することが示されています。 CMUパノプティックおよびMuPoTS-3Dデータセットでのパフォーマンスであり、野生のビデオに適用可能です。このホワイトペーパーでは、最初に身体部分の2.5D表現のセットを回帰し、次に3D絶対ポーズを再構築する新しいシステムを提案します。これらの2.5D表現に基づいて、深度認識パーツ関連付けアルゴリズムを使用します。 
[要約]提案されたアプローチは、体のパーツの2.5dモデルのセットを回帰することで機能します。次に、これらの2.5d表現に基づいて3d絶対ポーズを再構築します。実験は、システムが最先端のパフォーマンスを達成することを実証しましたcmuパノプティックとmupots-3Dデータセット</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Estimating Example Difficulty using Variance of Gradients -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_46.html">
      <font color="black">Estimating Example Difficulty using Variance of Gradients</font>
    </a>
  </h2>
  <font color="black">高いVOGスコアのデータポイントは、モデルが記憶を必要とする例を分類して過剰にインデックス化するのがより困難です。VOGは、難易度によってデータをランク付けし、 Human-in-the-Loop監査の最も困難な例。非定型の例を識別することは、モデルの安全な展開を通知するのに役立ち、さらに人間の検査が必要な例を分離し、モデルの動作に解釈可能性を提供します。 
[ABSTRACT]これらの例を特定することは、models.vogの安全な展開を通知するのに役立ちます。ループ監査で人間にとって最も困難な例のサブセットを表面化し、サブセットを明らかにするための良い方法です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: SNE-RoadSeg: Incorporating Surface Normal Information into Semantic
  Segmentation for Accurate Freespace Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_47.html">
      <font color="black">SNE-RoadSeg: Incorporating Surface Normal Information into Semantic
  Segmentation for Accurate Freespace Detection</font>
    </a>
  </h2>
  <font color="black">データ融合畳み込みニューラルネットワーク（CNN）で行われた最近の取り組みにより、セマンティックドライビングシーンのセグメンテーションが大幅に改善されました。さらに、RGB画像と両方の機能から機能を抽出して融合できるRoadSegと呼ばれるデータ融合CNNアーキテクチャを提案します。正確な自由空間検出のための推定表面法線情報。したがって、このペーパーでは、まず、表面法線推定器（SNE）という名前の新しいモジュールを紹介します。これは、高精度と効率で高密度の深度/視差画像から表面法線情報を推定できます。 
[要約]このペーパーでは、まず、密な深さ/視差の画像から表面の法線情報を推測できる新しいモジュールを紹介します。研究目的で、準備完了（r2d）道路という名前の大規模な合成自由空間検出データセットを公開しますデータセット</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Many-shot from Low-shot: Learning to Annotate using Mixed Supervision
  for Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_48.html">
      <font color="black">Many-shot from Low-shot: Learning to Annotate using Mixed Supervision
  for Object Detection</font>
    </a>
  </h2>
  <font color="black">私たちのOAMは、完全に監視されている2段階のオブジェクト検出メソッドと共同でトレーニングでき、追加のトレーニングアノテーションをオンザフライで提供します。ただし、これらのメソッドは、弱いトレーニングシグナルの\ emph {頻繁な}結果として、それらの強力に監視された対応物に対してパフォーマンスが大幅に低下します。部分的または特大の検出。これにより、完全に注釈が付けられた画像のローショットセットのみを必要とする完全なエンドツーエンドの戦略が実現します。 
[ABSTRACT]注釈は非常に時間がかかり、費用がかかるため、ヤンの監視あり少数ショットのオブジェクト検出方法の開発に動機を与えます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-21">
        <br><font color="black">2020-08-21</font>
      </time>
    </span>
</section>
<!-- paper0: Delving into Inter-Image Invariance for Unsupervised Visual
  Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_49.html">
      <font color="black">Delving into Inter-Image Invariance for Unsupervised Visual
  Representations</font>
    </a>
  </h2>
  <font color="black">コードとモデルはhttps://github.com/open-mmlab/OpenSelfSup。で入手できます。注意深く設計された比較と分析を通じて、教師なしの画像内および画像間不変性学習の統合をサポートする統合フレームワークを提案します。画像間の不変性を利用するための1つの主要な障害は、画像間のポジティブペアを確実に構築する方法が不明確であり、ペアアノテーションが利用できないため、それらから効果的な監視をさらに引き出すことです。 
[ABSTRACT]この作品では、3つの主要な構成要素からの画像間不変性学習に関する厳密で包括的な研究を提示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Making a Case for 3D Convolutions for Object Segmentation in Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_50.html">
      <font color="black">Making a Case for 3D Convolutions for Object Segmentation in Videos</font>
    </a>
  </h2>
  <font color="black">このために、効率的な3Dエンコーダーを活用し、新しい3Dグローバルコンボリューションレイヤーと3Dリファインメントモジュールで構成される3Dデコーダーアーキテクチャを提案します。シンプルで効果的なエンコーダー/デコーダーネットワークアーキテクチャーを提案します。標準のクロスエントロピー損失を使用してエンドツーエンドでトレーニングしました。この作業では、3D CNNが顕著なオブジェクトのセグメンテーションなどの高密度ビデオ予測タスクに効果的に適用できることを示します。 
[要約]当社のエンコーダー-デコーダーネットワークアーキテクチャは3dたたみ込みで構成されています。これらには、標準のクロスネバダ損失損失と標準の「セミセミセミファイナル」損失を使用してトレーニングできる3dたたみ込みネットワークが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: DeepSOCIAL: Social Distancing Monitoring and Infection Risk Assessment
  in COVID-19 Pandemic -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_51.html">
      <font color="black">DeepSOCIAL: Social Distancing Monitoring and Infection Risk Assessment
  in COVID-19 Pandemic</font>
    </a>
  </h2>
  <font color="black">また、移動軌跡からの時空間データと社会的距離違反の割合を統計的に分析することにより、オンラインリスク評価スキームを提供します。提案された方法論の効率は、オックスフォードタウンセンターデータセットで評価され、用語の点で優れています3つの最先端の方法と比較した精度と速度の比較。この研究では、一般的なディープニューラルネットワークベースのモデルを開発し、一般的な人の検出、追跡、群衆内の人間の距離の推定を行います。 CCTV防犯カメラ。 
[ABSTRACT]大多数の政府と国の保健当局は、ショッピングセンター、学校、その他の対象地域における必須の安全対策として2メートルの物理的な距離を設定しています。提案されたモデルには、yolov4ベースのフレームワークと正確な人々のための逆透視マッピングが含まれます検出と社会的距離の監視</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Attr2Style: A Transfer Learning Approach for Inferring Fashion Styles
  via Apparel Attributes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_52.html">
      <font color="black">Attr2Style: A Transfer Learning Approach for Inferring Fashion Styles
  via Apparel Attributes</font>
    </a>
  </h2>
  <font color="black">特に、モデルのエンコーダーは、最初にソースデータセットでトレーニングされ、低レベルの属性をキャプチャする潜在表現を取得します。逆に、低レベルの属性ベースのアノテーションははるかに簡単に利用できます。この問題に対処するために、十分な属性ベースのグラウンドトゥルースキャプションを持つソースデータセットでトレーニングされ、ターゲットデータセットのスタイルベースのキャプションを予測するために使用される転移学習ベースの画像キャプションモデルを提案します。 
[ABSTRACT]スタイルベースのキャプションに注釈を付けるには、ある程度のファッションドメインの専門知識が必要であり、コストと手作業に追加されます。モデルは、十分な例に基づいたグラウンドトゥルースキャプションを含むソースデータセットでトレーニングされ、スタイルの予測に使用されます- -ターゲットデータセットのベースのキャプション</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: DRR4Covid: Learning Automated COVID-19 Infection Segmentation from
  Digitally Reconstructed Radiographs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_53.html">
      <font color="black">DRR4Covid: Learning Automated COVID-19 Infection Segmentation from
  Digitally Reconstructed Radiographs</font>
    </a>
  </h2>
  <font color="black">感染を意識したDRRジェネレーターは、COVID-19感染の放射線徴候の強度を調整可能なDRRを生成し、DRRと正確に一致するピクセルレベルの感染アノテーションを生成できます。合成DRRにおけるCOVID-19感染の放射線学的徴候の強度を調整することによるCOVID-19感染。ドメイン適応モジュールは、ラベルなしの実際のCXRとラベル付きDRRでネットワークをトレーニングすることにより、DRRとCXR間のドメインの不一致を減らすために導入されています。最大平均不一致（MMD）に基づくドメイン適応モジュール、および分類ヘッダーとセグメンテーションヘッダーを持つFCNベースのネットワークを使用した、DRR4Covidのシンプルですが効果的な実装。 
[要約] drr4covidは、デジタルで再構成された放射線写真（drrs）からcxrsを使用して自動化されたcovidを学習するツールです。drr4covidは、covid-19感染の強度が調整可能なcxrsを生成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: AgingMapGAN (AMGAN): High-Resolution Controllable Face Aging with
  Spatially-Aware Conditional GANs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_54.html">
      <font color="black">AgingMapGAN (AMGAN): High-Resolution Controllable Face Aging with
  Spatially-Aware Conditional GANs</font>
    </a>
  </h2>
  <font color="black">これらの制限に対処するため、民族固有の老化情報と弱い空間監視を使用して高解像度画像の外観を変更し、老化プロセスをガイドするアプローチを提示します。既存のアプローチと顔の老化のデータセットは、平均に向かって歪んだ結果を生成し、個人のバリエーションや表情のしわが目に見えない、または見過ごされがちなため、顔の肥大などのグローバルパターンが優先されます。品質、制御、および高解像度画像での使用方法に関して、提案された方法の利点を示します。計算オーバーヘッドを制限しながら。 
[要約]提案された方法は、高精細画像で使用できます。顔の老化方法を制御することはほとんどできません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-25">
        <br><font color="black">2020-08-25</font>
      </time>
    </span>
</section>
<!-- paper0: 3D Face Anti-spoofing with Factorized Bilinear Coding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_55.html">
      <font color="black">3D Face Anti-spoofing with Factorized Bilinear Coding</font>
    </a>
  </h2>
  <font color="black">RGBとYCbCrスペースから差別的で融合した補足情報を抽出することにより、3D顔のなりすまし検出の原理的なソリューションを開発しました。画像とビデオの両方を備えた大規模なワックスフィギュア顔データベース（WFFD）も超現実的なものとして収集されました3D顔提示攻撃検出の研究を容易にするための攻撃。広範な実験結果は、提案された方法が、さまざまなイントラデータベースおよびデータベース間で、独自のWFFDと他の顔なりすましデータベースの両方で最先端のパフォーマンスを達成することを示しています。テストシナリオ。 
[要約]顔認識システムは、実際の顔に似たマテリアルの3D特性により混乱しやすくなります。複数のカラーチャネルの因数分解バイリニアコーディングに基づく当社の方法は、実際の画像と偽の画像の微妙な違いを学習することを目的としています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-12">
        <br><font color="black">2020-05-12</font>
      </time>
    </span>
</section>
<!-- paper0: Point Adversarial Self Mining: A Simple Method for Facial Expression
  Recognition in the Wild -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_56.html">
      <font color="black">Point Adversarial Self Mining: A Simple Method for Facial Expression
  Recognition in the Wild</font>
    </a>
  </h2>
  <font color="black">純粋にランダムな方法で領域を選択するランダムな消去と注意マップによって動作する敵対的な消去によって生成された領域と比較すると、PASMによって取得された特定された情報領域は、以前の結果とより順応性が高く、よりよく整合しています。顔領域は正確な予測に貢献します。具体的には、サンプルと事前トレーニング済みネットワークが与えられた場合、提案されたアプローチは、ポイント敵対攻撃ポリシーによって生成されたサンプル内の情報領域を特定します。拡張画像は、ネットワークを微調整して強化するために使用されますその一般性。 
[ABSTRACT]高い予測精度を達成するために、既存の作業のほとんどは、ネットワークアーキテクチャの操作と高度な損失項の設計を選択しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Side-Aware Boundary Localization for More Precise Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_57.html">
      <font color="black">Side-Aware Boundary Localization for More Precise Object Detection</font>
    </a>
  </h2>
  <font color="black">標準の境界ボックス回帰ブランチを提案された設計に置き換えると、Faster R-CNN、RetinaNet、およびCascade R-CNNがそれぞれ3.0％、1.7％、および0.9％大幅に改善されます。 2ステージおよび1ステージの検出フレームワーク..分散が大きい変位の存在下での正確な位置特定の難しさに取り組むために、2段階の位置特定スキームをさらに提案します。これは、バケット予測を通じて動きの範囲を予測し、次にピンポイントを特定します。予測バケット内の正確な位置。 
[要約]バウンディングボックスの証言の精度は依然として不十分であり、オブジェクト検出のパフォーマンスが制限されます。しかし、中心とサイズの予測に焦点を当てた主流のアプローチは、このタスクを達成するための最も効果的な方法ではありません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-09">
        <br><font color="black">2019-12-09</font>
      </time>
    </span>
</section>
<!-- paper0: Cross-regional oil palm tree counting and detection via multi-level
  attention domain adaptation network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_58.html">
      <font color="black">Cross-regional oil palm tree counting and detection via multi-level
  attention domain adaptation network</font>
    </a>
  </h2>
  <font color="black">MADANは4つの手順で構成されます。最初に、モデルの汎化能力を改善するためにバッチインスタンス正規化ネットワーク（BIN）ベースの特徴抽出を採用し、バッチ正規化とインスタンス正規化を統合しました。大規模なヤシの木のプランテーションの正確な評価を提供します。地域は、経済的および生態学的な側面の両方に意味のある影響をもたらす可能性があります。MADANは、ベースライン法（DAなし）と比較して、平均F1スコアの点で検出精度を14.98％向上させ、既存のドメイン適応より3.55％-14.49％優れたパフォーマンスを発揮しますメソッド。 
[要約]新しい研究は、クロスレベルの地域のヤシの木のカウントと検出を獲得するためにマルチレベルの注意領域適応ネットワーク（madan）を提案します。madanは検出精度を14向上させます。平均f1の観点から98％は、ベースライン法（daなし）</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Determinantal Point Process as an alternative to NMS -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CV/paper_59.html">
      <font color="black">Determinantal Point Process as an alternative to NMS</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、このホワイトペーパーで提案されている変更により、最新のオブジェクト検出パイプラインに一貫した改善がもたらされることを強く示しています。非最大抑制（NMS）に代わる決定論的ポイントプロセス（DPP）すべての最先端のオブジェクト検出フレームワークに不可欠なステップです。NMSをサブセット選択問題として提起し、DPPのようなフレームワークを直接組み込むことで、オブジェクト検出システムの全体的なパフォーマンスを改善できると考えています。 
[ABSTRACT] dppは、ステップアップの多様性を促進することが示されています。dppは、特定の特定の領域への一連の変更にリンクされています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: A Baseline Analysis for Podcast Abstractive Summarization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CL/paper_0.html">
      <font color="black">A Baseline Analysis for Podcast Abstractive Summarization</font>
    </a>
  </h2>
  <font color="black">ニュースとは異なり、ポッドキャストは長く、口語的で会話的で、コマーシャルやスポンサーシップのコンテンツが騒々しいため、ポッドキャストの自動要約は非常に困難です。エンドユーザーの聴取上の決定に影響する重要な要素であるポッドキャストの要約は、ポッドキャスト推奨システムおよび多くのダウンストリームアプリケーションにおける重要な機能です。これは、研究者が最新の事前トレーニング済みモデルを理解し、より優れたモデルを作成するための基盤を構築するのに役立つことを目的としています。 
[ABSTRACT] spotifyポッドキャストデータセットを使用したポッドキャストの分析。分析は、現在のユーザーのビューが事前情報の分析に基づいていることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-24">
        <br><font color="black">2020-08-24</font>
      </time>
    </span>
</section>
<!-- paper0: Inno at SemEval-2020 Task 11: Leveraging Pure Transfomer for Multi-Class
  Propaganda Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CL/paper_1.html">
      <font color="black">Inno at SemEval-2020 Task 11: Leveraging Pure Transfomer for Multi-Class
  Propaganda Detection</font>
    </a>
  </h2>
  <font color="black">私たちは、プロパガンダ手法を相互に区別する機能について、最適化された学習スキームを備えた純粋なトランスフォーマーベースのモデルをテストしました。 ..このペーパーは、SEMEVAL 2020タスク11「ニュース記事におけるプロパガンダ手法の検出」に対するチーム「イノ」の解決策を提示します。 
[要約] 2番目のサブタスクの目標は、ニュース記事のデータセットで18のプロパガンダ手法のうち11を分類することです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Decision Tree J48 at SemEval-2020 Task 9: Sentiment Analysis for
  Code-Mixed Social Media Text (Hinglish) -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CL/paper_2.html">
      <font color="black">Decision Tree J48 at SemEval-2020 Task 9: Sentiment Analysis for
  Code-Mixed Social Media Text (Hinglish)</font>
    </a>
  </h2>
  <font color="black">分類子は2セットのトレーニングデータでトレーニングされ、F1スコアは0.4972と0.5316になりました。システムの評価が行われたテストデータセットでツイートを分類するために、トレーニングデータの一部のみがシステムに提供されました。システムのパフォーマンスは、公式の競争評価指標F1スコアを使用して評価されました。 
[ABSTRACT]システムのパフォーマンスは、公式の競争評価指標f1スコアを使用して評価されました。システムは、テストシステムとは異なるレベルのデータを使用してテストされました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Discrete Word Embedding for Logical Natural Language Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CL/paper_3.html">
      <font color="black">Discrete Word Embedding for Logical Natural Language Understanding</font>
    </a>
  </h2>
  <font color="black">私たちは、「言い換え」タスクを実行することにより、埋め込みがシンボリックで古典的なプランニングソルバーと直接互換性があることを示しています。言い換えデータセットに追加のトレーニングが必要な場合、システムは言い換えクエリ（解の存在なし）に否定的に答えることができ、一部の近似解のみが存在することに答えることができます-次のような最近の巨大で純粋な神経言語モデルに欠けている機能GPT-3 ..不確実性を捕捉する能力がないために離散表現がうまく機能しないという従来の知識とは対照的に、私たちの表現は、いくつかのダウンストリームタスクでの連続表現に対して競争力があります。 
[ABSTRACT]私たちのシステムは言い換えの質問に否定的に答えることができ、正確な解決策はいくつかしか存在しないと答えることができます。gpt-3などの最近の巨大な純粋な神経言語モデルでは欠けている機能</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Item Tagging for Information Retrieval: A Tripartite Graph Neural
  Network based Approach -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CL/paper_4.html">
      <font color="black">Item Tagging for Information Retrieval: A Tripartite Graph Neural
  Network based Approach</font>
    </a>
  </h2>
  <font color="black">オープンデータセットとインダストリアルデータセットの両方での実験結果は、TagGNNアプローチが最新のマルチラベル分類アプローチよりも優れていることを示しています。特に、タグ付けは、情報検索（IR）の関連性マッチングを高めるための成功事例として認識されています。アイテムには、豊富なテキストの説明がありません。 
[要旨]マルチラベルのテキスト分類または画像注釈のいずれかについて多くの調査が行われました。これは、irの固有の特性が知られていないためです。アイテムの表現を充実させることに加えて、irで利用可能なログログを活用しますタスク</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Joint Embedding of Words and Category Labels for Hierarchical
  Multi-label Text Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CL/paper_5.html">
      <font color="black">Joint Embedding of Words and Category Labels for Hierarchical
  Multi-label Text Classification</font>
    </a>
  </h2>
  <font color="black">テキスト分類タスクで親カテゴリと子カテゴリの関係を十分に活用すると、分類のパフォーマンスを大幅に向上させることができます。このホワイトペーパーでは、階層型微調整順序ニューロンLSTM（HFT）に基づいて、テキストと親カテゴリの同時埋め込みを提案します。 -ONLSTM）for HTC ..テキスト分類は、分類ラベルの粒度の継続的な改良と分類ラベルスケールの拡大により、ますます困難になっています。 
[要約]いくつかの調査は、多数のカテゴリの問題で階層構造を活用する戦略に適用されています。テキスト分類タスクで親カテゴリと子カテゴリの関係を最大限に活用すると、分類のパフォーマンスを大幅に向上させることができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-06">
        <br><font color="black">2020-04-06</font>
      </time>
    </span>
</section>
<!-- paper0: Talk2Car: Taking Control of Your Self-Driving Car -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CL/paper_6.html">
      <font color="black">Talk2Car: Taking Control of Your Self-Driving Car</font>
    </a>
  </h2>
  <font color="black">人工知能の長期的な目標は、エージェントに自然言語を介して伝達されるコマンドを実行させることです。具体的には、自動運転の設定における問題を考慮します。ストリートシーン。ReferIt、RefCOCO、RefCOCO +、RefCOCOg、Cityscape-Ref、CLEVR-Refなどの関連データセットとの詳細な比較を提供します。 
[ABSTRACT] talk2carデータセットは、自動運転車用の自然言語で書かれたコマンドを含む最初のオブジェクト参照データセットです。これには、強力な状態を使用したパフォーマンス分析が含まれます-最先端のモデル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-24">
        <br><font color="black">2019-09-24</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Label Sentiment Analysis on 100 Languages with Dynamic Weighting
  for Label Imbalance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CL/paper_7.html">
      <font color="black">Multi-Label Sentiment Analysis on 100 Languages with Dynamic Weighting
  for Label Imbalance</font>
    </a>
  </h2>
  <font color="black">さらに、線形時間の複雑さでマクロf1スコアを最大化する最適なクラス固有のしきい値を選択する方法を導き出します。以前の静的重み付け方法とは異なり、トレーニング中に各クラスからの寄与のバランスをとる新しい動的重み付け方法を導入しますクラスの頻度に基づいて変化しない重み。モデルのコードを公開します。これにより、100言語で感情分析を実行して、さらに調査を進めることができます。 
[ABSTRACT]新しいメソッドは、マルチラベル設定に感情分析フレームワークを使用します。このメソッドは、状態、、および、および3つの異なる言語を提供します。これには、中世の競争で最高のパフォーマンスを発揮するメソッドが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Language Models and Word Sense Disambiguation: An Overview and Analysis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CL/paper_8.html">
      <font color="black">Language Models and Word Sense Disambiguation: An Overview and Analysis</font>
    </a>
  </h2>
  <font color="black">私たちの分析の主な結論の1つは、BERTは、各単語の意味で使用できる例の数が限られている場合でも、高レベルの意味の区別をキャプチャするのに適切な仕事を実行するということです。エンコードと単語の感覚の回復の制限..この記事では、名詞のあいまいさに関する有名なBERTモデルの詳細な定量的および定性的分析を提供します。 
[ABSTRACT]単語の意味の曖昧性解消（wsd）は、言語モデルの例です。言語モデルは、トレーニングデータとコンピューティングリソースの観点から、解決精度の高い名詞に近くなります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Machine learning approach of Japanese composition scoring and writing
  aided system's design -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CL/paper_9.html">
      <font color="black">Machine learning approach of Japanese composition scoring and writing
  aided system's design</font>
    </a>
  </h2>
  <font color="black">そして、これらのオートマトンを使用して、文法機能を抽出します。自然言語によって生成された記事を評価するとき、単語機能、文法機能、意味機能、テキスト構造など、多くの次元から記事を表示する必要があります。実験では、次の作業を行いました。1）単語分割ツールと辞書を使用して、記事の単語分割を実現し、単語の特徴を抽出し、記事の単語の複雑さの特徴を生成します。 
[ABSTRACT]それは何かを出力する過程で言語をよりリーンに向上させることができます。また、言語を作成することができ、構文のコンテンツがより懸念されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: A Multitask Deep Learning Approach for User Depression Detection on Sina
  Weibo -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/cs.CL/paper_10.html">
      <font color="black">A Multitask Deep Learning Approach for User Depression Detection on Sina
  Weibo</font>
    </a>
  </h2>
  <font color="black">FusionNet（FN）が提案され、同時に抽出された機能を使用してトレーニングされます。これらの機能は、複数の分類タスクと見なされます。ただし、機械学習に基づくうつ病の検出に関する既存の研究では、分類パフォーマンスが比較的低く、大幅な改善があることを示唆しています。機能エンジニアリングの改善の可能性。私たちの作業は、他のOSNプラットフォームの不況を検出する新しい方法も提供します。 
[要約]オンラインソーシャルネットワーク（osn）は、うつ病に苦しんでいる人々を検出するための別の視点を研究者に提供します。このツールには、weiboユーザーうつ病検出データセット（wu3d）が含まれています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: The Freesound Loop Dataset and Annotation Tool -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.AS/paper_0.html">
      <font color="black">The Freesound Loop Dataset and Annotation Tool</font>
    </a>
  </h2>
  <font color="black">自動ループの特性評価からアルゴリズムによる作曲までのアプリケーションで、コミュニティがさらに多くのデータの用途を見つけることを期待しています。音楽ループは電子音楽制作に不可欠な要素であり、さまざまな種類の事前に録音されたループへの高い需要があります。 styles ..また、開発したオンラインループアノテーターツールをコミュニティに提供します。 
[ABSTRACT] freesoundはクリエイティブコモンズライセンスの下でリリースされたオーディオ録音のコミュニティデータベースです。fsldはfsldを使用してテンポとキーを推定し、音楽トラックを生成し、ループ分離アルゴリズムを評価します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: DeepVOX: Discovering Features from Raw Audio for Speaker Recognition in
  Degraded Audio Signals -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.AS/paper_1.html">
      <font color="black">DeepVOX: Discovering Features from Raw Audio for Speaker Recognition in
  Degraded Audio Signals</font>
    </a>
  </h2>
  <font color="black">自動話者認識アルゴリズムは、通常、音声周波数を特徴付けるために、Mel-FrequencyやGammatoneフィルターバンクなどの事前定義されたフィルターバンクを使用します。次に、フィルターバンクのトレーニングに最適なデータサンプルを効率的にマイニングするために、適応トリプレットマイニング技術が開発されます。したがって、結果として得られる機能は、さまざまな種類のオーディオ劣化に一般化できない場合があります。 
[要旨] deepvoxフィルターバンクはドメインに基づいています-知識と限られた観測。これは、1d畳み込みニューラルネットワークが時間を学習するように設計されていることを意味します-生の音声から直接deepvoxと呼ばれるドメインフィルターバンク</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarially Training for Audio Classifiers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.AS/paper_2.html">
      <font color="black">Adversarially Training for Audio Classifiers</font>
    </a>
  </h2>
  <font color="black">2つのベンチマーク環境音データセットで実験を実行し、敵対者の予算割り当てに課せられた制限なしに、敵対的に訓練されたモデルの浮気率が90 \％を超える可能性があることを示します。つまり、敵対的攻撃はあらゆる規模で存在します、しかしそれらは非敵対的に訓練されたモデルと比較してより高い敵対的な摂動を必要とするかもしれません。この論文では、標的化および非標的化されたさまざまな敵対的攻撃に対する6つの高度なディープニューラルネットワークの堅牢性に対する敵対的訓練の潜在的な影響を調査します。 
[ABSTRACT] 2つのベンチマーク環境音データセットで実験を行います。これらは、敵対的な敵の標的のだまし率が90％を超える可能性があることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
<!-- paper0: TIV.lib: an open-source library for the tonal description of musical
  audio -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-27/eess.AS/paper_3.html">
      <font color="black">TIV.lib: an open-source library for the tonal description of musical
  audio</font>
    </a>
  </h2>
  <font color="black">ライブラリはクロスプラットフォームであり、Pythonとグラフィカルプログラミング言語Pure Dataで実装されており、オンラインとオフラインの両方のシナリオで使用できます。その主な新機能は、離散フーリエ変換に基づく知覚にヒントを得た色調間隔ベクトル空間に依存しています。そこから、複数の瞬間的かつグローバルな表現、記述子、およびメトリックが計算されます-たとえば、調和変化、不協和音、ダイアトニシティ、および音楽のキー。このペーパーでは、TIV.libを提示します。音楽オーディオ信号の。 
[ABSTRACT]これは、紙を演奏するために使用されたのはこれが初めてです。これは、さまざまな形式の焼けるような変換に基づいています。これには、音楽のキーを含むさまざまな例が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-26">
        <br><font color="black">2020-08-26</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
