<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
<title>Akari-2019-12-06の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
  <div class="header-logo">
    <a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
  </div>
</header>
<input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">

<div class="menubar">
<span class="bar"></span>
<span class="bar"></span>
<span class="bar"></span>
</div>

<ul>
<li><a id="home" href="../../index.html">Home</a></li>
<li><a id="about" href="../../teamAkariとは.html">About</a></li>
<li><a id="contact" href="../../contact.html">Contact</a></li>
<li><a id="contact" href="../../list/newest.html">New Papers</a></li>
<li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
</ul>

</label>
<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Adversarial Music: Real World Audio Adversary Against Wake-word
  Detection System -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.SD/paper_0.html">
      Adversarial Music: Real World Audio Adversary Against Wake-word
  Detection System
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      次に、変換に対する期待を考慮してオーディオの敵を計算し、微分可能なシンセサイザーを使用してオーディオの敵を実装しました。この実験は、エミュレートされたモデルの認識F1スコアを93.4％から11.0％に効果的に削減できることを示しています。およびデモ動画は\ url {https://www.junchengbillyli.com/AdversarialMusic}からアクセスできます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-31">
        <br>2019-10-31
      </time>
    </span>
  </h3>
</article>
<!-- paper0: VoxSRC 2019: The first VoxCeleb Speaker Recognition Challenge -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.SD/paper_1.html">
      VoxSRC 2019: The first VoxCeleb Speaker Recognition Challenge
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このホワイトペーパーでは、課題の概要を説明し、そのベースライン、結果、およびディスカッションを提供します。（i）YouTubeビデオから公開されている話者認識データセットと、グラウンドトゥルースアノテーションおよび標準化された評価ソフトウェア。 （ii）オーストリア、グラーツのInterspeech 2019で開催された公開チャレンジとワークショップ。VoxCelebSpeaker Recognition Challenge 2019は、現在の話者認識テクノロジーが制約のない、または「インザワイルド」データの話者をどれだけ識別できるかを評価することを目的としています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br>2019-12-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Towards Robust Neural Vocoding for Speech Generation: A Survey -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.SD/paper_2.html">
      Towards Robust Neural Vocoding for Speech Generation: A Survey
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本稿では、WaveNet、WaveRNN、WaveGlowなどの一般的に使用される3つのニューラルボコーダーを5つの異なるデータセットで交互にトレーニングします。さらに、将来の研究のための主観的人間評価のかなりの基準値を示します。 WaveNetは音声合成モデルにより適し、WaveRNNは音声変換アプリケーションにより適していることを示しています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br>2019-12-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: MelGAN-VC: Voice Conversion and Audio Style Transfer on arbitrarily long
  samples using Spectrograms -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.SD/paper_3.html">
      MelGAN-VC: Voice Conversion and Audio Style Transfer on arbitrarily long
  samples using Spectrograms
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      クリーンな音声録音のデータセット、およびノイズの多い実世界の音声サンプルのコレクションを使用してフレームワークをテストします。従来の音声変換方法は、同じ文を発音する複数のスピーカーの並列録音に依存します。ターゲットスピーカーのスタイルを柔軟にモデル化する機能を犠牲にすることなく、翻訳プロセスでの音声情報。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-08">
        <br>2019-10-08
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Self-Supervised Contextual Language Representation of Radiology Reports
  to Improve the Identification of Communication Urgency -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.CL/paper_0.html">
      Self-Supervised Contextual Language Representation of Radiology Reports
  to Improve the Identification of Communication Urgency
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      放射線レポートの大きなラベルのないコーパスでBERTモデルを事前にトレーニングし、コミュニケーションの緊急性のために最終的なテキスト分類子で結果のコンテキスト表現を使用しました。モデルは97.0％の精度、93.3％の再現率、迅速なコミュニケーションのための放射線レポートの特定における独立したテストセットで95.1％、word2vec表現に基づく以前の最先端モデルよりも大幅に優れています。ただし、これらの方法の広範なアプリケーションの主要なボトルネックは、大量の注釈付きトレーニングデータ。これはリソースを消費し、時間がかかります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br>2019-12-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Easy-to-Hard: Leveraging Simple Questions for Complex Question
  Generation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.CL/paper_1.html">
      Easy-to-Hard: Leveraging Simple Questions for Complex Question
  Generation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      前者はサブ質問をエンコードしてコピーし、後者はさらに複数の疑似サブ質問を採点および集計します。この論文は、知識グラフから複雑な質問を自動的に生成するための最初の取り組みの1つです。基本CoG2Qだけでなく、追加のトレーニング例として単純な質問を使用した拡張版も大幅に優れています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br>2019-12-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Effective Data Augmentation Approaches to End-to-End Task-Oriented
  Dialogue -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.CL/paper_2.html">
      Effective Data Augmentation Approaches to End-to-End Task-Oriented
  Dialogue
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      高価なクラウドソーシングの努力によりトレーニングデータを増強する以前の作業とは対照的に、エンドツーエンドのタスク指向の対話のために、単語レベルと文レベルの両方でデータを増強する4つの異なる自動アプローチを提案し、その影響に関する実証研究を実施します..綿密な分析により、本手法がユーザーの発話の多様性を適切に向上させ、エンドツーエンドモデルが機能を堅牢に学習できることがさらに確認されます。CamRest676およびKVRETデータセットの実験結果は、4つのデータ増加アプローチは、Success F1スコアの点で強力なベースラインを大幅に上回り、4つのアプローチのアンサンブルが2つのデータセットで最先端の結果を達成することができます。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br>2019-12-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Automatically Neutralizing Subjective Bias in Text -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.CL/paper_3.html">
      Automatically Neutralizing Subjective Bias in Text
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最後に、タスクに2つの強力なエンコーダーデコーダーベースラインを提案します。コーパスには180,000文のペアが含まれ、さまざまなフレーミング、前提条件、および偏った文からの態度を削除したWikipediaの編集に基づいています。 。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-21">
        <br>2019-11-21
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Can I Trust the Explainer? Verifying Post-hoc Explanatory Methods -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.CL/paper_4.html">
      Can I Trust the Explainer? Verifying Post-hoc Explanatory Methods
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      しかし、ニューラルネットワークは、正しい決定を下す場合でも、不合理な相関関係に依存することがよくあります。この作業では、現在の説明方法の2つの問題を特定します..機能選択の観点から説明方法の検証フレームワークを導入します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-04">
        <br>2019-10-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Measuring Social Bias in Knowledge Graph Embeddings -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.CL/paper_5.html">
      Measuring Social Bias in Knowledge Graph Embeddings
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      グラフの埋め込みがますます利用されるようになるにつれて、そのようなバイアスの存在を理解し、それらの影響を緩和するために講じる手順が重要であることが示唆されます。たとえば、グラフの埋め込みは、男性が銀行家であり、女性がより可能性が高いという情報をエンコードします私たちは、知識グラフ埋め込みにおける社会的バイアスに関する最初の研究を提示し、そのようなバイアスを測定するのに適した新しいメトリックを提案します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br>2019-12-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Learning to Predict Explainable Plots for Neural Story Generation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.CL/paper_6.html">
      Learning to Predict Explainable Plots for Neural Story Generation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      外部要約モデルを採用して潜在変数モデルをガイドし、トレーニングデータからアウトラインを生成する方法を学習します。ニューラルネットワークの使用はストーリー生成の改善に効果的であることが証明されていますが、説明可能な高レベルプロットの生成方法は依然として残っています大きな挑戦。モデルは、人間に説明可能な自然言語文であるアウトラインを、入力と出力をつなぐ高レベルのプロットを表す潜在変数として扱います。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br>2019-12-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art
  Baseline -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.CL/paper_7.html">
      Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art
  Baseline
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの最高の単一モデルは、Visual Dialogで最先端を達成し、以前の公開作品（モデルアンサンブルを含む）をNDCGおよびMRRで1％を超える絶対パフォーマンスで達成します。ダイアログは、VisDialデータセットのディープニューラルモデルを単独でトレーニングすることに焦点を当てており、大きな進歩をもたらしましたが、制限があり無駄です。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br>2019-12-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Complete Variable-Length Codes: An Excursion into Word Edit Operations -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.CL/paper_8.html">
      Complete Variable-Length Codes: An Excursion into Word Edit Operations
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この研究では、$ \ tau $ -independentまたは$ \ tau $ -closed可変長コードの場合の最大性と完全性との関係について説明します。言語X $ \ subseteq $ A *は、等式の場合は可変長コードですXの単語の中では必然的に些細なことです。言語Xは、A上の単語がXの単語の連結の要因である場合に完全です。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br>2019-12-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: 12-in-1: Multi-Task Vision and Language Representation Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.CL/paper_9.html">
      12-in-1: Multi-Task Vision and Language Representation Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      個別にトレーニングされたシングルタスクモデルと比較すると、これは約30億のパラメーターから2億7000万に減少すると同時に、タスク全体で平均2.05ポイントのパフォーマンス向上を実現します。さらに、単一のマルチタスクモデルからタスク固有のモデルを微調整することで、最先端のパフォーマンス以上のパフォーマンスを達成できることを示します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br>2019-12-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: SemEval-2015 Task 10: Sentiment Analysis in Twitter -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.CL/paper_10.html">
      SemEval-2015 Task 10: Sentiment Analysis in Twitter
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、（C）単一のツイート内のトピックに対する感情、（D）一連のツイート内のトピックに対する感情、（E）フレーズの前の極性の程度を予測するよう求める3つの新しいサブタスクを追加しました。 。2つは前年からのリランでした：（A）ツイートの文脈でフレーズで表現された感情、および（B）ツイートの全体的な感情..今年の共有タスクの競争は、5つの感情予測サブタスクで構成されました。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br>2019-12-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Iterative Answer Prediction with Pointer-Augmented Multimodal
  Transformers for TextVQA -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.CL/paper_11.html">
      Iterative Answer Prediction with Pointer-Augmented Multimodal
  Transformers for TextVQA
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この作業では、画像内のテキストの豊富な表現を伴うマルチモーダルトランスフォーマアーキテクチャに基づいたTextVQAタスクの新規モデルを提案します。さらに、動的なポインタネットワークを使用した反復回答デコードを可能にし、モデルが回答を形成できるようにします私たちのモデルは、モダリティ間およびイントラモダリティコンテキストに自己注意が適用される共通のセマンティック空間にそれらを埋め込むことにより、異なるモダリティを自然に均一に融合します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-14">
        <br>2019-11-14
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Exploration of Neural Machine Translation in Autoformalization of
  Mathematics in Mizar -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.CL/paper_12.html">
      Exploration of Neural Machine Translation in Autoformalization of
  Mathematics in Mizar
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この論文では、非形式的な数学を形式的な数学に自動的に変換しようとするいくつかの実験を共有します。自動形式化に使用可能なデータを増やし、結果を改善するために、カスタム型エラボレーションメカニズムを開発し、教師付き翻訳に統合します。 。モデルが教師付きであるか教師なしであるかに応じて、結果を比較および分析します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br>2019-12-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Massive vs. Curated Word Embeddings for Low-Resourced Languages. The
  Case of Yorùbá and Twi -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.CL/paper_13.html">
      Massive vs. Curated Word Embeddings for Low-Resourced Languages. The
  Case of Yorùbá and Twi
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      作業の出力として、コーパス、埋め込み、および両方の言語のテストスーツを提供します。公的に利用可能なコーパス内のノイズを分析し、2つの言語の高品質でノイズの多いデータを収集し、データ量も品質も同じです。また、表面構造と文字の両方から単語表現を学習するさまざまなアーキテクチャを使用して、これらの言語にとって重要であることがわかったすべての利用可能な情報をさらに活用します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br>2019-12-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Enriching Existing Conversational Emotion Datasets with Dialogue Acts
  using Neural Annotators -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.CL/paper_14.html">
      Enriching Existing Conversational Emotion Datasets with Dialogue Acts
  using Neural Annotators
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらの神経モデルは、対話行為ラベルで感情コーパスに注釈を付け、アンサンブルアノテーターが最終対話行為ラベルを抽出します。ただし、テキストおよびマルチモーダルの会話感情データセットのほとんどは、感情行為ラベルのみを含み、対話行為は含みません。 /同意の対話行為は、喜びの感情、悲しみのある謝罪、喜びの感謝でしばしば起こります。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-02">
        <br>2019-12-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Love Me, Love Me, Say (and Write!) that You Love Me: Enriching the
  WASABI Song Corpus with Lyrics Annotations -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.CL/paper_15.html">
      Love Me, Love Me, Say (and Write!) that You Love Me: Enriching the
  WASABI Song Corpus with Lyrics Annotations
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      リソースの作成は現在も進行中です。これまでのところ、コーパスには、上記の方法の出力で異なるレベルで注釈が付けられた歌詞付きの173万曲（一意の歌詞）が含まれています。このようなコーパスラベルと提供された方法は、音楽検索エンジンや音楽専門家（たとえば、
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br>2019-12-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Drug Repurposing for Cancer: An NLP Approach to Identify Low-Cost
  Therapies -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.CL/paper_16.html">
      Drug Repurposing for Cancer: An NLP Approach to Identify Low-Cost
  Therapies
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最新の言語モデリング技術を利用して各モジュールで有望なパフォーマンスを獲得し、個々のコンポーネントの将来の改善のためのベースラインアプローチとして扱うことを計画しています。主な貢献は、そのような証拠を得るために必要な自然言語処理パイプラインを定義することです。次のモジュール：クエリ、フィルタリング、癌タイプエンティティ抽出、治療関連分類、および研究タイプ分類。チームの主題の専門知識を使用して、これらの特定のドメイン固有タスクの独自のデータセットを作成します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-18">
        <br>2019-11-18
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Fine-Grained Emotion Classification of Chinese Microblogs Based on Graph
  Convolution Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/cs.CL/paper_17.html">
      Fine-Grained Emotion Classification of Chinese Microblogs Based on Graph
  Convolution Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      実験では、幸福、悲しみ、怒り、嫌悪、恐怖、驚きを含む中国のマイクロブログの感情分類カテゴリについて、モデルのF尺度は82.32％に達し、最新のアルゴリズムを5.90％超えています。ディープラーニングアプローチはSAで広く使用されていますが、構文情報を十分に活用していません。さらに、モデルの精度を向上させるために、パーセンタイルに基づくプーリング方法が提案されています。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br>2019-12-05
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: VoxSRC 2019: The first VoxCeleb Speaker Recognition Challenge -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/eess.AS/paper_0.html">
      VoxSRC 2019: The first VoxCeleb Speaker Recognition Challenge
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      （i）YouTubeビデオから公開されている話者認識データセットと、グラウンドトゥルースアノテーションおよび標準化された評価ソフトウェア。 （ii）オーストリア、グラーツのInterspeech 2019で開催されたパブリックチャレンジとワークショップ。VoxCelebSpeaker Recognition Challenge 2019は、現在のスピーカー認識技術が、制約のない、または「インザワイルド」データのスピーカーをどれだけ識別できるかを評価することを目的としています。このペーパーでは、課題の概要を説明し、そのベースライン、結果、およびディスカッションを提供します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br>2019-12-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Towards Robust Neural Vocoding for Speech Generation: A Survey -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/eess.AS/paper_1.html">
      Towards Robust Neural Vocoding for Speech Generation: A Survey
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、将来の研究のために主観的な人間の評価のかなりの基準値で結果を提示します。最近、テキストから音声への変換や音声変換を含む音声合成タスクでニューラルボコーダーが広く使用されています。特にトレーニングデータとテストデータの間に矛盾がある場合、WaveRNNよりも堅牢です。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br>2019-12-05
      </time>
    </span>
  </h3>
</article>
<!-- paper0: MelGAN-VC: Voice Conversion and Audio Style Transfer on arbitrarily long
  samples using Spectrograms -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/eess.AS/paper_2.html">
      MelGAN-VC: Voice Conversion and Audio Style Transfer on arbitrarily long
  samples using Spectrograms
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      追加のシャムネットワークは、ターゲットスピーカーのスタイルを柔軟にモデル化する能力を犠牲にすることなく、翻訳プロセスで音声情報を保存するのに役立ちます。クリーンな音声録音のデータセットとノイズの多い現実のコレクションでフレームワークをテストします。最後に、同じ方法を使用して音楽スタイルの転送を実行し、任意の長い音楽サンプルをあるジャンルから別のジャンルに変換し、フレームワークが柔軟であり、音声変換とは異なるオーディオ操作アプリケーションに使用できることを示します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-08">
        <br>2019-10-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Audio-Visual Target Speaker Extraction on Multi-Talker Environment using
  Event-Driven Cameras -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2019-12-06/eess.AS/paper_3.html">
      Audio-Visual Target Speaker Extraction on Multi-Talker Environment using
  Event-Driven Cameras
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      モデルのパフォーマンスは、フレームベースの方法で得られたものに近いです。この制限を克服するために、高時間分解能と低レイテンシのため、イベント駆動型カメラの使用を提案します。後処理の前に理想的な振幅マスクを使用して、クリーンなオーディオ信号を取得します。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br>2019-12-05
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="">
      
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本日更新された論文はありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="">
        <br>
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
