<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-08-21の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Towards Robust Neural Vocoding for Speech Generation: A Survey -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.SD/paper_0.html">
      <font color="black">Towards Robust Neural Vocoding for Speech Generation: A Survey</font>
    </a>
  </h2>
  <font color="black">すべてのボコーダーの自然さをもたらす主観的なMOSの結果は、今後の研究のために大量に提示されます。私たちの実験を通じて、WaveNetとWaveRNNが音声合成モデルに、Parallel WaveGANが音声変換アプリケーションに適していることを示しています。このペーパーでは、WaveNet、WaveRNN、FFTNet、Parallel WaveGANを含む4つの一般的なニューラルボコーダーを5つの異なるデータセットで交互にトレーニングします。 
[ABSTRACT]実際のデータでトレーニングされたボコーダーは、多くの場合、目に見えないシナリオの音声品質を低下させます。研究によると、ボコーダーは主観的研究のために低下することがよくあります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br><font color="black">2019-12-05</font>
      </time>
    </span>
</section>
<!-- paper0: WG-WaveNet: Real-Time High-Fidelity Speech Synthesis without GPU -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.SD/paper_1.html">
      <font color="black">WG-WaveNet: Real-Time High-Fidelity Speech Synthesis without GPU</font>
    </a>
  </h2>
  <font color="black">音声サンプルはオンラインで公開されています。PyTorchの実装は、8 GB未満のGPUメモリを使用してトレーニングでき、NVIDIA 1080Ti GPUで960 kHzを超えるレートで音声サンプルを生成します。実験により、生成された音声の品質は他の方法と同等です。 
[要約]提案されたモデルは、コンパクトなフローベースのモデルとポストフィルターに基づいています。他の波形生成モデルよりも必要なリソースが少なくなります。提案された方法は、44。1センチメートルの音声波形1. 2倍の速度で生成できます。リアルタイム</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-15">
        <br><font color="black">2020-05-15</font>
      </time>
    </span>
</section>
<!-- paper0: CoVoST 2 and Massively Multilingual Speech-to-Text Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.SD/paper_2.html">
      <font color="black">CoVoST 2 and Massively Multilingual Speech-to-Text Translation</font>
    </a>
  </h2>
  <font color="black">それにもかかわらず、現在のデータセットは限られた数の言語に対応しています。大量の多言語音声翻訳と低リソース言語ペアの音声翻訳の研究を促進することを目的として、21言語からの翻訳をカバーする大規模多言語音声翻訳コーパスCoVoST 2をリリースしますこれは、英語から15言語に翻訳されています。これは、総量と言語カバレッジの観点から、これまでに利用可能な最大のオープンデータセットを表しています。 
[ABSTRACT]これは、総量と言語カバレッジの観点から、現在までに利用可能な最大のオープンデータセットです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-20">
        <br><font color="black">2020-07-20</font>
      </time>
    </span>
</section>
<!-- paper0: asya: Mindful verbal communication using deep learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.SD/paper_3.html">
      <font color="black">asya: Mindful verbal communication using deep learning</font>
    </a>
  </h2>
  <font color="black">すべてのモデルは言語に依存せず、リアルタイムで実行できます。これらのモデルは、カスタマーサービスの改善、販売効果の高い会話、心理学、カップルセラピーなどのさまざまな分野に適用できます。asyaは、ディープラーニングで構成されるモバイルアプリケーションです。人間の声のスペクトルを分析し、ノイズ検出、話者ダイアライゼーション、性別検出、テンポ推定、および音声のみを使用した感情の分類を行うモデル。 
[ABSTRACT]すべてのモデルは言語に強く、分析できます。これらのモデルは、顧客サービスの改善、販売効果の高い会話、心理学、カップルセラピーに適用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: SoundSpaces: Audio-visual Navigation in 3D Environments -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.SD/paper_4.html">
      <font color="black">SoundSpaces: Audio-visual Navigation in 3D Environments</font>
    </a>
  </h2>
  <font color="black">見ることと聞くことの両方によって、エージェントは、鳴っているオブジェクトに移動することを学ぶ必要があります。SoundSpacesをさらに紹介します。これは、公に利用可能な2つの3D環境のセットの幾何学的音響シミュレーションに基づく、オーディオレンダリングの最初のデータセット（Matterport3Dとレプリカ）、そして新しいセンサーをサポートするためにハビタットをインストルメント化し、実世界のスキャン環境の配列に任意の音源を挿入できるようにします。私たちの結果は、オーディオが3D空間での視覚化された視覚的ナビゲーションと私たちの仕事に大きな利益をもたらすことを示しています視聴覚を備えた統合型AIの新しい研究の基礎を築きます。 
[ABSTRACT]オーディオを導入-複雑で音響的および視覚的にリアルな3D環境のためのビジュアルナビゲーション</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-24">
        <br><font color="black">2019-12-24</font>
      </time>
    </span>
</section>
<!-- paper0: A Generalized Framework for Domain Adaptation of PLDA in Speaker
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.SD/paper_5.html">
      <font color="black">A Generalized Framework for Domain Adaptation of PLDA in Speaker
  Recognition</font>
    </a>
  </h2>
  <font color="black">さらに、提案された正則化手法は、補間に関する堅牢性を保証します。提案された相関配置ベースの補間方法は、適応前のドメイン外PLDAモデルからのものと比較して、minCprimaryを最大30.5％削減し、minCprimaryも、最適な補間重みを使用する従来の線形補間方法よりも5.5％低くなります。特に、ここでは、以下に説明する2つの新しい手法を紹介します。 
[要約]提案された方法は紙のペーパーで提案されています。データの使用を制限するなど、いくつかの既存のプログラムが含まれています。また、利用可能なデータのより柔軟な使用を可能にします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Exploring the Best Loss Function for DNN-Based Low-latency Speech
  Enhancement with Temporal Convolutional Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.SD/paper_6.html">
      <font color="black">Exploring the Best Loss Function for DNN-Based Low-latency Speech
  Enhancement with Temporal Convolutional Networks</font>
    </a>
  </h2>
  <font color="black">また、低レイテンシバージョンのTasNetを実装します。これは、DNSチャレンジに提出し、オープンソーシングによって公開しました。提案された方法は、ボイスバンク+ DEMANDデータセットで効果的であり、他の最新の状態と比較して有利です。 -art methods ..最も適切な方法は、データセットの規模とタスクのタイプによって異なります。 
[ABSTRACT]時間-周波数マスキングはdnn音声強調に広く使用されています。ただし、tasnetやtasnetなどの時間領域法も提案されています。この方法は、音声バンク需要データセットに効果的です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-23">
        <br><font color="black">2020-05-23</font>
      </time>
    </span>
</section>
<!-- paper0: Using Multi-Resolution Feature Maps with Convolutional Neural Networks
  for Anti-Spoofing in ASV -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.SD/paper_7.html">
      <font color="black">Using Multi-Resolution Feature Maps with Convolutional Neural Networks
  for Anti-Spoofing in ASV</font>
    </a>
  </h2>
  <font color="black">提案された方法は、異なるウィンドウ長を使用して抽出された複数のスペクトログラムを積み重ねることにより、周波数分解能と時間分解能の両方を改善します。これらは、複数のチャネルの形で畳み込みニューラルネットワークに供給され、計算コストがわずかに増加しています。提案された方法の効率は、ASVspoof 2019データベースに準拠しています。 
[要約]提案された方法は、異なるウィンドウ長を使用して抽出された複数のスペクトログラムを使用します。この方法は、スプーフィング対策データベースに準拠しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Attention and Encoder-Decoder based models for transforming articulatory
  movements at different speaking rates -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.SD/paper_8.html">
      <font color="black">Attention and Encoder-Decoder based models for transforming articulatory
  movements at different speaking rates</font>
    </a>
  </h2>
  <font color="black">現在の作業では、滑らかな予測調音軌跡を生成するLSTMを使用したエンコーダーデコーダーアーキテクチャを提案します。提案されたAstNetを使用して持続時間がどのように変換されるかを調べるために、音素固有の持続時間分析を実行します。アテンションネットワークを展開します。これにより、DTWを使用して軌跡を異なる速度で調整する必要がなくなります。 
[ABSTRACT]調音動作は、調音動作をニュートラルから高速（n2f）およびニュートラルから低速（n2s）の発話速度に変換するために使用されています。現在の作業では、滑らかな予測調音軌跡を生成するlstmsを使用したエンコーダー/デコーダーアーキテクチャを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br><font color="black">2020-06-04</font>
      </time>
    </span>
</section>
<!-- paper0: Speaker-Utterance Dual Attention for Speaker and Utterance Verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.SD/paper_9.html">
      <font color="black">Speaker-Utterance Dual Attention for Speaker and Utterance Verification</font>
    </a>
  </h2>
  <font color="black">提案されたSUDAは、話者と発話の情報ストリーム間の相互作用を学習するためのアテンションマスクメカニズムを備えています。このホワイトペーパーでは、話者の特性と言語コンテンツ間の相互作用を利用して、話者検証と発話検証のパフォーマンスの両方を向上させる新しい手法を研究します。 。これは、関係のないものをマスクすることにより、それぞれのタスクに必要な情報のみに焦点を合わせるのに役立ちます。 
[ABSTRACT]統合されたニューラルネットワークで話者-発話二重注意（suda）のアイデアを実装します。提案されたsudaは、話者と発話情報ストリーム間の相互作用を学習する注意マスクメカニズムを備えています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Similarity-and-Independence-Aware Beamformer: Method for Target Source
  Extraction using Magnitude Spectrogram as Reference -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.SD/paper_10.html">
      <font color="black">Similarity-and-Independence-Aware Beamformer: Method for Target Source
  Extraction using Magnitude Spectrogram as Reference</font>
    </a>
  </h2>
  <font color="black">最尤推定によりこの抽出問題を解決するために、類似性を反映できる2種類のソースモデルを紹介します。この研究では、ソース抽出用の類似性と独立性を意識したビームフォーマー（SIBF）と呼ばれる新しい方法を紹介します。そのような抽出を実現するために、参照と抽出されたターゲット間の類似性、およびすべての潜在的なソース間の相互独立性を考慮して、デフレの独立したコンポーネント分析のフレームワークを拡張します。 
[ABSTRACT] sibfは、大まかなマグニチュードスペクトログラムを参照信号として使用してターゲット信号を抽出できます。メソッドは、chime3データセットを使用してsibfを作成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-01">
        <br><font color="black">2020-06-01</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Single Image Super-Resolution via a Holistic Attention Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.IV/paper_0.html">
      <font color="black">Single Image Super-Resolution via a Holistic Attention Network</font>
    </a>
  </h2>
  <font color="black">一方、CSAMは、各チャネルのすべての位置で信頼性を学習して、より有益な機能を選択的にキャプチャします。情報提供機能は、単一画像超解像タスクで重要な役割を果たします。特に、提案されたLAMは、レイヤー。 
[ABSTRACT]レイヤー、チャネル、位置の間の全体的な相互依存性をモデル化するための新しい全体的注意ネットワーク（han）を提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Generative View Synthesis: From Single-view Semantics to Novel-view
  Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.IV/paper_1.html">
      <font color="black">Generative View Synthesis: From Single-view Semantics to Novel-view
  Images</font>
    </a>
  </h2>
  <font color="black">さらにエンベロープを押し進めることを提案し、単一のセマンティックマップが与えられたシーンの複数の写実的なビューを合成できる\ emph {Generative View Synthesis}（GVS）を導入します。次に、レイヤードフィーチャをターゲットビューに投影して、最終的な小説ビューの画像.. https://gvsnet.github.ioのプロジェクトページにアクセスしてください。 
[ABSTRACT]デジタル画像を使用して、わずか1つの入力画像から新しいビューを作成できます。また、異なるデータセットを組み合わせることにより、写実的な画像に変換することもできます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Assessing the Quality-of-Experience of Adaptive Bitrate Video Streaming -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.IV/paper_2.html">
      <font color="black">Assessing the Quality-of-Experience of Adaptive Bitrate Video Streaming</font>
    </a>
  </h2>
  <font color="black">最も重要なことに、私たちの結果は、高度な最適化フレームワーク、機械学習戦略、または帯域幅予測子とは対照的に、より客観的なQoEモデル、または人間の知覚経験と行動のより良い理解が、ABRアルゴリズムのパフォーマンスを向上させる最も支配的な要因であることを示唆しています、過去10年間にABR研究の大部分が焦点を当てられていました。慎重に設計された一連の主観的実験により、各ビデオの人間の意見を収集します。データベースを使用したABRアルゴリズムとQoEモデルの後続のデータ分析とテスト/比較主観的な実験方法の有効性、ユーザーエクスペリエンスとソースコンテンツ間の相互作用、デバイスとエンコーダーの種類の表示、ユーザーエクスペリエンスのバイアスと好みの不均一性、動作の観点から、一連の新しい観察と興味深い発見につながりますABRアルゴリズム、および客観的なQoEモデルのパフォーマンス。 
[ABSTRACT] waterloosqoe-ivは、さまざまなソースコンテンツ、ビデオエンコーダー、ネットワークトレース、ABRアルゴリズム、および表示デバイスから作成された1350アダプティブストリーミングビデオで構成されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: iPhantom: a framework for automated creation of individualized
  computational phantoms and its application to CT organ dosimetry -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.IV/paper_3.html">
      <font color="black">iPhantom: a framework for automated creation of individualized
  computational phantoms and its application to CT organ dosimetry</font>
    </a>
  </h2>
  <font color="black">方法：患者のCT画像から、iPhantomセグメントはアンカー臓器を選択しました（例：iPhantomはアンカー臓器に対してダイス類似度係数（DSC）&gt; 0.6、その他すべての臓器に対してDSC 0.3-0.9の精度ですべての臓器位置を正確に予測しました。.目的：この研究の目的は、患者の医用画像を使用して患者固有のファントムまたはデジタルツイン（DT）を自動作成するための新しいフレームワーク、iPhantomを開発して検証することです。
[要約]フレームワークは、ctイメージングで放射線感受性臓器への放射線量を評価するために適用されます個々の患者の</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: DronePose: Photorealistic UAV-Assistant Dataset Synthesis for 3D Pose
  Estimation via a Smooth Silhouette Loss -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.IV/paper_4.html">
      <font color="black">DronePose: Photorealistic UAV-Assistant Dataset Synthesis for 3D Pose
  Estimation via a Smooth Silhouette Loss</font>
    </a>
  </h2>
  <font color="black">トレーニング中に、差別化可能なレンダリングを利用して、最新の直接回帰目標を新しい滑らかなシルエットの損失で補完します。結果は、従来のシルエット目標に対する質的および量的なパフォーマンスの向上を示しています。次に、フォトリアリスティックの共同可用性を活用しますシングルショット単眼姿勢推定モデルをトレーニングするための合成入力。 
[要約] UAVアシスタントの3Dローカリゼーションは重要なタスクです。次に、フォトリアリスティックと現在のプレゼントの共同利用可能性を活用します。また、単発単眼姿勢推定モデルを開発します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Real-Time Cardiac Cine MRI with Residual Convolutional Recurrent Neural
  Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.IV/paper_5.html">
      <font color="black">Real-Time Cardiac Cine MRI with Residual Convolutional Recurrent Neural
  Network</font>
    </a>
  </h2>
  <font color="black">放射線科医の評価に基づいて、ディープラーニングモデルは圧縮センシングよりも優れたパフォーマンスを示します。リアルタイムの心臓シネ再構成用に残差たたみ込みRNNを提案します。ただし、高速の画像取得を実現するには、リアルタイムシネMRI画像再構成に大きな課題を課すデータ。 
[ABSTRACT]これは、深層学習アプローチをカートルのリアルタイム心臓シネ再構成に適用した最初の作品です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-12">
        <br><font color="black">2020-08-12</font>
      </time>
    </span>
</section>
<!-- paper0: Partial Volume Segmentation of Brain MRI Scans of any Resolution and
  Contrast -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.IV/paper_6.html">
      <font color="black">Partial Volume Segmentation of Brain MRI Scans of any Resolution and
  Contrast</font>
    </a>
  </h2>
  <font color="black">PV-SynthSegは、PVの生成モデルを使用してHRラベルマップからLR画像をシミュレートし、画像やセグメンテーションがトレーニングで利用できない以前には見えなかったモダリティでも、任意のターゲットコントラストと解像度のスキャンをセグメント化するようにトレーニングできます。 3つのデータセットと2,680スキャンでの広範な実験による方法の精度と柔軟性。PVは、アトラスとテストスキャンの間に大きな解像度のギャップがある場合、たとえば厚いスライスで臨床スキャンをセグメント化する場合など、セグメンテーションで特に問題になります。高解像度アトラスを使用する場合。 
[ABSTRACT] pvは、有毒物質に複数の組織クラスが含まれている場合に発生し、基になるクラスの1つを表していない可能性がある画像強度を生じさせます。pv-synthseg、畳み込みニューラルネットワークは、（おそらく複数の-モーダル）低解像度（lr）スキャンおよび基礎となる高解像度（hr）セグメンテーション</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br><font color="black">2020-04-21</font>
      </time>
    </span>
</section>
<!-- paper0: Maxwell Parallel Imaging -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.IV/paper_7.html">
      <font color="black">Maxwell Parallel Imaging</font>
    </a>
  </h2>
  <font color="black">結果として得られるメソッドは、Maxwell Parallel Imaging（MPI）と呼ばれ、任意のシーケンス（2Dと3Dの両方）で任意の軌道と最小限のキャリブレーション信号を使用してシームレスに機能します。任意の加速スキャンからの再構成..この論文では、SMの基本的な特性に依存します-それらはマクスウェル方程式の解です-与えられた視野でサポートされるすべての可能なSM分布の部分空間を構築し、このサブスペースに属するSMのソリューション。 
[ABSTRACT] mpiは、すべてのデータセットに物理学に着想を得たソリューションを提供します。すべてのデータに物理学制度のメソッドを提供します。これは、最新のpiメソッドと比較されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data
  Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.IV/paper_8.html">
      <font color="black">Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data
  Generation</font>
    </a>
  </h2>
  <font color="black">Meta-Sim2は、所定の確率的シーン文法からルール展開を順次サンプリングすることを学習することによって続行します。実際の運転データセットでの実験は、監視なしで、オブジェクトなどのオブジェクトの離散構造統計をキャプチャするデータを生成することを学習できることを示しています周波数、実際の画像.. Meta-Sim2では、パラメーターに加えてシーンの構造を学習することを目指しています。これは、その離散的な性質のために困難な問題です。 
[ABSTRACT]リアルなシーンを作成するには、手順モデルを管理する多数のパラメータを専門家が注意深く調整する必要があります。これらには、有効な構造でオブジェクトを制御する魅力的なパラメータが含まれます。メタ-sim2は、パラメータに加えてシーン構造を学習することを目的としています、それはその離散的な性質のために挑戦的な問題です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Image quality assessment for closed-loop computer-assisted lung
  ultrasound -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.IV/paper_9.html">
      <font color="black">Image quality assessment for closed-loop computer-assisted lung
  ultrasound</font>
    </a>
  </h2>
  <font color="black">十分な品質のデータの場合、COVID-19陽性症例の検出における平均分類精度は、提案されたシステム内のネットワークのトレーニング中に見られなかった5つのホールドアウトテストデータセットで95％でした。診断支援モジュールは、十分な品質と見なされるデータ。品質評価モジュールからの閉ループフィードバックメカニズムによって保証されます。提案されたシステムは、2つの深層学習ベースのモデルで構成されます。 
[ABSTRACT]システムは2つのディープラーニングベースのケースで構成されます。システムは、新規性検出アルゴリズムを使用して、制御ケースの欠如に対処します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Multimodal Image-to-Image Translation via Mutual Information Estimation
  and Maximization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.IV/paper_10.html">
      <font color="black">Multimodal Image-to-Image Translation via Mutual Information Estimation
  and Maximization</font>
    </a>
  </h2>
  <font color="black">この論文では、条件付き生成敵対ネットワークにおける潜在コードと出力画像の間の統計的依存を単に奨励することにより、マルチモーダルな画像から画像への変換を実現できる新しいフレームワークを提示します。ドメインコンテンツとターゲットドメインスタイルを無料で提供します。最新の方法と比較して、さまざまなベンチマークの画像から画像への翻訳データセットに対して、監視ありおよび監視なしの設定で実験を行い、メソッドの有効性とシンプルさを示しています。マルチモーダルで高品質の結果を実現します。 
[要旨]さまざまなベンチマーク画像からスポットへの方法で、監視ありおよび監視なしの設定で実験を行います。この方法では、両方のソース画像ドメインからターゲット画像ドメインへの片側変換モデルを学習するだけで済みます。教師ありまたは教師なしマルチモーダル画像-から-画像への変換</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-08">
        <br><font color="black">2020-08-08</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic lung segmentation in routine imaging is primarily a data
  diversity problem, not a methodology problem -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.IV/paper_11.html">
      <font color="black">Automatic lung segmentation in routine imaging is primarily a data
  diversity problem, not a methodology problem</font>
    </a>
  </h2>
  <font color="black">多様なルーチンデータセット（n = 36）でトレーニングすると、標準的なアプローチ（U-net）では、Lung Tissue Research Consortiumなどのパブリックデータセット（0.94 $ \ pm $）でのトレーニングと比較して、DSC（0.97 $ \ pm $ 0.05）が高くなります。 0.13、p = 0.024）または解剖学3（0.92 $ \ pm $ 0.15、p = 0.001）。複数の疾患をカバーするルーチンデータ（n = 231）でトレーニングされ、参照メソッドと比較したU-netは0.98 $ \のDSCを生成しますpm $ 0.03対0.94 $ \ pm $ 0.12（p = 0.024）。6つ以上の異なる疾患パターンと3つの公開されたデータセットを使用して、ルーチンイメージングデータの評価を行いました。 
[要約]肺のセグメンテーションには、さまざまなアプローチが存在します。これらには、さまざまなデータセットでトレーニングおよび検証された高度なパイプラインが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-31">
        <br><font color="black">2020-01-31</font>
      </time>
    </span>
</section>
<!-- paper0: Deep learning-based transformation of the H&E stain into special stains
  improves kidney disease diagnosis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.IV/paper_12.html">
      <font color="black">Deep learning-based transformation of the H&E stain into special stains
  improves kidney disease diagnosis</font>
    </a>
  </h2>
  <font color="black">3人の腎臓病理学者による評価に続いて4人目の腎臓病理学者による判決に基づいて、既存のH＆E画像からの仮想特殊染色の生成により、16人の固有の被験者からサンプリングされたいくつかの非腫瘍性腎疾患の診断が改善されることを示します。 H＆E画像の特別な染色への仮想変換は、患者のコア標本スライドあたり1分未満で達成できます。この染色から染色への変換フレームワークは、追加の特別な染色が必要な場合の予備診断の品質を向上させるとともに、 3人の病理医によるN = 48診断の裁定により、仮想的に生成された特殊な染色により22の改善（45.8％）、23の一致（47.9％）および3の不一致（6.3 ％）、H＆E染色組織のみの使用と比較した場合。 
[ABSTRACT]最も一般的には、ヘマトキシリンとエオシン（h＆e）染色が診断ワークフローで使用されます。この研究では、教師あり学習のトリクローム、周期的酸-シッフおよびジョーンズシルバー染色の有用性を示しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Interpretation of Brain Morphology in Association to Alzheimer's Disease
  Dementia Classification Using Graph Convolutional Networks on Triangulated
  Meshes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.IV/paper_13.html">
      <font color="black">Interpretation of Brain Morphology in Association to Alzheimer's Disease
  Dementia Classification Using Graph Convolutional Networks on Triangulated
  Meshes</font>
    </a>
  </h2>
  <font color="black">皮質および皮質下の表面情報の利用を活用した提案手法では、ADDと健全な制御の問題について96.35％のテスト精度で他の機械学習手法よりも優れています。この研究で生成された可視化マップは、アルツハイマー型の認知症に関連する脳の病理学的変化の構造的局在。しばしば、自動化された医療診断のためのこれらのアプローチはまた、診断を行うことに関与する脳の領域の視覚的解釈性を欠いている。 
[要約]ディープラーニングメソッドは、最適化するために多くの場合、広範な学習パラメーターを必要とします。これらには、最先端のネットワークのための皮質下学習フレームワークが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-14">
        <br><font color="black">2020-08-14</font>
      </time>
    </span>
</section>
<!-- paper0: Revisiting Temporal Modeling for Video Super-resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.IV/paper_14.html">
      <font color="black">Revisiting Temporal Modeling for Video Super-resolution</font>
    </a>
  </h2>
  <font color="black">広範な実験により、提案されたRRNは非常に計算効率が高く、他の時間モデリング方法よりも細かい時間一貫性のあるVSR結果が生成されることが示されています。効率的なビデオ超解像のための新しいRecurrent Residual Network（RRN）を提案します。残差学習は、RNNのトレーニングを安定させるために利用され、その間、超解像パフォーマンスを向上させます。 
[要約]提案された方法は、広く使用されているいくつかのベンチマークで状態および現在の残余ネットワークの結果を達成しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Facial movement synergies and Action Unit detection from distal wearable
  Electromyography and Computer Vision -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.IV/paper_15.html">
      <font color="black">Facial movement synergies and Action Unit detection from distal wearable
  Electromyography and Computer Vision</font>
    </a>
  </h2>
  <font color="black">この方法は、独立成分分析（ICA）、非負行列因数分解（NNMF）、および結果の成分のソートに基づいており、各CVラベル付きアクションユニット（AU）に対応する可能性が最も高いものを決定します。遠位の筋電図が個々の顔の動きを検出する可能性。特定の顔面の行動単位（AU）を遠位の顔面筋電図とコンピュータビジョン（CV）から推定する新しい方法を提案します。 
[要旨]顔の動きの正確な原因は不明です。この方法は顔の動作単位（au）に基づいています。特定の筋肉活動を検出するために使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Uncertainty Estimation in Medical Image Denoising with Bayesian Deep
  Image Prior -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.IV/paper_16.html">
      <font color="black">Uncertainty Estimation in Medical Image Denoising with Bayesian Deep
  Image Prior</font>
    </a>
  </h2>
  <font color="black">ただし、大規模なデータセットでトレーニングされた深いモデルは、幻覚を起こし、解剖学的に存在しないアーティファクトを再構築された出力に作成する傾向があります。偶然と認識の両方の不確実性を定量化するためのドロップアウト。 
[ABSTRACT]大規模なデータセットでトレーニングされたディープモデルは幻覚を起こし、解剖学的に存在しないアーティファクトを作成する傾向があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Attentive One-Dimensional Heatmap Regression for Facial Landmark
  Detection and Tracking -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_0.html">
      <font color="black">Attentive One-Dimensional Heatmap Regression for Facial Landmark
  Detection and Tracking</font>
    </a>
  </h2>
  <font color="black">4つのベンチマークデータベースでの実験結果は、本手法の優位性を示しています。ランドマーク追跡のための時間調整メカニズムを使用して時間パターンをさらにキャプチャするトラッカー。最初に、x座標とy座標の周辺分布を表す1Dヒートマップの2つのグループを予測します。 
[要約] 1dヒートマップは、現在のヒートマップ53メソッドと比較して、空間の複雑さを大幅に軽減します。これらの1dヒートマップは、x座標とy座標に存在する正確な正確なパターンを予測します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-05">
        <br><font color="black">2020-04-05</font>
      </time>
    </span>
</section>
<!-- paper0: Fully Trainable and Interpretable Non-Local Sparse Models for Image
  Restoration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_1.html">
      <font color="black">Fully Trainable and Interpretable Non-Local Sparse Models for Image
  Restoration</font>
    </a>
  </h2>
  <font color="black">このアプローチをノイズ除去、jpeg非ブロック化、およびデモザイキングに適用し、わずか100Kのパラメーターで、いくつかの標準ベンチマークでのパフォーマンスが、次の順序を持つ可能性のある最新のメソッドと同等かそれ以上であることを示しますマグニチュード以上のパラメーター..両方の原則を活用し、（1）エンドツーエンドでトレーニング可能、（2）完全に解釈可能、および（3）より多くの画像復元のための一般的なフレームワークにつながる、ジョイントスパースの新しい微分可能な緩和を提案します。競合するディープラーニングアーキテクチャよりもコンパクトです。非ローカルな自己相似性とスパース性の原則は、自然な画像モデリングの強力な前提条件であることが証明されています。 
[ABSTRACT]スパース性へのシンプルシンプルシンプルセルフコントロールアプローチが証明されています。方法は、将来のための新しいモデルを作成することです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br><font color="black">2019-12-05</font>
      </time>
    </span>
</section>
<!-- paper0: OREBA: A Dataset for Objectively Recognizing Eating Behaviour and
  Associated Intake -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_2.html">
      <font color="black">OREBA: A Dataset for Objectively Recognizing Eating Behaviour and
  Associated Intake</font>
    </a>
  </h2>
  <font color="black">データ収集と注釈の詳細、およびセンサー処理の詳細を報告します。摂取ジェスチャーの自動検出は、食事の自動監視の重要な要素です。このニーズを満たすために、私たちは客観的に認識している摂食行動と関連する摂取量（ OREBA）データセット。 
[要約] orebaデータセットは、共同摂取状況の包括的なマルチセンサー記録を提供することを目的としています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-31">
        <br><font color="black">2020-07-31</font>
      </time>
    </span>
</section>
<!-- paper0: Attribute Prototype Network for Zero-Shot Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_3.html">
      <font color="black">Attribute Prototype Network for Zero-Shot Learning</font>
    </a>
  </h2>
  <font color="black">CUBデータセットについて、画像表現の改善された属性ローカライゼーション能力を確認します。このために、クラスレベルの属性のみを使用して差別的なグローバルおよびローカル機能を共同で学習する新しいゼロショット表現学習フレームワークを提案します。利点として、私たちのモデルは画像内の属性の視覚的証拠を指します。たとえば、
[要約]コードはwwwで公開されます。ウェンジアクス。 io / apn-zsl /。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-19">
        <br><font color="black">2020-08-19</font>
      </time>
    </span>
</section>
<!-- paper0: Single Image Super-Resolution via a Holistic Attention Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_4.html">
      <font color="black">Single Image Super-Resolution via a Holistic Attention Network</font>
    </a>
  </h2>
  <font color="black">具体的には、提案されたLAMは、レイヤー間の相関を考慮して、階層的特徴を適応的に強調します。ただし、チャネルアテンションは、各畳み込みレイヤーを異なるレイヤー間の相関を見落とす個別のプロセスとして扱います。一方、CSAMは、各チャンネルのすべての位置で信頼度を学習します。より有益な機能を選択的にキャプチャします。 
[ABSTRACT]レイヤー、チャネル、位置の間の全体的な相互依存性をモデル化するための新しい全体的注意ネットワーク（han）を提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Learning of Depth, Optical Flow and Pose with Occlusion
  from 3D Geometry -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_5.html">
      <font color="black">Unsupervised Learning of Depth, Optical Flow and Pose with Occlusion
  from 3D Geometry</font>
    </a>
  </h2>
  <font color="black">自動運転では、単眼シーケンスに多くの情報が含まれます。奥行きポーズとオプティカルフローによって再構築された画像は、閉塞領域では無効になるため、奥行き、ポーズ、オプティカルフローの教師なし学習でオクルージョン情報が使用されます。この方法はオプティカルフローネットワークのトレーニングで、いくつかの些細な不一致ピクセルを除外するためにも使用されます。 
[ABSTRACT]深度とポーズの教師なしトレーニングでは、オクルードされた領域を明示的にセグメント化できます。平均未満のマスクは、激しいネットワークとポーズのネットワークのトレーニングでの動きやイラストの変更によって干渉される不一致のピクセルをさらに除外するように設計されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-02">
        <br><font color="black">2020-03-02</font>
      </time>
    </span>
</section>
<!-- paper0: Contrastive Learning for Unpaired Image-to-Image Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_6.html">
      <font color="black">Contrastive Learning for Unpaired Image-to-Image Translation</font>
    </a>
  </h2>
  <font color="black">画像から画像への変換では、出力の各パッチは、ドメインに関係なく、入力の対応するパッチの内容を反映する必要があります。私たちのフレームワークが、対になっていない画像から画像への変換設定で片側変換を有効にすることを示します、品質を改善し、トレーニング時間を短縮します。特に、画像全体を操作するのではなく、多層パッチベースのアプローチを使用します。 
[要約]コントラスト学習に基づくフレームワークを使用して、2つの間の相互情報を最大化する簡単な方法を提案します。メソッドは、各「ドメイン」が単一の画像のみであるトレーニング設定に拡張することもできます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-30">
        <br><font color="black">2020-07-30</font>
      </time>
    </span>
</section>
<!-- paper0: iCaps: An Interpretable Classifier via Disentangled Capsule Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_7.html">
      <font color="black">iCaps: An Interpretable Classifier via Disentangled Capsule Networks</font>
    </a>
  </h2>
  <font color="black">この作業では、新規のクラス教師付き解絡アルゴリズムと追加の正則化器をそれぞれ使用して、これら2つの制限に対処します。3つのデータセットの定量的および定性的評価を通じて、結果の分類子であるiCapsが明確な根拠とともに予測を提供することを示しますただし、その解釈可能性を低下させる2つの制限があります。1）クラスカプセルには分類に関連しない情報も含まれます。2）クラスカプセルのオーバーラップによって表されるエンティティです。 
[ABSTRACT]結果の分類子icapsは、パフォーマンスを低下させることなく、背後にある明確な根拠とともに予測を提供することを示しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Generative View Synthesis: From Single-view Semantics to Novel-view
  Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_8.html">
      <font color="black">Generative View Synthesis: From Single-view Semantics to Novel-view
  Images</font>
    </a>
  </h2>
  <font color="black">まず、入力2Dセマンティックマップをフィーチャ空間のシーンの3Dレイヤー表現に持ち上げ、3D幾何学的構造のセマンティックラベルを保持します。対照的に、セマンティクスから画像への変換を、シーンの3Dレイアウト。これにより、セマンティック構造を保持する幾何学的に一貫した斬新なビューが生成されます。https：//gvsnet.github.ioのプロジェクトページにアクセスしてください。 
[ABSTRACT]デジタル画像を使用して、わずか1つの入力画像から新しいビューを作成できます。また、異なるデータセットを組み合わせることにより、写実的な画像に変換することもできます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Anchor-free Small-scale Multispectral Pedestrian Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_9.html">
      <font color="black">Anchor-free Small-scale Multispectral Pedestrian Detection</font>
    </a>
  </h2>
  <font color="black">挑戦的なKAISTマルチスペクトル歩行者検出ベンチマークで、最新の7.49％（25％改善）の最高の最新技術と比較して、5.68％のログ平均ミス率を達成しています。オブジェクトセンターに基づいて歩行者の表現を学習することを目指しています。このようにして、ネットワークアーキテクチャを簡素化し、特にオクルージョン下または低オブジェクト解像度での歩行者に対して、より高い検出パフォーマンスを実現できます。 
[ABSTRACT]これらのデータは、歩行者検出のパフォーマンスを向上させるために使用できます。これは、小規模な、または部分的に遮蔽されたケースに基づいています。結果は、小規模な歩行者の検出における私たちの方法の技術を示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-19">
        <br><font color="black">2020-08-19</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Learning Facial Parameter Regressor for Action Unit
  Intensity Estimation via Differentiable Renderer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_10.html">
      <font color="black">Unsupervised Learning Facial Parameter Regressor for Action Unit
  Intensity Estimation via Differentiable Renderer</font>
    </a>
  </h2>
  <font color="black">リグレッサは、ジェネレータを使用して、単一の顔画像からBDFMの物理的意味パラメータを適合させることができます。ジェネレータは、顔パラメータを微分可能なレンダラーとしてゲームの顔画像にマップします。さらに、ID損失、ループバック損失、および敵対的損失は、回帰結果を改善する可能性があります。ほとんどの既存の方法は、AUデータが限定された強度推定器を学習しますが、データセットからの汎化能力を欠いています。 
[要約]提案されたフレームワークは、特徴抽出器、ジェネレーター、および顔面プロファイルモデルで構成されます。新しい方法は、損失、ループバック損失、および敵対的損失を識別するために使用できます。結果は、実際の方法の有効性を示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Utilizing Explainable AI for Quantization and Pruning of Deep Neural
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_11.html">
      <font color="black">Utilizing Explainable AI for Quantization and Pruning of Deep Neural
  Networks</font>
    </a>
  </h2>
  <font color="black">説明可能なAIメソッドを使用すると、さまざまなニューロンや機能の重要性など、DNNの内部動作をよりよく理解できます。説明可能なAIの概念は、量子化やプルーニングなどのDNN圧縮メソッドを、これまでにない方法で改善する機会を提供しますこれまでに十分に調査されました。このホワイトペーパーでは、説明可能なAIメソッドを使用します。主に、DeepLIFTメソッドです。 
[ABSTRACT]説明可能なAIメソッドにより、さまざまなニューロンや機能の重要性など、DNSの内部動作をよりよく理解できます。これらには、ディープリフトメソッドが含まれます。これらの方法：説明可能</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Bookworm continual learning: beyond zero-shot learning and continual
  learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_12.html">
      <font color="black">Bookworm continual learning: beyond zero-shot learning and continual
  learning</font>
    </a>
  </h2>
  <font color="black">また、過去と未来の両方のクラスの機能が生成されるBCLに対処するための双方向想像（BImag）フレームワークを提案します。本の虫の継続的な学習（BCL）を提案します。ビジュアルモデルは継続的に更新できます。属性でフィーチャジェネレーターを調整すると、継続的な学習能力が実際に損なわれる可能性があることを観察し、この問題を緩和するために2つのバリアント（結合クラス属性条件付けと非対称生成）を提案します。 
[ABSTRACT]属性に基づいて特徴ジェネレーターを調整すると、継続的な学習能力が損なわれる可能性があることを確認しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-26">
        <br><font color="black">2020-06-26</font>
      </time>
    </span>
</section>
<!-- paper0: Spatial--spectral FFPNet: Attention-Based Pyramid Network for
  Segmentation and Classification of Remote Sensing Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_13.html">
      <font color="black">Spatial--spectral FFPNet: Attention-Based Pyramid Network for
  Segmentation and Classification of Remote Sensing Images</font>
    </a>
  </h2>
  <font color="black">高解像度とハイパースペクトルのリモートセンシング画像のセグメンテーションと分類の問題を検討します。この研究では、リモートセンシングデータセットのセグメンテーションと分類のための注意ベースのピラミッドネットワークを開発します。次に、エンドツーエンドの空間--spectral FFPNetは、ハイパースペクトル画像を分類するために提供されます。 
[要約]リモートセンシング画像に固有の大規模で複雑な構造は、既存のモデルを画像分類に直接適用すると、空間オブジェクト分布の多様性や情報抽出などの大きな課題を引き起こします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Weakly-supervised 3D Shape Completion in the Wild -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_14.html">
      <font color="black">Weakly-supervised 3D Shape Completion in the Wild</font>
    </a>
  </h2>
  <font color="black">さらに、学習されたポーズ推定は、部分的な点群の登録を容易にすることができます。実際のセンサーで取得された部分的な点群は通常、まばらで、ノイズが多く、整列していないため、実際のデータの3D形状の完成は重要ですが、困難です。このために、同じインスタンスに関連付けられた複数の部分観測が与えられた場合に、位置合わせのために3Dカノニカルシェイプと6-DoFポーズの両方を推定するための弱監視手法。 
[ABSTRACT]形状とポーズの監視なしで大規模なデータを通じて3D形状の完成を学習することが可能</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Simultaneously-Collected Multimodal Lying Pose Dataset: Towards In-Bed
  Human Pose Monitoring under Adverse Vision Conditions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_15.html">
      <font color="black">Simultaneously-Collected Multimodal Lying Pose Dataset: Towards In-Bed
  Human Pose Monitoring under Adverse Vision Conditions</font>
    </a>
  </h2>
  <font color="black">このタスクの1つは、ベッド内の人間の姿勢推定です。これは、多くのヘルスケアアプリケーションで重要な値を持っています。また、消灯や完全に覆われているなどの極端な条件下でグラウンドトゥルースポーズラベルを生成するための物理ハイパーパラメータ調整戦略を示します。シート/毛布..このペーパーでは、RGB、長波赤外線、深度、および圧力マップを含む複数のイメージングモダリティを使用してキャプチャされた109人の参加者からのベッド内ポーズ画像を含む、同時収集マルチモーダル横臥ポーズ（SLP）データセットを紹介します。 。 
[ABSTRACT] in-人間のポーズポーズは人間のポーズのモデルです。モデルモデルモデル極端な状態に対処するためのモデルモデルモデル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: iPhantom: a framework for automated creation of individualized
  computational phantoms and its application to CT organ dosimetry -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_16.html">
      <font color="black">iPhantom: a framework for automated creation of individualized
  computational phantoms and its application to CT organ dosimetry</font>
    </a>
  </h2>
  <font color="black">方法：患者のCT画像から、iPhantomセグメントはアンカー臓器を選択します（例：フレームワークは、個々の患者のCTイメージングにおける放射線感受性臓器への放射線量を評価するために適用されます。.目的：この研究は、新しいフレームワーク、iPhantomの開発と検証を目的としています患者の医用画像を使用した患者固有のファントムまたはデジタル双子（DT）の自動作成。
[要約]フレームワークは、個々の患者のctイメージングにおける放射線感受性臓器への放射線量を評価するために適用</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: DronePose: Photorealistic UAV-Assistant Dataset Synthesis for 3D Pose
  Estimation via a Smooth Silhouette Loss -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_17.html">
      <font color="black">DronePose: Photorealistic UAV-Assistant Dataset Synthesis for 3D Pose
  Estimation via a Smooth Silhouette Loss</font>
    </a>
  </h2>
  <font color="black">トレーニング中に、差別化可能なレンダリングを利用して、最新の直接回帰目標に斬新な滑らかなシルエットの損失を補います。次に、フォトリアリスティックな入力と合成された入力の両方の可用性を活用して、単発単眼姿勢推定モデルをトレーニングします。私たちの結果は、従来のシルエットの目標を超えた質的および量的なパフォーマンスの向上を示しています。 
[要約] UAVアシスタントの3Dローカリゼーションは重要なタスクです。次に、フォトリアリスティックと現在のプレゼントの共同利用可能性を活用します。また、単発単眼姿勢推定モデルを開発します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Partial Volume Segmentation of Brain MRI Scans of any Resolution and
  Contrast -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_18.html">
      <font color="black">Partial Volume Segmentation of Brain MRI Scans of any Resolution and
  Contrast</font>
    </a>
  </h2>
  <font color="black">部分ボリューム化（PV）は、おそらく、確率的アトラスを使用した脳MRIのベイズセグメンテーションにおける最後の重要な未解決の問題です。PVは、アトラスとテストスキャンの間に大きな解像度のギャップがある場合（セグメンテーションなど）に特に問題です。厚いスライス、または高解像度アトラスを使用する場合。PVはボクセルに複数の組織クラスが含まれている場合に発生し、基になるクラスの1つを代表しない可能性がある画像強度を生じさせます。 
[ABSTRACT] pvは、有毒物質に複数の組織クラスが含まれている場合に発生し、基になるクラスの1つを表していない可能性がある画像強度を生じさせます。pv-synthseg、畳み込みニューラルネットワークは、（おそらく複数の-モーダル）低解像度（lr）スキャンおよび基礎となる高解像度（hr）セグメンテーション</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-21">
        <br><font color="black">2020-04-21</font>
      </time>
    </span>
</section>
<!-- paper0: Document Visual Question Answering Challenge 2020 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_19.html">
      <font color="black">Document Visual Question Answering Challenge 2020</font>
    </a>
  </h2>
  <font color="black">課題は2つのタスクで構成されていました。課題により、新しい問題が発生しました-ドキュメントの画像に対する視覚的な質問応答。最初のタスクは、単一のドキュメントの画像に対して質問することに関するものです。 
[ABSTRACT]最初のタスクは、単一のドキュメント画像で質問することに関するものです。他のタスクには、単一の画像で質問することが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Deformable PV-RCNN: Improving 3D Object Detection with Learned
  Deformations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_20.html">
      <font color="black">Deformable PV-RCNN: Improving 3D Object Detection with Learned
  Deformations</font>
    </a>
  </h2>
  <font color="black">有益なコンテンツが存在する場所からインスタンス固有の機能を適応的に収集できる2D変形可能たたみ込みネットワークに触発された提案改良モジュールを提示します。KITTIデータセットに最新の結果を表示します。変形可能なPV-RCNNを提示します、高性能の点群ベースの3Dオブジェクト検出器。 
[要約]最先端の2ステージ検出器は、さまざまなオブジェクトスケール、さまざまなポイントクラウドネットワークに対応できません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Object Properties Inferring from and Transfer for Human Interaction
  Motions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_21.html">
      <font color="black">Object Properties Inferring from and Transfer for Human Interaction
  Motions</font>
    </a>
  </h2>
  <font color="black">私たちは同様のアクションを分析し、それらの間の微妙な違いを学習して、相互作用するオブジェクトの潜在的な特性を明らかにします。慣性モーションキャプチャデバイスを使用して、実行している俳優の多数のビデオと3D骨格モーションを収集しました。この推論により、オブジェクトプロパティからモーションを取得し、オブジェクトプロパティを特定のモーションに転送します。 
[ABSTRACT]モーション認識方法は、人間の相互作用のモーションのみからモーションを推測することを学習します。モーションだけからそのような潜在的なオブジェクトのプロパティを推測することができます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: ViewSynth: Learning Local Features from Depth using View Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_22.html">
      <font color="black">ViewSynth: Learning Local Features from Depth using View Synthesis</font>
    </a>
  </h2>
  <font color="black">また、さまざまなデータセット間での3DキーポイントマッチングにおけるViewSynthの一般化可能性も示します。ビューの合成を学習することで、特徴の抽出に、可視だけでなくシーンのオクルードされた部分に関する情報もエンコードすることを明示的に推奨します。制限に対処するにはこれらのメソッドの中で、我々は共同で学習するフレームワークViewSynthを提案します：（1）提案されたコントラストマッチングロスを使用した深度画像からの視点不変キーポイント記述子、および（2）提案されたビュー合成モジュールを使用した異なる視点からの深度画像のビュー合成合成損失を表示します。 
[要約]深度モダリティでは、viewsynthが3Dキーポイントマッチングおよびカメラ位置確認タスクで最先端の深度およびRGBローカル特徴抽出技術よりも優れていることを示しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-22">
        <br><font color="black">2019-11-22</font>
      </time>
    </span>
</section>
<!-- paper0: Neural Light Transport for Relighting and View Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_23.html">
      <font color="black">Neural Light Transport for Relighting and View Synthesis</font>
    </a>
  </h2>
  <font color="black">定性的および定量的な実験は、ニューラルLT（NLT）が、以前の作業で必要な両方の問題を個別に処理することなく、再ライティングおよびビュー合成の最先端のソリューションよりも優れていることを示しています。この戦略により、ネットワークは複雑なマテリアル効果（サブサーフェススキャタリングなど）およびグローバルイルミネーション、拡散LTの物理的な正確さ（ハードシャドウなど）を保証します。シーンのライトトランスポート（LT）は、さまざまな照明および表示方向でのシーンの見え方、およびシーンのLTは、任意の照明の下で新しいビューの合成を可能にします。 
[ABSTRACT]これは、同じシーンの画像を含む新しい画像パターンの作成の結果であり、選択したビューから新しい画像を作成する方法です。これは、新しい画像のネットワークを使用して作成できることを意味します。異なるテクニック</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-09">
        <br><font color="black">2020-08-09</font>
      </time>
    </span>
</section>
<!-- paper0: Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data
  Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_24.html">
      <font color="black">Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data
  Generation</font>
    </a>
  </h2>
  <font color="black">Meta-Sim2では、パラメーターに加えてシーンの構造を学習することを目指しています。これは、離散的な性質のために困難な問題です。問題の離散的な性質のため、強化学習を使用してモデルをトレーニングし、トレーニングの成功の鍵となる、合成画像とターゲット画像間の特徴空間の相違。プロジェクトページ：https://nv-tlabs.github.io/meta-sim-structure/。 
[ABSTRACT]リアルなシーンを作成するには、手順モデルを管理する多数のパラメータを専門家が注意深く調整する必要があります。これらには、有効な構造でオブジェクトを制御する魅力的なパラメータが含まれます。メタ-sim2は、パラメータに加えてシーン構造を学習することを目的としています、それはその離散的な性質のために挑戦的な問題です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: DeepGMR: Learning Latent Gaussian Mixture Models for Registration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_25.html">
      <font color="black">DeepGMR: Learning Latent Gaussian Mixture Models for Registration</font>
    </a>
  </h2>
  <font color="black">この構成により、ネットワークはSE（3）不変の特徴空間を学習し、リアルタイムで一般化可能で、ノイズに対してロバストなグローバル登録メソッドを生成できます。 ..合成データと実際のデータ全体で、提案された方法は、最新のジオメトリベースおよび学習ベースの登録方法と比較した場合、良好なパフォーマンスを示します。 
[ABSTRACT]提案された方法は、状態-ofr、学習ベース、および学習ベースの登録方法と比較すると、優れたパフォーマンスを示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Grasping Detection Network with Uncertainty Estimation for
  Confidence-Driven Semi-Supervised Domain Adaptation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_26.html">
      <font color="black">Grasping Detection Network with Uncertainty Estimation for
  Confidence-Driven Semi-Supervised Domain Adaptation</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、提案されたネットワークがコーネルの把握データセットで高い成功率を達成できることを示しており、非常に限られたデータでのドメイン適応の場合、信頼主導の平均教師は、特に評価損失で10％以上、元の平均教師および直接トレーニングよりも優れています過剰適合とモデルの発散を回避するため。このアプローチは、学生モデルが一貫性の損失から誤った/有害な情報を学習するのを大幅に防ぎ、学習の進行をスピードアップし、モデルの精度を向上させます。提案された把持検出ネットワークは、特に予測を提供します特徴ピラミッドネットワーク（FPN）を利用した不確実性推定メカニズム、および平均教師半教師あり学習は、このような不確実性情報を利用して、信頼度の高いラベルなしデータのみの一貫性の損失を強調します。先生。 
[要約]システムは知識の欠如に基づいており、効果的であると想定されています。これは、学生モデルが光沢から学ぶのを妨げるためです。これは、システムを開発できないという理論と矛盾します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: SoundSpaces: Audio-visual Navigation in 3D Environments -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_27.html">
      <font color="black">SoundSpaces: Audio-visual Navigation in 3D Environments</font>
    </a>
  </h2>
  <font color="black">さらに、SoundSpacesを導入します。これは、公に利用可能な2組の3D環境（Matterport3Dとレプリカ）の幾何学的音響シミュレーションに基づくオーディオレンダリングの最初のデータセットであり、新しいセンサーをサポートするためにHabitatを装備し、実世界のスキャンされたさまざまな環境に任意の音源を挿入します。複雑な音響的および視覚的にリアルな3D環境のために、オーディオビジュアルナビゲーションを導入します。世界を移動することは当然、多感覚体験ですが、今日の具体化されたエージェントは耳が聞こえません-環境に対する視覚的な認識のみに制限されます。 
[ABSTRACT]オーディオを導入-複雑で音響的および視覚的にリアルな3D環境のためのビジュアルナビゲーション</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-24">
        <br><font color="black">2019-12-24</font>
      </time>
    </span>
</section>
<!-- paper0: Simultaneous Detection and Tracking with Motion Modelling for Multiple
  Object Tracking -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_28.html">
      <font color="black">Simultaneous Detection and Tracking with Motion Modelling for Multiple
  Object Tracking</font>
    </a>
  </h2>
  <font color="black">DMM-Netは、人気のUA-DETRACチャレンジで12.80 @ 120+ fpsのPR-MOTAスコアを達成しました。これは、パフォーマンスが向上し、桁違いに高速です。DMMNetでの深層学習に対するOmni-MOTの適合性を実証し、私たちのネットワークパブリックのソースコード。また、MOT評価で検出器の影響を排除するために正確なグラウンドトゥルースアノテーションを提供する車両追跡用の合成大規模パブリックデータセットOmni-MOTを提供しています。 
[ABSTRACT]ディープモーションモデリングネットワーク（dmm-net）は、複数のオブジェクトのモーションパラメーターを推定して、エンドツーエンドの方法でジョイントの検出と関連付けを実行できます。これらの出力は、効率的なmotのトラックレットを更新するために簡単に使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Multimodal Image-to-Image Translation via Mutual Information Estimation
  and Maximization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_29.html">
      <font color="black">Multimodal Image-to-Image Translation via Mutual Information Estimation
  and Maximization</font>
    </a>
  </h2>
  <font color="black">さらに、私たちの方法は、ソースドメインのコンテンツとターゲットドメインのスタイルの絡み合いを無料で解消します。最先端の方法と比較して、さまざまなベンチマークの画像から画像への変換データセットに対して、監視ありおよび監視なしの設定で実験を行います、マルチモーダルで高品質の結果を達成するための方法の有効性とシンプルさを示しています。この論文では、潜在コードと統計コード間の統計的依存性を促進するだけでマルチモーダルな画像から画像への変換を達成できる新しいフレームワークを提示します。条件付き生成敵対ネットワークの出力画像。 
[要旨]さまざまなベンチマーク画像からスポットへの方法で、監視ありおよび監視なしの設定で実験を行います。この方法では、両方のソース画像ドメインからターゲット画像ドメインへの片側変換モデルを学習するだけで済みます。教師ありまたは教師なしマルチモーダル画像-から-画像への変換</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-08">
        <br><font color="black">2020-08-08</font>
      </time>
    </span>
</section>
<!-- paper0: Accuracy and Performance Comparison of Video Action Recognition
  Approaches -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_30.html">
      <font color="black">Accuracy and Performance Comparison of Video Action Recognition
  Approaches</font>
    </a>
  </h2>
  <font color="black">モデルの精度は、提案された新しい精度メトリックに加えて、標準のトップ1およびトップ5精度メトリックを使用して評価されます。過去数年間、ビデオアクション認識システムおよびモデルに大きな関心が寄せられてきました。精度と計算パフォーマンスの結果の比較は、さまざまなトレーニング環境、ハードウェア仕様、ハイパーパラメーター、パイプライン、および推論方法によって混濁しています。 
[ABSTRACT]結果は、さまざまなトレーニング環境、ハイパーパラメータ、パイプライン、および可能性のある方法に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Unsupervised Image Classification for Deep Representation Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_31.html">
      <font color="black">Unsupervised Image Classification for Deep Representation Learning</font>
    </a>
  </h2>
  <font color="black">さらに、転移学習ベンチマークの実験により、マルチラベル画像分類、オブジェクト検出、セマンティックセグメンテーション、少数ショット画像分類などの他のダウンストリームタスクへの一般化が検証されました。詳細な解釈については、ディープクラスタリングとの関係をさらに分析し、対比学習..埋め込みクラスタリングを使用せずに、教師なし画像分類フレームワークを提案します。これは、標準の教師ありトレーニング方法に非常に似ています。 
[要約]埋め込みクラスタリングを使用せずに、教師なし画像分類フレームワークを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-20">
        <br><font color="black">2020-06-20</font>
      </time>
    </span>
</section>
<!-- paper0: Text-based Localization of Moments in a Video Corpus -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_32.html">
      <font color="black">Text-based Localization of Moments in a Video Corpus</font>
    </a>
  </h2>
  <font color="black">この課題を克服するために、私たちは、モーメントとセンテンスの効果的な共同埋め込みスペースを学習する階層型モーメントアラインメントネットワーク（HMAN）を提案します。3つのベンチマークテキストベースのビデオモーメント検索データセット-Charades-STA、DiDeMo、およびActivityNetの定性的および定量的結果キャプション-私たちの方法がビデオのコーパス内のモーメントの時間的ローカリゼーションの提案されたタスクで有望なパフォーマンスを達成することを示します。ビデオ内のモーメント間の微妙な違いを学習することに加えて、HMANは文に基づいてビデオ間のグローバルセマンティック概念の区別に焦点を当てていますクエリ。 
[ABSTRACT]これらの作品は、関連する動画がすでに知られていることを前提としています。それらは、その関連する動画にのみ瞬間をローカライズしようとします。これらの作品は、動画内の瞬間間の微妙な違いも学習します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: ISSAFE: Improving Semantic Segmentation in Accidents by Fusing
  Event-based Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_33.html">
      <font color="black">ISSAFE: Improving Semantic Segmentation in Accidents by Fusing
  Event-based Data</font>
    </a>
  </h2>
  <font color="black">最先端のモデルと比較して、私たちのアプローチは、提案された評価セットで30.0％mIoUを達成し、9.9％のパフォーマンス向上を達成します。セグメンテーションパフォーマンスのベンチマークとして、Cityscapesを参照して11フレームごとに手動で注釈を付けます。ただし、モデル一般的なデータセットで訓練されたこれらの難しいシーンに適用すると、パフォーマンスが大幅に低下する可能性があります。 
[要旨]事故シナリオでのセマンティックセグメンテーションに関するめったに対処されないタスクを、関連する大規模データセットdada-seg。安全性のベンチマークのために提示します。11番目のイベントごとに手動で都市景観に注釈が付けられます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: DISIR: Deep Image Segmentation with Interactive Refinement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_34.html">
      <font color="black">DISIR: Deep Image Segmentation with Interactive Refinement</font>
    </a>
  </h2>
  <font color="black">画像のみに基づく初期出力から始めて、ネットワークは、画像とユーザーアノテーションの連結を使用して、このセグメンテーションマップをインタラクティブに調整します。およそ5000ピクセル。このペーパーでは、航空写真のマルチクラスセグメンテーションのためのインタラクティブなアプローチを紹介します。 
[要約]画像と注釈を利用するディープニューラルネットワークに基づいています。注釈はネットワークの入力を変更します-重みではありません-高速でスムーズなプロセスを可能にします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-31">
        <br><font color="black">2020-03-31</font>
      </time>
    </span>
</section>
<!-- paper0: Automatic lung segmentation in routine imaging is primarily a data
  diversity problem, not a methodology problem -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_35.html">
      <font color="black">Automatic lung segmentation in routine imaging is primarily a data
  diversity problem, not a methodology problem</font>
    </a>
  </h2>
  <font color="black">多様なルーチンデータセット（n = 36）でトレーニングすると、標準的なアプローチ（U-net）では、Lung Tissue Research Consortiumなどのパブリックデータセット（0.94 $ \ pm $）でのトレーニングと比較して、DSC（0.97 $ \ pm $ 0.05）が高くなります。 0.13、p = 0.024）または解剖学3（0.92 $ \ pm $ 0.15、p = 0.001）。6つ以上の異なる疾患パターンと3つの公開データセットを使用して、ルーチンの画像データで評価を行いました。解剖学的構造の自動セグメンテーションは画像分析の重要なステップです。 
[要約]肺のセグメンテーションには、さまざまなアプローチが存在します。これらには、さまざまなデータセットでトレーニングおよび検証された高度なパイプラインが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-31">
        <br><font color="black">2020-01-31</font>
      </time>
    </span>
</section>
<!-- paper0: Deep learning-based transformation of the H&E stain into special stains
  improves kidney disease diagnosis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_36.html">
      <font color="black">Deep learning-based transformation of the H&E stain into special stains
  improves kidney disease diagnosis</font>
    </a>
  </h2>
  <font color="black">3人の腎臓病理学者による評価に続いて、4人目の腎臓病理学者による裁定に基づいて、既存のH＆E画像からの仮想特殊染色の生成により、16人の固有の被験者からサンプリングされたいくつかの非腫瘍性腎疾患の診断が改善されることを示します。 H＆E画像の特殊染色への仮想変換は、患者のコア標本スライドあたり1分未満で達成できます。この染色から染色への変換フレームワークは、追加の特殊染色が必要な場合の予備診断の品質を向上させるとともに、時間とコストを削減し、ヘルスケアシステムと患者の負担を軽減します。最も一般的には、ヘマトキシリンおよびエオシン（H＆E）染色が診断ワークフローで使用され、がん診断のゴールドスタンダードです。 
[ABSTRACT]最も一般的には、ヘマトキシリンとエオシン（h＆e）染色が診断ワークフローで使用されます。この研究では、教師あり学習のトリクローム、周期的酸-シッフおよびジョーンズシルバー染色の有用性を示しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Does MAML really want feature reuse only? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_37.html">
      <font color="black">Does MAML really want feature reuse only?</font>
    </a>
  </h2>
  <font color="black">BOILは、特徴の再利用が迅速な学習よりも効率的であるという仮説とは反対の方向であることに注意してください。結果は、勾配ベースのメタ学習アプローチでの迅速な学習が必要であることを意味します。最近、特徴の再利用が仮定されています。効率的な表現にほとんど変更を加えません。これは、迅速な学習ではなくMAMLによるメタ初期化モデルのパフォーマンスの主要な要因であり、表現に大きな変化をもたらします。 
[ABSTRACT] mamlは、最も代表的な勾配ベースのメタ学習アルゴリズムの1つです。沸騰アルゴリズムは、急速な学習に大きく依存しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: PhysCap: Physically Plausible Monocular 3D Motion Capture in Real Time -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_38.html">
      <font color="black">PhysCap: Physically Plausible Monocular 3D Motion Capture in Real Time</font>
    </a>
  </h2>
  <font color="black">私たちのアルゴリズムは、最初に純粋に運動学的に3D人間のポーズをキャプチャします。ビデオはhttp://gvv.mpi-inf.mpg.de/projects/PhysCapで入手できます。私たちの方法は、物理的に妥当ではない、時間的に安定したグローバルな3D人間の動きを、物理的に妥当でない姿勢、床の貫通、足のスケートなしで、リアルタイムおよび一般的なシーンのビデオからキャプチャします。 
[要約]ビデオは25 fpsで単一のカラーカメラによってキャプチャされました。これは、物理的にもっともらしい、リアルタイムのマーカーの最初のアルゴリズムを示しています-人間の3Dモーションキャプチャは少ない</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Localizing Anomalies from Weakly-Labeled Videos -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_39.html">
      <font color="black">Localizing Anomalies from Weakly-Labeled Videos</font>
    </a>
  </h2>
  <font color="black">異常なビデオの外観の違いに触発されて、隣接する時間セグメントの進化が異常なセグメントのローカライズのために評価されます。このために、高次コンテキストエンコーディングモデルが提案され、意味論的表現を抽出するだけでなく、動的な変動も測定しますそのため、時間的コンテキストを効果的に利用できます。しかし、それらのほとんどは、時間的ドメインのビデオ内の異常イベントを正確に特定できません。 
[要約]この論文では、異常ビデオ内の異常セグメントの時間的ローカライズに焦点を当てた、不適切に監視された異常ローカライゼーション（wsal）メソッドを提案します。比較すると、セマンティック表現を抽出して動的変動を測定する高次コンテキスト認識モデルが提案されます。さらに、直接の意味が最終的な異常スコアを取得するために効率的に集約されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: LabelEnc: A New Intermediate Supervision Method for Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_40.html">
      <font color="black">LabelEnc: A New Intermediate Supervision Method for Object Detection</font>
    </a>
  </h2>
  <font color="black">コードはhttps://github.com/megvii-model/LabelEncで入手できます。私たちのアプローチは主に2段階のトレーニング手順を含みます。さらに、補助構造はトレーニング中にのみ存在します。つまり、
[ABSTRACT]新しい方法はラベル空間でオートエンコーダを使用します。検出バックボーンへの補助的な中間監視として機能します。方法さまざまな検出システムを約2％改善</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-07">
        <br><font color="black">2020-07-07</font>
      </time>
    </span>
</section>
<!-- paper0: Attention-based Fully Gated CNN-BGRU for Russian Handwritten Text -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_41.html">
      <font color="black">Attention-based Fully Gated CNN-BGRU for Russian Handwritten Text</font>
    </a>
  </h2>
  <font color="black">また、Tahnからの複数の出力機能と入力機能を利用して完全にゲート化されたレイヤーを提案します。この提案された作業は、より良い結果を達成し、手書きカザフ語とロシア語データベース（HKR）でモデルを実験しました。 HKRデータセットで最初に作業し、他のほとんどの既存のモデルに最先端の結果を示します。複数の双方向GRUおよび注意メカニズムがサポートする、完全にゲートされたCNNに基づく新しいディープニューラルネットワークモデルを開発しました。最初のテストデータセットで0.045文字エラー率（CER）、0.192ワードエラー率（WER）および0.253シーケンスエラー率（SER）を達成し、2番目のテストデータセットで0.064 CER、0.24 WERおよび0.361 SERを達成する機能。 
[要約]私たちは、完全にゲートされたcnnに基づいた新しいディープニューラルネットワークモデルを開発します。これは、複数の双方向のGRUと機能を操作する注意メカニズムによってサポートされています。当社の研究は、HKRデータセットの最初の作業です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-12">
        <br><font color="black">2020-08-12</font>
      </time>
    </span>
</section>
<!-- paper0: Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh
  Recovery from a 2D Human Pose -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_42.html">
      <font color="black">Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh
  Recovery from a 2D Human Pose</font>
    </a>
  </h2>
  <font color="black">入力としての2D人間のポーズは、2つのドメイン間で比較的均一な幾何学的特性を持ちながら、基本的な人体関節情報を提供します。さまざまなベンチマークデータセットで、Pose2Meshが以前の3D人間のポーズおよびメッシュ推定方法よりも優れていることを示しています。最初の弱点これらの方法のうちの1つは、実験室などの制御された環境からの列車データと、野生の環境からのテストデータとの画像の見え方が異なるため、見かけのドメインギャップの問題です。 
[ABSTRACT]最初の弱点は、ドメイン内ギャップ問題です。これは、野生の環境からのトレーニングデータとテストデータの画像の外観が異なるためです。pose2meshは、3次元を推定する新しいグラフ畳み込みニューラルネットワーク（graphcnn）ベースのシステムです。 2D人間のポーズから直接人間のメッシュエッジの座標</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Line detection via a lightweight CNN with a Hough Layer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_43.html">
      <font color="black">Line detection via a lightweight CNN with a Hough Layer</font>
    </a>
  </h2>
  <font color="black">さらに、ライン検出に使用される現在のデータセットのいくつかの主要な不整合を指摘します。このホワイトペーパーでは、ネットワークニューロンにグローバルストリップのような受容性を持たせることができる、埋め込まれたパラメータフリーのハフ層を備えたライン検出用の軽量CNNを提案します。フィールド..しかし、ディープラーニングの進歩により、ライン検出へのトレーニング可能なアプローチが一般的になりました。 
[要約]従来の畳み込みネットワークには、ライン検出のタスクに適用すると2つの固有の問題があると主張します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: An Inter-Layer Weight Prediction and Quantization for Deep Neural
  Networks based on a Smoothly Varying Weight Hypothesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_44.html">
      <font color="black">An Inter-Layer Weight Prediction and Quantization for Deep Neural
  Networks based on a Smoothly Varying Weight Hypothesis</font>
    </a>
  </h2>
  <font color="black">私たちの包括的な実験は、提案された方法が、ディープニューラルネットワークでの以前の量子化ベースの圧縮方法と比較して、同じ精度レベルではるかに高い重み圧縮率を達成することを示しています。 ..つまり、提案された損失は、隣接する2つのレイヤー間で連結された重みが同じ値になるように重みを正規化します。 
[要約]この論文では、すべての畳み込み層の重み間の予測残差を量子化する新しい圧縮方法を提案します。さらに、提案された減量と通常の減量を備えたilwpを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-07-16">
        <br><font color="black">2019-07-16</font>
      </time>
    </span>
</section>
<!-- paper0: Investigating the Effect of Intraclass Variability in Temporal
  Ensembling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_45.html">
      <font color="black">Investigating the Effect of Intraclass Variability in Temporal
  Ensembling</font>
    </a>
  </h2>
  <font color="black">私たちの実験を通して、（a）クラス内の高い変動性を提供するデータセットでは精度が大幅に低下していること、（b）データセット全体で一貫してより高い精度を提供するシード画像が多いこと、（c）実際にシードタイプが全体的な効率。精度のスペクトルが低くなり、高くなります。さらに、すべての実験に基づいて、KMNISTは時間的エンサンブルの強力なベースラインであることがわかります。時間的アンサンブルは、半教師付きアプローチであり、深いトレーニングを可能にします。少数のラベル付き画像を含むニューラルネットワークモデル。 
[要約]これは、クラス内変動の影響に関する最初の研究です。これには、シードサイズとシードタイプへの焦点が含まれます。結果は、いくつかのテストに基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: $β$-Variational Classifiers Under Attack -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_46.html">
      <font color="black">$β$-Variational Classifiers Under Attack</font>
    </a>
  </h2>
  <font color="black">特に、正しく分類された入力データをわずかに変更する小さな敵対的な摂動を合成して、ネットワークが自信を持ってそれを誤って分類してしまう可能性があります。この論文では、特定のクラスのメソッドである$ \ beta $ -Variational Classifiersの分析を行います。これは、特定の分類タスクを解決するだけでなく、入力分布から新しいサンプルを生成できる生成コンポーネントも提供します。ディープニューラルネットワークは、コンピュータービジョンの分野で得られた画期的な成果により、近年多くの注目を集めています。 。 
[ABSTRACT]研究により、予測のロバスト性が限定的であることが示されています。これにより、改善しようとするさまざまな方法が大量に発生しました。これらには、これらの摂動の存在を見つけることが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Monocular Expressive Body Regression through Body-Driven Attention -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_47.html">
      <font color="black">Monocular Expressive Body Regression through Body-Driven Attention</font>
    </a>
  </h2>
  <font color="black">ほとんどの既存の方法は、体の一部にのみ焦点を当てています。最近のいくつかのアプローチでは、顔と手を含む3D体モデルを使用して、画像から表現力豊かな3D人間を再構築しています。主な貢献は3つあります。 
[ABSTRACT]ほとんどの既存の方法は身体の一部にのみ焦点を当てていますが、新しい方法は入力として2dキーポイントを必要とします。これにより、身体画像がニューラルネットワーク用に縮小されたときに手と顔の推定が難しくなります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Co-Saliency Detection with Co-Attention Fully Convolutional Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_48.html">
      <font color="black">Co-Saliency Detection with Co-Attention Fully Convolutional Network</font>
    </a>
  </h2>
  <font color="black">具体的には、共同注意モジュールはFCNの高レベルの畳み込み層にプラグインされます。これにより、共通の顕著なオブジェクトに大きな注意の重みを割り当て、背景と一般的でないディストラクタに小さな注意の重みを割り当てて、最終的な検出パフォーマンスを向上させることができます。我々は、共同注意FCN（CA-FCN）と呼ばれる共同注意モジュール組み込みFCNフレームワークを提案します。3つの人気の共通顕著性ベンチマークデータセットでの広範な実験は、提案されたCA-FCNの優位性を実証します。ほとんどの場合、芸術。 
[要約]完全な畳み込みネットワーク（fcn）を使用していくつかの試みが行われました。情報が不足しているため、既存のモデルはしばしば抽出された特徴を区別せずに使用し、表現に冗長性をもたらします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Interpretation of Brain Morphology in Association to Alzheimer's Disease
  Dementia Classification Using Graph Convolutional Networks on Triangulated
  Meshes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_49.html">
      <font color="black">Interpretation of Brain Morphology in Association to Alzheimer's Disease
  Dementia Classification Using Graph Convolutional Networks on Triangulated
  Meshes</font>
    </a>
  </h2>
  <font color="black">私たちの研究で生成された視覚化マップは、アルツハイマー型の認知症に関連する脳の病理学的変化の構造的局在に関する現在の知識との対応を示しています。25回の試行のモンテカルロクロスでそのパフォーマンスを観察することにより、モデルの有効性を確認します。 -検証..提案された方法は皮質および皮質下の表面情報の使用を活用しており、ADDと健全な制御の問題について、96.35％のテスト精度で他の機械学習方法よりも優れています。 
[要約]ディープラーニングメソッドは、最適化するために多くの場合、広範な学習パラメーターを必要とします。これらには、最先端のネットワークのための皮質下学習フレームワークが含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-14">
        <br><font color="black">2020-08-14</font>
      </time>
    </span>
</section>
<!-- paper0: Linear Attention Mechanism: An Efficient Attention for Semantic
  Segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_50.html">
      <font color="black">Linear Attention Mechanism: An Efficient Attention for Semantic
  Segmentation</font>
    </a>
  </h2>
  <font color="black">コードは、https：//github.com/lironui/Linear-Attention-Mechanism。で入手できます。セマンティックセグメンテーションで行われた実験は、線形注意メカニズムの有効性を実証しました。メモリと計算コストがはるかに少なく、ドット積の注意に近いものです。 
[ABSTRACT]効率的な設計により、注意メカニズムとニューラルネットワークの統合がより柔軟で多用途に</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-29">
        <br><font color="black">2020-07-29</font>
      </time>
    </span>
</section>
<!-- paper0: A Benchmark for Inpainting of Clothing Images with Irregular Holes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_51.html">
      <font color="black">A Benchmark for Inpainting of Clothing Images with Irregular Holes</font>
    </a>
  </h2>
  <font color="black">実験は、拡張部分畳み込み（DPConv）が他のインペインティング戦略と比較して定量的なインペインティングパフォーマンスを向上させることを示しています。特に、マスクサイズがイメージの20％以上の場合に、パフォーマンスが向上します。\ keywords {イメージインペインティング、ファッションイメージの理解、拡張された畳み込み、部分的な畳み込み。インテリジェントなファッション分析システムへの実際的な影響にもかかわらず、衣服の画像の修復はまだ広範囲にわたって検討されていません。 
[ABSTRACT]衣服の画像の修復はまだ十分に検討されていません。たとえば、部分的な描画の拡張バージョンの使用を紹介します。提案された方法は、完全に透明なマスクを形成するために必要なレイヤー数を削減します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-09">
        <br><font color="black">2020-07-09</font>
      </time>
    </span>
</section>
<!-- paper0: Revisiting Temporal Modeling for Video Super-resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_52.html">
      <font color="black">Revisiting Temporal Modeling for Video Super-resolution</font>
    </a>
  </h2>
  <font color="black">広範な実験により、提案されたRRNは非常に計算効率が高く、他の時間モデリング方法よりも細かい時間一貫性のあるVSR結果が生成されることが示されています。効率的なビデオ超解像のための新しいRecurrent Residual Network（RRN）を提案します。残差学習は、RNNのトレーニングを安定させるために利用され、その間、超解像パフォーマンスを向上させます。 
[要約]提案された方法は、広く使用されているいくつかのベンチマークで状態および現在の残余ネットワークの結果を達成しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-13">
        <br><font color="black">2020-08-13</font>
      </time>
    </span>
</section>
<!-- paper0: Facial movement synergies and Action Unit detection from distal wearable
  Electromyography and Computer Vision -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_53.html">
      <font color="black">Facial movement synergies and Action Unit detection from distal wearable
  Electromyography and Computer Vision</font>
    </a>
  </h2>
  <font color="black">電極がソース筋肉に直接配置されていない場合でも、ボリューム伝導を利用して関連する筋肉活動を検出します。 ..しかし、EMGを遠位側で測定するには、顔の動きの正確な原因が不明であることを伴います。 
[要旨]顔の動きの正確な原因は不明です。この方法は顔の動作単位（au）に基づいています。特定の筋肉活動を検出するために使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Yet Another Intermediate-Level Attack -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_54.html">
      <font color="black">Yet Another Intermediate-Level Attack</font>
    </a>
  </h2>
  <font color="black">誘発された敵対的損失を予測するための中間レベルの不一致（敵対的な入力のセットとそれらの無害な対応物との間）の線形マッピングを確立することにより、マルチステップベースライン攻撃の最適化手順を最大限に活用することを目指します。実験結果以前の最新技術よりもかなり優れていることを示しています。コードはhttps://github.com/qizhangli/ila-plus-plusにあります。 
[要約]この方法は、ベースラインの敵の例のブラックボックス転送性を向上させるために提案されています。このメソッドは、ブラックボックス転送性を強化するために使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Uncertainty Estimation in Medical Image Denoising with Bayesian Deep
  Image Prior -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CV/paper_55.html">
      <font color="black">Uncertainty Estimation in Medical Image Denoising with Bayesian Deep
  Image Prior</font>
    </a>
  </h2>
  <font color="black">これをモンテカルロドロップアウトのベイジアンアプローチに拡張して、偶然的および認識論的不確実性の両方を定量化します。実験結果は、我々のアプローチが十分に較正された不確実性をもたらすことを示しています。 
[ABSTRACT]大規模なデータセットでトレーニングされたディープモデルは幻覚を起こし、解剖学的に存在しないアーティファクトを作成する傾向があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: SARG: A Novel Semi Autoregressive Generator for Multi-turn Incomplete
  Utterance Restoration -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_0.html">
      <font color="black">SARG: A Novel Semi Autoregressive Generator for Multi-turn Incomplete
  Utterance Restoration</font>
    </a>
  </h2>
  <font color="black">生成の自動回帰とテキスト編集のシーケンスラベリングに触発されて、高効率と柔軟性を備えた新しい半自己回帰ジェネレーター（SARG）を提案します。推論速度がより速い最新モデル..オープンドメインの対話システムは、大規模な会話データとディープラーニングの開発により大きな成功を収めてきましたが、同時参照と情報の省略が頻繁に行われるため、マルチターンシナリオは依然として課題です。 
[ABSTRACT]復元に関する実験、200kは、提案されたモデルが最新のモデルより大幅に優れていることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-04">
        <br><font color="black">2020-08-04</font>
      </time>
    </span>
</section>
<!-- paper0: Quda: Natural Language Queries for Visual Data Analytics -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_1.html">
      <font color="black">Quda: Natural Language Queries for Visual Data Analytics</font>
    </a>
  </h2>
  <font color="black">Qudaと呼ばれる新しいデータセットを提示して、V-NLIが自由形式の自然言語を理解できるようにします。このデータセットには、最新の状態の導入を支援する10の低レベル分析タスクの注釈が付けられた14; 035のさまざまなユーザークエリが含まれています複雑な人間の言語を解析するための最新の技術。視覚化指向の自然言語インターフェース（V-NLI）は、近年探求および開発されてきました。 
[ABSTRACT] qudaは、学術出版物に記載されている設計タスクを分析することで、視覚化コミュニティのさまざまなアプリケーションに有益です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-07">
        <br><font color="black">2020-05-07</font>
      </time>
    </span>
</section>
<!-- paper0: Towards a Decomposable Metric for Explainable Evaluation of Text
  Generation from AMR -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_2.html">
      <font color="black">Towards a Decomposable Metric for Explainable Evaluation of Text
  Generation from AMR</font>
    </a>
  </h2>
  <font color="black">この作業では、これらの問題を緩和し、2つの柱に基づいて構築される自動メトリックである$ \ mathcal {M} \ mathcal {F} _ \ beta $を提案します。両方の原則が満たされると、理論的および実験的に示されますAMR-to-textシステムの評価には、スコアの説明可能性を含む、いくつかの利点があります。最初の柱は、保存$ \ mathcal {M} $を意味する原則です。これは、元のAMRグラフが生成された文。 
[ABSTRACT]これは、amrのより抽象的なドメインからより具体的な文のドメインへのマッピングにより、ベータベータ文を意図することができるためです。この概念は、生成された文から元のamrグラフを再構築する方法を測定します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: OCoR: An Overlapping-Aware Code Retriever -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_3.html">
      <font color="black">OCoR: An Overlapping-Aware Code Retriever</font>
    </a>
  </h2>
  <font color="black">異なる人が使用する異なる名前間の重複は、2つの異なる名前が潜在的に関連している可能性があることを示します（たとえば、「メッセージ」と「msg」）。コード内の識別子と自然言語の説明内の単語間の重複は、コードスニペットとコードの取得は、開発者がオープンソースプロジェクトでコードスニペットを再利用するのに役立ちます。さらに、OCoRのさまざまなコンポーネントのパフォーマンスを理解するのに役立ついくつかの詳細な実験も行いました。 
[ABSTRACT]コードの取得は、一連のコードから最も関連性の高いコードを検索することを目的としています。ただし、これらのアプローチでは、重要な機能をキャプチャできません。「overlaps」と呼ばれる新しいコード。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-12">
        <br><font color="black">2020-08-12</font>
      </time>
    </span>
</section>
<!-- paper0: Language Models as Few-Shot Learner for Task-Oriented Dialogue Systems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_4.html">
      <font color="black">Language Models as Few-Shot Learner for Task-Oriented Dialogue Systems</font>
    </a>
  </h2>
  <font color="black">異なる点として、GPT-2（Radford et al。、2019）やGPT-3（Brown et al。、2020）などの言語モデルでは、いくつかの例でモデルを準備することにより、数回の学習が可能です。重要なのは、このアプローチの現在の制限事項、および将来の作業への影響の可能性について説明します。タスク指向の対話システムは、4つの接続されたモジュール、つまり自然言語理解（NLU）、対話状態追跡（DST）、対話ポリシー（DP）および自然言語生成（NLG）。 
[ABSTRACT]研究課題は、データ収集に関連する高コストを考慮して、最小量のサンプル（つまり、数ショット）で各モジュールを学習することです。これらの方法には、各タスクの詳細な手順と一連のパラメーターが必要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-14">
        <br><font color="black">2020-08-14</font>
      </time>
    </span>
</section>
<!-- paper0: CoVoST 2 and Massively Multilingual Speech-to-Text Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_5.html">
      <font color="black">CoVoST 2 and Massively Multilingual Speech-to-Text Translation</font>
    </a>
  </h2>
  <font color="black">大規模な多言語音声翻訳と低リソース言語ペアの音声翻訳の研究を促進することを目的として、21言語から英語へ、および英語から15言語への翻訳をカバーする大規模な多言語音声翻訳コーパスCoVoST 2をリリースします。データの健全性チェックは、CC0ライセンスの下でリリースされるデータの品質に関する証拠を提供します。また、広範な音声認識、2か国語および多言語の機械翻訳、および音声翻訳のベースラインも提供します。 
[ABSTRACT]これは、総量と言語カバレッジの観点から、現在までに利用可能な最大のオープンデータセットです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-20">
        <br><font color="black">2020-07-20</font>
      </time>
    </span>
</section>
<!-- paper0: LTIatCMU at SemEval-2020 Task 11: Incorporating Multi-Level Features for
  Multi-Granular Propaganda Span Identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_6.html">
      <font color="black">LTIatCMU at SemEval-2020 Task 11: Incorporating Multi-Level Features for
  Multi-Granular Propaganda Span Identification</font>
    </a>
  </h2>
  <font color="black">最終モデルは、多数決のアンサンブルであり、組み込まれた知識のさまざまなサブセットを活用してさまざまな宣伝クラスの境界を学習し、テストリーダーボードで$ 4 ^ {th} $の位置を獲得します。より良い表現学習を促進するために、10kのコーパスも収集します。ニュース記事、およびモデルの微調整に使用します。「マルチグラニュラー」モデルは、単語、文、およびドキュメントレベルの構文、意味論的および実用的な影響機能など、さまざまなレベルのテキスト粒度で言語知識を組み込んでおり、モデルを大幅に改善します。言語に依存しないバリアントと比較したパフォーマンス。 
[要約]どのトークンがプロパガンダを示しているかを識別する新しいモデルを導入します。また、10,000件のニュース記事のコーパスを収集し、それをナレッジに使用します。モデルはwwwで入手できます。 github。 com / sopu / propagandasemeval20。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Transformer based Multilingual document Embedding model -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_7.html">
      <font color="black">Transformer based Multilingual document Embedding model</font>
    </a>
  </h2>
  <font color="black">3つ目は、NMT変換損失関数に新しい距離制約損失を追加することです。2つ目は、T-LASERは再帰がないため、エンコーダーで並列計算を高速化し、テキストの埋め込みを生成できます。この距離制約損失により、さらに並列文の埋め込みをベクトル空間で近づけます。距離制約付きトレーニング済みT-LASERモデルをcT-LASERと呼びます。 
[要約]トランスフォーマーベースのセンテンスパー/エンベディングモデルであるt-laserにより、3つの重要な改善が行われます。このモデルにより、エンコーダーで速度の並列計算が可能になり、テキストの埋め込みが生成されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-19">
        <br><font color="black">2020-08-19</font>
      </time>
    </span>
</section>
<!-- paper0: How Have We Reacted To The COVID-19 Pandemic? Analyzing Changing Indian
  Emotions Through The Lens of Twitter -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_8.html">
      <font color="black">How Have We Reacted To The COVID-19 Pandemic? Analyzing Changing Indian
  Emotions Through The Lens of Twitter</font>
    </a>
  </h2>
  <font color="black">メンタルウェルビーイングの懸念が高まる中、潜在的な脅威を予測し、予防策を講じるために、公衆の影響のダイナミクスを分析することが不可欠になっています。人間の心の状態は、わずかなバイナリ感情よりもニュアンスがあります。ここでは、ツイートから人々の感情を識別するディープラーニングベースのシステムを提案します。 
[要約] 1300万人を超えるパンデミックの確定症例。確定症例数は250万人に増加しました。人々はソーシャルメディアを使用して感情を表現しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Lite Training Strategies for Portuguese-English and English-Portuguese
  Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_9.html">
      <font color="black">Lite Training Strategies for Portuguese-English and English-Portuguese
  Translation</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、適度なハードウェア（9日間で単一の8 GBゲームGPU）でトレーニングされている間、モデルが最先端のモデルに匹敵するパフォーマンスを発揮することを示しています。データ、モデル、およびコードは、https：//で入手できます。 github.com/unicamp-dl/Lite-T5-Translation .. WMT20生物医学翻訳共有タスクへの提出についても説明します。 
[ABSTRACT]結果は、適度なハードウェアでトレーニングされている間、私たちのモデルが最先端の競争力のあるパフォーマンスを持っていることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Assigning function to protein-protein interactions: a weakly supervised
  BioBERT based approach using PubMed abstracts -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_10.html">
      <font color="black">Assigning function to protein-protein interactions: a weakly supervised
  BioBERT based approach using PubMed abstracts</font>
    </a>
  </h2>
  <font color="black">動機：タンパク質間相互作用（PPI）は正常細胞と病変細胞の両方でタンパク質の機能に重要であり、多くの重要なタンパク質機能は相互作用によって媒介されます。これらの相互作用の性質に関する知識は、生物学を分析するネットワークの構築に重要ですデータ：方法：注釈付きの機能を持つ相互作用するタンパク質のペアと、PubMedデータベースからの関連するアブストラクトを含む、IntAct PPIデータベースから弱く監視されたデータセットを作成します。この作業は、PPI機能抽出のための生物医学アブストラクトの分析がオンラインデータベースでキャプチャされた機能で注釈が付けられた相互作用の数を増やします。 
[要旨] 1800万のpubmed ppisをスキャンして、3253の新しいタイプのppisを識別します。これらには、リン酸化およびアセチル化の相互作用が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: An Experimental Study of Deep Neural Network Models for Vietnamese
  Multiple-Choice Reading Comprehension -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_11.html">
      <font color="black">An Experimental Study of Deep Neural Network Models for Vietnamese
  Multiple-Choice Reading Comprehension</font>
    </a>
  </h2>
  <font color="black">私たちの実験には、ベトナム語の6つの異なる単語の埋め込みでのCo-matchモデルと多肢選択読解のためのBERTモデルの使用が含まれます。ViMMRCコーパスでは、BERTモデルの精度はテストセットで61.28％です。 MRCはベトナム語などのリソースの少ない言語で行われています。 
[要約]単語表現は、機械読解の精度に最も影響を与える非常に重要な手法です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Constructing a Knowledge Graph from Unstructured Documents without
  External Alignment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_12.html">
      <font color="black">Constructing a Knowledge Graph from Unstructured Documents without
  External Alignment</font>
    </a>
  </h2>
  <font color="black">WikiMoviesやMetaQAなどのベンチマークデータセットを使用して実験を行います。ナレッジグラフ（KG）は多くのNLPタスクに関連しますが、信頼できるドメイン固有のKGの構築には時間がかかり、費用がかかります。この問題を克服するために、小説を提案します外部配置を必要としない非構造化ドキュメントからKGを自動的に構築し、目的の情報を抽出するためのその使用を検討する方法。 
[ABSTRACT]このメソッドは、18kドキュメントでkgを正常に作成および検索できます。検索タスクtask taskで69. 7％ヒット@ 10（oracleモデルに近い）を達成しました。このメソッドは、wi-kimツールを使用して知識を抽出します非構造化ドキュメントからのタプル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Checkworthiness in Automatic Claim Detection Models: Definitions and
  Analysis of Datasets -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_13.html">
      <font color="black">Checkworthiness in Automatic Claim Detection Models: Definitions and
  Analysis of Datasets</font>
    </a>
  </h2>
  <font color="black">現在のデータセットの特性については、データが非常に不均衡でノイズが多いだけでなく、範囲と言語も限定的であると主張されています。ファクトチェック組織でのチェックに値するクレーム選択手順の詳細な分析と状態分析-最新のクレーム検出データセット、チェックワージネスは、時空間的かつ状況依存の価値を持つ概念として定義され、伝達する客観性の正確さを検証する必要があります。自動化されたファクトチェックに対する公的、専門的、学術的な関心は劇的に高まっています。過去10年間で増加し、ファクトチェック手順の最初のステップの1つである、いわゆるチェックに値するクレームの選択を自動化することを目指しています。 
[要旨]事実間のチェックワージネスの定義と特性についてはほとんど合意がありません-checkers.checkworthinessは実際にはトレーニングとテストに使用されるデータセットに反映されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Discovering Useful Sentence Representations from Large Pretrained
  Language Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_14.html">
      <font color="black">Discovering Useful Sentence Representations from Large Pretrained
  Language Models</font>
    </a>
  </h2>
  <font color="black">さらに重要なことは、複雑な最適化アルゴリズムを必要とせずに、基盤となる言語モデルをまったく微調整することなく、これらの文をほぼ完全に回復することです。「ユニバーサル」と見なされるには、デコーダーがターゲット文$ s $の暗黙的な表現を持っている必要があります。その表現で条件付けされたときに正確にその文を回復できるようにします。トランスフォーマーベースのモデルの3つの表現注入手法と、この表現空間との間で文をマッピングする3つの付随する方法を提示して比較します。 
[ABSTRACT]ユニバーサルデコーダーは、ユニバーサルデコーダーとして使用するように構成できます。これらの例は、標準的なロービング法を使用して簡単に見つけることができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Language Models as Knowledge Bases: On Entity Representations, Storage
  Capacity, and Paraphrased Queries -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_15.html">
      <font color="black">Language Models as Knowledge Bases: On Entity Representations, Storage
  Capacity, and Paraphrased Queries</font>
    </a>
  </h2>
  <font color="black">LMが何百万ものエンティティを表現できるようにする3つのエンティティ表現を探索し、LMの世界知識の言い換えクエリに関する詳細なケーススタディを提示することで、言語モデルが実際に知識ベースとして機能できることの概念実証を提供します。このパラダイムの主な利点、つまりさまざまな自然言語の言い換えを使用してKBにクエリを実行することは、これまで十分に検討されていませんでした。ここで、LMをKBとして扱うための2つの基本要件を定式化します。（i）を含む多数のファクトを保存する機能多数のエンティティと（ii）格納されたファクトをクエリする機能。 
[要約]新しいシステムでは、単一のトークン名が一般的なlmボキャブラリで見つかった21kエンティティのみを処理できることが示唆されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Demographics Should Not Be the Reason of Toxicity: Mitigating
  Discrimination in Text Classifications with Instance Weighting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_16.html">
      <font color="black">Demographics Should Not Be the Reason of Toxicity: Mitigating
  Discrimination in Text Classifications with Instance Weighting</font>
    </a>
  </h2>
  <font color="black">その結果、これらのデータセットを使用してトレーニングされたモデルは、「同性愛者」という言葉だけが原因で、「彼女は私を同性愛者であることを幸せにします」のような文を乱用と見なす場合があります。最近のテキスト分類の使用の急増に伴い、研究者はテキスト分類データセットに特定の意図しないバイアスがあることを発見しました。この形式化に基づいて、インスタンスを使用して非差別分布を回復することにより、モデルにとらわれない偏見トレーニングフレームワークをさらに提案します重み付け。事前に定義された人口統計学的識別用語のセット以外に、追加のリソースや注釈は必要ありません。 
[ABSTRACT]新しいデータセットは、この方法がテキスト分類データセットの意図しないバイアスの影響を軽減できることを示しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-29">
        <br><font color="black">2020-04-29</font>
      </time>
    </span>
</section>
<!-- paper0: Institutional Grammar 2.0 Codebook -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_17.html">
      <font color="black">Institutional Grammar 2.0 Codebook</font>
    </a>
  </h2>
  <font color="black">コードブックは、ドキュメントの準備に関連するプリコーディング手順の概要を提供する前に、IG 2.0の基本的な概念をカバーしています。このドキュメントは、エンコーディングプロセスで使用され、コードブック全体で参照される分類法の概要をさらに提供します。このコードブックは特にポリシーコーディングのコンテキストでIG 2.0の運用面に焦点を当てています。 
[ABSTRACT]このコードブックは、igの改訂版、制度的文法2. 0（ig 2. 0）のガイドラインを提供します。この目的のために、包括性、透明性、および特異性に関して文法を改訂します。また、規制制度の声明のコーディングに加えて、構成的制度の声明</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Quantification of BERT Diagnosis Generalizability Across Medical
  Specialties Using Semantic Dataset Distance -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_18.html">
      <font color="black">Quantification of BERT Diagnosis Generalizability Across Medical
  Specialties Using Semantic Dataset Distance</font>
    </a>
  </h2>
  <font color="black">1つの専門分野でトレーニングされたモデルは、混合テストセットまたは外部テストセットよりも内部テストセットでパフォーマンスが優れていました（それぞれ平均AUC 0.92、0.87、および0.83; p = 0.016）。さらに、既存のモデルが新しいモデルでどのように実行されるかを示す定量的メトリックはありません。データ..モデルがより多くの専門分野でトレーニングされると、テストのパフォーマンスが向上します（p &lt;1e-4）。 
[ABSTRACT]ディープラーニアモデルは新しいデータから学習できる場合があります。既存のモデルが新しいデータでどのように機能するかを示す定量的な指標はありません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-14">
        <br><font color="black">2020-08-14</font>
      </time>
    </span>
</section>
<!-- paper0: Speaker-Utterance Dual Attention for Speaker and Utterance Verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_19.html">
      <font color="black">Speaker-Utterance Dual Attention for Speaker and Utterance Verification</font>
    </a>
  </h2>
  <font color="black">提案されたSUDAは、話者と発話の情報ストリーム間の相互作用を学習するアテンションマスクメカニズムを特徴としています。 RSR2015コーパスで実施された調査は、提案されたSUDAが、注意マスクなしでフレームワーク、および話者と発話の両方の検証のためのいくつかの競合システムより優れていることを確認します。 
[ABSTRACT]統合されたニューラルネットワークで話者-発話二重注意（suda）のアイデアを実装します。提案されたsudaは、話者と発話情報ストリーム間の相互作用を学習する注意マスクメカニズムを備えています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Inducing Language-Agnostic Multilingual Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_20.html">
      <font color="black">Inducing Language-Agnostic Multilingual Representations</font>
    </a>
  </h2>
  <font color="black">コードとモデルを利用できるようにします。XNLIのタスクと、選択した19言語で難易度の異なるリファレンスなしのMT評価を評価します。実験では、ゼロの可能性を示す多言語表現の言語に依存しない動作を示します-遠くの低リソース言語へのクロスリンガルなショットの転送、およびすべてのタスクと言語で平均8.9ポイント（M-BERT）と18.2ポイント（XLM-R）のパフォーマンスギャップの削減。 
[要約]言語を削除するための3つのアプローチ-埋め込みの副産物としての識別力を高める特定の手段とバリエーション-実験では、ゼロショットクロスの可能性を明らかにする言語-多言語表現の診断動作を実証しました-遠方の低言語へのリンガル転送-リソース言語</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Scruples: A Corpus of Community Ethical Judgments on 32,000 Real-Life
  Anecdotes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_21.html">
      <font color="black">Scruples: A Corpus of Community Ethical Judgments on 32,000 Real-Life
  Anecdotes</font>
    </a>
  </h2>
  <font color="black">私たちの実証分析の重要な要点は、規範が常にクリーンカットされているわけではないということです。多くの状況は自然に分かれます。私たちのデータセットは、最先端のニューラル言語モデルに大きな課題を提示し、改善の余地を残しています。AIシステムが人々の日常生活の一部として増加するにつれて、次のことがますます重要になります。彼らは人々の倫理規範を理解しています。 
[ABSTRACT]マシン倫理に対する新しいデータ駆動型アプローチを調査します。このようなタスクで可能な限り最高のパフォーマンスを推定する新しい方法を提示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Controlling Dialogue Generation with Semantic Exemplars -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_22.html">
      <font color="black">Controlling Dialogue Generation with Semantic Exemplars</font>
    </a>
  </h2>
  <font color="black">見本自体の単語ではなく、見本のセマンティックフレームに基づいて対話の生成を制御すると、見本の応答に存在する意味の意味と会話の目標を維持しながら、生成された応答の一貫性が向上することを示します。ローカルで一貫性のある応答ですが、特定の目標を達成するために必要な応答をきめ細かく制御できません。応答生成を制御する有望な方法は、モデルに基づいた生成です。このモデルでは、トレーニングデータから取得した、または手書きの例示応答を編集します。談話レベルの目標に戦略的に取り組み、新しい対話のコンテキストに適合させる。 
[ABSTRACT]サンプル-ベースのレスポンスレスポンスの生成は、control.modelsがトレーニングデータから取得したサンプルレスポンスを編集する方法です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Do Syntax Trees Help Pre-trained Transformers Extract Information? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/cs.CL/paper_23.html">
      <font color="black">Do Syntax Trees Help Pre-trained Transformers Extract Information?</font>
    </a>
  </h2>
  <font color="black">私たちの経験的分析は、これらの構文が導入されたトランスフォーマーがSRLおよび関係抽出タスクに関する最新の結果を取得することを示しています。依存構造を組み込むための2つの異なる戦略を提案および調査します。トランスフォーマーの出力、および構文構造をトランスフォーマーの注意レイヤーに注入するジョイントフュージョンアプローチ。この作業では、3つの代表的な情報抽出タスクで依存関係ツリーを事前トレーニング済みトランスフォーマーに組み込むことの有用性を体系的に研究します。ロールラベリング（SRL）、名前付きエンティティの認識、および関係の抽出。 
[ABSTRACT]組み込まれた依存関係ツリー情報の効果は不明確なままです。これらのモデルはパフォーマンスのエンコード方法に基づいています。ただし、たとえば、</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: Towards Robust Neural Vocoding for Speech Generation: A Survey -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.AS/paper_0.html">
      <font color="black">Towards Robust Neural Vocoding for Speech Generation: A Survey</font>
    </a>
  </h2>
  <font color="black">すべてのボコーダーの自然さをもたらす大量の主観的MOS結果が今後の研究のために提示されます。このペーパーでは、WaveNet、WaveRNN、FFTNet、Parallel WaveGANを含む4つの一般的なニューラルボコーダーを5つの異なるデータセットで交互にトレーニングします。話者の多様性は、言語よりもユニバーサルボコーダーを実現するためにはるかに重要です。 
[ABSTRACT]実際のデータでトレーニングされたボコーダーは、多くの場合、目に見えないシナリオの音声品質を低下させます。研究によると、ボコーダーは主観的研究のために低下することがよくあります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-05">
        <br><font color="black">2019-12-05</font>
      </time>
    </span>
</section>
<!-- paper0: WG-WaveNet: Real-Time High-Fidelity Speech Synthesis without GPU -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.AS/paper_1.html">
      <font color="black">WG-WaveNet: Real-Time High-Fidelity Speech Synthesis without GPU</font>
    </a>
  </h2>
  <font color="black">PyTorchの実装は、8 GB未満のGPUメモリを使用してトレーニングでき、NVIDIA 1080Ti GPUで960 kHzを超えるレートでオーディオサンプルを生成します。また、生成されたオーディオの品質は他の方法と同等であることも実験で示されています。音声サンプルはオンラインで公開されています。 
[要約]提案されたモデルは、コンパクトなフローベースのモデルとポストフィルターに基づいています。他の波形生成モデルよりも必要なリソースが少なくなります。提案された方法は、44。1センチメートルの音声波形1. 2倍の速度で生成できます。リアルタイム</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-15">
        <br><font color="black">2020-05-15</font>
      </time>
    </span>
</section>
<!-- paper0: CoVoST 2 and Massively Multilingual Speech-to-Text Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.AS/paper_2.html">
      <font color="black">CoVoST 2 and Massively Multilingual Speech-to-Text Translation</font>
    </a>
  </h2>
  <font color="black">大規模な多言語音声翻訳と低リソース言語ペアの音声翻訳の研究を促進することを目的として、21言語から英語へ、および英語から15言語への翻訳をカバーする大規模な多言語音声翻訳コーパスCoVoST 2をリリースします。現在のデータセットは限られた数の言語をカバーしています。音声翻訳は最近、ベンチマークデータセットの開発により、最近ますます人気のある研究トピックになっています。 
[ABSTRACT]これは、総量と言語カバレッジの観点から、現在までに利用可能な最大のオープンデータセットです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-20">
        <br><font color="black">2020-07-20</font>
      </time>
    </span>
</section>
<!-- paper0: asya: Mindful verbal communication using deep learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.AS/paper_3.html">
      <font color="black">asya: Mindful verbal communication using deep learning</font>
    </a>
  </h2>
  <font color="black">当社の話者ダイアライゼーションモデルは、テストデータセットで95％を超える精度を備えています。これらのモデルは、カスタマーサービスの改善、販売効果の高い会話、心理学、カップルセラピーなどのさまざまな分野に適用できます。asyaは、人間の声のスペクトルを分析し、ノイズ検出、話者ダイアライゼーション、性別検出、テンポ推定、および音声のみを使用した感情の分類を行う学習モデル。 
[ABSTRACT]すべてのモデルは言語に強く、分析できます。これらのモデルは、顧客サービスの改善、販売効果の高い会話、心理学、カップルセラピーに適用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: SoundSpaces: Audio-visual Navigation in 3D Environments -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.AS/paper_4.html">
      <font color="black">SoundSpaces: Audio-visual Navigation in 3D Environments</font>
    </a>
  </h2>
  <font color="black">さらに、SoundSpacesを導入します。これは、公に利用可能な2組の3D環境（Matterport3Dとレプリカ）の幾何学的音響シミュレーションに基づくオーディオレンダリングの最初のデータセットであり、新しいセンサーをサポートするためにHabitatを装備し、実世界のスキャンされた一連の環境に任意の音源を挿入します。私たちの結果は、オーディオが3D空間での具体的な視覚ナビゲーションに大きな利益をもたらすことを示しており、私たちの仕事は、具体的なAIによる視覚的知覚の新しい研究の基礎を築いています。世界は自然に多感覚体験ですが、今日の具体化されたエージェントは耳が聞こえない---環境に対する視覚的な認識のみに制限されています。 
[ABSTRACT]オーディオを導入-複雑で音響的および視覚的にリアルな3D環境のためのビジュアルナビゲーション</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-24">
        <br><font color="black">2019-12-24</font>
      </time>
    </span>
</section>
<!-- paper0: A Generalized Framework for Domain Adaptation of PLDA in Speaker
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.AS/paper_5.html">
      <font color="black">A Generalized Framework for Domain Adaptation of PLDA in Speaker
  Recognition</font>
    </a>
  </h2>
  <font color="black">提案された相関配置ベースの補間方法は、適応前のドメイン外PLDAモデルからのものと比較して、minCprimaryを最大30.5％削減し、minCprimaryも、最適な補間重みを使用する従来の線形補間方法よりも5.5％低くなります。さらに、提案された正則化技術は、補間に関するロバスト性を保証します。この論文は、話者認識における確率的線形判別分析（PLDA）のドメイン適応のための一般化されたフレームワークを提案します。 
[要約]提案された方法は紙のペーパーで提案されています。データの使用を制限するなど、いくつかの既存のプログラムが含まれています。また、利用可能なデータのより柔軟な使用を可能にします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Exploring the Best Loss Function for DNN-Based Low-latency Speech
  Enhancement with Temporal Convolutional Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.AS/paper_6.html">
      <font color="black">Exploring the Best Loss Function for DNN-Based Low-latency Speech
  Enhancement with Temporal Convolutional Networks</font>
    </a>
  </h2>
  <font color="black">最も適切な方法は、データセットの規模とタスクのタイプによって異なります。提案された方法は、ボイスバンク+ DEMANDデータセットで効果的であり、他の最先端の方法と比較して有利です。 -DNSチャレンジに提出し、オープンソース化して公開したTasNetのレイテンシバージョン。 
[ABSTRACT]時間-周波数マスキングはdnn音声強調に広く使用されています。ただし、tasnetやtasnetなどの時間領域法も提案されています。この方法は、音声バンク需要データセットに効果的です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-23">
        <br><font color="black">2020-05-23</font>
      </time>
    </span>
</section>
<!-- paper0: Using Multi-Resolution Feature Maps with Convolutional Neural Networks
  for Anti-Spoofing in ASV -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.AS/paper_7.html">
      <font color="black">Using Multi-Resolution Feature Maps with Convolutional Neural Networks
  for Anti-Spoofing in ASV</font>
    </a>
  </h2>
  <font color="black">提案された方法の効率は、ASVspoof 2019データベースに準拠しています。これらは、複数のチャネルの形でたたみ込みニューラルネットワークに供給され、計算コストをわずかに増加させるだけで、入力信号からより多くの情報を抽出できます。中心的な考え方は、スプーフィング防止ネットワークで一般的に使用される機能マップは、単一長のスライディングウィンドウによって抽出されることが多いため、オーディオセグメントの識別表現を構築するには不十分であるという問題を緩和することです。 
[要約]提案された方法は、異なるウィンドウ長を使用して抽出された複数のスペクトログラムを使用します。この方法は、スプーフィング対策データベースに準拠しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Attention and Encoder-Decoder based models for transforming articulatory
  movements at different speaking rates -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.AS/paper_8.html">
      <font color="black">Attention and Encoder-Decoder based models for transforming articulatory
  movements at different speaking rates</font>
    </a>
  </h2>
  <font color="black">現在の作業では、LSTMを使用したエンコーダーデコーダアーキテクチャを提案します。これにより、滑らかな予測調音軌道が生成されます。調音動作の範囲は発話速度と相関しているため、変換された調音運動の振幅を元の速度とは異なる速度で分析します提案されたAstNetがN2FとN2Sの調音運動の範囲をどの程度予測するかを調べるために、対応するものを調べます。提案されたAstNetを使用して持続時間がどの程度うまく変換されるかを調べるために、音素固有の持続時間分析を実行します。 
[ABSTRACT]調音動作は、調音動作をニュートラルから高速（n2f）およびニュートラルから低速（n2s）の発話速度に変換するために使用されています。現在の作業では、滑らかな予測調音軌跡を生成するlstmsを使用したエンコーダー/デコーダーアーキテクチャを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br><font color="black">2020-06-04</font>
      </time>
    </span>
</section>
<!-- paper0: Speaker-Utterance Dual Attention for Speaker and Utterance Verification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.AS/paper_9.html">
      <font color="black">Speaker-Utterance Dual Attention for Speaker and Utterance Verification</font>
    </a>
  </h2>
  <font color="black">提案されたSUDAは、話者と発話の情報ストリーム間の相互作用を学習するアテンションマスクメカニズムを特徴としています。 （SUDA）統合ニューラルネットワーク。 
[ABSTRACT]統合されたニューラルネットワークで話者-発話二重注意（suda）のアイデアを実装します。提案されたsudaは、話者と発話情報ストリーム間の相互作用を学習する注意マスクメカニズムを備えています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-20">
        <br><font color="black">2020-08-20</font>
      </time>
    </span>
</section>
<!-- paper0: Similarity-and-Independence-Aware Beamformer: Method for Target Source
  Extraction using Magnitude Spectrogram as Reference -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-08-21/eess.AS/paper_10.html">
      <font color="black">Similarity-and-Independence-Aware Beamformer: Method for Target Source
  Extraction using Magnitude Spectrogram as Reference</font>
    </a>
  </h2>
  <font color="black">SIBFは、大まかなマグニチュードスペクトログラムを基準信号として使用してターゲット信号を抽出できます。SIBFの利点は、音声強調などのターゲット強調方法によって生成されるスペクトログラムと比較して、正確なターゲット信号を取得できることです。ディープニューラルネットワーク（DNN）に基づいています。最尤推定によってこの抽出問題を解決するために、類似性を反映できる2種類のソースモデルを紹介します。 
[ABSTRACT] sibfは、大まかなマグニチュードスペクトログラムを参照信号として使用してターゲット信号を抽出できます。メソッドは、chime3データセットを使用してsibfを作成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-01">
        <br><font color="black">2020-06-01</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
