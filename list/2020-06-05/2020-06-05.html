<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157706143-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157706143-1');
</script>

<title>Akari-2020-06-05の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/menu_bar.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<link rel='stylesheet' type='text/css' href='../../css/custom.css'>

</head>
<body>
<div class="container">
<!--
	header
	-->
<header role="banner">
		<div class="header-logo">
			<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
		</div>
	</header>
  <input type="checkbox" id="cp_navimenuid">
<label class="menu" for="cp_navimenuid">
<div class="menubar">
	<span class="bar"></span>
	<span class="bar"></span>
	<span class="bar"></span>
</div>
  <ul>
    <li><a id="home" href="../../index.html">Home</a></li>
    <li><a id="about" href="../../teamAkariとは.html">About</a></li>
    <li><a id="contact" href="../../contact.html">Contact</a></li>
    <li><a id="contact" href="../../list/newest.html">New Papers</a></li>
    <li><a id="contact" href="../../list/back_number.html">Back Numbers</a></li>
  </ul>

</label>

<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
cs.SD
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label1">
cs.CL
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label2">
eess.AS
  </label></li>
<!-- field: biorxiv.physiology -->
  <li class="hidden_box">
    <label for="label3">
biorxiv.physiology
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<svg class='line_field-papers' viewBox='0 0 390 2'>
  <path fill='transparent'  id='__1' d='M 0 2 L 390 0'>
  </path>
</svg>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020 -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.SD/paper_0.html">
      End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      そのトレーニングプロセスは、このホワイトペーパーの主な焦点であり、i）転移学習（ASR事前トレーニングおよび知識抽出）、ii）データ拡張（SpecAugment、タイムストレッチおよび合成データ）、iii）マークされた合成データと実際のデータの組み合わせに基づいています。異なるドメインとして、およびiv）CTC損失を使用したマルチタスク学習。提供されたセグメンテーションを使用しました。MuST-CEn-Deテストセットで最高のモデルは29 BLEUを記録しました。これは、最近の論文と比較して優れた結果です。 、およびVADでセグメント化された同じデータの23.7 BLEUは、この特定のデータ条件に対処するソリューションを研究する必要性を示しています。 
[ABSTRACT]タスクは、英語のtedトークの音声をドイツ語のテキストに翻訳するシステムの能力を評価します。タスクには、合成言語言語language.itが含まれます。これは、音声データ用のトランスフォーマーの適応に基づいています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving Speaker Identification using Network Knowledge in Criminal
  Conversational Data -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.SD/paper_1.html">
      Improving Speaker Identification using Network Knowledge in Criminal
  Conversational Data
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      提案された方法は、2人以上の話者を含む会話に適用できます。CSIでは、このアプローチがベースラインの話者の精度を1.3％絶対（1.5％相対）、会話の精度を3.7％絶対（4.7％相対）上回ることを示しています。データ..犯罪捜査の文脈での会話の正確さの測定基準も紹介します。 
[ABSTRACT]犯罪現場の調査（csi）テレビ番組を犯罪会話データの潜在的な候補として紹介します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-03">
        <br>2020-06-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A study on more realistic room simulation for far-field keyword spotting -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.SD/paper_2.html">
      A study on more realistic room simulation for far-field keyword spotting
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ソースコードはPyroomacousticsパッケージで利用可能になり、他の人がこれらの技術を自分の作業に組み込むことができるようになります。クリーンでノイズの多い遠方場条件下での一連の再録音では、最大$ 35.8 \％$の相対的な改善が見られます一般的に使用されている（単一吸収係数）画像ソース法を超えています。アブレーション研究を通じて、ウェイクワードタスクを使用して、測定されたRIRのグラウンドトゥルースセットと比較して、これらの要因の影響を測定します。 
[ABSTRACT]クリーンでノイズのある遠距離場の状態の再録音の保留セットで、最大35ドルを示しました。一般的に使用されている部屋のインパルス応答（riyr）画像ソースメソッドに対して多くの改善を示しました。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: PJS: phoneme-balanced Japanese singing voice corpus -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.SD/paper_3.html">
      PJS: phoneme-balanced Japanese singing voice corpus
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      これらの問題を回避する方法として、音素バランスを保証し、CC BY-SA 4.0でライセンスされるPJS（音素バランスの日本語歌声）コーパスを構築し、音素バランスのとれた音声コーパスを使用してメロディーを構成しました。歌声コーパスは歌声合成の開発に役立ちますが、既存のコーパスには2つの重要な問題があります。データの不均衡（歌声コーパスは、話す声のコーパスとは異なり、音素のバランスを保証しません）と著作権の問題（合法的にデータを共有できない）です。非常に適用可能で再現性のある歌声合成研究に使用できる無料の日本語歌声コーパス。 
[要約]歌声コーパスは歌声合成の開発に役立ちます。しかし、既存のコーパスには2つの重大な問題があります。データの不均衡と著作権の問題です。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: CSTNet: Contrastive Speech Translation Network for Self-Supervised
  Speech Representation Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.SD/paper_4.html">
      CSTNet: Contrastive Speech Translation Network for Self-Supervised
  Speech Representation Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      音声に対応するテキスト翻訳を取得するのは比較的簡単です。この作業では、音声とそれに対応するテキスト翻訳という2つのモダリティ間の相関関係を利用して、音声表現学習のためのマルチモーダル機械学習フレームワークを提供します。学習した表現を評価することにより電話認識タスクでは、検索タスクを実行する学習の副産物として、オーディオエンコーダーの内部表現に言語表現が現れることを示します。 
[ABSTRACT]多くの危険にさらされている言語には正字法の形式はありませんが、通常はバイリンガルで、リソースの高い言語で訓練された話者がいます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Online End-to-End Neural Diarization with Speaker-Tracing Buffer -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.SD/paper_5.html">
      Online End-to-End Neural Diarization with Speaker-Tracing Buffer
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この不整合の問題を回避するために、スピーカートレースバッファーと呼ばれる提案された方法は、自己注意メカニズム内の以前のチャンクで決定されたスピーカー順列情報を維持して、正しいスピーカートレースを維持します。これらの結果は、従来のオンラインクラスタリング手法に基づくよりもはるかに優れています。遅延が1.5秒のx-vectorで、それぞれ26.90％と25.45％のDERを達成しました。我々の実験結果は、スピーカートレースバッファーを備えた提案されたオンラインSA-EENDが、CALLHOMEで12.84％、21.64％のDERを達成したことを示しています。 1秒のレイテンシの自発日本語コーパス用。 
[要約]スピーカートレースバッファーを使用して提案されたオンラインサウンドは、callhomonsに対してder％5.8％を達成しました。この方法は、オフラインのダイアライゼーションエラー率（der）に基づいています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multi-talker ASR for an unknown number of sources: Joint training of
  source counting, separation and ASR -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.SD/paper_6.html">
      Multi-talker ASR for an unknown number of sources: Joint training of
  source counting, separation and ASR
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、システムは、WSJ0-4mixデータベースでの実験に示されているように、トレーニング中に見られたよりも多くの話者に一般化します。実験は、シミュレーションされたクリーンな混合物のカウント精度、音源分離、音声認識で非常に有望なパフォーマンスを示します。 WSJ0-2mixとWSJ0-3mixから。マルチトーカーの重複した音声の分離と認識へのほとんどのアプローチは、同時にアクティブな話者の数が与えられることを前提としていますが、実際の状況では、それは通常不明です。 
[要旨] wsj0-2mixデータベースに新しい注目度の高いエラー率を設定します。また、データベースにクリーンスポットエラー率を設定します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Time-Domain Multi-modal Bone/air Conducted Speech Enhancement -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.SD/paper_7.html">
      Time-Domain Multi-modal Bone/air Conducted Speech Enhancement
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、この新しいSEマルチモーダル構造でEF以外のLF戦略を採用すると、より良い結果が得られます。さらに、早期融合（EF）と後期融合（LF）の2つのアンサンブル学習ベースの戦略を検討します。 、2種類の音声信号を統合し、ディープラーニングベースの完全たたみ込みネットワークを採用して拡張を行います。ただし、ビデオクリップには通常大量のデータが含まれており、計算リソースのコストが高く、複雑になる可能性があります。 SEシステム。 
[ABSTRACT]ビデオクリップとビデオクリップには大量のデータが含まれる可能性があります。ただし、コンピューティングリソースの点でコストが高くなります。これは、骨伝導と空気伝導の信号を使用するseシステムが原因です。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-22">
        <br>2019-11-22
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Training Keyword Spotting Models on Non-IID Data with Federated Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.SD/paper_8.html">
      Training Keyword Spotting Models on Non-IID Data with Federated Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      リソースの制約を克服するために、メモリを集中的に使用するMTRデータの拡張をSpecAugmentに置き換えます。これにより、誤った拒否率が56％減少します。オンデバイスデータのフィッティングに関連するアルゴリズムの制約（本質的に独立しておらず、同一に分散されている）を克服するために、大規模なフェデレーションシミュレーションを使用して、最適化アルゴリズムとハイパーパラメーター構成の徹底的な実証研究を行います。 
[ABSTRACT]オンデバイスデータのフィッティングに関連するアルゴリズムの制約を克服するために、アルゴリズムとハイパーパラメーター構成の徹底的な調査を実施します。例をテストするために、教師と生徒のトレーニングを調査します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-21">
        <br>2020-05-21
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020 -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.CL/paper_0.html">
      End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの最高のモデルは、MuST-C En-Deテストセットで29 BLEUを記録しました。これは、最近の論文と比較して優れた結果であり、VADでセグメント化された同じデータで23.7 BLEUであり、この特定のデータ条件に対処するソリューションの研究の必要性を示しています。 。テストトークは2つのバージョンで提供されます。1つは自動ツールですでにセグメント化されたデータを含み、もう1つはセグメント化されていない生データです。そのトレーニングプロセスはこのホワイトペーパーの主な焦点であり、以下に基づいています。学習（ASR事前トレーニングおよび知識抽出）、ii）データ拡張（SpecAugment、タイムストレッチおよび合成データ）、iii）異なるドメインとしてマークされた合成データと実際のデータの組み合わせ、およびiv）CTC損失を使用したマルチタスク学習。 
[ABSTRACT]タスクは、英語のtedトークの音声をドイツ語のテキストに翻訳するシステムの能力を評価します。タスクには、合成言語言語language.itが含まれます。これは、音声データ用のトランスフォーマーの適応に基づいています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Using Self-Training to Improve Back-Translation in Low Resource Neural
  Machine Translation -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.CL/paper_1.html">
      Using Self-Training to Improve Back-Translation in Low Resource Neural
  Machine Translation
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この手法は、ベースラインの低リソースIWSLT&#39;14英語-ドイツ語およびIWSLT&#39;15英語-ベトナム語の後方変換モデルをそれぞれ11.06および1.5 BLEU改善することが示されました。改良された英語-ドイツ語後方モデルによって生成された合成データは、 2.7 BLEUによる標準の逆変換を使用してトレーニングされた別のフォワードモデルよりもパフォーマンスが優れたフォワードモデル。リソースが少ない状況では、使用可能な並列データは通常、標準のトレーニングに必要な定性的合成データを生成できるバックワードモデルをトレーニングするには不十分です。翻訳モデル。 
[要約]システムの品質は、多くの研究で最終qutモデルのパフォーマンスに影響を与えることが示されています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: The SOFC-Exp Corpus and Neural Approaches to Information Extraction in
  the Materials Science Domain -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.CL/paper_2.html">
      The SOFC-Exp Corpus and Neural Approaches to Information Extraction in
  the Materials Science Domain
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      また、新しいデータセットに基づいて対処できるさまざまなタスクの強力なニューラルネットワークベースのモデルを紹介します。すべてのタスクで、BERT埋め込みを使用すると、パフォーマンスが大幅に向上しますが、タスクの複雑さが増すため、繰り返し上位のニューラルネットワークは有益なようです。このホワイトペーパーでは、材料科学の領域における新しい挑戦的な情報抽出タスクについて説明します。 
[要約]コーパスとアノテーター間の合意の調査により、提案された名前付きエンティティの認識とスロット充填タスクの複雑さが実証されました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: pyBART: Evidence-based Syntactic Transformations for IE -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.CL/paper_3.html">
      pyBART: Evidence-based Syntactic Transformations for IE
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      イベント構造と多くの字句関係を明示的にする、幅広いカバレッジ、データ駆動型、言語的に正しい変換のセットを紹介します。これらの構文依存関係は、構文関係を正確に反映するように設計されており、意味関係を明示的にしません。英語のUDツリーを拡張UDグラフまたは表現に変換するための使いやすいオープンソースのPythonライブラリであるpyBARTを紹介します。 
[ABSTRACT]これらの表現には、下流のアプリケーションに役立つコンテンツワード間の多くの明示的な接続がありません。これらの表現は、Pythonユーザーが利用できず、ユニバーサルカバレッジによっても駆動されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-04">
        <br>2020-05-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Syntactic Search by Example -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.CL/paper_4.html">
      Syntactic Search by Example
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ウィキペディアシステムのデモは、https：//allenai.github.io/spikeで入手できます。これにより、構文ベースのクエリの迅速な探索、開発、改良が可能になります。2つのコーパス、英語のWikipediaと、英語で公開された要約のコレクションに対するクエリを使用したシステムを示します。 
[要約]ウィキペディアシステムでは、ユーザーが基礎となる構文表現の詳細を知る必要はありません。代わりに、ユーザーが主要な構文表現の詳細を知る必要のない軽量の検索エンジンを導入します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Response to LiveBot: Generating Live Video Comments Based on Visual and
  Textual Contexts -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.CL/paper_5.html">
      Response to LiveBot: Generating Live Video Comments Based on Visual and
  Textual Contexts
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      元のLivebotペーパーで報告されたベースラインの結果を再現しようとしたところ、プロジェクトのコードベースを使用して再現された結果とペーパーで報告された数値の間に違いが見つかりました。この状況をさらに詳しく調べると、これは、プロジェクトコードの問題（トレーニングセットとテストセット間の明白でない重複を含む）。これにより、既存のビデオストリームと既存の視聴者のコメントの両方からライブビデオコメントを自動生成できます。 
[ABSTRACT]これにより、既存の動画ストリームと既存の視聴者のコメントの両方からライブ動画コメントを自動生成できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: CSTNet: Contrastive Speech Translation Network for Self-Supervised
  Speech Representation Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.CL/paper_6.html">
      CSTNet: Contrastive Speech Translation Network for Self-Supervised
  Speech Representation Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ここでは、音声から言語表現を抽出できるたたみ込みニューラルネットワークオーディオエンコーダーを構築します。音声に対応するテキスト翻訳を取得するのは比較的簡単です。電話認識タスクで学習した表現を評価することにより、言語表現が検索タスクを実行する学習の副産物としてのオーディオエンコーダーの内部表現。 
[ABSTRACT]多くの危険にさらされている言語には正字法の形式はありませんが、通常はバイリンガルで、リソースの高い言語で訓練された話者がいます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multi-talker ASR for an unknown number of sources: Joint training of
  source counting, separation and ASR -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.CL/paper_7.html">
      Multi-talker ASR for an unknown number of sources: Joint training of
  source counting, separation and ASR
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、WSJ0-4mixデータベースを使用した実験で示されているように、私たちのシステムは、トレーニング中にこれまで見たよりも多くの話者にうまく一般化します。マルチトーカー重複音声分離および認識へのほとんどのアプローチは、同時にアクティブな話者の数が私たちの実験は、WSJ0-2mixとWSJ0-3mixからのシミュレートされたクリーンな混合物でのカウント精度、音源分離、音声認識において非常に有望なパフォーマンスを示しています。 
[要旨] wsj0-2mixデータベースに新しい注目度の高いエラー率を設定します。また、データベースにクリーンスポットエラー率を設定します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Embeddings of Label Components for Sequence Labeling: A Case Study of
  Fine-grained Named Entity Recognition -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.CL/paper_8.html">
      Embeddings of Label Components for Sequence Labeling: A Case Study of
  Fine-grained Named Entity Recognition
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ただし、ほとんどのシーケンスラベリングモデルではそのようなラベルコンポーネントは考慮されませんが、Personなどのラベル間で共有されるコンポーネントは、ラベル予測に役立ちます。一般に、シーケンスラベリングで使用されるラベルは、さまざまなタイプの要素で構成されます。この作業では、ラベルコンポーネント情報を埋め込みとしてモデルに統合することを提案します。 
[ABSTRACT]この作業では、ラベルコンポーネント情報を埋め込みとして人物に統合することを提案します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-02">
        <br>2020-06-02
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Seq2Seq AI Chatbot with Attention Mechanism -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.CL/paper_9.html">
      Seq2Seq AI Chatbot with Attention Mechanism
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ディープラーニングの登場により、これらのモデルはすぐにエンドツーエンドのトレーニング可能なニューラルネットワークに置き換えられました。人工知能または機械学習技術を使用したインテリジェントな会話型エージェントの開発は、自然言語処理の分野で興味深い問題です。 
[ABSTRACT]インテリジェントラーニングは一連の人工的なコミュニケーション形式の最新版です。テクノロジーはディープラーニングの台頭とともに開発されました
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Personalizing Grammatical Error Correction: Adaptation to Proficiency
  Level and L1 -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.CL/paper_10.html">
      Personalizing Grammatical Error Correction: Adaptation to Proficiency
  Level and L1
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの研究は5の習熟度レベルと12の異なる言語をカバーし、3つの異なる適応シナリオを比較するその種の中で最も広いです：熟練度レベルのみ、第一言語のみ、または両方の側面に同時に適応します。最初の結果を提示します。数千の注釈付きの文のみを使用して、汎用ニューラルGECシステムを熟練度レベルとライターの第一言語の両方に適合させることについて。両方のシナリオに合わせると、最大のパフォーマンス改善（3.6 F0.5）が達成されることを示します。強いベースラインに。 
[ABSTRACT]この研究は5の習熟度レベルと12の異なる言語をカバーする、その種の中で最も幅広いものです
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Experiments on Paraphrase Identification Using Quora Question Pairs
  Dataset -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.CL/paper_11.html">
      Experiments on Paraphrase Identification Using Quora Question Pairs
  Dataset
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      Quoraの質問ペアデータセットをモデル化して類似の質問を特定しました。特徴抽出には、Count Vectorizerを含むBag of Wordsと、XGBoostとCatBoostのユニグラムを使用したTerm Frequency-Inverse Document Frequencyを使用しました。前作からのアプローチ。 
[ABSTRACT]私たちが使用するデータセットはquora.itによって提供されています。これはワードピーストークナイザーのデータセットに似ています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Using Aspect Extraction Approaches to Generate Review Summaries and User
  Profiles -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.CL/paper_12.html">
      Using Aspect Extraction Approaches to Generate Review Summaries and User
  Profiles
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      2番目の実験は、ユーザーが書いたレビューによってユーザーを表すために、回復されたアスペクト分布の適合性に焦点を当てています。$ k $の平均ベースラインは、この設定で非常によく機能します。1つ目は、レビューからさまざまなアスペクトの標準的な文章を抽出することです、そして代替案に対して人間の評価者によって判断されます。 
[ABSTRACT]ユーザーは、特定の側面の観点からレビューまたはレビュースニペットを調査することを望みます。最初の実験は、レビューからさまざまな側面の聖書の文章を抽出することであり、代替に対して人間の評価者によって判断されます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2018-04-23">
        <br>2018-04-23
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Stopwords in Technical Language Processing -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.CL/paper_13.html">
      Stopwords in Technical Language Processing
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      エンジニアリングコンテキストでの情報検索、索引付け、トピックモデリングのための自然言語処理技術のアプリケーションはますます増えています。研究者は、一般的な英語用に派生したすぐに利用できるストップワードリストを使用していますが、エンジニアリング分野の専門用語には、非常に頻繁に情報語ではなく、技術的な言語処理アプリケーション用の標準ストップワードリストはありません。このようなタスクの標準コンポーネントは、データの非情報コンポーネントであるストップワードの削除です。 
[ABSTRACT]ストップワードはデータの情報を提供しないコンポーネントです。これらはデータの情報を提供しない部分です。これらは削除するために必要です
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Linguists Who Use Probabilistic Models Love Them: Quantification in
  Functional Distributional Semantics -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.CL/paper_14.html">
      Linguists Who Use Probabilistic Models Love Them: Quantification in
  Functional Distributional Semantics
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      このアカウントを、Rational Speech Actsフレームワークでの一般的な数量化のモデリングに関する最近の作業に結び付け、これをロバの文章のモデリングに拡張します。最後に、一般的な数量詞が実用的に複雑でありながら、正確な数量詞よりも計算が簡単な方法について説明します。このホワイトペーパーでは、正確な量指定子が曖昧な述語で使用された場合に、前の定式化が些細な真理値を与える方法を示します。 
[ABSTRACT] i改善されたアカウントを提案し、曖昧な数量化を合理的な事前cps上の分布として扱うことでこの問題を回避します。一般的な数量詞が実際の複雑さと正確な数量詞よりも計算的に単純な方法の両方を説明します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: CiwGAN and fiwGAN: Encoding information in acoustic data to model
  lexical learning with Generative Adversarial Networks -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.CL/paper_15.html">
      CiwGAN and fiwGAN: Encoding information in acoustic data to model
  lexical learning with Generative Adversarial Networks
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      アーキテクチャは、生成されたオーディオ出力から潜在コードを取得することを学習するネットワークを導入します。したがって、字句学習は、ディープニューラルネットワークにデータを出力させ、固有の情報をその音響出力から取得できるようにするアーキテクチャから出現したものとしてモデル化されます。ディープニューラルネットワークは、人間の音声の単語に対応する情報を生の音響データにエンコードしますか？ 
[要約] ciwgan、fiwgan、fiwganは、教師なしの語彙学習の例です。これらは、出力データのdcganアーキテクチャとinfoganを組み合わせています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: ColBERT: Efficient and Effective Passage Search via Contextualized Late
  Interaction over BERT -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.CL/paper_16.html">
      ColBERT: Efficient and Effective Passage Search via Contextualized Late
  Interaction over BERT
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ColBERTは、BERTを使用してクエリとドキュメントを個別にエンコードし、細かい類似性をモデル化する安価で強力な相互作用ステップを採用するレイトインタラクションアーキテクチャを導入します。深いLMを使用しながら、ドキュメント表現をオフラインで事前に計算する機能を同時に取得し、クエリ処理を大幅に高速化します。大規模なドキュメントコレクションから直接エンドツーエンドで検索するためのインデックス。 
[ABSTRACT]これらのlmsに基づくランク付けモデルは、以前のアプローチよりも桁違いにコストが増加します。colbertのプルーニング-フレンドリーな相互作用メカニズムにより、大規模なドキュメントコレクションから直接、エンドツーエンドツーエンドの検索で確率インデックスを活用できます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-27">
        <br>2020-04-27
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Description Based Text Classification with Reinforcement Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.CL/paper_17.html">
      Description Based Text Classification with Reinforcement Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      単一ラベル分類、マルチラベル分類、多面的感情分析などの幅広いテキスト分類タスクで、強力なベースラインよりもパフォーマンスが大幅に向上することがわかります。説明とテキストを連結して分類子に送り、現在のラベルをテキストに割り当てる必要があるかどうか。提案された戦略では、モデルをラベルに関して最も目立つテキストに注意を向けさせます。これは注意のハードバージョンと見なすことができ、パフォーマンスの向上につながります。 
[ABSTRACT]提案された戦略は、モデルがラベルに関して最も重要なテキストに注意を払うように強制します。これは注意のハードバージョンと見なすことができ、より良いパフォーマンスにつながります
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-08">
        <br>2020-02-08
      </time>
    </span>
  </h3>
</article>
<!-- paper0: M3P: Learning Universal Representations via Multitask Multilingual
  Multimodal Pre-training -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.CL/paper_18.html">
      M3P: Learning Universal Representations via Multitask Multilingual
  Multimodal Pre-training
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      また、商用検索エンジンのログから8つの言語で大量の（テキストクエリ、画像、コンテキスト）トリプレットを収集して、新しい多言語画像言語データセット（MILD）を構築します。評価は、M3Pが（i）多言語タスクと英語のマルチモーダルタスクで同等の結果を達成できることを示しています。これら2つのタイプのタスクについて個別に事前トレーニングされた最新モデルと比較して、（ii）新しい状態を取得できます。 -ゼロショットまたは少数ショット設定での英語以外のマルチモーダルタスクに関する最新の結果。M3Pの一般化機能を検証するために、さまざまな種類のダウンストリームタスク用に事前トレーニング済みモデルを微調整します。テキスト検索、多言語画像キャプション、マルチモーダル機械翻訳、多言語自然言語推論、多言語テキスト生成。 
[要約]多言語、マルチモーダルタスクは達成できますが、マルチモーダルは完全にトレーニングされていません。デュアルデュアルデュアル英語エンジンモデルは、ゼロショットまたは少数ショット設定の設定で英語以外のマルチモーダルタスクの新しい結果を学習することもできます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Training Keyword Spotting Models on Non-IID Data with Federated Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/cs.CL/paper_19.html">
      Training Keyword Spotting Models on Non-IID Data with Federated Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      最後に、例にラベルを付ける（デバイス上のデータの可視性がゼロの場合）ために、教師と生徒のトレーニングを検討します。リソースの制約を克服するために、メモリを集中的に使用するMTRデータの拡張をSpecAugmentに置き換えます。これにより、誤った拒否率が56％削減されます。オンデバイスデータのフィッティングに関連するアルゴリズムの制約（本質的に独立しておらず、同一に分散されている）を克服するために、大規模なフェデレーションシミュレーションを使用して、最適化アルゴリズムとハイパーパラメーター構成の徹底的な実証研究を行います。 
[ABSTRACT]オンデバイスデータのフィッティングに関連するアルゴリズムの制約を克服するために、アルゴリズムとハイパーパラメーター構成の徹底的な調査を実施します。例をテストするために、教師と生徒のトレーニングを調査します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-21">
        <br>2020-05-21
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020 -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/eess.AS/paper_0.html">
      End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの最高のモデルは、MuST-C En-Deテストセットで29 BLEUを記録しました。これは、最近の論文と比較して優れた結果であり、VADでセグメント化された同じデータで23.7 BLEUであり、この特定のデータ条件に対処するソリューションの研究の必要性を示しています。 。そのトレーニングプロセスはこのホワイトペーパーの主な焦点であり、i）転移学習（ASR事前トレーニングおよび知識抽出）、ii）データ拡張（SpecAugment、タイムストレッチおよび合成データ）、iii）合成データと実際のデータの組み合わせに基づいています。異なるドメインとしてマークされ、iv）CTC損失を使用したマルチタスク学習。最後に、単語レベルの知識抽出によるトレーニングが完了した後、STモデルはラベル平滑化クロスエントロピーを使用して微調整されます。 
[ABSTRACT]タスクは、英語のtedトークの音声をドイツ語のテキストに翻訳するシステムの能力を評価します。タスクには、合成言語言語language.itが含まれます。これは、音声データ用のトランスフォーマーの適応に基づいています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Improving Speaker Identification using Network Knowledge in Criminal
  Conversational Data -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/eess.AS/paper_1.html">
      Improving Speaker Identification using Network Knowledge in Criminal
  Conversational Data
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      犯罪捜査は会話データの収集に依存しています。調査員はソーシャルネットワーク分析ツールを使用して、ネットワーク内の最も中心的な人物とさまざまなコミュニティを特定します。提案された方法は、2人以上の話者が関与する会話に適用できます。 
[ABSTRACT]犯罪現場の調査（csi）テレビ番組を犯罪会話データの潜在的な候補として紹介します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-03">
        <br>2020-06-03
      </time>
    </span>
  </h3>
</article>
<!-- paper0: A study on more realistic room simulation for far-field keyword spotting -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/eess.AS/paper_2.html">
      A study on more realistic room simulation for far-field keyword spotting
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ソースコードはPyroomacousticsパッケージで利用可能になり、他の人がこれらの技術を自分の作業に組み込むことができるようになります。クリーンでノイズの多い遠方場条件下での一連の再録音の差し控えにより、最大$ 35.8 \％$の相対改善一般的に使用されている（単一吸収係数）画像ソース法を超えています。この目的のために、室内インパルス応答（RIR）生成に次の要素を組み込むことの影響を調べます：空気吸収、表面および周波数依存係数マテリアル、確率的レイトレーシング。 
[ABSTRACT]クリーンでノイズのある遠距離場の状態の再録音の保留セットで、最大35ドルを示しました。一般的に使用されている部屋のインパルス応答（riyr）画像ソースメソッドに対して多くの改善を示しました。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: PJS: phoneme-balanced Japanese singing voice corpus -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/eess.AS/paper_3.html">
      PJS: phoneme-balanced Japanese singing voice corpus
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本論文では、適用性と再現性の高い歌声合成研究に利用できる無料の日本語歌声コーパスを紹介する。これらの問題を回避する方法として、音素バランスを保証するPJS（音素バランス和歌声）コーパスを構築した。はCC BY-SA 4.0でライセンスされており、音素バランスのとれた音声コーパスを使用してメロディーを作成しました。歌声コーパスは歌声合成の開発に役立ちますが、既存のコーパスには2つの重要な問題があります：データの不均衡（歌声コーパスではありません）話す声のコーパスとは異なり、音素のバランスを保証します）および著作権の問題（合法的にデータを共有することはできません）。 
[要約]歌声コーパスは歌声合成の開発に役立ちます。しかし、既存のコーパスには2つの重大な問題があります。データの不均衡と著作権の問題です。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: CSTNet: Contrastive Speech Translation Network for Self-Supervised
  Speech Representation Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/eess.AS/paper_4.html">
      CSTNet: Contrastive Speech Translation Network for Self-Supervised
  Speech Representation Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      ここでは、音声から言語表現を抽出できる畳み込みニューラルネットワークオーディオエンコーダーを構築します。音声に対応するテキスト翻訳を取得するのは比較的簡単です。オーディオエンコーダーは、対照的な学習フレームワークで音声翻訳検索タスクを実行するようにトレーニングされています。 
[ABSTRACT]多くの危険にさらされている言語には正字法の形式はありませんが、通常はバイリンガルで、リソースの高い言語で訓練された話者がいます
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Online End-to-End Neural Diarization with Speaker-Tracing Buffer -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/eess.AS/paper_5.html">
      Online End-to-End Neural Diarization with Speaker-Tracing Buffer
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      この不整合の問題を回避するために、スピーカートレースバッファーと呼ばれる提案された方法は、自己注意メカニズム内の以前のチャンクで決定されたスピーカー順列情報を維持して、正しいスピーカートレースを維持します。これらの結果は、従来のオンラインクラスタリング手法に基づくよりもはるかに優れています。遅延が1.5秒のx-ベクトル上で、それぞれ26.90％と25.45％のDERを達成しました。完全に監視された自己注意メカニズム（SA-EEND）を使用したエンドツーエンドのスピーカーのダイアライゼーションは、状態から大幅な改善を実現しました-of-artクラスタリングベースの方法、特に重複する場合。 
[要約]スピーカートレースバッファーを使用して提案されたオンラインサウンドは、callhomonsに対してder％5.8％を達成しました。この方法は、オフラインのダイアライゼーションエラー率（der）に基づいています
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Multi-talker ASR for an unknown number of sources: Joint training of
  source counting, separation and ASR -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/eess.AS/paper_6.html">
      Multi-talker ASR for an unknown number of sources: Joint training of
  source counting, separation and ASR
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      私たちの実験は、WSJ0-2mixとWSJ0-3mixからのシミュレートされたクリーンな混合でのカウント精度、音源分離、音声認識で非常に有望なパフォーマンスを示しています。さらに、このシステムは、トレーニング中に見られたよりも多くのスピーカーに一般化されています。 WSJ0-4mixデータベースを使った実験で。特に、WSJ0-2mixデータベースに新しい最先端の単語エラー率を設定しました。 
[要旨] wsj0-2mixデータベースに新しい注目度の高いエラー率を設定します。また、データベースにクリーンスポットエラー率を設定します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br>2020-06-04
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Time-Domain Multi-modal Bone/air Conducted Speech Enhancement -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/eess.AS/paper_7.html">
      Time-Domain Multi-modal Bone/air Conducted Speech Enhancement
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      さらに、この新しいSEマルチモーダル構造でEF以外のLF戦略を採用すると、より良い結果が得られます。さらに、早期融合（EF）と後期融合（LF）の2つのアンサンブル学習ベースの戦略を検討します。 、2つのタイプの音声信号を統合し、ディープラーニングベースの完全畳み込みネットワークを採用して強化を行います。マンダリンコーパスでの実験結果は、この新たに提示されたマルチモーダル（骨および空気伝導信号を統合する） ）SE構造は、さまざまな音声評価メトリックにおいて、単一ソースのSEの対応物（骨または空気伝導信号のみ）を大幅に上回っています。 
[ABSTRACT]ビデオクリップとビデオクリップには大量のデータが含まれる可能性があります。ただし、コンピューティングリソースの点でコストが高くなります。これは、骨伝導と空気伝導の信号を使用するseシステムが原因です。
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-22">
        <br>2019-11-22
      </time>
    </span>
  </h3>
</article>
<!-- paper0: Training Keyword Spotting Models on Non-IID Data with Federated Learning -->
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-06-05/eess.AS/paper_8.html">
      Training Keyword Spotting Models on Non-IID Data with Federated Learning
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本番品質のキーワードスポッティングモデルは、統合学習を使用してデバイス上でトレーニングでき、中央でトレーニングされたモデルと同等の誤った受け入れ率と誤った拒否率を達成できることを示しています。最後に、例にラベルを付けます（デバイスのデータ）、教師と生徒のトレーニングを探索します。リソースの制約を克服するために、メモリを集中的に使用するMTRデータの拡張をSpecAugmentに置き換えます。これにより、誤った拒否率が56％削減されます。 
[ABSTRACT]オンデバイスデータのフィッティングに関連するアルゴリズムの制約を克服するために、アルゴリズムとハイパーパラメーター構成の徹底的な調査を実施します。例をテストするために、教師と生徒のトレーニングを調査します
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-21">
        <br>2020-05-21
      </time>
    </span>
  </h3>
</article>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<article itemscope itemtype="https://schema.org/Blog">
  <h2 class="entry-title" itemprop="headline">
    <a href="">
      
    </a>
  </h2>
  <h3 class="entry-abstract" itemprop="abstract">
      本日更新された論文はありません
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="">
        <br>
      </time>
    </span>
  </h3>
</article>
</div>
</main>
<footer role="contentinfo">
  <div class="hr"></div>
  <address>
    <div class="avatar-bottom">
      <a href="https://twitter.com/akari39203162">
        <img src="../../images/twitter.png">
      </a>
    </div>
    <div class="avatar-bottom">
      <a href="https://www.miraimatrix.com/">
        <img src="../../images/mirai.png">
      </a>
    </div>

  <div class="copyright">Copyright &copy;
    <a href="../../teamAkariについて">Akari</a> All rights reserved.
  </div>
  </address>
</footer>

</div>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
       HTML: ["input/TeX","output/HTML-CSS"],
       TeX: {
              Macros: {
                       bm: ["\\boldsymbol{#1}", 1],
                       argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                       argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
              extensions: ["AMSmath.js","AMSsymbols.js"],
              equationNumbers: { autoNumber: "AMS" } },
       extensions: ["tex2jax.js"],
       jax: ["input/TeX","output/HTML-CSS"],
       tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: true },
       "HTML-CSS": { availableFonts: ["TeX"],
                     linebreaks: { automatic: true } }
   });
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     }
   });
</script>

<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


</body>
</html>
