<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-11-19の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Focusing Phenomena in Linear Discrete Inverse Problems in Acoustics -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.SD/paper_0.html">
      <font color="black">Focusing Phenomena in Linear Discrete Inverse Problems in Acoustics</font>
    </a>
  </h2>
  <font color="black">次に、均一な線形アレイを使用して複数のサウンドゾーンを再作成するアプリケーションを検討します。一方、その最小化により、ソースが各ポイントで選択的にフォーカスできる理想的なフォーカス状態が得られ、他のすべてのポイントでヌルが作成されます..線形離散逆問題に固有の集束操作が形式化されます。 
[概要]開発は音場再生のコンテキストで行われます。ここで、ソース強度は、指定された圧力場を再現するために必要な逆解です。1点でのクロストークの最大化は、システムの線形依存性につながります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-01">
        <br><font color="black">2020-11-01</font>
      </time>
    </span>
</section>
<!-- paper0: Tie Your Embeddings Down: Cross-Modal Latent Spaces for End-to-end
  Spoken Language Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.SD/paper_1.html">
      <font color="black">Tie Your Embeddings Down: Cross-Modal Latent Spaces for End-to-end
  Spoken Language Understanding</font>
    </a>
  </h2>
  <font color="black">異なるクロスモーダル損失にわたって、2つの公開されているE2EデータセットでCMLSモデルをトレーニングし、提案されたトリプレット損失関数が最高のパフォーマンスを達成することを示します。エンドツーエンド（E2E）音声言語理解（SLU）システムは音声信号から直接話された発話のセマンティクス..さまざまなマルチモーダル損失を使用して、意味的に強力な事前トレーニング済みBERTモデルから取得した、音響埋め込みをテキスト埋め込みに近づけるように明示的にガイドすることを提案します。 
[概要] e2eシステムのトレーニングは、主にペアのテキストメッセージが不足しているため、依然として課題です。このシステムを使用して、テキスト埋め込みに近づくように音響埋め込みをトレーニングできます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Streaming Transformer ASR with Blockwise Synchronous Beam Search -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.SD/paper_2.html">
      <font color="black">Streaming Transformer ASR with Blockwise Synchronous Beam Search</font>
    </a>
  </h2>
  <font color="black">HKUSTおよびAISHELL-1Mandarin、LibriSpeech English、およびCSJ Japaneseタスクの評価は、提案されたストリーミングTransformerアルゴリズムが、特に知識蒸留技術を使用する場合に、単調チャンクワイズアテンション（MoChA）を含む従来のオンラインアプローチよりも優れていることを示しています。検討したすべてのタスクで、バッチモデルやその他のストリーミングベースのTransformerメソッドと同等またはそれ以上のパフォーマンスを実現します。この論文では、ストリーミングE2E TransformerASRを実行するエンコーダのブロックワイズ処理に基づく新しいブロックワイズ同期ビーム検索アルゴリズムを提案します。 
[ABSTRACT]トランスフォーマーは、ブロック境界検出手法を使用してポイントを作成します。予測された各理論の信頼性スコアは、仮説の認識の終わりと繰り返されるトークンに基づいて評価されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-25">
        <br><font color="black">2020-06-25</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Channel Automatic Speech Recognition Using Deep Complex Unet -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.SD/paper_3.html">
      <font color="black">Multi-Channel Automatic Speech Recognition Using Deep Complex Unet</font>
    </a>
  </h2>
  <font color="black">マルチチャネル自動音声認識（ASR）システムのフロントエンドモジュールは、主にマイクアレイ技術を使用して、残響とエコーを伴うノイズの多い条件で強化された信号を生成します。一方、認識精度を向上させるために、いくつかのトレーニング戦略を使用して提案された方法を調査します。エコーを使用した1000時間の実世界のXiaoMiスマートスピーカーデータについて..実験によると、提案されたDCUnet-MTL方式は、アレイ処理とシングルチャネル音響を使用した従来のアプローチと比較して、相対文字エラー率（CER）を約12.2％削減します。モデル。 
[ABSTRACT] front-t-lineは、従来の信号処理に比べて有望な改善を示していますが、複数開発システムの改善も示しています。これには、無線通信や認識精度を向上させるための学習が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: CAA-Net: Conditional Atrous CNNs with Attention for Explainable
  Device-robust Acoustic Scene Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.SD/paper_4.html">
      <font color="black">CAA-Net: Conditional Atrous CNNs with Attention for Explainable
  Device-robust Acoustic Scene Classification</font>
    </a>
  </h2>
  <font color="black">マルチデバイスASCに注意を払って条件付き畳み込みCNNを提案します。音響シーン分類（ASC）は、オーディオ信号が記録される環境を分類することを目的としています。提案されたシステムには、ASCブランチとデバイス分類ブランチが含まれています。 CNN。 
[ABSTRACT]畳み込みニューラルネットワーク（cnns）がascに正常に適用されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Context-aware RNNLM Rescoring for Conversational Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.SD/paper_5.html">
      <font color="black">Context-aware RNNLM Rescoring for Conversational Speech Recognition</font>
    </a>
  </h2>
  <font color="black">4つの異なる会話テストセットの結果は、私たちのアプローチが、1回目のパスデコードおよび一般的なラティススコアリングと比較して、それぞれ最大13.1％および6％の相対文字エラー率（CER）の削減をもたらすことを示しています。会話型音声認識は、フリースタイルのスピーキングと長期的なコンテキスト依存性のため、やりがいのあるタスクです。トピックやスピーカーターンなどの会話中に持続する性質をさらに活用するために、再スコアリング手順を新しいコンテキスト認識方式に拡張します。 
[ABSTRACT]以前の作業では、長距離コンテキストのモデリングを検討しました。隣接する文を話者や意図情報などのさまざまなタグワードと連結することにより、コンテキストの依存関係をキャプチャします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: WPD++: An Improved Neural Beamformer for Simultaneous Speech Separation
  and Dereverberation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.SD/paper_6.html">
      <font color="black">WPD++: An Improved Neural Beamformer for Simultaneous Speech Separation
  and Dereverberation</font>
    </a>
  </h2>
  <font color="black">複素スペクトルドメインのスケール不変の信号対雑音比（C-Si-SNR）および振幅ドメインの平均二乗誤差（Mag-MSE）を含む多目的損失は、拡張されたものに複数の制約を課すように適切に設計されています。スピーチとドライクリーン信号の望ましいパワー..従来のWPDの強化されたビームフォーミングモジュールと共同トレーニングのための多目的損失関数による「WPD ++」と呼ばれる改良されたニューラルWPDビームフォーマーを提案します。共同トレーニングは複素数値のマスク推定器とWPD ++ビームフォーマーをエンドツーエンドの方法で最適化します。 
[概要]ノイズキャンセリングコンポーネントは進歩する可能性があります。ノイズベースのノイズベースのノイズベースのノイズノイズベースのノイズベースのコンポーネントは引き続き利用可能です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Expanding Access to Music Technology -- Rapid Prototyping Accessible
  Instrument Solutions For Musicians With Intellectual Disabilities -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.SD/paper_7.html">
      <font color="black">Expanding Access to Music Technology -- Rapid Prototyping Accessible
  Instrument Solutions For Musicians With Intellectual Disabilities</font>
    </a>
  </h2>
  <font color="black">適応された楽器のデザインの評価は、従来の楽器や製品を評価するためのフレームワークとは大きく異なる場合がありますが、そのような焦点を絞った個別のデザインの結果には、さまざまな用途が考えられます。表現、および可能性のあるキュレーション-適応としての音の結果..楽器はさまざまなセンサーを採用し、結果として得られる音楽コントロールをMIDI経由でソフトウェアサウンドジェネレーターに送信します。 
[ABSTRACT]楽器によって開発され、さまざまなセンサーを使用して開発され、結果として得られた音楽コントロールをMIDI経由でソフトウェアサウンドジェネレーターに送信します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Ultra-Lightweight Speech Separation via Group Communication -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.SD/paper_8.html">
      <font color="black">Ultra-Lightweight Speech Separation via Group Communication</font>
    </a>
  </h2>
  <font color="black">サブバンド周波数LSTM（F-LSTM）アーキテクチャに動機付けられて、特徴ベクトルがより小さなグループに分割され、小さな処理ブロックがグループ間通信を実行するために使用されるグループ通信（GroupComm）を紹介します。サブバンド出力が連結された標準のF-LSTMモデルでは、超小型モジュールがすべてのグループに並列に適用されるため、モデルサイズを大幅に縮小できます。このペーパーでは、単純なモデル設計パラダイムを提供します。パフォーマンスを犠牲にすることなく、超軽量モデルを明示的に設計します。 
[概要]グループ通信（groupcomm）は、35.6倍少ないパラメータと2.3倍少ない操作で同等のパフォーマンスを達成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Audio-visual Multi-channel Recognition of Overlapped Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.SD/paper_9.html">
      <font color="black">Audio-visual Multi-channel Recognition of Overlapped Speech</font>
    </a>
  </h2>
  <font color="black">\ textit {TFマスキング}、\ textit {filter \＆sum}、および\ textit {マスクベースのMVDR}ビームフォーミングアプローチに基づく一連のオーディオビジュアルマルチチャネル音声分離フロントエンドコンポーネントが開発されました。不変性に動機付けられました。音響信号の破損に対する視覚モダリティの評価について、このペーパーでは、緊密に統合された分離フロントエンドと認識バックエンドを備えたオーディオビジュアルマルチチャネルオーバーラップ音声認識システムを紹介します。オーバーラップ音声の自動音声認識（ASR）は依然として非常に困難なタスクです。現在まで。 
[概要]提案されたマルチチャネルavsrシステムはベースラインオーディオを上回ります-asrシステムのみが最大6.81％（相対26. 83％）および22. 22. 22. 87％。提案されたマルチトールavsrシステム35,000ドルの費用で開発されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br><font color="black">2020-05-18</font>
      </time>
    </span>
</section>
<!-- paper0: Dynamic Prosody Generation for Speech Synthesis using Linguistics-Driven
  Acoustic Embedding Selection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.SD/paper_10.html">
      <font color="black">Dynamic Prosody Generation for Speech Synthesis using Linguistics-Driven
  Acoustic Embedding Selection</font>
    </a>
  </h2>
  <font color="black">私たちの結果は、このアプローチが複雑な発話とロングフォームリーディング（LFR）の韻律と自然さを改善することを示しています。セマンティック機能と構文機能の両方の寄与を分析します。Text-to-Speech（TTS）の最近の進歩は孤立した文を検討する際の、人間に近い能力に対する品質と自然さの向上。 
[概要]人間の発話の新しい動的な動的変化と適応性を提案します。私たちの結果は、このアプローチが複雑な発話の韻律と自然さを改善することを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-02">
        <br><font color="black">2019-12-02</font>
      </time>
    </span>
</section>
<!-- paper0: A study on more realistic room simulation for far-field keyword spotting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.SD/paper_11.html">
      <font color="black">A study on more realistic room simulation for far-field keyword spotting</font>
    </a>
  </h2>
  <font color="black">ソースコードはPyroomacousticsパッケージで利用可能になり、他の人がこれらのテクニックを自分の作業に組み込むことができます。クリーンでノイズの多い遠方界条件下での再録音のホールドアウトセットで、最大$ 35.8 \％$の相対的な改善を示します一般的に使用されている（単一吸収係数）画像ソース法よりも..アブレーション研究を通じて、ウェイクワードタスクを使用して、測定されたRIRのグラウンドトゥルースセットと比較して、これらの要因の影響を測定します。 
[要約]クリーンでノイズの多い遠方界条件の再記録のホールドアウトセットで、最大35ドルを示しました。一般的に使用されている部屋のインパルス応答（riyr）画像ソース方式よりも多くの改善を示しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br><font color="black">2020-06-04</font>
      </time>
    </span>
</section>
<!-- paper0: Reducing Spelling Inconsistencies in Code-Switching ASR using
  Contextualized CTC Loss -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.SD/paper_12.html">
      <font color="black">Reducing Spelling Inconsistencies in Code-Switching ASR using
  Contextualized CTC Loss</font>
    </a>
  </h2>
  <font color="black">既存のCTCベースのアプローチとは対照的に、コンテキストグラウンドトゥルースはモデルの推定パスから取得されるため、CCTC損失はフレームレベルの位置合わせを必要としません。文字のスペルの一貫性を促進するために、コンテキスト化された接続主義者の時間分類（CCTC）損失を提案します。より高速な推論を可能にするベースの非自己回帰ASR ..コードスイッチング（CS）は、自動音声認識（ASR）、特に文字ベースのモデルにとって依然として課題です。 
[要約]文字ベースのモデルからの結果は損失に苦しんでいます。言語にリンクされています-一貫性のないスペル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-16">
        <br><font color="black">2020-05-16</font>
      </time>
    </span>
</section>
<!-- paper0: Any-to-Many Voice Conversion with Location-Relative Sequence-to-Sequence
  Modeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.SD/paper_13.html">
      <font color="black">Any-to-Many Voice Conversion with Location-Relative Sequence-to-Sequence
  Modeling</font>
    </a>
  </h2>
  <font color="black">次に、マルチスピーカーの位置相対注意ベースのseq2seq合成モデルをトレーニングして、ボトルネックの特徴からスペクトルの特徴を再構築し、生成された音声の話者ID制御のために話者の表現を条件付けます。BNEは音声認識機能から取得され、スペクトルの特徴から話者に依存しない、密度の高い、豊かな言語表現を抽出するために利用されます。このペーパーでは、任意対多の位置相対、シーケンス対シーケンス（seq2seq）ベースの非並列音声変換アプローチを提案します。 
[ABSTRACT]提案されたアプローチは、ボトルネック特徴抽出器（bne）とseq2seqベースの合成モジュールを組み合わせたものです。提案されたアプローチは、任意のvcをサポートするように簡単に拡張でき、評価に従って高性能を実現します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-06">
        <br><font color="black">2020-09-06</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Robust Deep Neural Networks for Affect and Depression
  Recognition from Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.SD/paper_14.html">
      <font color="black">Towards Robust Deep Neural Networks for Affect and Depression
  Recognition from Speech</font>
    </a>
  </h2>
  <font color="black">私たちのモデルは、感情とうつ病の予測において非常に有望な結果を示しています。DeepEmoAudioNetは、オーディオ信号の時間周波数表現とその周波数スペクトルの視覚的表現から学習します。EmoAudioNetのコードはGitHubで公開されています：https：// github.com/AliceOTHMANI/EmoAudioNet 
[ABSTRACT]インテリジェントシステムは、初期段階でmdd診断を強化します。注目度の高いインテリジェントシステムは、診断を強化できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-01">
        <br><font color="black">2019-11-01</font>
      </time>
    </span>
</section>
<!-- paper0: Vertical-Horizontal Structured Attention for Generating Music with
  Chords -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.SD/paper_15.html">
      <font color="black">Vertical-Horizontal Structured Attention for Generating Music with
  Chords</font>
    </a>
  </h2>
  <font color="black">私たちのモデルは、時間に沿った時間的関係だけでなく、キー間の構造的関係もキャプチャします。実験結果は、コード内の音符のキャプチャにおいて、モデルがベースラインMusicVAEよりも優れたパフォーマンスを発揮することを示しています。私たちは後者に焦点を当てています。 
[概要]音楽の作成はジェネレータテキストとは異なります。モデルのパフォーマンスはベースラインのmusicvaeよりも優れています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: A Generalized Deep Learning Framework for Whole-Slide Image Segmentation
  and Analysis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.IV/paper_0.html">
      <font color="black">A Generalized Deep Learning Framework for Whole-Slide Image Segmentation
  and Analysis</font>
    </a>
  </h2>
  <font color="black">病変検出のタスクのCAMELYON16テストデータ（n = 139）では、達成されたFROCスコアは0.86であり、pNステージングのタスクのCAMELYON17テストデータ（n = 500）では、達成されたコーエンのカッパスコアは0.9090（3番目）でした。オープンリーダーボードで）。組織病理学組織分析のための深層学習ベースのフレームワークを提案します。腫瘍セグメンテーションのタスクのDigestPathテストデータ（n = 212）で、0.782のダイススコアが達成されました（チャレンジの4番目）。 。 
[概要]画像のサイズと組織病理学タスクのバリエーションにより、組織病理学画像分析の統合フレームワークを開発することが課題になります。この課題は、カメリオン（乳がん転移）、ダイジェストパス（結腸がん）、 paip（肝臓がん）データセット</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-01">
        <br><font color="black">2020-01-01</font>
      </time>
    </span>
</section>
<!-- paper0: All-Optical Information Processing Capacity of Diffractive Surfaces -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.IV/paper_1.html">
      <font color="black">All-Optical Information Processing Capacity of Diffractive Surfaces</font>
    </a>
  </h2>
  <font color="black">多数のトレーニング可能な表面で構成されるより深い回折ネットワークは、より大きな入力視野とより大きな出力視野の間の複素数値線形変換の高次元部分空間をカバーでき、次の点で深さの利点を示します。単一のトレーニング可能な回折面と比較した場合の、さまざまな画像分類タスクの統計的推論、学習、および一般化機能の比較。ここでは、全光学式を実行するようにトレーニングされた回折面によって形成されるコヒーレント光ネットワークの情報処理能力を分析します。与えられた入力視野と出力視野の間の計算タスク..入力視野と出力視野の間の複素数値変換をカバーする全光学解空間の次元が、次の数に線形に比例することを示します。入力フィールドと出力フィールドの範囲によって決定される限界までの、光ネットワーク内の回折面-オブビュー。 
[概要]これらの進歩により、光と物質の相互作用と回折を通じてコンピューターと機械学習のタスクを実行できるトレーニング可能な表面を設計する機会も開かれました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-25">
        <br><font color="black">2020-07-25</font>
      </time>
    </span>
</section>
<!-- paper0: Compensating for visibility artefacts in photoacoustic imaging with a
  deep learning approach providing prediction uncertainties -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.IV/paper_2.html">
      <font color="black">Compensating for visibility artefacts in photoacoustic imaging with a
  deep learning approach providing prediction uncertainties</font>
    </a>
  </h2>
  <font color="black">さらに、この作業はニューラルネットワーク予測の信頼性を定量化することを目的としています。最後に、実験データセットのサイズを大幅に制限するために、シミュレーションデータで転移学習を使用する可能性に取り組みます。サンプルの写真をグラウンドトゥルース画像として使用するトレーニングセットとテストセット。 
[ABSTRACT]これらの問題を処理するために深層学習アプローチが提案されています。シミュレーションとシミュレーションデータで実証されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-23">
        <br><font color="black">2020-06-23</font>
      </time>
    </span>
</section>
<!-- paper0: CVEGAN: A Perceptually-inspired GAN for Compressed Video Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.IV/paper_3.html">
      <font color="black">CVEGAN: A Perceptually-inspired GAN for Compressed Video Enhancement</font>
    </a>
  </h2>
  <font color="black">CVEGANジェネレーターは、新しいMul2Resブロック（複数レベルの残差学習ブランチを含む）、拡張残余非ローカルブロック（ERNB）、および拡張畳み込みブロック注意モジュール（ECBAM）の使用から恩恵を受けます。CVEGANは完全に統合されています。 MPEG HEVCビデオコーディングテストモデル（HM16.20）と実験結果は、両方のコーディングツールの既存の最先端アーキテクチャに比べて大幅なコーディングの向上（アンカーと比較して、PPで最大28％、SRAで最大38％）を示しています。複数のデータセットにまたがって..ERNBは、表現能力を向上させるためにディスクリミネーターにも採用されています。 
[概要] cveganジェネレーターは、新しいmul2resブロック、強化された残差非ローカルブロック（ernb）、および強化された畳み込みブロック注意モジュールの使用から恩恵を受けます。トレーニング戦略も、ビデオ圧縮アプリケーション用に特別に再設計されました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Transfer Learning for Automated Diagnosis of Skin Lesions from
  Photographs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.IV/paper_4.html">
      <font color="black">Deep Transfer Learning for Automated Diagnosis of Skin Lesions from
  Photographs</font>
    </a>
  </h2>
  <font color="black">EfficientNet、MnasNet、MobileNet、DenseNet、SqueezeNet、ShuffleNet、GoogleNet、ResNet、ResNeXt、VGG、および転送学習がある場合とない場合の単純なCNNを比較します。この目的のために、事前にトレーニングされたモデルパラメーターを活用してさまざまな転送学習アプローチを調査しました。 ImageNetでメラノーマ検出を微調整します。メラノーマは皮膚がんの最も一般的な形態ではありませんが、最も致命的です。 
[概要]モバイルネットワーク、efficiencynet（転送学習あり）は、受信者動作特性曲線（auroc）の下の領域で最高の平均パフォーマンスを達成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-06">
        <br><font color="black">2020-11-06</font>
      </time>
    </span>
</section>
<!-- paper0: Tuning-free Plug-and-Play Proximal Algorithm for Inverse Imaging
  Problems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.IV/paper_5.html">
      <font color="black">Tuning-free Plug-and-Play Proximal Algorithm for Inverse Imaging
  Problems</font>
    </a>
  </h2>
  <font color="black">これは、線形および非線形の例示的な逆イメージング問題の両方で一般的であり、特に、圧縮センシングMRIおよび位相回復で有望な結果を示します。私たちのアプローチの重要な部分は、パラメーターの自動検索のためのポリシーネットワークを開発することです。モデルフリーとモデルベースの深層強化学習を組み合わせて効果的に学習します。この作業では、ペナルティパラメータ、ノイズ除去強度、終了時間を含む内部パラメータを自動的に決定できる、チューニングフリーのPnP近位アルゴリズムを紹介します。 。 
[ABSTRACT] pnpは、特にディープラーニングベースのデノイザーの統合により、大きな数値的成功を収めました。イメージング条件とさまざまなシーンコンテンツの点で、高い不一致にわたって高品質の結果を取得する必要があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-22">
        <br><font color="black">2020-02-22</font>
      </time>
    </span>
</section>
<!-- paper0: 3D Grid-Attention Networks for Interpretable Age and Alzheimer's Disease
  Prediction from Structural MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.IV/paper_6.html">
      <font color="black">3D Grid-Attention Networks for Interpretable Age and Alzheimer's Disease
  Prediction from Structural MRI</font>
    </a>
  </h2>
  <font color="black">アルツハイマー病ニューロイメージングイニシアチブ（ADNI）の4つのフェーズからの4,561の3テスラT1強調MRIスキャンに基づく評価では、年齢とAD予測の顕著性マップは部分的に重複していましたが、低レベルの機能は高レベルの機能よりも重複していました。脳年齢予測ネットワークはまた、ADと健康な対照群を別の最先端の方法よりもよく区別しました。結果として得られる視覚分析は、臨床診断を予測するために重要な解釈可能な特徴パターンを区別できます。 
[概要] 3D畳み込みニューラルネットワーク上に構築し、オノノンの異なる層に2つの注意モジュールを追加しました。結果の視覚的分析により、診断の予測に重要な解釈可能な特徴パターンを区別できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Efficient image retrieval using multi neural hash codes and bloom
  filters -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.IV/paper_7.html">
      <font color="black">Efficient image retrieval using multi neural hash codes and bloom
  filters</font>
    </a>
  </h2>
  <font color="black">画像検索タスクにニューラルネットワークを使用する従来のアプローチでは、特徴抽出に上位層を使用する傾向があります。取得した特徴マップは、最初に上位層の画像を比較することにより、階層的な粗い方法から細かい方法で画像検索プロセスでさらに使用されます。この論文は、複数のニューラルハッシュコードを使用して画像を検索し、事前に誤検出を特定することでブルームフィルタを使用してクエリの数を制限するための、効率的で修正されたアプローチを提供することを目的としています。 
[概要]ローカルディープ畳み込みニューラルネットワークの使用は、特徴マップを作成するために下位層と上位層の両方の機能の能力を組み合わせます。これらは、pcaを使用して圧縮され、変更されたマルチkを使用したバイナリシーケンス後にブルームフィルターに供給されます。アプローチ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-06">
        <br><font color="black">2020-11-06</font>
      </time>
    </span>
</section>
<!-- paper0: Voxelwise principal component analysis of dynamic
  [S-methyl-11C]methionine PET data in glioma patients -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.IV/paper_8.html">
      <font color="black">Voxelwise principal component analysis of dynamic
  [S-methyl-11C]methionine PET data in glioma patients</font>
    </a>
  </h2>
  <font color="black">現実的な数値シミュレーションにより、ノイズに対する方法論の堅牢性を実証します。この作業は、以前のように、神経膠腫における
[S-メチル-11C]メチオニンPETデータの静的分析よりも動的分析の付加価値に関するさらに有望な結果を提供します。 O-（2- 
[18F]フルオロエチル）-L-チロシンについて実証..13人の神経膠腫患者の2番目のコホートで、結果のパラメトリックマップを標準の1組織および2組織コンパートメント薬物動態（PK）モデルによって提供されるものと比較します。 。 
[概要]これらの研究のほとんどは、事前定義されたボリューム内の平均時間活動曲線（tac）からの手作りのクオリプレミアまたはセミ定量的な動的特徴に基づいています。これらは、腫瘍の攻撃性を局所的に評価するのに役立つ可能性があります。これらのマップは、治療および分析用</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Deep learning models for gastric signet ring cell carcinoma
  classification in whole slide images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.IV/paper_9.html">
      <font color="black">Deep learning models for gastric signet ring cell carcinoma
  classification in whole slide images</font>
    </a>
  </h2>
  <font color="black">それぞれ約500枚の画像からなる4つの異なるテストセットでモデルを評価しました。主に細胞形態とびまん性浸潤性のために病理学者による検出がより困難になる傾向があり、進行した段階で検出されると予後が悪くなります。最良のモデルは、4つのテストセットすべてで少なくとも0.99のレシーバーオペレーター曲線（ROC）曲線下面積（AUC）を達成し、SRCCWSI分類の最高のベースラインパフォーマンスを設定しました。 
[要約]最良のモデルは、4つのテストセットすべてで少なくとも0.99の曲線（auc）の下の受信者操作曲線（roc）領域を達成しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Towards online monitoring and data-driven control: a study of
  segmentation algorithms for infrared images of the powder bed -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.IV/paper_10.html">
      <font color="black">Towards online monitoring and data-driven control: a study of
  segmentation algorithms for infrared images of the powder bed</font>
    </a>
  </h2>
  <font color="black">識別されたアルゴリズムは、選択的レーザー焼結および選択的レーザー溶融機に容易に適用して、上記の各制限に対処し、プロセス制御を大幅に改善できます。これらの制限に対処するために、各赤外線画像をセグメント化する30を超えるセグメンテーションアルゴリズムを研究します。前景と背景..ますます多くの選択的レーザー焼結および選択的レーザー溶融機が軸外赤外線カメラを使用して、オンライン監視およびデータ駆動型制御機能を改善しています。 
[概要]これらのカメラからの赤外線画像を処理するためのアルゴリズムがまだ深刻に不足しています。これには、レーザートラックのオンライン監視機能の欠如や、データ駆動型の方法での画像の不十分な前処理が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: SoftSeg: Advantages of soft versus binary training for image
  segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.IV/paper_11.html">
      <font color="black">SoftSeg: Advantages of soft versus binary training for image
  segmentation</font>
    </a>
  </h2>
  <font color="black">これは、（i）前処理とデータ拡張後の二値化なし、（ii）正規化されたReLU最終活性化層（シグモイドの代わり）、および（iii）回帰損失関数（従来のダイス損失の代わり）を使用することによって実現されます。 、単一の「ハード」ラベルを割り当てると、有害な近似が生じる可能性があります。開発されたトレーニングパイプラインは、既存の深層学習アーキテクチャのほとんどに簡単に組み込むことができます。 
[ABSTRACT] softsegは、組織のインターフェースで一貫したソフト予測を生成します。小さなオブジェクトの感度が向上します。softsegは問題の解決を目的としています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Convolutional Autoencoder for Blind Hyperspectral Image Unmixing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.IV/paper_12.html">
      <font color="black">Convolutional Autoencoder for Blind Hyperspectral Image Unmixing</font>
    </a>
  </h2>
  <font color="black">提案されたアーキテクチャは、畳み込み層とそれに続くオートエンコーダで構成されます。次に、これらの潜在的な特性から、デコーダはアーキテクチャの入力にあるモノクロ画像のロールアウト画像を再構築します。実際のハイパースペクトルデータの実験結果は、提案されたアルゴリズムが存在量推定で既存のアンミキシング方法よりも優れており、それぞれRMSEとSADをメトリックとして使用した端成分抽出の競合結果を生成すると結論付けています。 
[概要]ハイパースペクトル画像に対してブラインドアンミキシングを実行するための新しいアーキテクチャが提案されています。エンコーダは、畳み込み層を介して生成された特徴空間を潜在空間表現に変換します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: RSINet: Rotation-Scale Invariant Network for Online Visual Tracking -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.IV/paper_13.html">
      <font color="black">RSINet: Rotation-Scale Invariant Network for Online Visual Tracking</font>
    </a>
  </h2>
  <font color="black">さらに、シャムベースのトラッカーは、余分なバックグラウンドノイズを含み、移動するオブジェクトの回転とスケール変換を正確に推定できないため、追跡パフォーマンスを低下させる可能性がある、軸に沿った境界ボックスを生成することによって、追跡されるオブジェクトの新しい状態を推測します。さらに、追跡モデルは、時空間エネルギー制御の下で適応的に最適化および更新されます。これにより、モデルの安定性と信頼性、および高い追跡効率が保証されます。ほとんどのシャムネットワークベースのトラッカーは、モデルの更新なしで追跡プロセスを実行し、ターゲット固有の学習はできません。適応的に変化。 
[概要]私たちのrsinetトラッカーは、ターゲット-ディストラクタ識別ブランチと回転-スケール推定ブランチで構成されています。回転とスケールの知識は、マルチタスク学習方法によってエンドツーエンドで明示的に学習できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: FROST: Faster and more Robust One-shot Semi-supervised Training -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.IV/paper_14.html">
      <font color="black">FROST: Faster and more Robust One-shot Semi-supervised Training</font>
    </a>
  </h2>
  <font color="black">私たちの実験は、ラベルのないデータの構成が不明な場合に、FROSTがうまく機能することを示しています。つまり、ラベルのないデータに各クラスの数が等しくなく、どのトレーニングクラスにも属さない分布外の例が含まれている可能性がある場合です。このホワイトペーパーでは、次のようなワンショットの半教師あり学習方法を紹介します。最先端の方法よりも1桁速くトレーニングし、堅牢です。高性能、トレーニングの速度、ハイパーパラメータに対する感度の低さにより、FROSTはワンショット半教師ありトレーニングの最も実用的な方法になっています。 。 
[概要]半教師あり学習の新しい方法はトレーニングに時間がかかりますが、パフォーマンスはラベル付けされたデータの選択に敏感です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Self-Supervised Physics-Guided Deep Learning Reconstruction For
  High-Resolution 3D LGE CMR -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.IV/paper_15.html">
      <font color="black">Self-Supervised Physics-Guided Deep Learning Reconstruction For
  High-Resolution 3D LGE CMR</font>
    </a>
  </h2>
  <font color="black">この作業では、この自己教師あり学習アプローチを3Dイメージングに拡張し、3Dボリュームの小さなトレーニングデータベースサイズに関連する課題に取り組みます。最近、完全になくてもPG-DL技術のトレーニングを可能にする自己教師あり学習アプローチが提案されました。サンプリングされたデータ..PG-DLメソッドのトレーニングは、通常、完全にサンプリングされたデータを参照として必要とする教師ありの方法で実行されます。これは、3D LGECMRでは困難です。 
[概要] 3Dスキャンは、2Dイメージングと比較して、カバレッジと解像度が向上しています。これは、科学に基づく深層学習（pg-dl）アプローチによるものです。最近、トレーニングを可能にするために、自己教師あり学習アプローチが提案されました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: TJU-DHD: A Diverse High-Resolution Dataset for Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_0.html">
      <font color="black">TJU-DHD: A Diverse High-Resolution Dataset for Object Detection</font>
    </a>
  </h2>
  <font color="black">4つの異なる検出器（つまり、1ステージRetinaNet、アンカーフリーFCOS、2ステージFPN、およびカスケードR-CNN）を使用して、オブジェクト検出と歩行者検出に関する実験が行われます。さらに、人気のあるデータセット（たとえば、特定のシナリオから収集されたKITTIとCitypersons）は、画像とインスタンスの数、解像度、および多様性に制限があります。問題の解決を試みるために、多様な高解像度データセット（TJU-DHDと呼ばれる）を構築します。 
[概要]大規模、豊富、多様性、高解像度のデータセットは、より優れたオブジェクト検出方法を開発する上で重要な役割を果たします。これらの車両は、画像とインスタンスの数、解像度、オブジェクトの多様性に制限があります。データセット115、354の高解像度画像と709、330のラベル付きオブジェクトが含まれています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Profiles: Detecting Out-Distribution & Adversarial Samples
  in Pre-trained CNNs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_1.html">
      <font color="black">Adversarial Profiles: Detecting Out-Distribution & Adversarial Samples
  in Pre-trained CNNs</font>
    </a>
  </h2>
  <font color="black">この目的のために、1つの敵対的攻撃生成手法のみを使用して各クラスの敵対的プロファイルを作成します。MNISTデータセットを使用したこのアプローチの初期評価では、敵対的プロファイルベースの検出が少なくとも92のアウトディストリビューション例と59％の検出に効果的であることが示されています。敵対的例の例..畳み込みニューラルネットワーク（CNN）の精度は高いにもかかわらず、敵対的および外部配布の例に対して脆弱です。 
[概要]これらの提案には、cnnを再トレーニングする必要なしに、事前にトレーニングされたcnnに対して敵対者とアウトディストリビューションの例を検出する方法が含まれています。次に、作成された敵対者プロファイルを各入力に適用する事前トレーニングされたcnnの周りに検出器をラップします。出力を使用して、入力が正当であるかどうかを判断します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: More Informed Random Sample Consensus -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_2.html">
      <font color="black">More Informed Random Sample Consensus</font>
    </a>
  </h2>
  <font color="black">私たちの方法は、均一ベースライン法と比較してより良い結果を示します。提案された方法の仮説サンプリングステップでは、データは、データポイントがインライアセットにある可能性に基づいてデータをソートする、提案したソートアルゴリズムでソートされます。ただし、この均一なサンプリング戦略では、多くの問題に関するすべての情報を十分に活用しているわけではありません。 
[要約]提案された方法は、データポイントがインライアセットに含まれる可能性に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: ADCPNet: Adaptive Disparity Candidates Prediction Network for Efficient
  Real-Time Stereo Matching -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_3.html">
      <font color="black">ADCPNet: Adaptive Disparity Candidates Prediction Network for Efficient
  Real-Time Stereo Matching</font>
    </a>
  </h2>
  <font color="black">したがって、多様なオブジェクトのさまざまな補正要件を満たし、効率的な2段階フレームワークを設計するための動的オフセット予測モジュールを提案します。最近、粗い方法から細かい方法に基づくステレオネットワークにより、大規模なメモリの制約と速度の制限が大幅に緩和されました。スケールネットワークモデル..複数のデータセットとプラットフォームでの評価結果は、提案されたネットワークが、精度と速度の点で、特にモバイルデバイス向けの最先端の軽量モデルよりも優れていることを明確に示しています。 
[ABSTRACT]視差予測は、視差モデルを支援するために使用できます。複数のモデルは、視差に基づいています-独立した畳み込み</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: End-to-End Object Detection with Adaptive Clustering Transformer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_4.html">
      <font color="black">End-to-End Object Detection with Adaptive Clustering Transformer</font>
    </a>
  </h2>
  <font color="black">ACTは、トレーニングなしで元の自己注意モジュールを置き換えるドロップインモジュールにすることができます。Transformerを使用したエンドツーエンドのオブジェクト検出（DETR）は、Transformerを使用してオブジェクト検出を実行し、次のような2段階のオブジェクト検出で同等のパフォーマンスを実現することを提案します。 Faster-RCNN .. ACTは、局所性鋭敏型ハッシュ（LSH）を使用してクエリ機能を適応的にクラスター化し、プロトタイプとキーの相互作用を使用してクエリとキーの相互作用を近似します。 
[ABSTRACT] actは、トレーニングなしで元のセルフアテンションモジュールを置き換えるアルゴリズムのドロップインです。コードは、実験の複製と検証を容易にするための補足として利用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: EfficientPose: An efficient, accurate and scalable end-to-end 6D multi
  object pose estimation approach -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_5.html">
      <font color="black">EfficientPose: An efficient, accurate and scalable end-to-end 6D multi
  object pose estimation approach</font>
    </a>
  </h2>
  <font color="black">これらのアプローチは、最初に2Dターゲットを検出することを目的としています。複数のオブジェクトとインスタンスの固有の処理、融合されたシングルショット2Dオブジェクト検出、および6Dポーズ推定により、私たちのアプローチは、26 FPSを超える複数のオブジェクト（8）でエンドツーエンドで実行され、多くの人にとって非常に魅力的です。実世界のシナリオ..また、6D拡張と呼ばれる、パフォーマンスと一般化を改善するための直接6Dポーズ推定アプローチのための新しい拡張方法を提案します。 
[概要]私たちの方法は、非常に正確で、効率的で、幅広いコンピューティングリソースにわたってスケーラブルです。他のアプローチが苦しんでいる複数のオブジェクトを処理するときのランタイムの大幅な増加を排除します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-09">
        <br><font color="black">2020-11-09</font>
      </time>
    </span>
</section>
<!-- paper0: gradSLAM: Dense SLAM meets Automatic Differentiation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_6.html">
      <font color="black">gradSLAM: Dense SLAM meets Automatic Differentiation</font>
    </a>
  </h2>
  <font color="black">精度を犠牲にすることなく、微分可能な信頼領域オプティマイザ、表面測定と融合スキーム、およびレイキャスティングを提案します。ただし、典型的な高密度SLAMシステムのいくつかのコンポーネントは微分不可能です。この作業では、ポーズをとるための方法論であるgradSLAMを提案します。微分可能な計算グラフとしてのSLAMシステム。これは、勾配ベースの学習とSLAMを統合します。 
[ABSTRACT] slamは、生のセンサーをロボットベースの状態（s）全体の分布に変換する操作です。これにより、異なるシステムから学習できますが、一般的な高密度プロジェクトのいくつかのコンポーネントは区別できません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-23">
        <br><font color="black">2019-10-23</font>
      </time>
    </span>
</section>
<!-- paper0: Res-GCNN: A Lightweight Residual Graph Convolutional Neural Networks for
  Human Trajectory Forecasting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_7.html">
      <font color="black">Res-GCNN: A Lightweight Residual Graph Convolutional Neural Networks for
  Human Trajectory Forecasting</font>
    </a>
  </h2>
  <font color="black">他の方法と比較して、提案された方法は、予測精度と時間効率を考慮したオンボードアプリケーションの強力な可能性を示しています。コードはGitHubで公開されます。提案されたRes-GCNNは非常に軽量で、パラメータは約6.4キロです。パラメータサイズの点で他のすべての方法を上回っています。私たちの実験結果は、0.65メートルに達する最終変位誤差（FDE）で、最先端技術を13.3％上回っていることを示しています。 
[概要]提案された方法は、交通交通交通エージェントを予測するために使用されます。それは、残差グラフ畳み込みニューラルネットワーク（res-gcnn）によって開発されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: A Generalized Deep Learning Framework for Whole-Slide Image Segmentation
  and Analysis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_8.html">
      <font color="black">A Generalized Deep Learning Framework for Whole-Slide Image Segmentation
  and Analysis</font>
    </a>
  </h2>
  <font color="black">CAMELYON（乳がん転移）、DigestPath（結腸がん）、PAIP（肝臓がん）データセットを含むいくつかのオープンソースデータセットで、トレーニングと推論を含むフレームワークの一般化可能性を示します。CAMELYON16テストデータ（n = 139）病変検出のタスクでは、達成されたFROCスコアは0.86であり、pNステージングのタスクで達成されたCAMELYON17テストデータ（n = 500）では、達成されたコーエンのカッパスコアは0.9090（オープンリーダーボードで3番目）でした。 。組織病理学組織分析のための深い学習ベースのフレームワークを提案します。 
[概要]画像のサイズと組織病理学タスクのバリエーションにより、組織病理学画像分析の統合フレームワークを開発することが課題になります。この課題は、カメリオン（乳がん転移）、ダイジェストパス（結腸がん）、 paip（肝臓がん）データセット</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-01">
        <br><font color="black">2020-01-01</font>
      </time>
    </span>
</section>
<!-- paper0: DASZL: Dynamic Action Signatures for Zero-shot Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_9.html">
      <font color="black">DASZL: Dynamic Action Signatures for Zero-shot Learning</font>
    </a>
  </h2>
  <font color="black">オリンピックスポーツとUCF101のデータセットでこの方法を評価します。ここで、モデルは複数の実験パラダイムの下で新しい最先端技術を確立します。最後に、既製のオブジェクト検出器を使用して、完全に非アクティブなアクティビティを認識できることを示します。追加のトレーニングなしのnovo設定..この論文では、動的アクションシグネチャの構成としてアクティビティをモデル化するきめ細かい認識へのアプローチを提示します。 
[概要]現在、一連のアクティビティを監視する作業はありません。これらには、ゼロショットアクティビティ認識が含まれます。この場合、検出器は、単純な第一原理ステートマシンから「オンザフライ」で構成されます。また、このメソッドを拡張して、独自の方法を形成します。ビデオ内のアクティビティのゼロショットジョイントセグメンテーションと分類のためのフレームワーク</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-08">
        <br><font color="black">2019-12-08</font>
      </time>
    </span>
</section>
<!-- paper0: All-Optical Information Processing Capacity of Diffractive Surfaces -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_10.html">
      <font color="black">All-Optical Information Processing Capacity of Diffractive Surfaces</font>
    </a>
  </h2>
  <font color="black">ここでは、与えられた入力視野と出力視野の間で全光学計算タスクを実行するように訓練された回折面によって形成されたコヒーレント光ネットワークの情報処理能力を分析します。全光学の次元性を示します。入力視野と出力視野の間の複素数値変換をカバーするソリューション空間は、入力フィールドと出力フィールドの範囲によって決定される限界まで、光ネットワーク内の回折面の数に線形に比例します。 -ビュー..これらの分析と結論は、たとえば、プラズモニックおよび/または誘電体ベースのメタ表面や、全光学プロセッサの形成に使用できるフラット光学系など、さまざまな形態の回折表面に広く適用できます。 
[概要]これらの進歩により、光と物質の相互作用と回折を通じてコンピューターと機械学習のタスクを実行できるトレーニング可能な表面を設計する機会も開かれました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-25">
        <br><font color="black">2020-07-25</font>
      </time>
    </span>
</section>
<!-- paper0: Positive-Congruent Training: Towards Regression-Free Model Updates -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_11.html">
      <font color="black">Positive-Congruent Training: Towards Regression-Free Model Updates</font>
    </a>
  </h2>
  <font color="black">また、参照モデル自体を複数のディープニューラルネットワークのアンサンブルとして選択できる場合、新しいモデルの精度に影響を与えることなく、負の反転をさらに減らすことができることもわかりました。正の合同（PC）トレーニングは、エラー率を減らすことを目的としています。同時に、負の反転を減らし、モデルの蒸留とは異なり、正の予測でのみ参照モデルとの一致を最大化します。AIシステムのさまざまなバージョンの動作の不整合を減らすことは、全体的なエラーを減らすことと同じくらい重要です。 
[概要]画像分類では、サンプル抵抗は「ネガティブフリップ」として表示されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Dense Contrastive Learning for Self-Supervised Visual Pre-Training -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_12.html">
      <font color="black">Dense Contrastive Learning for Self-Supervised Visual Pre-Training</font>
    </a>
  </h2>
  <font color="black">ベースラインメソッドMoCo-v2と比較して、このメソッドはごくわずかな計算オーバーヘッドを導入しますが（1％未満遅い）、オブジェクト検出、セマンティックセグメンテーション、インスタンスセグメンテーションなどのダウンストリームの高密度予測タスクに転送するときに一貫して優れたパフォーマンスを示します。最先端の方法を大幅に上回っています。具体的には、強力なMoCo-v2ベースラインを超えて、PASCAL VOCオブジェクト検出で2.0％AP、COCOオブジェクト検出で1.1％APの大幅な改善を達成しています。 COCOインスタンスセグメンテーションで0.9％AP、PASCAL VOCセマンティックセグメンテーションで3.0％mIoU、Cityscapesセマンティックセグメンテーションで1.8％mIoU ..コードはhttps://git.io/AdelaiDet 
[ABSTRACT]で入手できます。入力画像の2つのビュー間のピクセルレベルでのペアワイズコントラスト（dis）類似性損失</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Your "Labrador" is My "Dog": Fine-Grained, or Not -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_13.html">
      <font color="black">Your "Labrador" is My "Dog": Fine-Grained, or Not</font>
    </a>
  </h2>
  <font color="black">その単純さのおかげで、私たちの方法は既存のFGVCフレームワークの上に簡単に実装でき、パラメーターはありません。そのために、単一ラベル分類からトップダウンのFGVCの従来の設定を再考します。事前定義された粗いラベルから細かいラベルへの階層のトラバーサル-答えが「犬」-&gt;「銃犬」-&gt;「レトリーバー」-&gt;「ラブラドール」になるように..実験は、私たちの方法が達成することを示しています新しいFGVC設定で優れたパフォーマンスを発揮し、従来のシングルラベルFGVC問題でも最先端のパフォーマンスよりも優れたパフォーマンスを発揮します。 
[概要]きめ細かい視覚的分類（fgvc）は前者に到達するように努めていますが、専門家ではない私たちの大多数はおそらく「犬」で十分でしょう。標準標準ラベル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: CG-ATTACK: Modeling the Conditional Distribution of Adversarial
  Perturbations to Boost Black-Box Attack -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_14.html">
      <font color="black">CG-ATTACK: Modeling the Conditional Distribution of Adversarial
  Perturbations to Boost Black-Box Attack</font>
    </a>
  </h2>
  <font color="black">本研究では、サロゲートモデルで上記の効率的なトレーニング方法で事前トレーニングされたc-Glowモデルに基づく新しい転送メカニズムを設計することにより、新しいスコアベースのブラックボックス敵対攻撃方法を提案し、敵対的転送可能性とターゲットモデルへのクエリ..ディープニューラルネットワーク（DNN）に対する敵対的な例は、近年広く開発されています。この目的のために、条件付き生成フローモデル（c -グロー）、複雑なデータ分布をキャプチャする強力な機能を示します。 
[概要]これは、敵対的な摂動を生み出す上で重要な役割を果たす可能性があります。しかし、提案された方法は、攻撃の成功率とターゲットの効率の両方で優れています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-15">
        <br><font color="black">2020-06-15</font>
      </time>
    </span>
</section>
<!-- paper0: Visual Forecasting of Time Series with Image-to-Image Regression -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_15.html">
      <font color="black">Visual Forecasting of Time Series with Image-to-Image Regression</font>
    </a>
  </h2>
  <font color="black">それでも、開業医は、チャートやプロットなどの視覚化に依存して予測を推論することがよくあります。視覚的予測アプローチの成功は、連続的な数値回帰問題を連続的なターゲット信号の量子化によって離散領域に変換するという事実に起因します。ピクセル空間に..私たちの実験は、視覚的予測は周期的なデータには効果的ですが、株価などの不規則なデータにはやや少ないことを示しています。 
[ABSTRACT]既存のモデルは、将来の値を予測するために古典的な統計手法に依存しています。ただし、人間が直感的に行うのと同様に、視覚的な予測を作成することがよくあります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Keep your Eyes on the Lane: Real-time Attention-guided Lane Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_16.html">
      <font color="black">Keep your Eyes on the Lane: Real-time Attention-guided Lane Detection</font>
    </a>
  </h2>
  <font color="black">さらに、アブレーション研究は、実際に役立つ効率のトレードオフオプションに関する議論とともに実行されます。モデルは、文献で最も広く使用されている3つのデータセットで広範囲に評価されました。したがって、この作業は新しいアンカーを提案します。グローバル情報を集約するベースの注意メカニズム。 
[概要]この作業では、特徴プーリングステップにアンカーを使用するlaneatt：アンカーベースのディープレーン検出モデルを提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-22">
        <br><font color="black">2020-10-22</font>
      </time>
    </span>
</section>
<!-- paper0: DeepNAG: Deep Non-Adversarial Gesture Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_17.html">
      <font color="black">DeepNAG: Deep Non-Adversarial Gesture Generation</font>
    </a>
  </h2>
  <font color="black">最初に、DeepGANと呼ばれるジェスチャ合成用の新しいデバイスに依存しないGANモデルについて説明します。この作業では両方の問題に取り組みます。評価を通じて、DeepGANとDeepNAGの有用性を、データ拡張を使用して5つの認識機能をトレーニングする2つの代替手法と比較します。 6つのデータセット。 
[ABSTRACT] deepganのジェネレーターは、優れた画像データ拡張パフォーマンスを示していますが、ジェスチャ合成におけるそれらの適合性は十分な注目を集めていません。AmazonMechanicalTurk Studyを介して、生成されたサンプルの知覚品質をさらに調査します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Weakly Supervised Instance Segmentation by Deep Community Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_18.html">
      <font color="black">Weakly Supervised Instance Segmentation by Deep Community Learning</font>
    </a>
  </h2>
  <font color="black">この問題に対処するために、境界ボックス回帰、インスタンスマスク生成、インスタンスセグメンテーション、および特徴抽出を備えたオブジェクト検出の正のフィードバックループを備えた統合ディープニューラルネットワークアーキテクチャを設計します。アルゴリズムの実装は、プロジェクトのWebページで入手できます。 ：https：//cv.snu.ac.kr/research/WSIS_CL ..提案されたアルゴリズムは、高速R-CNNやマスクR-CNNなどの追加のトレーニングなしで、弱く監視された設定で最先端のパフォーマンスを実現します。標準のベンチマークデータセット。 
[概要]このタスクは、監視が不十分なオブジェクト検出の組み合わせに基づいています。同じクラスのすべての個々のオブジェクトが識別され、個別にセグメント化されるように設計されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-01-30">
        <br><font color="black">2020-01-30</font>
      </time>
    </span>
</section>
<!-- paper0: Effectiveness of Arbitrary Transfer Sets for Data-free Knowledge
  Distillation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_19.html">
      <font color="black">Effectiveness of Arbitrary Transfer Sets for Data-free Knowledge
  Distillation</font>
    </a>
  </h2>
  <font color="black">ただし、これらのアプローチはどちらも複雑な最適化（GANトレーニングまたは1つのサンプルを合成するためのいくつかのバックプロパゲーションステップ）を伴い、多くの場合計算コストが高くなります。MNIST、FMNIST、CIFAR-10、CIFAR-100などの複数のベンチマークデータセットでの広範な実験を通じて、このデータセットが「ターゲットクラスバランス」である場合に、任意のデータを使用して知識蒸留を実行することの驚くべき有効性を検証します。この論文では、簡単な代替手段として、公開されているランダムノイズなどの「任意の転送セット」の有効性を調査します。合成データセットと自然データセット。これらはすべて、視覚的または意味的な内容の点で、元のトレーニングデータセットとはまったく関係がありません。 
[ABSTRACT]教師モデルのトレーニングに使用されるデータセットは「転送セット」として選択されます。ただし、既存のアプローチは、元のトレーニングデータセットを表す合成セットを繰り返し構成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Viewpoint-aware Progressive Clustering for Unsupervised Vehicle
  Re-identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_20.html">
      <font color="black">Viewpoint-aware Progressive Clustering for Unsupervised Vehicle
  Re-identification</font>
    </a>
  </h2>
  <font color="black">この問題を処理するために、教師なし車両Re-IDの新しい視点認識クラスタリングアルゴリズムを提案します。ただし、車両画像はさまざまな視点で大きな外観の変化を示すため、これらの方法を車両Re-IDに直接一般化することはできません。最近、教師なし。個人のRe-IDメソッドは、ドメイン適応またはクラスタリングベースの手法を検討することで印象的なパフォーマンスを実現します。 
[概要]現在、ほとんどの既存の方法は、監視された方法で車両のre-idタスクを処理します。これは、時間と労力の両方を消費します。しかし、これらの方法を直接一般化することができます。たとえば、車両の画像は、さまざまな視点で大きな外観の変化を示します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: CVEGAN: A Perceptually-inspired GAN for Compressed Video Enhancement -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_21.html">
      <font color="black">CVEGAN: A Perceptually-inspired GAN for Compressed Video Enhancement</font>
    </a>
  </h2>
  <font color="black">CVEGANはMPEGHEVCビデオコーディングテストモデル（HM16.20）に完全に統合されており、実験結果は、既存の状態に比べて大幅なコーディングの向上（PPで最大28％、SRAで最大38％）を示しています。複数のデータセットにわたる両方のコーディングツールのアートアーキテクチャ..提案されたネットワークは、後処理（PP）と空間解像度適応（SRA）の2つの典型的なビデオ圧縮拡張ツールのコンテキストで完全に評価されています。ERNBも採用されています。表現能力を向上させるために弁別器で。 
[概要] cveganジェネレーターは、新しいmul2resブロック、強化された残差非ローカルブロック（ernb）、および強化された畳み込みブロック注意モジュールの使用から恩恵を受けます。トレーニング戦略も、ビデオ圧縮アプリケーション用に特別に再設計されました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: MUST-GAN: Multi-level Statistics Transfer for Self-driven Person Image
  Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_22.html">
      <font color="black">MUST-GAN: Multi-level Statistics Transfer for Self-driven Person Image
  Generation</font>
    </a>
  </h2>
  <font color="black">DeepFashionデータセットの実験結果は、最先端の教師ありおよび教師なし方法と比較した場合の方法の優位性を示しています。次に、外観とポーズを再融合するために、それらをポーズガイドジェネレータに転送します。ポーズガイド人物の画像生成には通常、ペアのソースターゲット画像を使用してトレーニングを監視することが含まれます。これにより、データ準備の労力が大幅に増加し、モデルの適用が制限されます。 
[ABSTRACT]モデルは、人物画像から外観の特徴を解きほぐして転送し、それらをポーズの特徴とマージして、ソースの人物画像自体を再構築します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Transfer Learning for Automated Diagnosis of Skin Lesions from
  Photographs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_23.html">
      <font color="black">Deep Transfer Learning for Automated Diagnosis of Skin Lesions from
  Photographs</font>
    </a>
  </h2>
  <font color="black">EfficientNet、MnasNet、MobileNet、DenseNet、SqueezeNet、ShuffleNet、GoogleNet、ResNet、ResNeXt、VGG、および転送学習の有無にかかわらず単純なCNNを比較します。世界の遠隔地で、財政的制約のため、または2020年にCOVID-19キャンセル..この目的のために、メラノーマ検出の微調整を伴うImageNetで事前にトレーニングされたモデルパラメータを活用することにより、さまざまな転移学習アプローチを調査しました。 
[概要]モバイルネットワーク、efficiencynet（転送学習あり）は、受信者動作特性曲線（auroc）の下の領域で最高の平均パフォーマンスを達成します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-06">
        <br><font color="black">2020-11-06</font>
      </time>
    </span>
</section>
<!-- paper0: Removing the Background by Adding the Background: Towards Background
  Robust Self-supervised Video Representation Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_24.html">
      <font color="black">Removing the Background by Adding the Background: Towards Background
  Robust Self-supervised Video Representation Learning</font>
    </a>
  </h2>
  <font color="black">この方法を\ emph {背景消去}（BE）と呼びます。背景へのモデルの依存を軽減するために、背景を追加して背景の影響を取り除くことを提案します。次に、モデルに気を散らす特徴を強制的に引き出します。ビデオと元のビデオの機能が近くなるため、モデルは背景の影響に抵抗するように明示的に制限され、動きの変化に焦点を当てます。 
[概要]ビデオの方法を「背景消去」（be）と呼びます。これは、モーションではなくビデオの背景に基づいています。これにより、モデルが背景の変更に対して脆弱になります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-12">
        <br><font color="black">2020-09-12</font>
      </time>
    </span>
</section>
<!-- paper0: The Criminality From Face Illusion -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_25.html">
      <font color="black">The Criminality From Face Illusion</font>
    </a>
  </h2>
  <font color="black">顔からの犯罪性の予測は、最初は他の顔の分析と似ているように見えるかもしれませんが、顔からの犯罪性アルゴリズムを作成する試みは必然的に失敗する運命にあり、最近の出版物で明らかに有望な実験結果は不十分な実験計画から生じる幻想であると主張します。顔の錯覚による犯罪性を信じるには、潜在的に大きな社会的コストがかかる可能性があります。顔の画像の自動分析により、人の性別、年齢、人種、顔の表情、体重指数、およびその他のさまざまな指数と状態に関する予測を生成できます。 ..最近のいくつかの出版物は、犯罪者/非犯罪者としての人物のステータスを予測するために、人物の顔の画像を分析することに成功したと主張しています。 
[要約]最近のいくつかの出版物は、犯罪者または非犯罪者としての人物のステータスを予測するために、人物の顔の画像を分析することに成功したと主張しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-06">
        <br><font color="black">2020-06-06</font>
      </time>
    </span>
</section>
<!-- paper0: Continuous Emotion Recognition with Spatiotemporal Convolutional Neural
  Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_26.html">
      <font color="black">Continuous Emotion Recognition with Spatiotemporal Convolutional Neural
  Networks</font>
    </a>
  </h2>
  <font color="black">FERシステムと感情の次元表現に3D-CNNを使用した研究はほとんどないため、ビデオに不可欠な伝達学習を操作するために、事前にトレーニングされた2D-CNNモデルの重みの膨張を可能にする膨張3D-CNNアーキテクチャを提案します。ベースのアプリケーション..RAF-DBおよびSEWA-DBデータセットの実験結果は、これらの微調整されたアーキテクチャにより、生のピクセル画像から時空間情報を効果的にエンコードでき、現在の状態よりもはるかに優れた結果を達成できることを示しています。 -art ..ただし、実際にキャプチャされたビデオシーケンスや、次元モデルなどのより複雑な感情表現を使用すると、ディープFERシステムにはより識別力のある特徴表現を学習する機能があります。 
[概要]ディープファーシステムには、より多くの神経機能を学習する機能があります。これらのビデオが3d --cnnで使用されるのはこれが初めてです。これにより、事前にトレーニングされた2d --cnnモデルの重量膨張が可能になります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Tuning-free Plug-and-Play Proximal Algorithm for Inverse Imaging
  Problems -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_27.html">
      <font color="black">Tuning-free Plug-and-Play Proximal Algorithm for Inverse Imaging
  Problems</font>
    </a>
  </h2>
  <font color="black">この作業では、ペナルティパラメータ、ノイズ除去強度、終了時間を含む内部パラメータを自動的に決定できる、チューニングフリーのPnP近位アルゴリズムを紹介します。これは、線形および非線形の両方の例示的な逆イメージング問題で一般的です。特に、圧縮センシングMRIと位相回復で有望な結果を示します。私たちのアプローチの重要な部分は、パラメータの自動検索のためのポリシーネットワークを開発することです。これは、モデルフリーとモデルベースの混合深部強化学習によって効果的に学習できます。 。 
[ABSTRACT] pnpは、特にディープラーニングベースのデノイザーの統合により、大きな数値的成功を収めました。イメージング条件とさまざまなシーンコンテンツの点で、高い不一致にわたって高品質の結果を取得する必要があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-22">
        <br><font color="black">2020-02-22</font>
      </time>
    </span>
</section>
<!-- paper0: UP-DETR: Unsupervised Pre-training for Object Detection with
  Transformers -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_28.html">
      <font color="black">UP-DETR: Unsupervised Pre-training for Object Detection with
  Transformers</font>
    </a>
  </h2>
  <font color="black">私たちの実験では、UP-DETRは、PASCAL VOCおよびCOCOデータセットでより高速な収束とより高い精度でDETRのパフォーマンスを大幅に向上させます。コードはまもなく利用可能になります。事前トレーニング中に、2つの重要な問題に対処します。マルチタスク学習とマルチクエリローカリゼーション。 
[ABSTRACT]オブジェクト検出のためにdetr（up-detr）を事前トレーニングする口実タスクを提案します。cnnバックボーンをフリーズし、パッチ検出と共同で最適化されたパッチ機能再構築ブランチを提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Crop Height and Plot Estimation for Phenotyping from Unmanned Aerial
  Vehicles using 3D LiDAR -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_29.html">
      <font color="black">Crop Height and Plot Estimation for Phenotyping from Unmanned Aerial
  Vehicles using 3D LiDAR</font>
    </a>
  </h2>
  <font color="black">私たちのアルゴリズムは、二乗平均平方根誤差（RMSE）が6.1 cmの112プロットで、畑の草丈を推定することができました。無人航空機（UAV）..このツールは、現実的な3Dプラントおよび地形モデルを使用してランダム化されたファームを作成します。 
[概要]表現型ファームの作成に使用できるツールチェーンを紹介します。これは、3D植物表現型の最初のデータセットです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-30">
        <br><font color="black">2019-10-30</font>
      </time>
    </span>
</section>
<!-- paper0: RGBT Salient Object Detection: A Large-scale Dataset and Benchmark -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_30.html">
      <font color="black">RGBT Salient Object Detection: A Large-scale Dataset and Benchmark</font>
    </a>
  </h2>
  <font color="black">VT5000には、アルゴリズムの堅牢性を調査するために、さまざまなシーンと環境で収集された11の課題があります。この作業は、グラウンドトゥルースアノテーションを備えた5000の空間的に整列されたRGBT画像ペアを含むVT5000という名前のRGBT画像データセットに貢献します。 VT5000データセットおよび他の2つのパブリックデータセットの最先端の方法よりも優れています。 
[概要]ほとんどの作品は、実際のアプリケーションのパフォーマンスを制限するrgbベースのデータセットに焦点を当てています。新しい研究の利点は、大規模なデータセットと包括的なベンチマークの欠如によって制限されています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-07">
        <br><font color="black">2020-07-07</font>
      </time>
    </span>
</section>
<!-- paper0: A Hierarchical Multi-Modal Encoder for Moment Localization in Video
  Corpus -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_31.html">
      <font color="black">A Hierarchical Multi-Modal Encoder for Moment Localization in Video
  Corpus</font>
    </a>
  </h2>
  <font color="black">一般的な検索システムは、ビデオ全体または事前定義されたビデオセグメントのいずれかでクエリに応答しますが、すべての可能なセグメントを徹底的に検索することが困難な、トリミングされていないセグメント化されていないビデオで未定義のセグメントをローカライズすることは困難です。ビデオの表現は、時間領域のさまざまなレベルの粒度を説明する必要があります。この問題に対処するために、粗粒度のクリップレベルと細粒度の両方でビデオをエンコードするHierArchical Multi-Modal EncodeR（HAMMER）を提案します。複数のサブタスク、つまり、ビデオ検索、セグメントの時間的ローカリゼーション、およびマスクされた言語モデリングに基づいて、さまざまなスケールで情報を抽出するためのフレームレベル。 
[概要] gran granは、グラニュラービデオエンコーダー（ハンマー）の発案によるものです。これは、細粒度のクリップレベルと細粒度のフレームレベルの両方でビデオをエンコードします。これは、ビデオの取得とその後の検索を含む複数のサブタスクに基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Self-Gradient Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_32.html">
      <font color="black">Self-Gradient Networks</font>
    </a>
  </h2>
  <font color="black">ディープニューラルネットワークの敵対的な脆弱性の問題が発見されて以来、敵対的な防御メカニズムが提案されてきましたが、この問題を完全に理解して対処するには長い道のりがあります。この仮説に動機付けられて、ディープニューラルネットワークアーキテクチャが明示的にタップできる場合は、トレーニング中に独自の勾配フローに変換することで、防御能力を大幅に向上させることができます。勾配フロー情報は、自己勾配ネットワーク内で活用され、標準のトレーニングプロセスで達成できる以上の摂動安定性を実現します。 
[ABSTRACT]ディープニューラルネットワークアーキテクチャは、敵対的な摂動に対してより堅牢になるように設計されています。自己膨潤ネットワークの概念は、ディープニューラルネットワークの敵対的な脆弱性の問題の発見に触発されました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Efficient image retrieval using multi neural hash codes and bloom
  filters -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_33.html">
      <font color="black">Efficient image retrieval using multi neural hash codes and bloom
  filters</font>
    </a>
  </h2>
  <font color="black">得られた特徴マップは、最初に上位層の画像を意味的に類似した画像と比較し、次に構造的類似性を探して徐々に下位層に向かって移動することにより、階層的な粗い方法から細かい方法で画像検索プロセスでさらに使用されます。複数のニューラルハッシュコードを使用した画像検索の効率的で修正されたアプローチを提供し、事前に誤検知を特定することでブルームフィルタを使用したクエリの数を制限することを目的としています。画像検索タスクにニューラルネットワークを使用する従来のアプローチでは、特徴抽出に上位層を使用する傾向があります。 
[概要]ローカルディープ畳み込みニューラルネットワークの使用は、特徴マップを作成するために下位層と上位層の両方の機能の能力を組み合わせます。これらは、pcaを使用して圧縮され、変更されたマルチkを使用したバイナリシーケンス後にブルームフィルターに供給されます。アプローチ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-06">
        <br><font color="black">2020-11-06</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Positional and Relational Feature Learning for Rotation-Invariant
  Point Cloud Analysis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_34.html">
      <font color="black">Deep Positional and Relational Feature Learning for Rotation-Invariant
  Point Cloud Analysis</font>
    </a>
  </h2>
  <font color="black">ネットワークの入力としてのポイントの距離や角度など、一部の幾何学的フィーチャは回転不変ですが、ポイントの位置情報を失います。ネットワークは階層的であり、位置フィーチャ埋め込みブロックとリレーショナルフィーチャ埋め込みブロックの2つのモジュールに依存しています。 。入力としてポイントクラウドを処理する場合、モジュールとネットワーク全体の両方が回転不変であることが証明されています。 
[ABSTRACT]ポイントベースのディープネットワークは、通常、ポイント座標に基づいて大まかに整列された3D形状を認識するように設計されていますが、形状の回転によってパフォーマンスが低下します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: A Divide et Impera Approach for 3D Shape Reconstruction from Multiple
  Views -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_35.html">
      <font color="black">A Divide et Impera Approach for 3D Shape Reconstruction from Multiple
  Views</font>
    </a>
  </h2>
  <font color="black">私たちのアプローチは3つのステップに分かれています。単一または複数の画像からオブジェクトの3D形状を推定することは、ディープラーニングによる最近のブレークスルーのおかげで人気が高まっています。オブジェクトのまばらなビューから始めて、最初にそれらを次のように位置合わせします。すべてのペア間の相対ポーズを推定することによる共通の座標系。 
[要約]この論文は、与えられたビューからの可視情報をマージすることにより、視点バリアントの再構築に依存することを提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Liquid Warping GAN with Attention: A Unified Framework for Human Image
  Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_36.html">
      <font color="black">Liquid Warping GAN with Attention: A Unified Framework for Human Image
  Synthesis</font>
    </a>
  </h2>
  <font color="black">すべてのコードとデータセットはhttps://impersonator.org/work/impersonator-plus-plus.htmlで入手できます。また、人間の動きの模倣、外観の転送、および外観の転送を評価するために、新しいデータセット、つまりiPERデータセットを構築します。新しいビュー合成..これは、トレーニングが完了すると、モデルを使用してこれらすべてのタスクを処理できることを意味します。 
[ABSTRACT]モデルは、一度トレーニングされると、これらすべてのタスクを処理するために使用できます。ただし、これらは位置情報のみを表現し、人のパーソナライズされた形状を特徴付けたり、手足の回転をモデル化したりすることはできません。目に見えないソース画像、1つまたはいくつか-ショットの敵対的学習が適用されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Optimized Loss Functions for Object detection: A Case Study on Nighttime
  Vehicle Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_37.html">
      <font color="black">Optimized Loss Functions for Object detection: A Case Study on Nighttime
  Vehicle Detection</font>
    </a>
  </h2>
  <font color="black">この論文では、分類とローカリゼーションのために2つの損失関数の両方を同時に最適化します。最後に、夜間の車両検出のための十分な実験が2つのデータセットで行われました。さらに、予測間のマハラノビス距離を組み込むことにより、MIoUという名前の新しいローカリゼーション損失が提案されます。ボックスとターゲットボックス。DIoU損失の勾配の不一致の問題を排除し、ローカリゼーションの精度をさらに向上させます。 
[概要]この論文では、分類とローカリゼーションの両方の損失関数を同時に最適化します。この方法は、陽性サンプルのローカリゼーション精度を向上させるために使用されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-11">
        <br><font color="black">2020-11-11</font>
      </time>
    </span>
</section>
<!-- paper0: 3D-FRONT: 3D Furnished Rooms with layOuts and semaNTics -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_38.html">
      <font color="black">3D-FRONT: 3D Furnished Rooms with layOuts and semaNTics</font>
    </a>
  </h2>
  <font color="black">3D-FRONT（レイアウトとセマンティクスを備えた3D家具付きの部屋）、プロがデザインしたレイアウトによって強調された合成屋内シーンの新しい大規模で包括的なリポジトリ、およびスタイルのある高品質のテクスチャ3Dモデルが配置された多数の部屋を紹介します互換性..フロアプランとレイアウトデザインはプロの作品から直接調達されていますが、家具のスタイル、色、質感に関するインテリアデザインは、エキスパートデザインとして一貫したスタイルを実現するために開発した推奨システムに基づいて慎重にキュレーションされています。特に新しいデータセットの長所に合わせて調整された、インテリアシーン合成とテクスチャ合成の2つのアプリケーションを示します。 
[概要]私たちのデータセットは、学術コミュニティやそれ以外の場所でも無料で利用できます。レイアウトから個々のオブジェクトのテクスチャの詳細まで、私たちのデータセットは無料で利用できます。さらに、7、302のシーンオブジェクトはすべて高品質のテクスチャが付属しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Inspecting state of the art performance and NLP metrics in image-based
  medical report generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_39.html">
      <font color="black">Inspecting state of the art performance and NLP metrics in image-based
  medical report generation</font>
    </a>
  </h2>
  <font color="black">この記事では、最先端の（SOTA）モデルを弱いベースラインと比較することにより、この進歩を対比します。このタスクの評価方法は、臨床精度を正しく測定するためにさらに研究する必要があり、理想的には医師がこの目的に貢献する必要があると結論付けます。 。単純で単純なアプローチでも、ほとんどの従来のNLPメトリックでほぼSOTAのパフォーマンスが得られることを示します。 
[概要]ほとんどの作品は、生成されたレポートを評価します。最新のモデルを弱いベースラインと比較することで、この進歩を対比します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Active Surface Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_40.html">
      <font color="black">Deep Active Surface Models</font>
    </a>
  </h2>
  <font color="black">グラフ畳み込みネットワークにシームレスに統合できるレイヤーを実装して、許容可能な計算コストで高度な滑らかさの事前分布を適用できます。アクティブサーフェスモデルは、複雑な3Dサーフェスのモデル化に役立つ長い歴史がありますが、で使用されているのはアクティブコンターのみです。ディープネットワークと組み合わせて、データ項とそれらを制御するメタパラメータマップを生成するだけです。結果として得られるディープアクティブサーフェスモデルは、従来の正規化損失項を使用して3Dサーフェスに滑らかさの優先順位を課す同等のアーキテクチャよりも優れていることを示します。 2D画像からの再構成および3Dボリュームセグメンテーション用。 
[概要]結果として得られるディープアクティブサーフェスモデルは、アーキテクチャよりも優れています。結果は、結果がモデルよりも優れていることを示します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Deep learning models for gastric signet ring cell carcinoma
  classification in whole slide images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_41.html">
      <font color="black">Deep learning models for gastric signet ring cell carcinoma
  classification in whole slide images</font>
    </a>
  </h2>
  <font color="black">それぞれ約500枚の画像からなる4つの異なるテストセットでモデルを評価しました。最良のモデルは、4つのテストセットすべてで少なくとも0.99の受信者操作曲線（ROC）曲線下面積（AUC）を達成し、最高のベースラインパフォーマンスを設定しました。 SRCC WSI分類..病理学者がSRCCを検出するのを支援できる計算病理学ツールは、非常に有益です。 
[要約]最良のモデルは、4つのテストセットすべてで少なくとも0.99の曲線（auc）の下の受信者操作曲線（roc）領域を達成しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Puzzle-AE: Novelty Detection in Images through Solving Puzzles -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_42.html">
      <font color="black">Puzzle-AE: Novelty Detection in Images through Solving Puzzles</font>
    </a>
  </h2>
  <font color="black">自己教師あり学習（SSL）メソッドの口実タスクとしてのパズル解決は、意味的に意味のある機能を学習する能力を以前に証明しました。ただし、ショートカットソリューションは、ジグソーパズルを含むSSLタスクでは大きな課題です。多くの場合とは異なります。競合他社の場合、提案されたフレームワークは安定していて、高速で、データ効率が高く、無原則の早期停止を必要としません。 
[ABSTRACT] u-codeはこの目的に効果的であることが証明されていますが、他のaeベースのフレームワークと同様の再構成エラーを使用してトレーニングした場合、トレーニングデータに適合しません。提案されたフレームワークは安定していて、高速で、データ効率が高く、原則のない早期停止は必要ありません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-29">
        <br><font color="black">2020-08-29</font>
      </time>
    </span>
</section>
<!-- paper0: Explicitly Learning Topology for Differentiable Neural Architecture
  Search -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_43.html">
      <font color="black">Explicitly Learning Topology for Differentiable Neural Architecture
  Search</font>
    </a>
  </h2>
  <font color="black">具体的には、トポロジ変数のセットと組み合わせ確率分布を導入して、ターゲットトポロジを明示的に示します。導入したトポロジ変数は、操作変数やスーパーネットの重みと一緒に学習でき、さまざまなDARTSバリアントに適用できます。スーパーネット内の無効なトポロジを抑制するためのパッシブアグレッシブ正規化。 
[ABSTRACT] toponasという名前の新しいメソッドは、操作の選択を分離するように設計されています。これはsupernetnetnetに適用されますが、esesesesesesesesesesを抑制するために使用できます。ただし、実行にかかる時間は関係ありません。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Towards online monitoring and data-driven control: a study of
  segmentation algorithms for infrared images of the powder bed -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_44.html">
      <font color="black">Towards online monitoring and data-driven control: a study of
  segmentation algorithms for infrared images of the powder bed</font>
    </a>
  </h2>
  <font color="black">識別されたアルゴリズムは、選択的レーザー焼結および選択的レーザー溶融機に容易に適用して、上記の制限のそれぞれに対処し、プロセス制御を大幅に改善できます。選択的レーザー焼結および選択的レーザー溶融機の数が増え、軸外赤外線を使用しています。オンライン監視およびデータ駆動型制御機能を改善するためのカメラ。これらの制限に対処するために、各赤外線画像を前景と背景にセグメント化する30を超えるセグメンテーションアルゴリズムを研究しています。 
[概要]これらのカメラからの赤外線画像を処理するためのアルゴリズムがまだ深刻に不足しています。これには、レーザートラックのオンライン監視機能の欠如や、データ駆動型の方法での画像の不十分な前処理が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Self-Supervised 3D Keypoint Learning for Ego-motion Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_45.html">
      <font color="black">Self-Supervised 3D Keypoint Learning for Ego-motion Estimation</font>
    </a>
  </h2>
  <font color="black">プロクラステスの残差ポーズ補正に基づく微分可能なモーションからの構造モジュールを介して外観と幾何学的マッチングを組み合わせることにより、キーポイントと深度推定ネットワークを共同で学習します。この作業では、ラベルのないビデオから直接深度認識キーポイントの教師あり学習を提案します。 ..実世界の条件で自律車両のロバストで正確な自我運動推定を行うために、自己監視キーポイントを最先端の視覚オドメトリフレームワークに統合する方法について説明します。 
[ABSTRACT]状態ベースの方法は、ホモグラフィ適応を介してトレーニングサンプルを生成し、単一の画像から既知のキーポイントの一致を使用して2D合成ビューを作成します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-07">
        <br><font color="black">2019-12-07</font>
      </time>
    </span>
</section>
<!-- paper0: SoftSeg: Advantages of soft versus binary training for image
  segmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_46.html">
      <font color="black">SoftSeg: Advantages of soft versus binary training for image
  segmentation</font>
    </a>
  </h2>
  <font color="black">開発されたトレーニングパイプラインは、既存のディープラーニングアーキテクチャのほとんどに簡単に組み込むことができます。これは、無料で利用できるディープラーニングツールボックスivadomed（https://ivadomed.org）にすでに実装されています。その結果、単一の「ハード」を割り当てます。ラベルは、有害な近似をもたらす可能性があります。 
[ABSTRACT] softsegは、組織のインターフェースで一貫したソフト予測を生成します。小さなオブジェクトの感度が向上します。softsegは問題の解決を目的としています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Visuo-Linguistic Question Answering (VLQA) Challenge -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_47.html">
      <font color="black">Visuo-Linguistic Question Answering (VLQA) Challenge</font>
    </a>
  </h2>
  <font color="black">最初に、VLQAサブセットを解決するための既存の最良の視覚言語アーキテクチャを調査し、それらが適切に推論できないことを示します。次に、ベースラインパフォーマンスがわずかに優れたモジュラーメソッドを開発しますが、それでも人間のパフォーマンスよりはるかに遅れています。特定の画像テキストモダリティに関する共同推論を導き出し、質問応答設定でVisuo-Linguistic Question Answering（VLQA）チャレンジコーパスをコンパイルする新しいタスク。 
[概要]データセット、コード、リーダーボードは米国で入手可能です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-01">
        <br><font color="black">2020-05-01</font>
      </time>
    </span>
</section>
<!-- paper0: Convolutional Autoencoder for Blind Hyperspectral Image Unmixing -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_48.html">
      <font color="black">Convolutional Autoencoder for Blind Hyperspectral Image Unmixing</font>
    </a>
  </h2>
  <font color="black">次に、これらの潜在的な特性から、デコーダーはアーキテクチャーの入力にあるモノクロ画像のロールアウト画像を再構築します。実際のハイパースペクトルデータの実験結果は、提案されたアルゴリズムが存在量推定で既存のアンミキシング方法よりも優れており、RMSEとSADをそれぞれメトリックとして使用した端成分抽出の競争力のある結果を生成すると結論付けています。畳み込み層から潜在空間表現まで生成された特徴空間。 
[概要]ハイパースペクトル画像に対してブラインドアンミキシングを実行するための新しいアーキテクチャが提案されています。エンコーダは、畳み込み層を介して生成された特徴空間を潜在空間表現に変換します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: RSINet: Rotation-Scale Invariant Network for Online Visual Tracking -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_49.html">
      <font color="black">RSINet: Rotation-Scale Invariant Network for Online Visual Tracking</font>
    </a>
  </h2>
  <font color="black">さらに、追跡モデルは、時空間エネルギー制御の下で適応的に最適化および更新され、モデルの安定性と信頼性、および高い追跡効率を保証します。さらに、シャムベースのトラッカーは、軸を生成することによって追跡対象の新しい状態を推測します。余分なバックグラウンドノイズを含み、移動するオブジェクトの回転とスケール変換を正確に推定できないため、追跡パフォーマンスが低下する可能性がある整列された境界ボックス。ほとんどのシャムネットワークベースのトラッカーは、モデルを更新せずに追跡プロセスを実行し、ターゲット固有を学習できません。適応的に変化。 
[概要]私たちのrsinetトラッカーは、ターゲット-ディストラクタ識別ブランチと回転-スケール推定ブランチで構成されています。回転とスケールの知識は、マルチタスク学習方法によってエンドツーエンドで明示的に学習できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Semantic Scene Completion using Local Deep Implicit Functions on LiDAR
  Data -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_50.html">
      <font color="black">Semantic Scene Completion using Local Deep Implicit Functions on LiDAR
  Data</font>
    </a>
  </h2>
  <font color="black">この連続表現は、空間離散化を必要とせずに、広範な屋外シーンの幾何学的およびセマンティックプロパティをエンコードするのに適していることを示します（したがって、シーンの詳細レベルとカバーできるシーンの範囲との間のトレードオフを回避します）。グローバルシーンその後、完了関数はローカライズされた関数パッチから組み立てられます。私たちの実験は、私たちの方法が、与えられたシーンの密な3D記述にデコードできる強力な表現を生成することを確認します。 
[ABSTRACT]私たちの方法は、ボクセル化に基づかない連続シーン表現を生成します。グローバルシーン完了関数は、その後、ローカライズされたパッチから組み立てられます。私たちの方法のパフォーマンスは、セマンティックキティシーン完了ベンチマークの最先端を超えています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: A Multi-class Approach -- Building a Visual Classifier based on Textual
  Descriptions using Zero-Shot Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_51.html">
      <font color="black">A Multi-class Approach -- Building a Visual Classifier based on Textual
  Descriptions using Zero-Shot Learning</font>
    </a>
  </h2>
  <font color="black">見えているクラスはモデルをトレーニングしたクラスであり、見えないクラスはモデルをテストするクラスです。このドメインでの初期の研究はバイナリ分類器の開発に焦点を当てていましたが、この論文では、マルチクラス分類器を紹介します。ゼロショット学習アプローチを使用します。ZSLは、クラスを2つのタイプ（表示クラスと非表示クラス）として区別します。 
[ABSTRACT]分類器は、伝達学習の概念を使用します。これらには、ゼロショット学習（zsl）と標準の自然言語処理技術が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Dehazing Cost Volume for Deep Multi-view Stereo in Scattering Media with
  Airlight and Scattering Coefficient Estimation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_52.html">
      <font color="black">Dehazing Cost Volume for Deep Multi-view Stereo in Scattering Media with
  Airlight and Scattering Coefficient Estimation</font>
    </a>
  </h2>
  <font color="black">霧や煙などの散乱媒体における学習ベースのマルチビューステレオ（MVS）法を、デヘイズコストボリュームと呼ばれる新しいコストボリュームで提案します。また、エアライトなどの散乱パラメータを推定する方法も提案します。 、およびデヘイズコストボリュームに必要な散乱係数。デヘイズコストボリュームを持つネットワークの出力深度は、これらのパラメータの関数と見なすことができます。したがって、これらは、モーションからの構造のステップで取得されたスパース3Dポイントクラウドを使用して幾何学的に最適化されます。 
[概要]実際の霧のシーンへのデヘイズコストボリュームの適用可能性を示しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: FROST: Faster and more Robust One-shot Semi-supervised Training -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_53.html">
      <font color="black">FROST: Faster and more Robust One-shot Semi-supervised Training</font>
    </a>
  </h2>
  <font color="black">私たちの実験は、ラベルのないデータの構成が不明な場合に、FROSTがうまく機能することを示しています。つまり、ラベルのないデータに各クラスの数が等しくなく、どのトレーニングクラスにも属さない分布外の例が含まれている可能性がある場合です。このホワイトペーパーでは、次のようなワンショットの半教師あり学習方法を紹介します。最先端の方法よりも1桁速くトレーニングし、堅牢です。高性能、トレーニングの速度、ハイパーパラメータに対する感度の低さにより、FROSTはワンショット半教師ありトレーニングの最も実用的な方法になっています。 。 
[概要]半教師あり学習の新しい方法はトレーニングに時間がかかりますが、パフォーマンスはラベル付けされたデータの選択に敏感です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Online Exemplar Fine-Tuning for Image-to-Image Translation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_54.html">
      <font color="black">Online Exemplar Fine-Tuning for Image-to-Image Translation</font>
    </a>
  </h2>
  <font color="black">実験結果は、私たちのフレームワークが目に見えない画像ペアへの一般化力を持つのに効果的であり、集中的なトレーニングフェーズを必要とする最先端を明らかに上回っていることを証明しています。私たちのフレームワークはオフライントレーニングフェーズを必要としません。既存の方法の主な課題でしたが、オンラインでの最適化を可能にするために事前にトレーニングされたネットワークです。2つのサブネットワーク、つまり通信の微調整と複数のGAN反転を設計し、これらのネットワークパラメータと潜在コードを最適化します。明確に定義された損失関数を備えた、事前にトレーニングされたもの。 
[概要]この概念は、オンラインエグザンプラと呼ばれる入力画像ペアの分析に基づいています。微調整（oeft）。ネットワークを入力ペア自体に微調整します。システムはオフライントレーニングを必要としませんが、事前にトレーニングされたネットワークにはトレーニングが必要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Restore from Restored: Video Restoration with Pseudo Clean Video -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_55.html">
      <font color="black">Restore from Restored: Video Restoration with Pseudo Clean Video</font>
    </a>
  </h2>
  <font color="black">具体的には、提案された方法は、複数の連続するフレームにわたって存在する多数の同様のパッチ（すなわち、パッチの繰り返し）を利用することができる。これらのパッチは、ベースラインネットワークのパフォーマンスを大幅に向上させることができます。私たちの実験では、提案された方法を最先端のノイズ除去装置に適用し、微調整されたネットワークがノイズ除去パフォーマンスを大幅に向上させることを示しています。提案された自己監視ベースの学習アルゴリズムを使用して、微調整されたビデオノイズ除去ネットワークの復元パフォーマンスを分析し、FCNが隣接するフレーム間の正確な登録を必要とせずに繰り返しパッチを利用できることを示します。 
[概要]この方法は、完全畳み込みニューラルネットワーク（fcn）を試行しています。正確なオプティカルフローの推定と登録手順なしで、ビデオのノイズ除去パフォーマンスを向上させることができます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-09">
        <br><font color="black">2020-03-09</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial collision attacks on image hashing functions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_56.html">
      <font color="black">Adversarial collision attacks on image hashing functions</font>
    </a>
  </h2>
  <font color="black">さらに、ハッシュ関数の出力以外のポイントを攻撃することで、攻撃者は特定のアルゴリズムの詳細を知る必要がなくなり、異なるハッシュサイズまたはモデルアーキテクチャ間で転送される衝突が発生します。知覚アルゴリズムを使用した画像のハッシュは重複画像検出の問題を解決するための一般的なアプローチ..これらの手法を使用すると、攻撃者は重複画像検出サービスの画像ルックアップテーブルを汚染し、未定義または望ましくない動作を引き起こす可能性があります。 
[概要]攻撃者は、重複画像検出サービスのルックアップテーブルを汚染し、未定義または望ましくない動作を引き起こす可能性があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Masked Linear Regression for Learning Local Receptive Fields for Facial
  Expression Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_57.html">
      <font color="black">Masked Linear Regression for Learning Local Receptive Fields for Facial
  Expression Synthesis</font>
    </a>
  </h2>
  <font color="black">これらのGANは、テストとトレーニングの分布が類似している限り、フォトリアリスティックな結果を生成します。したがって、顔の表情の局所的でまばらな構造を利用するリッジ回帰の制約付きバージョンを提案します。このモデルをマスクされた回帰と見なします。地元の受容分野を学ぶ。 
[概要]式合成モデルのパラメータ数を大幅に削減できます。提案されたアルゴリズムは、ガニメーションの状態と比較されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Self-Supervised Physics-Guided Deep Learning Reconstruction For
  High-Resolution 3D LGE CMR -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_58.html">
      <font color="black">Self-Supervised Physics-Guided Deep Learning Reconstruction For
  High-Resolution 3D LGE CMR</font>
    </a>
  </h2>
  <font color="black">最近、完全にサンプリングされたデータなしでPG-DL手法のトレーニングを可能にするために、教師あり学習アプローチが提案されました。PG-DLメソッドのトレーニングは、通常、完全にサンプリングされたデータを参照として必要とする教師ありの方法で実行されます。これは3DLGEでは困難です。 CMR ..この作業では、この自己教師あり学習アプローチを3Dイメージングに拡張し、3Dボリュームの小さなトレーニングデータベースサイズに関連する課題に取り組みます。 
[概要] 3Dスキャンは、2Dイメージングと比較して、カバレッジと解像度が向上しています。これは、科学に基づく深層学習（pg-dl）アプローチによるものです。最近、トレーニングを可能にするために、自己教師あり学習アプローチが提案されました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: AttentiveNAS: Improving Neural Architecture Search via Attentive
  Sampling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_59.html">
      <font color="black">AttentiveNAS: Improving Neural Architecture Search via Attentive
  Sampling</font>
    </a>
  </h2>
  <font color="black">最近、2ステージNAS、例えば。単純化のために均一サンプリングが広く使用されていますが、検索プロセスの主な焦点であるモデルのパフォーマンスパレートフロントにとらわれないため、モデルの精度をさらに向上させる機会を逃しています。この作業では、AttentiveNASを提案します。これは、パレートのパフォーマンスを向上させるためにネットワークをサンプリングすることに焦点を当てています。 
[ABSTRACT] 2段階のnasは、トレーニング中に検索スペースからサンプリングする必要があります。これは、最終的に検索されるモデルの精度に直接影響します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Layer-Wise Data-Free CNN Compression -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_60.html">
      <font color="black">Layer-Wise Data-Free CNN Compression</font>
    </a>
  </h2>
  <font color="black">また、この方法をエンドツーエンドの生成方法と組み合わせた場合の、EfficientNet B0のデータフリープルーニングに関する最先端のパフォーマンスを示します。レイヤーごとのトレーニングデータを効率的に生成する方法と、レイヤーワイズ圧縮中に精度を維持するためのネットワーク..データフリーネットワーク圧縮の問題を、いくつかの独立したレイヤーワイズ圧縮に分割します。 
[概要]私たちのデータ-無料の方法では、同等の最先端の方法よりも14倍から450倍少ないフロップが必要です。レイヤーごとのトレーニングデータを効率的に生成する方法と、ネットワークを事前調整して精度を維持する方法を示します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Fixing the train-test resolution discrepancy: FixEfficientNet -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_61.html">
      <font color="black">Fixing the train-test resolution discrepancy: FixEfficientNet</font>
    </a>
  </h2>
  <font color="black">たとえば、追加のトレーニングデータなしでトレーニングされたFixEfficientNet-B0は、530万個のパラメーターを使用してImageNetで79.3％のトップ1精度を達成します。これにより、トレーニング画像とテスト画像の不一致が修正されます。300Mのラベルなし画像の弱い監視で事前トレーニングされ、FixResでさらに最適化されたEfficientNet-L2は、88.5％のトップ1精度（トップ5：98.7％）を達成し、新しい単一のクロップでImageNetの最先端。 
[概要] fixefficientnetネットワークは、同じ数のパラメーターで初期アーキテクチャよりも優れています。たとえば、私たちの改善は、過剰適合の傾向が少ないimagenet-v2の実験設定にとどまります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-18">
        <br><font color="black">2020-03-18</font>
      </time>
    </span>
</section>
<!-- paper0: Shaping Deep Feature Space towards Gaussian Mixture for Visual
  Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_62.html">
      <font color="black">Shaping Deep Feature Space towards Gaussian Mixture for Visual
  Classification</font>
    </a>
  </h2>
  <font color="black">GM損失は、入力の特徴分布とトレーニングセットの間の不一致に基づいて、敵対的な例などの異常な入力を区別するために簡単に使用できます。提案されたモデルは、追加のトレーニング可能なパラメーターを使用せずに簡単かつ効率的に実装できます。広範な評価は、提案された方法が画像分類だけでなく、さまざまな脅威モデルの下での強力な攻撃によって生成された敵対的な例の堅牢な検出でも良好に機能することを示しています。 
[概要]視覚的分類のための深層ニューラルネットワークに対してガウス混合（gm）損失関数が提案されています。これを使用して、敵対攻撃に対して堅牢に実行するようにモデルをトレーニングできます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Privileged Knowledge Distillation for Online Action Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_63.html">
      <font color="black">Privileged Knowledge Distillation for Online Action Detection</font>
    </a>
  </h2>
  <font color="black">将来のフレームを明示的に予測する他のOAD手法と比較して、私たちのアプローチは、予測できない不要で一貫性のないビジュアルコンテンツの学習を回避し、2つの人気のあるOADベンチマークであるTVSeriesとTHUMOS14で最先端の精度を実現します。これは、（i）カリキュラム学習手順をスケジュールし、（ii）情報ギャップを縮小し、学習パフォーマンスを向上させるために、学生モデルに補助ノードを挿入します。知識蒸留を使用して、特権情報をオフライン教師からオンライン学生に転送します。 。 
[概要]この論文は、オンライン行動検出のための特権ベースのフレームワークを備えた新しい学習を提示しました。トレーニング段階でのみ観察可能な将来のフレームは、特権情報の形式と見なされます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Fast Motion Understanding with Spatiotemporal Neural Networks and
  Dynamic Vision Sensors -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_64.html">
      <font color="black">Fast Motion Understanding with Spatiotemporal Neural Networks and
  Dynamic Vision Sensors</font>
    </a>
  </h2>
  <font color="black">$ {\ theta} $の24.73 {\ deg}エラー、18.4mmの平均離散半径予測エラー、および衝突予測エラーまでの中央値25.03％で、23.4m / sで移動するおもちゃのダーツに対するシステムの結果を強調表示します。 ..結合されたネットワークは、オブジェクトの衝突までの予想時間と、離散化された極グリッド上の予測された衝突点の両方を出力することを学習します。このペーパーでは、高速モーションについて推論するためのダイナミックビジョンセンサー（DVS）ベースのシステムを紹介します。 。 
[概要]この論文では、動物が15mを超える速度で小さくて速く接近する物体にどのように反応するかによって動機付けられた方法を提示します。この方法は、視覚システムと畳み込みニューラルネットワークを組み合わせて、関連する時空間特徴を効率的に抽出します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Weight mechanism: adding a constant in concatenation of series connect -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_65.html">
      <font color="black">Weight mechanism: adding a constant in concatenation of series connect</font>
    </a>
  </h2>
  <font color="black">したがって、融合層間のギャップを狭め、融合中のノイズの影響を減らすことが重要です。当然、直接法では、それらを組み合わせて、連結または追加によって失われた詳細情報を取得します。具体的には、新しいアーキテクチャを設計します。重量メカニズムをテストするために融合U-Netと名付けられ、0.12％のmIoUの改善も得られます。 
[概要]特徴融合で流れる写真表現は、セマンティックゲインと完全には一致しません。代わりに、畳み込み操作により、特徴マップ間のギャップと直列接続の狭さが減少します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-07">
        <br><font color="black">2020-03-07</font>
      </time>
    </span>
</section>
<!-- paper0: Diverse Plausible Shape Completions from Ambiguous Depth Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_66.html">
      <font color="black">Diverse Plausible Shape Completions from Ambiguous Depth Images</font>
    </a>
  </h2>
  <font color="black">評価するために、ネットワークから一連の完了をサンプリングし、各テスト観測に対して一連のもっともらしい形状一致を構築し、一連の形状に対して定義されたもっともらしい多様性メトリックを使用して比較します。Shapenetマグと部分的に遮蔽されたYCBを使用して実験を実行します。オブジェクトを見つけて、私たちの方法が曖昧さの少ないデータセットで同等に機能し、多くの形状が観測された深度画像にもっともらしく適合する場合、既存の方法よりも優れていることを発見します。デコーダーへの値。 
[ABSTRACT]既存の方法では、単一の形状にわずかな変化しか生じない傾向があります。これらの機能はトレーニング中に既知であり、エンコーダーに教師あり損失を追加し、デコーダーにノイズのない値を追加できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Restore from Restored: Single Image Denoising with Pseudo Clean Image -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_67.html">
      <font color="black">Restore from Restored: Single Image Denoising with Pseudo Clean Image</font>
    </a>
  </h2>
  <font color="black">内部の自己相似パッチ（つまり、パッチの繰り返し）を利用することで、ベースラインネットワークを特定の入力画像に適合させることができます。ただし、このような方法では、教師あり方法と比較して、ガウスノイズなどの既知のノイズタイプでのパフォーマンスが比較的低くなります。したがって、外部情報と内部情報を組み合わせるために、テスト時に疑似トレーニングセットを使用して、完全に事前トレーニングされたデノイザーを微調整します。 
[概要]多くの教師ありノイズ除去アプローチは、大規模な外部トレーニングデータセットを使用して満足のいく結果を生成できます。しかし、最近の教師ありアプローチは、特定のテスト入力からの情報を利用して、入力画像のノイズを除去できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-09">
        <br><font color="black">2020-03-09</font>
      </time>
    </span>
</section>
<!-- paper0: FixBi: Bridging Domain Spaces for Unsupervised Domain Adaptation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_68.html">
      <font color="black">FixBi: Bridging Domain Spaces for Unsupervised Domain Adaptation</font>
    </a>
  </h2>
  <font color="black">ドメイン不変表現を学習するための教師なしドメイン適応（UDA）手法は、目覚ましい進歩を遂げました。拡張ドメインから、補完的な特性を持つソース優勢モデルとターゲット優勢モデルをトレーニングします。信頼性ベースの学習手法を使用します。たとえば、信頼性の高い予測を使用した双方向マッチングや、信頼性の低い予測を使用した自己ペナルティにより、モデルは相互に、または独自の結果から学習できます。 
[要約]新しい方法では、お互いから、または独自の結果から学習できます。新しい方法では、信頼性に基づく学習方法を使用します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: CGAP2: Context and gap aware predictive pose framework for early
  detection of gestures -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_69.html">
      <font color="black">CGAP2: Context and gap aware predictive pose framework for early
  detection of gestures</font>
    </a>
  </h2>
  <font color="black">さらに、アブレーション研究は、ポーズ予測モジュールに高いコンテキスト情報を提供することは、予測認識に悪影響を与える可能性があることを示しています。CGAP2のパフォーマンスは、MPJPEメトリックを使用してHuman3.6Mデータセットで評価されます。CGAP2ポーズ予測モジュールは、3D畳み込みレイヤーを使用します。提供されるポーズフレームの数、各ポーズフレーム間の時間差、および予測されるポーズフレームの数によって異なります。 
[ABSTRACT] anticip2は、エンコーダーとデコーダーのアーキテクチャをポーズ予測モジュールと組み合わせて使用し、将来のフレームを予測します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Attentional Separation-and-Aggregation Network for Self-supervised
  Depth-Pose Learning in Dynamic Scenes -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CV/paper_70.html">
      <font color="black">Attentional Separation-and-Aggregation Network for Self-supervised
  Depth-Pose Learning in Dynamic Scenes</font>
    </a>
  </h2>
  <font color="black">次に、動的認識学習のために移動物体を自動的に検出する自動選択アプローチを紹介します。この問題に対処するために、シーンの識別と抽出を学習できる注意分離および集約ネットワーク（ASANet）を提案します。注意メカニズムによる静的および動的特性..経験的実験は、私たちの方法がKITTIベンチマークで最先端のパフォーマンスを達成できることを示しています。 
[概要]モーションネットは、カメラのエゴモーションとシーンのモーションフィールドを予測できます。その後、2つの別々のデコーダーとアサネットが続きます。このメソッドは、キティベンチマークで移動パフォーマンスを実現できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Sequence-Level Mixed Sample Data Augmentation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_0.html">
      <font color="black">Sequence-Level Mixed Sample Data Augmentation</font>
    </a>
  </h2>
  <font color="black">SCANやセマンティック解析などの強力な構成の一般化を必要とするタスクでは、SeqMixはさらに改善を提供します。経験的な成功にもかかわらず、ニューラルネットワークは自然言語の構成の側面をキャプチャするのが依然として困難です。SeqMixは5つの異なる翻訳で一貫して約1.0BLEUの改善をもたらします。強力なTransformerベースライン上のデータセット。 
[ABSTRACT] seqmixは、シーケンスからシーケンスへの問題の神経モデルにおける構成的動作を促進するための単純なデータ拡張アプローチを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Out-of-Task Training for Dialog State Tracking Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_1.html">
      <font color="black">Out-of-Task Training for Dialog State Tracking Models</font>
    </a>
  </h2>
  <font color="black">ダイアログ状態追跡（DST）は、深刻なデータスパース性に悩まされています。この作業では、関連のないNLPタスクからの非ダイアログデータを利用して、ダイアログ状態トラッカーをトレーニングします。多くの自然言語処理（NLP）タスクは、転送学習とマルチの恩恵を受けます。 -タスク学習。ダイアログでは、これらの方法は利用可能なデータの量とダイアログアプリケーションの特異性によって制限されます。 
[概要]これにより、dstに固有のデータスパース性の問題を軽減するために、無関係なnlpコーパスが豊富に存在するようになります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Efficient Computation of Expectations under Spanning Tree Distributions -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_2.html">
      <font color="black">Efficient Computation of Expectations under Spanning Tree Distributions</font>
    </a>
  </h2>
  <font color="black">ボーナスとして、エントロピーの勾配、KL発散、KL発散の勾配など、文献に欠落している量のアルゴリズムを提供します。正確性と効率性の厳密な証明を通じてフレームワークを検証します。場合によっては、私たちのアプローチは既存のアルゴリズムの効率と一致し、場合によっては、文の長さの係数で実行時の複雑さを軽減します。 
[概要]フレームワークが既知のアルゴリズムで効率的に収集される方法を示しました。たとえば、私たちのアプローチは既存のアルゴリズムの効率と一致します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-29">
        <br><font color="black">2020-08-29</font>
      </time>
    </span>
</section>
<!-- paper0: Beyond I.I.D.: Three Levels of Generalization for Question Answering on
  Knowledge Bases -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_3.html">
      <font color="black">Beyond I.I.D.: Three Levels of Generalization for Question Answering on
  Knowledge Bases</font>
    </a>
  </h2>
  <font color="black">代わりに、KBQAモデルには、iid、compositional、zero-shotの3つのレベルの組み込み一般化を含めることをお勧めします。さらに、新しいBERTベースのKBQAモデルを提案します。より強力なKBQAモデルの開発を容易にするため一般化では、64,331の質問を含む新しい大規模で高品質のデータセットGrailQAを構築してリリースし、3つの一般化レベルすべての評価設定を提供します。 
[要約] iii dは、大規模なkbで合理的に達成可能でも望ましいものでもない可能性があります。これは、1。真のユーザー分布をキャプチャするのが難しいためです。巨大なスペースからランダムにサンプリングされたトレーニング例は、データが非常に非効率的です。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-16">
        <br><font color="black">2020-11-16</font>
      </time>
    </span>
</section>
<!-- paper0: Topology of Word Embeddings: Singularities Reflect Polysemy -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_4.html">
      <font color="black">Topology of Word Embeddings: Singularities Reflect Polysemy</font>
    </a>
  </h2>
  <font color="black">私たちの見解は、単義語と多義語がそれらの近隣のトポロジーに基づいて区別できることを示唆しています。この見解をサポートする2種類の経験的証拠を提示します。これは、単語の実際の意味の数とよく相関します。（2）競争力のある結果を生み出す、Word Sense Induction＆Disambiguationに関するSemEval-2010タスクに対する単純でトポロジー的に動機付けられたソリューションを提案します。 
[要約]私たちは、より正確には、彼らがつまんで生きることを期待すべきだと主張します：空間の単一の商。この観点をサポートする2種類の証拠を提示します。永続的なホモロジーに基づく多義性の尺度を紹介します。それは単語の実際の意味の数とよく相関します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Tie Your Embeddings Down: Cross-Modal Latent Spaces for End-to-end
  Spoken Language Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_5.html">
      <font color="black">Tie Your Embeddings Down: Cross-Modal Latent Spaces for End-to-end
  Spoken Language Understanding</font>
    </a>
  </h2>
  <font color="black">異なるクロスモーダル損失にわたって、2つの公開されているE2EデータセットでCMLSモデルをトレーニングし、提案されたトリプレット損失関数が最高のパフォーマンスを達成することを示します。このペーパーでは、E2Eシステムをオーディオ付きのマルチモーダルモデルとして扱います。とテキストはその2つのモダリティとして機能し、クロスモーダル潜在空間（CMLS）アーキテクチャを使用します。このアーキテクチャでは、共有潜在空間が「音響」と「テキスト」の埋め込み間で学習されます。異なるマルチモーダル損失を使用して明示的に提案します。意味的に強力な事前トレーニング済みBERTモデルから取得された、音響埋め込みをテキスト埋め込みに近づけるようにガイドします。 
[概要] e2eシステムのトレーニングは、主にペアのテキストメッセージが不足しているため、依然として課題です。このシステムを使用して、テキスト埋め込みに近づくように音響埋め込みをトレーニングできます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: SRLGRN: Semantic Role Labeling Graph Reasoning Network -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_6.html">
      <font color="black">SRLGRN: Semantic Role Labeling Graph Reasoning Network</font>
    </a>
  </h2>
  <font color="black">提案されたアプローチは、最近の最先端モデルと比較して、HotpotQAディストラクタ設定ベンチマークで競争力のあるパフォーマンスを示しています。提案されたグラフは、タイプセンテンス（質問、タイトル、およびその他のセンテンス）のノードを含む異種のドキュメントレベルのグラフです。 ）、およびノードとして引数を含み、エッジとして述語を含む文ごとのセマンティックロールラベリングサブグラフ..この作業は、マルチホップ質問回答（QA）に関する学習と推論の課題を扱います。 
[要約]提案されたシステムは、文の意味構造に基づいています。それは、裏付けとなる事実と答えを共同で見つけることを含みます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-07">
        <br><font color="black">2020-10-07</font>
      </time>
    </span>
</section>
<!-- paper0: Language Acquisition Environment for Human-Level Artificial Intelligence -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_7.html">
      <font color="black">Language Acquisition Environment for Human-Level Artificial Intelligence</font>
    </a>
  </h2>
  <font color="black">この論文では、この能力のモデルの研究を容易にする環境を構築するための継続的な取り組みを紹介します。エージェントは、人間の子供と同じように最初の言葉を話すことを学ぶ必要があります。この環境では、の明確な定義はありません。タスクまたはそれらのタスクを達成するときに与えられる報酬。 
[概要]他人の言語経験から学ぶことは、人間の知性を他の人から排除する本質的な特徴であると推測します。この記事は論文に掲載されました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: EasyTransfer -- A Simple and Scalable Deep Transfer Learning Platform
  for NLP Applications -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_8.html">
      <font color="black">EasyTransfer -- A Simple and Scalable Deep Transfer Learning Platform
  for NLP Applications</font>
    </a>
  </h2>
  <font color="black">文献では、多くのNLPアプリケーションにディープトランスファーラーニング（TL）アルゴリズムを適用することに成功していますが、この目的のためにシンプルでスケーラブルなTLツールキットを構築するのは簡単ではありません。豊富なAPI抽象化、スケーラブルなアーキテクチャ、 NLPアプリケーションの開発を容易にする包括的なディープTLアルゴリズム。このギャップを埋めるために、EasyTransferプラットフォームは、NLPアプリケーションのディープTLアルゴリズムの開発を容易にするように設計されています。 
[概要] easytransferプラットフォームは、nlpアプリケーション用のディープtlアルゴリズムの開発を容易にするように設計されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Improving Document-Level Sentiment Analysis with User and Product
  Context -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_9.html">
      <font color="black">Improving Document-Level Sentiment Analysis with User and Product
  Context</font>
    </a>
  </h2>
  <font color="black">IMDB、Yelp 2013、およびYelp 2014のデータセットでの実験結果は、最良の場合に2パーセントポイントを超える最先端の改善を示しています。同じユーザーによって書かれたレビューの表現を明示的に保存することでこれを実現します。同じ製品を使用して、特定の1人のユーザーと製品のすべてのレビューをモデルに記憶させます。さらに、前の作業で使用した階層アーキテクチャを削除して、テキスト内の単語が互いに直接一致できるようにします。 
[概要]現在の製品に関連する過去のレビューが含まれていることを調査します。ただし、他のユーザーが作成したものでも、同じデザインが改善されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Predictions For Pre-training Language Models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_10.html">
      <font color="black">Predictions For Pre-training Language Models</font>
    </a>
  </h2>
  <font color="black">私たちの方法の改善は安定していて注目に値します。次に、タスク固有の損失とマスクされた言語モデルの損失に疑似ラベルを使用して事前トレーニングします。業界のNLPアプリケーションでは、ユーザーによって大量のデータが生成されます。 
[概要]微調整されたモデルは、ユーザーが生成したラベルなしデータに疑似ラベルを付けます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Generating universal language adversarial examples by understanding and
  enhancing the transferability across neural models -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_11.html">
      <font color="black">Generating universal language adversarial examples by understanding and
  enhancing the transferability across neural models</font>
    </a>
  </h2>
  <font color="black">特に、ネットワークアーキテクチャ、入力形式、単語の埋め込み、モデル容量などのさまざまな要因が敵対的攻撃の転送可能性にどのように影響するかを調査するために、広範な実験を実施します。これらの研究に基づいて、ユニバーサルブラックボックス攻撃アルゴリズムを提案します。これは、敵対的例を誘発して、ほとんどすべての既存のモデルを攻撃する可能性があります。ただし、敵対的例の転送可能性と、普遍的な敵対的例の生成方法に関する体系的な研究は不足しています。 
[概要]この論文では、テキスト分類モデルに対する敵対的攻撃の転送可能性を調査します。この論文では、そのような要因を体系的に調査します。これらの調査では、敵対的例を誘発して既存のほぼすべてのモデルを攻撃できるユニバーサルブラックワード置換アルゴリズムを提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: A Hierarchical Multi-Modal Encoder for Moment Localization in Video
  Corpus -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_12.html">
      <font color="black">A Hierarchical Multi-Modal Encoder for Moment Localization in Video
  Corpus</font>
    </a>
  </h2>
  <font color="black">未解決の課題は、ビデオの表現が時間領域のさまざまなレベルの粒度を説明する必要があることです。この問題に取り組むために、両方の粗いクリップでビデオをエンコードするHierArchical Multi-Modal EncodeR（HAMMER）を提案します。レベルときめ細かいフレームレベルにより、複数のサブタスク、つまりビデオ検索、セグメント時間ローカリゼーション、マスクされた言語モデリングに基づいてさまざまなスケールで情報を抽出します。私たちのアプローチは、以前の方法や強力なベースラインを上回り、新しい状態を確立します。このタスクの最先端。 
[概要] gran granは、グラニュラービデオエンコーダー（ハンマー）の発案によるものです。これは、細粒度のクリップレベルと細粒度のフレームレベルの両方でビデオをエンコードします。これは、ビデオの取得とその後の検索を含む複数のサブタスクに基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: An improved Bayesian TRIE based model for SMS text normalization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_13.html">
      <font color="black">An improved Bayesian TRIE based model for SMS text normalization</font>
    </a>
  </h2>
  <font color="black">提案されたTrieの統計的特性に関する2つの定理を証明し、それらを使用して、単語の発生確率の偏りのない一貫した推定量であると主張します。さらに、モデルを雑音のあるチャネルベースのエラー修正のパラダイムに融合し、ヒューリスティックを提供します。ダメラウ・レーベンシュタインの距離1を超えるために..また、シミュレーションを実行して、主張を裏付け、提案されたスキームが以前の作業よりも優れていることを示します。 
[概要]トライデータ構造に基づく確率論的アプローチが文献で提案されました。テレベースのシミュレーションよりもパフォーマンスが優れていることがわかりました。これらには、既存のトライベースモデルへの構造変更が含まれます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-04">
        <br><font color="black">2020-08-04</font>
      </time>
    </span>
</section>
<!-- paper0: Inspecting state of the art performance and NLP metrics in image-based
  medical report generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_14.html">
      <font color="black">Inspecting state of the art performance and NLP metrics in image-based
  medical report generation</font>
    </a>
  </h2>
  <font color="black">この記事では、最先端の（SOTA）モデルを弱いベースラインと比較することにより、この進歩を対比します。単純で単純なアプローチでも、ほとんどの従来のNLPメトリックでSOTAに近いパフォーマンスが得られることを示します。このタスクの評価方法は、臨床的精度を正しく測定するためにさらに研究する必要があり、理想的にはこの目的に貢献するために医師を巻き込む必要があります。 
[概要]ほとんどの作品は、生成されたレポートを評価します。最新のモデルを弱いベースラインと比較することで、この進歩を対比します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: KddRES: A Multi-level Knowledge-driven Dialogue Dataset for Restaurant
  Towards Customized Dialogue System -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_15.html">
      <font color="black">KddRES: A Multi-level Knowledge-driven Dialogue Dataset for Restaurant
  Towards Customized Dialogue System</font>
    </a>
  </h2>
  <font color="black">コーパスとベンチマークモデルは公開されています。KddRESの公開は、現在の対話データセットの必要な補足であり、それぞれにカスタマイズされた対話システムを構築するなど、社会の中小企業（SME）にとってより適切で価値があると考えています。レストラン..それに加えて、セマンティック情報をより適切にキャプチャするために、きめ細かいスロットとインテントを設計しました。 
[概要]香港のレストラン（kddres）向けの最初の広東語の知識主導型対話データセットを公開します。これは、特定の1つのレストランへの複数回の会話の情報に基づいています。kddresのリリースは、現在の対話データセットの必要な補足になる可能性があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: LAVA: Latent Action Spaces via Variational Auto-encoding for Dialogue
  Policy Optimization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_16.html">
      <font color="black">LAVA: Latent Action Spaces via Variational Auto-encoding for Dialogue
  Policy Optimization</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、補助タスクを活用して潜在変数の分布を形成する3つの方法を検討します。事前トレーニング、事前情報の取得、マルチタスク学習です。このような方法でトレーニングされたポリシーには、専門家が定義したアクションは必要ありません。スペースですが、大きなアクションスペースと長い軌道を処理する必要があるため、RLは実用的ではありません。エンドツーエンドの設定では、システムの語彙全体をアクションとして、単語レベルの順次意思決定プロセスで応答を構築できます。スペース。 
[要約]応答は、単語レベルの意思決定プロセスを介して構築できます。これは、変分モデルの潜在空間をアクション空間として使用して使用できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Pre-training Multilingual Neural Machine Translation by Leveraging
  Alignment Information -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_17.html">
      <font color="black">Pre-training Multilingual Neural Machine Translation by Leveraging
  Alignment Information</font>
    </a>
  </h2>
  <font color="black">実験結果は、mRASPがこれらのターゲットペアで直接トレーニングする場合と比較して大幅なパフォーマンスの向上を達成することを示しています。機械翻訳（MT）に関する次の質問を調査します。共通のシードとして機能する単一のユニバーサルMTモデルを開発し、派生物を取得して改善できるか任意の言語ペアのモデル？驚いたことに、mRASPは、トレーニング前のコーパスでは決して発生しないエキゾチックな言語での翻訳品質を向上させることさえできます。 
[概要] mraspは、ユニバーサル多言語ニューラル機械翻訳モデルを事前トレーニングするためのアプローチです。公開データセットのみを使用して、32の言語ペアでmraspモデルを事前トレーニングします。mraspは、複数の低リソース言語ペアを検証するのは初めてです。に使える</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-07">
        <br><font color="black">2020-10-07</font>
      </time>
    </span>
</section>
<!-- paper0: Visuo-Linguistic Question Answering (VLQA) Challenge -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_18.html">
      <font color="black">Visuo-Linguistic Question Answering (VLQA) Challenge</font>
    </a>
  </h2>
  <font color="black">まず、VLQAサブセットを解決するための既存の最良の視覚言語アーキテクチャを調査し、それらが適切に推論できないことを示します。VLQAは、視覚言語コンテキストを推論するための優れたベンチマークになると考えています。特定の画像テキストモダリティに関する共同推論を導き出し、質問回答設定でVisuo-Linguistic Question Answering（VLQA）チャレンジコーパスをコンパイルします。 
[概要]データセット、コード、リーダーボードは米国で入手可能です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-01">
        <br><font color="black">2020-05-01</font>
      </time>
    </span>
</section>
<!-- paper0: A Model to Measure the Spread Power of Rumors -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_19.html">
      <font color="black">A Model to Measure the Spread Power of Rumors</font>
    </a>
  </h2>
  <font color="black">（iii）T検定の結果は、SPR基準がFRとTRを大幅に区別できることを示しています。さらに、噂の真実性を検証する新しい方法としても役立ちます。今日、ソーシャルメディアで毎日やり取りされる投稿のかなりの部分が（ii）提案されたRSPMMアプローチはコンテキスト機能のみに焦点を当て、既存の手法は構造とコンテンツ機能に基づいていますが、RSPMMはかなり優れた結果を達成しています（Fメジャー= 83％）。 
[ABSTRACT]提案された噂の拡散電力測定モデル（rspmm）は、文学ベースのアプローチを使用してsprを計算します。提案されたrspmmは、Twitterと電報から収集された2つのラベル付きデータセットで検証されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-18">
        <br><font color="black">2020-02-18</font>
      </time>
    </span>
</section>
<!-- paper0: Supertagging Combinatory Categorial Grammar with Attentive Graph
  Convolutional Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_20.html">
      <font color="black">Supertagging Combinatory Categorial Grammar with Attentive Graph
  Convolutional Networks</font>
    </a>
  </h2>
  <font color="black">CCGbankで実行された実験は、スーパータグ付けと構文解析の両方の点で、私たちのアプローチが以前のすべての研究よりも優れていることを示しています。さらなる分析は、CCGスーパータグ付けを強化するために単語ペアから識別的に学習するアプローチの各コンポーネントの有効性を示しています。コンテキスト情報を活用する新しいソリューションを通じてニューラルCCGスーパータグ付けを強化するために注意深いグラフ畳み込みネットワークを提案します。 
[概要]以前の研究では、コンテキスト機能を活用するための取り組みは限られていましたが、既存の研究ではこの言語を使用できませんでした。代わりに、レキシコンから抽出されたチャンクからグラフを作成し、グラフに注意を向けます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-10-13">
        <br><font color="black">2020-10-13</font>
      </time>
    </span>
</section>
<!-- paper0: The Ubiqus English-Inuktitut System for WMT20 -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_21.html">
      <font color="black">The Ubiqus English-Inuktitut System for WMT20</font>
    </a>
  </h2>
  <font color="black">英語-イヌクティトゥット語の翻訳タスクは、データの選択、準備、トークン化から品質評価に至るまで、あらゆる段階で困難です。このペーパーでは、UbiqusのWMT20英語-イヌクティトゥット語の共有ニュース翻訳タスクへの提出について説明します。提出のみは、多言語アプローチに基づいており、いくつかの凝集言語でTransformerモデルを共同でトレーニングします。 
[概要]私たちのメインシステム、そして唯一の提出は、多言語アプローチに基づいており、いくつかの膠着語でトランスフォーマーモデルをトレーニングします。イヌクティトゥット語の特殊性と低リソースコンテキストの両方のために問題が発生します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Do Fine-tuned Commonsense Language Models Really Generalize? -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_22.html">
      <font color="black">Do Fine-tuned Commonsense Language Models Really Generalize?</font>
    </a>
  </h2>
  <font color="black">また、定性分析や一貫性分析などの選択的調査を実施して、問題をより深く理解します。これらは常識的なベンチマークであるため、常識的な推論を一般化するモデルでは、複数の常識的なベンチマークでパフォーマンスが大幅に低下することはありません。5つの一般的なベンチマークを使用します。 、複数のコントロールと統計分析により、微調整された常識的な言語モデルは、実験のセットアップに中程度の変更を加えても、まだ十分に一般化されておらず、実際、データセットの偏りの影響を受けやすい可能性があるという明確な証拠が見つかりました。 
[概要]この調査は、5つの一般的なベンチマーク、複数のコントロール、および統計分析を使用して行われました。彼らは、微調整された常識的な言語モデルが、実験の設定に中程度の変更を加えても、まだ十分に一般化されていないという証拠を発見しました。データセットバイアスの影響を受けやすい</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Audio-visual Multi-channel Recognition of Overlapped Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_23.html">
      <font color="black">Audio-visual Multi-channel Recognition of Overlapped Speech</font>
    </a>
  </h2>
  <font color="black">分離コンポーネントと認識コンポーネント間のエラーコストの不一致を減らすために、コネクティビスト時間分類（CTC）損失関数、またはスケール不変の信号対雑音比（Si-SNR）を使用したマルチタスク基準補間を使用して共同で微調整しました。エラーコスト..音響信号の破損に対する視覚モダリティの不変性に動機付けられて、この論文は、緊密に統合された分離フロントエンドと認識バックエンドを特徴とするオーディオビジュアルマルチチャネルオーバーラップ音声認識システムを提示します。 -チャネルAVSRシステムは、ベースラインの音声のみのASRシステムを最大6.81 \％（26.83 \％相対）および22.22 \％（56.87 \％相対）上回っています。それぞれ、リップリーディングセンテンス2（LRS2）データセットの再生。 
[概要]提案されたマルチチャネルavsrシステムはベースラインオーディオを上回ります-asrシステムのみが最大6.81％（相対26. 83％）および22. 22. 22. 87％。提案されたマルチトールavsrシステム35,000ドルの費用で開発されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br><font color="black">2020-05-18</font>
      </time>
    </span>
</section>
<!-- paper0: On the use of Self-supervised Pre-trained Acoustic and Linguistic
  Features for Continuous Speech Emotion Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_24.html">
      <font color="black">On the use of Self-supervised Pre-trained Acoustic and Linguistic
  Features for Continuous Speech Emotion Recognition</font>
    </a>
  </h2>
  <font color="black">著者の知る限り、このペーパーでは、wav2vecとBERTのような事前トレーニング済み機能の併用が、通常は少量のラベル付きトレーニングデータを特徴とする継続的なSERタスクの処理に非常に関連していることを示す最初の研究を紹介します。 -特徴抽出のトレーニングは、音声とテキストのコンテンツのより良い連続表現を得るためにますます研究されているアプローチです。よく知られている一致相関係数（CCC）によって評価され、私たちの実験は、0.592ではなく0.825のCCC値に到達できることを示しています。 AlloSatデータセットにword2vec単語埋め込みと組み合わせてMFCCを使用する場合。 
[概要] wav2vecとcamembertの研究者は、フランスの大規模な感情データベースであるallosatでの音声から継続的な感情認識を実行するために、私たちのデータを代表しています。最先端技術は、価数、覚醒、好みの次元を調査します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Mind Your Inflections! Improving NLP for Non-Standard Englishes with
  Base-Inflection Encoding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_25.html">
      <font color="black">Mind Your Inflections! Improving NLP for Non-Standard Englishes with
  Base-Inflection Encoding</font>
    </a>
  </h2>
  <font color="black">語彙効率を定量的に評価するための先行研究がなかったので、そうするためのメトリックを提案します。BITEを使用するモデルは、明示的なトレーニングなしで非標準の語尾変化を持つ方言によりよく一般化され、翻訳モデルはBITEでトレーニングされるとより速く収束します。私たちのエンコーディングが、人気のあるデータ駆動型サブワードトークナイザーの語彙効率を改善することを示しています。 
[概要]ブラウザが人気のあるデータ駆動型サブワードトークナイザーの語彙効率を向上させることを示します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-30">
        <br><font color="black">2020-04-30</font>
      </time>
    </span>
</section>
<!-- paper0: Dynamic Prosody Generation for Speech Synthesis using Linguistics-Driven
  Acoustic Embedding Selection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_26.html">
      <font color="black">Dynamic Prosody Generation for Speech Synthesis using Linguistics-Driven
  Acoustic Embedding Selection</font>
    </a>
  </h2>
  <font color="black">セマンティック機能と構文機能の両方の寄与を分析します。私たちの結果は、このアプローチが複雑な発話とロングフォームリーディング（LFR）の韻律と自然さを改善することを示しています。Text-to-Speech（TTS）の最近の進歩は孤立した文を検討する際の、人間に近い能力に対する品質と自然さの向上。 
[概要]人間の発話の新しい動的な動的変化と適応性を提案します。私たちの結果は、このアプローチが複雑な発話の韻律と自然さを改善することを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-02">
        <br><font color="black">2019-12-02</font>
      </time>
    </span>
</section>
<!-- paper0: Palomino-Ochoa at SemEval-2020 Task 9: Robust System based on
  Transformer for Code-Mixed Sentiment Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_27.html">
      <font color="black">Palomino-Ochoa at SemEval-2020 Task 9: Robust System based on
  Transformer for Code-Mixed Sentiment Classification</font>
    </a>
  </h2>
  <font color="black">実際、私たちのシステムは、簡単に再現できる0.755の加重F1スコア値を生成します。ソースコードと実装の詳細が利用可能になります。したがって、提出された29のシステムの中で、私たちのアプローチ（dplominopと呼ばれる）は4位にランクされています。 SemEval2020タスク9のSentimixSpanglishテストセットについて。スペイン語と英語の混合感情分類タスクを実行するための転移学習システムを紹介します。 
[概要]私たちの提案は、利用可能な最先端の言語モデルを使用しています。私たちのアプローチは、センチミックススパングリッシュテストセットで4位にランクされています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Reducing Spelling Inconsistencies in Code-Switching ASR using
  Contextualized CTC Loss -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_28.html">
      <font color="black">Reducing Spelling Inconsistencies in Code-Switching ASR using
  Contextualized CTC Loss</font>
    </a>
  </h2>
  <font color="black">既存のCTCベースのアプローチとは対照的に、コンテキストグラウンドトゥルースはモデルの推定パスから取得されるため、CCTC損失はフレームレベルの位置合わせを必要としません。文字のスペルの一貫性を促進するために、コンテキスト化された接続主義時間分類（CCTC）損失を提案します。より高速な推論を可能にするベースの非自己回帰ASR ..通常のCTC損失でトレーニングされた同じモデルと比較して、私たちの方法はCSと単一言語コーパスの両方で一貫してASRパフォーマンスを改善しました。 
[要約]文字ベースのモデルからの結果は損失に苦しんでいます。言語にリンクされています-一貫性のないスペル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-16">
        <br><font color="black">2020-05-16</font>
      </time>
    </span>
</section>
<!-- paper0: Any-to-Many Voice Conversion with Location-Relative Sequence-to-Sequence
  Modeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_29.html">
      <font color="black">Any-to-Many Voice Conversion with Location-Relative Sequence-to-Sequence
  Modeling</font>
    </a>
  </h2>
  <font color="black">BNEは音素認識機能から取得され、スペクトルの特徴から話者に依存しない、密度の高い、豊かな言語表現を抽出するために使用されます。次に、マルチスピーカーの位置相対注意ベースのseq2seq合成モデルをトレーニングして、ボトルからスペクトルの特徴を再構築します。ネック機能、生成された音声の話者ID制御のための話者表現の条件付け..seq2seqベースのモデルを使用して長いシーケンスを整列させることの難しさを軽減するために、入力スペクトル特徴を時間次元に沿ってダウンサンプリングし、合成モデルにロジスティック（MoL）注意メカニズムの離散化された混合。 
[ABSTRACT]提案されたアプローチは、ボトルネック特徴抽出器（bne）とseq2seqベースの合成モジュールを組み合わせたものです。提案されたアプローチは、任意のvcをサポートするように簡単に拡張でき、評価に従って高性能を実現します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-06">
        <br><font color="black">2020-09-06</font>
      </time>
    </span>
</section>
<!-- paper0: Combining Prosodic, Voice Quality and Lexical Features to Automatically
  Detect Alzheimer's Disease -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_30.html">
      <font color="black">Combining Prosodic, Voice Quality and Lexical Features to Automatically
  Detect Alzheimer's Disease</font>
    </a>
  </h2>
  <font color="black">両方のタスクは、韻律と音声品質に基づいて音声から28の特徴を抽出し、語彙と話者交替情報に基づいて文字起こしから51の特徴を抽出して実行されました。これは、アルツハイマー病の自動検出における有望な結果を示しています。音声および語彙の特徴..私たちの結果は、ランダムフォレスト分類器を使用して最大87.5％の分類精度を達成し、提供されたテストセットで確率的勾配降下を伴う線形回帰を使用して4.54のRMSEを達成しました。 
[概要]音声データの分析は、広告自動検出システムの開発に不可欠です。lexis、年齢、解像度、広告状態は、2つの異なるタスクを実行するためのトレーニングセットとして使用されるものの1つです。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Master Thesis: Neural Sign Language Translation by Learning Tokenization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_31.html">
      <font color="black">Master Thesis: Neural Sign Language Translation by Learning Tokenization</font>
    </a>
  </h2>
  <font color="black">現在のトークン化アプローチの調査から始め、いくつかの実験でその弱点を説明します。SL間の知識の伝達を可能にし、翻訳品質をBLEU-4で5ポイント、ROUGEスコアで8ポイント向上させることに成功しました。ソリューションを提供するために、追加の監督を活用するために、学習、マルチタスク学習、教師なしドメイン適応をこの研究に移します。 
[概要]トークン化の部分は、手話（sl）ビデオを表現して他の部分にフィードする方法に焦点を当てています。これは、世界中の研究者によって研究されています。これは、一般的な手話レベルのトークン化の開発を目指しているためです。層</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Diverse and Non-redundant Answer Set Extraction on Community QA based on
  DPPs -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_32.html">
      <font color="black">Diverse and Non-redundant Answer Set Extraction on Community QA based on
  DPPs</font>
    </a>
  </h2>
  <font color="black">日本のCQAサイトに焦点を当てたデータセットを作成し、このデータセットでの実験により、提案された方法がいくつかのベースライン方法よりも優れていることが実証されました。 BERTを使用して..このペーパーでは、回答をランク付けするのではなく、多様で非冗長な回答セットを選択するという新しいタスクを提案します。 
[ABSTRACT]私たちの方法は決定的な点過程に基づいています。それは答えの重要性と答え間の類似性を計算します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: FewJoint: A Few-shot Learning Benchmark for Joint Language Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/cs.CL/paper_33.html">
      <font color="black">FewJoint: A Few-shot Learning Benchmark for Joint Language Understanding</font>
    </a>
  </h2>
  <font color="black">また、互換性のあるFSLプラットフォームを提供して、実験のセットアップを容易にします。このペーパーでは、NLPの新しいFew-ShotLearningベンチマークであるFewJointを紹介します。これは結果の比較ではかなり非効率的であり、したがって累積的な進歩を妨げます。 
[概要]自然言語処理（nlp）でのfslの進行ははるかに遅い</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-17">
        <br><font color="black">2020-09-17</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: Focusing Phenomena in Linear Discrete Inverse Problems in Acoustics -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.AS/paper_0.html">
      <font color="black">Focusing Phenomena in Linear Discrete Inverse Problems in Acoustics</font>
    </a>
  </h2>
  <font color="black">次に、均一な線形アレイを使用して、複数のサウンドゾーンを再作成するアプリケーションを検討します。一方、その最小化により、ソースが各ポイントで選択的にフォーカスできる理想的なフォーカス状態が得られ、他のすべてのポイントでヌルが作成されます。 ..すべての場合において、周波数の関数として理想的な焦点を維持する能力は、光源または制御点の形状の比例した変化を必要とします。 
[概要]開発は音場再生のコンテキストで行われます。ここで、ソース強度は、指定された圧力場を再現するために必要な逆解です。1点でのクロストークの最大化は、システムの線形依存性につながります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-01">
        <br><font color="black">2020-11-01</font>
      </time>
    </span>
</section>
<!-- paper0: Evaluation of Error and Correlation-Based Loss Functions For Multitask
  Learning Dimensional Speech Emotion Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.AS/paper_1.html">
      <font color="black">Evaluation of Error and Correlation-Based Loss Functions For Multitask
  Learning Dimensional Speech Emotion Recognition</font>
    </a>
  </h2>
  <font color="black">これら2つの損失関数によるテスト予測の散布図でも、CCCスコアで測定された結果が確認されました。一致相関係数（CCC）損失を伴う相関ベースの損失関数を使用すると、エラーベースの損失よりもパフォーマンスが向上することがわかりました。平均二乗誤差（MSE）損失と平均絶対誤差（MAE）を使用した、平均CCCスコアの観点からの関数。損失関数の選択は、機械学習の重要な部分です。 
[概要]この論文では、スランプで一般的に使用される2つの異なる損失関数を評価しました。結果は2ミリ秒と2つのデータセットと一致しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-24">
        <br><font color="black">2020-03-24</font>
      </time>
    </span>
</section>
<!-- paper0: Tie Your Embeddings Down: Cross-Modal Latent Spaces for End-to-end
  Spoken Language Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.AS/paper_2.html">
      <font color="black">Tie Your Embeddings Down: Cross-Modal Latent Spaces for End-to-end
  Spoken Language Understanding</font>
    </a>
  </h2>
  <font color="black">エンドツーエンド（E2E）音声言語理解（SLU）システムは、音声信号から直接音声発話のセマンティクスを推測できます。異なるクロスモーダル損失にわたって、2つの公開されているE2EデータセットでCMLSモデルをトレーニングします。提案されたトリプレット損失関数が最高のパフォーマンスを達成すること。クロスモーダルスペースのないE2Eモデルと比較してそれぞれ1.4％と4％の相対的な改善を達成し、以前に公開されたCMLSモデルと比較して0.7％と1％の相対的な改善を達成します。 $ L_2 $損失を使用します。 
[概要] e2eシステムのトレーニングは、主にペアのテキストメッセージが不足しているため、依然として課題です。このシステムを使用して、テキスト埋め込みに近づくように音響埋め込みをトレーニングできます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Streaming Transformer ASR with Blockwise Synchronous Beam Search -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.AS/paper_3.html">
      <font color="black">Streaming Transformer ASR with Blockwise Synchronous Beam Search</font>
    </a>
  </h2>
  <font color="black">当社のストリーミングASRモデルは、検討対象のすべてのタスクにおいて、バッチモデルやその他のストリーミングベースのTransformerメソッドと同等またはそれ以上のパフォーマンスを実現します。 .. HKUSTおよびAISHELL-1Mandarin、LibriSpeech English、およびCSJ Japaneseタスクの評価は、提案されたストリーミングTransformerアルゴリズムが、特に知識蒸留技術を使用する場合に、単調なチャンクワイズアテンション（MoChA）を含む従来のオンラインアプローチよりも優れていることを示しています。 
[ABSTRACT]トランスフォーマーは、ブロック境界検出手法を使用してポイントを作成します。予測された各理論の信頼性スコアは、仮説の認識の終わりと繰り返されるトークンに基づいて評価されます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-25">
        <br><font color="black">2020-06-25</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-Channel Automatic Speech Recognition Using Deep Complex Unet -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.AS/paper_4.html">
      <font color="black">Multi-Channel Automatic Speech Recognition Using Deep Complex Unet</font>
    </a>
  </h2>
  <font color="black">この論文では、マルチチャネル音響モデルのフロントエンドとして、強力な複合値Unet構造の音声強調モデルであるディープコンプレックスUnet（DCUnet）のアーキテクチャを採用し、それらをマルチチャネル音響モデルに統合することを提案します。タスク学習（MTL）フレームワークと比較のためのカスケードフレームワーク..実験によると、提案されたDCUnet-MTLメソッドは、アレイ処理とシングルチャネル音響モデルを使用した従来のアプローチと比較して、相対文字エラー率（CER）を約12.2％削減します。 。一方、エコーを使用した1000時間の実世界のXiaoMiスマートスピーカーデータの認識精度を向上させるために、いくつかのトレーニング戦略を使用して提案された方法を調査します。 
[ABSTRACT]フロントTラインは、従来の信号処理に比べて有望な改善を示していますが、複数の開発システムの改善も示しています。これには、無線通信や認識精度を向上させるための学習が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: CAA-Net: Conditional Atrous CNNs with Attention for Explainable
  Device-robust Acoustic Scene Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.AS/paper_5.html">
      <font color="black">CAA-Net: Conditional Atrous CNNs with Attention for Explainable
  Device-robust Acoustic Scene Classification</font>
    </a>
  </h2>
  <font color="black">提案されたシステムには、両方ともCNNによってモデル化されたASCブランチとデバイス分類ブランチが含まれています。マルチデバイスASCに注意して条件付きアトラスCNNを提案します。アトラスCNNの中間層を視覚化して分析します。 
[ABSTRACT]畳み込みニューラルネットワーク（cnns）がascに正常に適用されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Context-aware RNNLM Rescoring for Conversational Speech Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.AS/paper_6.html">
      <font color="black">Context-aware RNNLM Rescoring for Conversational Speech Recognition</font>
    </a>
  </h2>
  <font color="black">ラティスの再スコアリングでは、隣接する文のラティスもタグワードによって最初のパスでデコードされた結果に関連付けられます。会話型音声認識は、自由な発話と長期的なコンテキスト依存性のため、困難なタスクと見なされます。 4つの異なる会話テストセットは、私たちのアプローチが、1回目のパスデコードおよび一般的なラティススコアリングと比較して、それぞれ最大13.1％および6％の相対文字エラー率（CER）の削減をもたらすことを示しています。 
[ABSTRACT]以前の作業では、長距離コンテキストのモデリングを検討しました。隣接する文を話者や意図情報などのさまざまなタグワードと連結することにより、コンテキストの依存関係をキャプチャします。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: WPD++: An Improved Neural Beamformer for Simultaneous Speech Separation
  and Dereverberation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.AS/paper_7.html">
      <font color="black">WPD++: An Improved Neural Beamformer for Simultaneous Speech Separation
  and Dereverberation</font>
    </a>
  </h2>
  <font color="black">従来のWPDの強化されたビームフォーミングモジュールと共同トレーニング用の多目的損失関数によって「WPD ++」と呼ばれる改良されたニューラルWPDビームフォーマを提案します。複雑なスペクトルドメインスケール不変信号を含む多目的損失-ノイズ比（C-Si-SNR）とマグニチュードドメイン平均二乗誤差（Mag-MSE）は、強化された音声とドライクリーン信号の必要なパワーに複数の制約を課すように適切に設計されています。ビームフォーミングモジュールが改善されました。時空間相関を利用することによって。 
[概要]ノイズキャンセリングコンポーネントは進歩する可能性があります。ノイズベースのノイズベースのノイズベースのノイズノイズベースのノイズベースのコンポーネントは引き続き利用可能です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Expanding Access to Music Technology -- Rapid Prototyping Accessible
  Instrument Solutions For Musicians With Intellectual Disabilities -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.AS/paper_8.html">
      <font color="black">Expanding Access to Music Technology -- Rapid Prototyping Accessible
  Instrument Solutions For Musicians With Intellectual Disabilities</font>
    </a>
  </h2>
  <font color="black">適応された楽器のデザインの評価は、従来の楽器や製品を評価するためのフレームワークとは大きく異なる場合がありますが、そのような焦点を絞った個別のデザインの結果にはさまざまな用途が考えられます。自由と自由の間のバランスについて慎重に検討されました。表現、および可能性のあるキュレーション-適応としての音の結果..楽器はさまざまなセンサーを採用し、結果として得られる音楽コントロールをMIDI経由でソフトウェアサウンドジェネレーターに送信します。 
[ABSTRACT]楽器によって開発され、さまざまなセンサーを使用して開発され、結果として得られた音楽コントロールをMIDI経由でソフトウェアサウンドジェネレーターに送信します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
<!-- paper0: Ultra-Lightweight Speech Separation via Group Communication -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.AS/paper_9.html">
      <font color="black">Ultra-Lightweight Speech Separation via Group Communication</font>
    </a>
  </h2>
  <font color="black">サブバンド周波数-LSTM（F-LSTM）アーキテクチャに動機付けられて、グループ通信（GroupComm）を導入します。この場合、特徴ベクトルは小さなグループに分割され、小さな処理ブロックを使用してグループ間通信が実行されます。サブバンド出力が連結されている標準のF-LSTMモデルでは、超小型モジュールがすべてのグループに並列に適用されるため、モデルサイズを大幅に縮小できます。実験結果は、強力なベースラインモデルと比較してGroupCommはすでに軽量であり、35.6分の1のパラメーターと2.3分の1の操作で同等のパフォーマンスを実現できます。 
[概要]グループ通信（groupcomm）は、35.6倍少ないパラメータと2.3倍少ない操作で同等のパフォーマンスを達成できます。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-17">
        <br><font color="black">2020-11-17</font>
      </time>
    </span>
</section>
<!-- paper0: Audio-visual Multi-channel Recognition of Overlapped Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.AS/paper_10.html">
      <font color="black">Audio-visual Multi-channel Recognition of Overlapped Speech</font>
    </a>
  </h2>
  <font color="black">重複した音声の自動音声認識（ASR）は、これまで非常に困難な作業でした。分離コンポーネントと認識コンポーネント間のエラーコストの不一致を減らすために、接続者の時間分類（CTC）損失関数を使用して共同で微調整しました。スケール不変の信号対雑音比（Si-SNR）エラーコストを使用したマルチタスク基準補間..音響信号の破損に対する視覚モダリティの不変性に動機付けられて、この論文は、緊密に機能するオーディオビジュアルマルチチャネルオーバーラップ音声認識システムを提示します。統合された分離フロントエンドと認識バックエンド。 
[概要]提案されたマルチチャネルavsrシステムはベースラインオーディオを上回ります-asrシステムのみが最大6.81％（相対26. 83％）および22. 22. 22. 87％。提案されたマルチトールavsrシステム35,000ドルの費用で開発されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-18">
        <br><font color="black">2020-05-18</font>
      </time>
    </span>
</section>
<!-- paper0: Dynamic Prosody Generation for Speech Synthesis using Linguistics-Driven
  Acoustic Embedding Selection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.AS/paper_11.html">
      <font color="black">Dynamic Prosody Generation for Speech Synthesis using Linguistics-Driven
  Acoustic Embedding Selection</font>
    </a>
  </h2>
  <font color="black">しかし、人間のようなコミュニケーションを実現するためにまだ不足しているのは、人間の音声の動的な変化と適応性です。この作業は、TTSシステムで、特に次のような文体の音声に対して、より動的で自然なイントネーションを実現する問題を解決しようとします。ニュースキャスターのスピーチスタイル..Text-to-Speech（TTS）の最近の進歩により、孤立した文を検討する際の人間に近い能力の品質と自然さが向上しました。 
[概要]人間の発話の新しい動的な動的変化と適応性を提案します。私たちの結果は、このアプローチが複雑な発話の韻律と自然さを改善することを示しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-12-02">
        <br><font color="black">2019-12-02</font>
      </time>
    </span>
</section>
<!-- paper0: A study on more realistic room simulation for far-field keyword spotting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.AS/paper_12.html">
      <font color="black">A study on more realistic room simulation for far-field keyword spotting</font>
    </a>
  </h2>
  <font color="black">ソースコードはPyroomacousticsパッケージで利用可能になり、他の人がこれらの手法を作業に組み込むことができるようになります。ドメイン内データを微調整せずに遠方場キーワードスポッティングシステムをトレーニングするためのより現実的な部屋シミュレーションの影響を調査します。クリーンでノイズの多い遠方場条件下での再記録のホールドアウトセットで、一般的に使用されている（単一吸収係数）画像ソース法に比べて最大$ 35.8 \％$の相対的な改善を示します。 
[要約]クリーンでノイズの多い遠方界条件の再記録のホールドアウトセットで、最大35ドルを示しました。一般的に使用されている部屋のインパルス応答（riyr）画像ソース方式よりも多くの改善を示しました。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-04">
        <br><font color="black">2020-06-04</font>
      </time>
    </span>
</section>
<!-- paper0: Reducing Spelling Inconsistencies in Code-Switching ASR using
  Contextualized CTC Loss -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.AS/paper_13.html">
      <font color="black">Reducing Spelling Inconsistencies in Code-Switching ASR using
  Contextualized CTC Loss</font>
    </a>
  </h2>
  <font color="black">既存のCTCベースのアプローチとは対照的に、コンテキストグラウンドトゥルースはモデルの推定パスから取得されるため、CCTC損失はフレームレベルの位置合わせを必要としません。文字のスペルの一貫性を促進するために、コンテキスト化された接続主義時間分類（CCTC）損失を提案します。より高速な推論を可能にするベースの非自己回帰ASR ..通常のCTC損失でトレーニングされた同じモデルと比較して、私たちの方法はCSと単一言語コーパスの両方で一貫してASRパフォーマンスを改善しました。 
[要約]文字ベースのモデルからの結果は損失に苦しんでいます。言語にリンクされています-一貫性のないスペル</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-16">
        <br><font color="black">2020-05-16</font>
      </time>
    </span>
</section>
<!-- paper0: Any-to-Many Voice Conversion with Location-Relative Sequence-to-Sequence
  Modeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.AS/paper_14.html">
      <font color="black">Any-to-Many Voice Conversion with Location-Relative Sequence-to-Sequence
  Modeling</font>
    </a>
  </h2>
  <font color="black">次に、マルチスピーカーの位置相対注意ベースのseq2seq合成モデルをトレーニングして、ボトルネックの特徴からスペクトルの特徴を再構築し、生成された音声の話者ID制御のために話者の表現を条件付けます。BNEは音声認識機能から取得され、スペクトルの特徴から話者に依存しない、密度の高い、豊かな言語表現を抽出するために利用されます。提案されたVCアプローチは、任意のVC（ワンショットVCとも呼ばれます）をサポートするように簡単に拡張でき、それに応じて高性能を実現します。客観的および主観的な評価に。 
[ABSTRACT]提案されたアプローチは、ボトルネック特徴抽出器（bne）とseq2seqベースの合成モジュールを組み合わせたものです。提案されたアプローチは、任意のvcをサポートするように簡単に拡張でき、評価に従って高性能を実現します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-09-06">
        <br><font color="black">2020-09-06</font>
      </time>
    </span>
</section>
<!-- paper0: Towards Robust Deep Neural Networks for Affect and Depression
  Recognition from Speech -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.AS/paper_15.html">
      <font color="black">Towards Robust Deep Neural Networks for Affect and Depression
  Recognition from Speech</font>
    </a>
  </h2>
  <font color="black">EmoAudioNetのコードは、GitHub（https://github.com/AliceOTHMANI/EmoAudioNet）で公開されています。インテリジェントな監視システムと感情コンピューティングアプリケーションは、ヘルスケアを強化するために近年出現しました。この論文では、音声からの感情とうつ病の認識のためのEmoAudioNetと呼ばれる新しいディープニューラルネットワークアーキテクチャを紹介します。 
[ABSTRACT]インテリジェントシステムは、初期段階でmdd診断を強化します。注目度の高いインテリジェントシステムは、診断を強化する可能性があります。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-11-01">
        <br><font color="black">2019-11-01</font>
      </time>
    </span>
</section>
<!-- paper0: Vertical-Horizontal Structured Attention for Generating Music with
  Chords -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-11-19/eess.AS/paper_16.html">
      <font color="black">Vertical-Horizontal Structured Attention for Generating Music with
  Chords</font>
    </a>
  </h2>
  <font color="black">私たちのモデルは、時間の経過に伴う時間的関係だけでなく、キー間の構造関係もキャプチャします。さらに、この方法は、五度圏の構成を維持し、メジャーキーとマイナーキーを音程ベクトルから区別し、意味のある構造を表すため、音楽理論と一致します。音楽フレーズ間..音楽では、複数の音符で構成されるコードは、複数の楽器の混合または単一の楽器の複数のキーの組み合わせのいずれかから発生します。 
[概要]音楽の作成はジェネレータテキストとは異なります。モデルのパフォーマンスはベースラインのmusicvaeよりも優れています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-11-18">
        <br><font color="black">2020-11-18</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
