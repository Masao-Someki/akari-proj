<!DOCTYPE html>
<html lang="ja-jp">
<head>
<meta charset="utf-8">
<meta name="generator" content="Akari" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/normalize.css">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<title>Akari-2020-09-01の記事</title>
<link rel='stylesheet' type='text/css' href='../../css/normalize.css'>
<link rel='stylesheet' type='text/css' href='../../css/skelton.css'>
<link rel='stylesheet' type='text/css' href='../../css/field.css'>
<body>
<div class="container">
<!--
	header
	-->

	<nav class="navbar navbar-expand-lg navbar-dark bg-dark">
	  <a class="navbar-brand" href="index.html" align="center">
			<font color="white">Akari<font></a>
	</nav>

<br>
<br>
<div class="container" align="center">
	<header role="banner">
			<div class="header-logo">
				<a href="../../index.html"><img src="../../images/akari.png" width="100" height="100"></a>
			</div>
	</header>
<div>

<br>
<br>

<nav class="navbar navbar-expand-lg navbar-light bg-dark">
  Menu
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item active">

        <a class="nav-link" href="../../index.html"><font color="white">Home</font></a>
      </li>
			<li class="nav-item">
				<a id="about" class="nav-link" href="../../teamAkariとは.html"><font color="white">About</font></a>
			</li>
			<li class="nav-item">
				<a id="contact" class="nav-link" href="../../contact.html"><font color="white">Contact</font></a>
			</li>
			<li class="nav-item">
				<a id="newest" class="nav-link" href="../../list/newest.html"><font color="white">New Papers</font></a>
			</li>
			<li class="nav-item">
				<a id="back_number" class="nav-link" href="../../list/backnumber.html"><font color="white">Back Number</font></a>
			</li>
    </ul>
  </div>
</nav>


<!--
	fields
	-->
<div class="topnav">
<!-- field: cs.SD -->
  <li class="hidden_box">
    <label for="label0">
<font color="black">cs.SD</font>
  </label></li>
<!-- field: eess.IV -->
  <li class="hidden_box">
    <label for="label1">
<font color="black">eess.IV</font>
  </label></li>
<!-- field: cs.CV -->
  <li class="hidden_box">
    <label for="label2">
<font color="black">cs.CV</font>
  </label></li>
<!-- field: cs.CL -->
  <li class="hidden_box">
    <label for="label3">
<font color="black">cs.CL</font>
  </label></li>
<!-- field: eess.AS -->
  <li class="hidden_box">
    <label for="label4">
<font color="black">eess.AS</font>
  </label></li>
</div>

<!--
 horizontal line between field and titles.
 -->
<!--
分野と論文の間の線
-->

<br><hr><br>
<!-- 
 papers 
 -->
<main role="main">
  <div class="hidden_box">
    <input type="checkbox" id="label0"/>
    <div class="hidden_show">
<!-- paper0: Neural Granular Sound Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.SD/paper_0.html">
      <font color="black">Neural Granular Sound Synthesis</font>
    </a>
  </h2>
  <font color="black">また、元の粒子が合成用に保存されないことも意味します。このモデルは、ピッチのかかったノートやピッチのないドラムや環境ノイズなど、さまざまなタイプのライブラリに適用できます。この提案の主な利点は、結果として得られる粒子空間が反転可能であることです。つまり、その次元を移動するときに、サウンドを継続的に合成できます。 
[ABSTRACT]グラニュラーシンセシスは、サウンドを制御するために使用するようにプログラムできます。しかし、このグレインスペースの品質は、ディスクリプターの品質によって制限されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-04">
        <br><font color="black">2020-08-04</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label1"/>
    <div class="hidden_show">
<!-- paper0: Augmented Reality-Based Advanced Driver-Assistance System for Connected
  Vehicles -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/eess.IV/paper_0.html">
      <font color="black">Augmented Reality-Based Advanced Driver-Assistance System for Connected
  Vehicles</font>
    </a>
  </h2>
  <font color="black">ただし、ヒューマンマシンの連携とインターフェイス、つまり高度な運転者支援システム（ADAS）として運転者へのガイダンス情報を視覚化する方法に焦点を当てた研究は少なくなります。この使用例として、信号なし交差点シナリオが採用されています。ドライバーは、交差点で完全に停止することなく、ARガイダンスの下で交差点を横断する接続車両を運転できます。シミュレーション環境は、サンフランシスコの道路ネットワークに基づくUnityゲームエンジンに組み込まれ、 the-loop（HITL）シミュレーションは、移動時間とエネルギー消費に関する提案システムの有効性を検証するために行われます。 
[ABSTRACT]拡張現実（ar）-ベースのadasは、複数の接続された車両によって協調的に計算されたガイダンス情報を視覚化できます。アップグレードシステムは、サンフランシスコの道路ネットワークに基づくシステムに組み込まれています。旅行時間とエネルギー消費に関する提案されたシステムの有効性を検証する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Evaluating Knowledge Transfer In Neural Network for Medical Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/eess.IV/paper_1.html">
      <font color="black">Evaluating Knowledge Transfer In Neural Network for Medical Images</font>
    </a>
  </h2>
  <font color="black">生徒のネットワークが小さなデータセット（ターゲットデータセット）でトレーニングされている場合、および教師と生徒のドメインが異なる場合に、提案されたネットワークのパフォーマンスを調査します。トレーニングデータのサイズが小さいことに加えて、教師を優先することの明確な利点も示します-転移学習などの他の知識転移技術と比較した、医用画像設定におけるクロスドメイン知識転移の学生学習フレームワーク。特に、教師と学生の学習フレームワークは、CNNモデルのROC曲線（AUC）の下の領域を改善します。 CheXpert（n = 5k）の小さなサンプルは4％、ChestX-ray8（n = 5.6k）のサンプルは9％。 
[要約]医療画像処理タスクで大規模な状態のデータが不足していることが提案されています。これは、注釈付きの大規模な画像データが不足しているためです。これらには、学生ネットワークの実装におけるネットワークの動作が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Switchable Deep Beamformer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/eess.IV/paper_2.html">
      <font color="black">Switchable Deep Beamformer</font>
    </a>
  </h2>
  <font color="black">Bモード集束超音波を使用した実験結果は、さまざまなアプリケーションに対して提案された方法の柔軟性と有効性を確認します。残念ながら、現在の技術では、アプリケーションごとに個別のビームフォーマーをトレーニングして保存する必要があり、大量のスキャナーリソースを必要とします。ビームフォーマーは、画像後処理アルゴリズムをビームフォーミングと組み合わせることができるという点で多用途です。 
[ABSTRACT]ディープビームフォーマーは、画像後処理アルゴリズムをビームフォーミングと組み合わせることができるという点で用途が広い</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: A Novel Measure to Evaluate Generative Adversarial Networks Based on
  Direct Analysis of Generated Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/eess.IV/paper_3.html">
      <font color="black">A Novel Measure to Evaluate Generative Adversarial Networks Based on
  Direct Analysis of Generated Images</font>
    </a>
  </h2>
  <font color="black">ここでは、他の分類子への入力として使用する代わりに、画像を直接分析してGANを評価する基本的な方法を検討します。3）多様性：生成された画像は互いに異なります。GANのパフォーマンスは、 3つの側面による画像ジェネレータ：1）創造性：実画像の非複製。 
[要約]最近の多くの論文では、画像処理のさまざまな分野におけるガンの理論と応用について取り上げています。ガンは理想的なガンの3つの側面に基づいており、ガンのパフォーマンスを評価するために類似度スコア（ls）を設計し、それを適用しています。いくつかの典型的なガンを評価する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-27">
        <br><font color="black">2020-02-27</font>
      </time>
    </span>
</section>
<!-- paper0: Extracting full-field subpixel structural displacements from videos via
  deep learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/eess.IV/paper_4.html">
      <font color="black">Extracting full-field subpixel structural displacements from videos via
  deep learning</font>
    </a>
  </h2>
  <font color="black">結果は、完全で疎なモーションフィールドの監視により、訓練されたネットワークが十分なテクスチャコントラストを持つピクセルとそのサブピクセルモーションを識別できることを示しています。このペーパーでは、畳み込みニューラルネットワーク（CNN）に基づくディープラーニングフレームワークを開発します。ビデオからフルフィールドサブピクセル構造変位のリアルタイム抽出を可能にします。トレーニング済みネットワークのパフォーマンスは、他の構造のさまざまなビデオでテストされ、フルフィールドモーション（変位時間履歴など）を抽出します。訓練されたネットワークは、十分なテクスチャコントラストを持つピクセルの全フィールド微妙な変位を正確に抽出する一般化可能性を備えています。 
[ABSTRACT] 2つの新しいcnnアーキテクチャが1つのラボのデータセットでテストされています。ネットワークは、十分なテクスチャとそのサブピクセル履歴を持つモデルを識別できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Left atrial ejection fraction estimation using SEGANet for fully
  automated segmentation of CINE MRI -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/eess.IV/paper_5.html">
      <font color="black">Left atrial ejection fraction estimation using SEGANet for fully
  automated segmentation of CINE MRI</font>
    </a>
  </h2>
  <font color="black">私たちの仕事は、マルチスライスCINE MRIからLAボリュームと機能バイオマーカーを自動的に推定する可能性を切り開き、現在のシングルスライス法の制限を回避し、AF患者の心房機能の特性を改善します。LAEFとaEFも一致しています文献値を使用し、AF患者では健康なボランティアよりも有意に高いです。SEGANetは、専用のデータ増強スキームを使用してトレーニングされ、LAをすべての心臓フェーズにわたって、短軸動的（CINE）磁気共鳴画像（MRI）で取得しました。完全な心臓カバー。 
[要約]体積機能バイオマーカーの分析に使用されるラバイオマーカーのメトリック。これらは畳み込みニューラルネットワーク（ダイネット）に基づいていました。これらには、推定されたラefおよびaef（0）と0.5 m（0.5％）が含まれていました。 la ef（aef）が古いデータにリンクされているという事実に基づいて</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Plug-and-Play Image Restoration with Deep Denoiser Prior -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/eess.IV/paper_6.html">
      <font color="black">Plug-and-Play Image Restoration with Deep Denoiser Prior</font>
    </a>
  </h2>
  <font color="black">次に、モジュラーパーツとして深いノイズ除去器を半分の二次分割ベースの反復アルゴリズムにプラグインして、さまざまな画像復元の問題を解決します。ソースコードはhttps://github.com/cszn/DPIRで入手できます。プラグアンドプレイの画像復元の限界に合わせて、非常に柔軟で効果的なCNNノイズ除去器をトレーニングすることにより、事前にベンチマークの深いノイズ除去器を設定しました。 
[要約]ノイズ除去器は非常に柔軟なモデルモデルモデルであり、効果的なcnn denoiser.itはプラグアンドプレイの限界を押し上げるために使用されます。また、イメージの復元を使用します。新しいモデルの実験モデルを作成するためにも使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: BIAS: Transparent reporting of biomedical image analysis challenges -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/eess.IV/paper_7.html">
      <font color="black">BIAS: Transparent reporting of biomedical image analysis challenges</font>
    </a>
  </h2>
  <font color="black">BIASステートメントは、適用分野、画像モダリティ、または評価されたタスクカテゴリに関係なく、生物医学画像分析の課題の報告の透明性を向上させることを目的としています。この記事では、BIASステートメントの作成方法について説明し、生物医学画像分析の作成者が作成したチェックリストを示します。チャレンジについては、チャレンジに関する論文をレビューに含める際に提出に含めることをお勧めします。チェックリストの目的は、レビュープロセスを標準化および促進し、関連情報を明確にすることで、チャレンジ結果の解釈可能性と再現性を高めることです。 
[要約]ラボの画像分析の課題（バイアス）イニシアチブにより、課題の報告に関する一連の推奨事項が作成されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-09">
        <br><font color="black">2019-10-09</font>
      </time>
    </span>
</section>
<!-- paper0: Multimodal Image-to-Image Translation via Mutual Information Estimation
  and Maximization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/eess.IV/paper_8.html">
      <font color="black">Multimodal Image-to-Image Translation via Mutual Information Estimation
  and Maximization</font>
    </a>
  </h2>
  <font color="black">さらに、私たちの方法は、ソースドメインのコンテンツとターゲットドメインのスタイルの絡み合いを無料で解消します。最先端の方法と比較して、さまざまなベンチマークの画像から画像への変換データセットに対して、監視ありおよび監視なしの設定で実験を行います、マルチモーダルで高品質の結果を達成するための方法の有効性とシンプルさを示しています。このホワイトペーパーでは、潜在コードと条件付き生成敵対ネットワークの出力画像。 
[要旨]さまざまなベンチマーク画像からスポットへの方法で、監視ありおよび監視なしの設定で実験を行います。この方法では、両方のソース画像ドメインからターゲット画像ドメインへの片側変換モデルを学習するだけで済みます。教師ありまたは教師なしマルチモーダル画像-から-画像への変換</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-08">
        <br><font color="black">2020-08-08</font>
      </time>
    </span>
</section>
<!-- paper0: Unpaired Learning of Deep Image Denoising -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/eess.IV/paper_9.html">
      <font color="black">Unpaired Learning of Deep Image Denoising</font>
    </a>
  </h2>
  <font color="black">クリーンでノイズの多い画像のペアになっていないセットからブラインド画像のノイズ除去ネットワークを学習するタスクを調査します。ノイズ除去ネットワークのペアになっていない学習を容易にするために、この論文では、自己教師あり学習と知識抽出を組み込むことによる2段階のスキームを紹介します。知識の抽出については、最初に学習したノイズモデルをクリーンな画像に適用して、トレーニング画像のペアのセットを合成し、最初の段階で実際のノイズの多い画像と対応するノイズ除去結果を使用して、別のペアのセットを形成します。 
[要約]このペーパーでは、自己管理学習と知識抽出を組み合わせた2段階のスキームを示しました。たとえば、1x1のノイズのあるクリーンな画像を積み重ねてネットワークを採用し、各画像のノイズレベルマップを推定しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Compensation Tracker: Data Association Method for Lost Object -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/eess.IV/paper_10.html">
      <font color="black">Compensation Tracker: Data Association Method for Lost Object</font>
    </a>
  </h2>
  <font color="black">これは、提案された方法がモデルの追跡パフォーマンスを効果的に改善できることを示しています。特に、高密度シナリオの2020データセットでは、マルチオブジェクト追跡の精度が66％に達しました。実験では、このホワイトペーパーで設計された補正トラッカーを使用した後、 、評価指標はMOTチャレンジデータセットでさまざまな程度で改善されています。 
[要約]新しい論文は、molmanフィルターと予測修正に基づいた補正トラッカーを開発しました。これは、密な方向のデータセットのデータセットに基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-27">
        <br><font color="black">2020-08-27</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label2"/>
    <div class="hidden_show">
<!-- paper0: Introducing Representations of Facial Affect in Automated Multimodal
  Deception Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_0.html">
      <font color="black">Introducing Representations of Facial Affect in Automated Multimodal
  Deception Detection</font>
    </a>
  </h2>
  <font color="black">ユニモーダルサポートベクターマシン（SVM）およびSVMベースのマルチモーダルフュージョンメソッドを実験して、効果的な機能、モダリティ、および欺瞞を検出するためのモデリングアプローチを特定しました。モデルでの顔の影響の重要性の実証は、自動化の将来の開発に影響を与え、動機付けします。偽装やその他の社会的行動を実際にモデル化して検出するための感情認識機械学習アプローチ。すべての動画で、虚偽的で真実な話者は、顔の価数と顔の覚醒に大きな違いを示し、感情と感情に関する既存の心理理論に計算上のサポートを提供しています欺くこと。 
[要旨] aff-wildデータベースでトレーニングされたシステムを使用して、話し手から顔の価数と顔の覚醒の連続的な表現を抽出しました。顔の影響は、テスト時に91％のaucを達成する最高のパフォーマンスのマルチモーダルアプローチ（アダプティブブースティング）に貢献しましたトレーニングセットに参加していないスピーカー</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Receptive Multi-granularity Representation for Person Re-Identification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_1.html">
      <font color="black">Receptive Multi-granularity Representation for Person Re-Identification</font>
    </a>
  </h2>
  <font color="black">この目的に向けて、ローカルパーティションは、均一なストライプの有意性のバランスが取れたアクティブ化を使用して、適応的にプールされます。データセット内およびデータセット間の評価に関する広範な実験により、提案されたアプローチの有効性が実証されています。境界ボックス内で人が出現する領域の差異。 
[ABSTRACT]現在のストライプベースの機能学習アプローチは、優れた精度を提供しましたが、多様性、サンプリング、および堅牢性の間で適切なトレードオフを行いません。これは、リジッドパーティションとミスアラインメントの間の競合に対する部分的なセマンティックの不一致の影響を簡単に受けます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Visual Camera Re-Localization from RGB and RGB-D Images Using DSAC -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_2.html">
      <font color="black">Visual Camera Re-Localization from RGB and RGB-D Images Using DSAC</font>
    </a>
  </h2>
  <font color="black">ニューラルネットワークは、いわゆるシーン座標を予測します。フレームワークは、ディープニューラルネットワークと完全に微分可能なポーズ最適化で構成されています。入力画像と環境の3Dシーン空間との密な対応。 
[ABSTRACT]システムは柔軟です、rtt。 。 。システムは応答性があります。システムはトレーニングに使用できますが、必須ではありません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-27">
        <br><font color="black">2020-02-27</font>
      </time>
    </span>
</section>
<!-- paper0: Adversarial Patch Camouflage against Aerial Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_3.html">
      <font color="black">Adversarial Patch Camouflage against Aerial Detection</font>
    </a>
  </h2>
  <font color="black">最初に、無人の空中監視の使用例にパッチベースの敵対的攻撃を適用します。パッチは大きな軍事資産の上に置かれ、画像上を走る自動検出器からそれらを偽装します。軍事資産を見えないように隠す従来の方法たとえば、カモフラージュネットを使用することにより、カモフラージュです。特に、敵対的な攻撃は、画像内の人物検出を禁止するために成功裏に実証されており、特定のパターンが人物の前で押されたパッチを必要とし、それにより、本質的に人物を検出器に偽装します。 
[要約]別のタイプのカモフラージュは、自動オブジェクト検出器の直接の誤解を招くものです。これは、画像内の人の検出を禁止するために成功裏に実証されています。これには、人の前で特定のパターンを保持するパッチが必要です。検出器</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Cheaper Pre-training Lunch: An Efficient Paradigm for Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_4.html">
      <font color="black">Cheaper Pre-training Lunch: An Efficient Paradigm for Object Detection</font>
    </a>
  </h2>
  <font color="black">これらの設計には、空間利用を改善するための新しい入力パターンだけでなく、事前学習済みモデルの有効受容野を拡張するための新しい学習目標も含まれています。この論文では、一般的かつ効率的な事前学習パラダイム、モンタージュ事前学習を提案します。オブジェクト検出のためのトレーニング。モンタージュ事前トレーニングの効率と効果は、MS-COCOデータセットでの広範な実験によって検証されます。この結果は、モンタージュ事前トレーニングを使用したモデルが、同等以上のパフォーマンスを達成できることを示しています。 ImageNet事前トレーニングと比較した検出パフォーマンス。 
[ABSTRACT]モンタージュの事前トレーニングでは、ターゲット検出データセットのみが必要です。モデルではerfを使用しています-適応型の密な分類戦略</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-04-25">
        <br><font color="black">2020-04-25</font>
      </time>
    </span>
</section>
<!-- paper0: Evaluating Knowledge Transfer In Neural Network for Medical Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_5.html">
      <font color="black">Evaluating Knowledge Transfer In Neural Network for Medical Images</font>
    </a>
  </h2>
  <font color="black">学生ネットワークが小さなデータセット（ターゲットデータセット）でトレーニングされている場合、および教師と学生のドメインが異なる場合に、提案されたネットワークのパフォーマンスを調査します。特に、教師と生徒の学習フレームワークはROC曲線（AUC）の下の領域を改善しますCheXpert（n = 5k）の小さなサンプルと4％のChestX-ray8（n = 5.6k）のCNNモデルの比較。しかし、ディープラーニングを医用画像にうまく統合するにはまだ課題があります。注釈付きの大きな画像データの不足によるタスク。 
[要約]医療画像処理タスクで大規模な状態のデータが不足していることが提案されています。これは、注釈付きの大規模な画像データが不足しているためです。これらには、学生ネットワークの実装におけるネットワークの動作が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Switchable Deep Beamformer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_6.html">
      <font color="black">Switchable Deep Beamformer</font>
    </a>
  </h2>
  <font color="black">Bモードに焦点を合わせた超音波を使用した実験結果は、さまざまなアプリケーションに対する提案された方法の柔軟性と有効性を確認します。特に、スイッチはAdaptive Instanace Normalization（AdaIN）レイヤーを介して実装されるため、AdaINを変更するだけでさまざまな出力を生成できます。コード..ディープニューラルネットワークを使用したディープビームフォーマの最近の提案は、適応型および圧縮型ビームフォーマの計算効率の高い代替手段として大きな注目を集めています。 
[ABSTRACT]ディープビームフォーマーは、画像後処理アルゴリズムをビームフォーミングと組み合わせることができるという点で用途が広い</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: RecSal : Deep Recursive Supervision for Visual Saliency Prediction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_7.html">
      <font color="black">RecSal : Deep Recursive Supervision for Visual Saliency Prediction</font>
    </a>
  </h2>
  <font color="black">さらに、この追加情報を利用するための新しいアーキテクチャも設計し、追加の監視がない基本モデルよりも優れたパフォーマンスを実現していることを示しています。この情報を生物学にヒントを得た方法で利用すると、使用せずに予測パフォーマンスを向上させることができます。膨大な数のパラメータを持つモデル。最先端の顕著性予測手法は、モデルアーキテクチャまたは損失関数に基づいて開発されます。 1つのターゲット顕著性マップを生成するトレーニング中。 
[ABSTRACT]公開されている顕著性予測データセットを使用して、最終的な集計マップだけでなく、各刺激の詳細情報を作成できます。追加のコンテキストを提供するために、（a）領域固有の顕著性と（b）パラメータの統計を抽出して使用することを提案します私たちのネットワークへ</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: A Novel Measure to Evaluate Generative Adversarial Networks Based on
  Direct Analysis of Generated Images -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_8.html">
      <font color="black">A Novel Measure to Evaluate Generative Adversarial Networks Based on
  Direct Analysis of Generated Images</font>
    </a>
  </h2>
  <font color="black">ここでは、他の分類子への入力として使用する代わりに、画像を直接分析してGANを評価する基本的な方法を検討します。3）多様性：生成された画像は互いに異なります。GANはいくつかの異なる画像を生成するべきではありません繰り返し画像。 
[要約]最近の多くの論文では、画像処理のさまざまな分野におけるガンの理論と応用について取り上げています。ガンは理想的なガンの3つの側面に基づいており、ガンのパフォーマンスを評価するために類似度スコア（ls）を設計し、それを適用しています。いくつかの典型的なガンを評価する</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-02-27">
        <br><font color="black">2020-02-27</font>
      </time>
    </span>
</section>
<!-- paper0: An Integrated Approach to Produce Robust Models with High Efficiency -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_9.html">
      <font color="black">An Integrated Approach to Produce Robust Models with High Efficiency</font>
    </a>
  </h2>
  <font color="black">ただし、このスパースはアドバサルリアルトレーニングでは構造化されていません。敵対的なトレーニングがDNNのクリーンな画像の精度とスパースの構造を損なうという問題を解決するために、DNNが自然な精度を維持し、チャネルを改善するのに役立つトレードオフ損失関数を設計します。 sparsity ..この作業では、収束緩和量子化アルゴリズムBinary-Relax（BR）を、Feynman-Kac Formalism（EnResNet）を介して堅牢な敵対訓練済みモデルResNets Ensembleに適用することにより、両方の機能を取得しようとします。 
[ABSTRACT]敵対的トレーニングはdnnを堅牢にする最も一般的な方法ですが、敵対的トレーニングはdnnsを緩和する最も効果的な方法です。このモデルは、チャネルのスパース性を改善し、自然な精度を維持するように設計されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: RESA: Recurrent Feature-Shift Aggregator for Lane Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_10.html">
      <font color="black">RESA: Recurrent Feature-Shift Aggregator for Lane Detection</font>
    </a>
  </h2>
  <font color="black">RESAは、レーンの強力な形状優先度を利用し、行と列にまたがるピクセルの空間関係をキャプチャします。レーン注釈に固有のまばらな監視信号でも、レーン検出タスクは依然として困難です。スライスごとの情報伝達の助けを借りて、RESAは、外観の手掛かりが弱い困難なシナリオでレーンを正確に推測できます。 
[ABSTRACT] cnnのwi-fiは、生の画像から微妙な車線の特徴をキャッチするために一般的なシーンでネットワークがトレーニングされた最初の例です。スライスごとの情報通信の助けを借りて、resaは難しいシナリオで車線を正確に推測できます弱い手掛かりで</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Shape Defense -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_11.html">
      <font color="black">Shape Defense</font>
    </a>
  </h2>
  <font color="black">コードは次の場所にあります：〜\ url {https://github.com/aliborji/Shapedefence.git} ..さらに、a）エッジ情報は他の敵対的なトレーニング方法にも役立つこと、およびb）エッジでトレーニングされたCNNは拡張入力は、RGB画像のみでトレーニングされたCNNよりも、モーションブラー、インパルスノイズ、JPEG圧縮などの自然な画像の破損に対してより堅牢です。エッジは不変のわずかな摂動に対して不変であるという観察に基づいて、2つのアルゴリズムが提案されています。 
[要旨]形状モデレートをCNNに組み込んで堅牢性を向上させる方法を探る</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: iLGaCo: Incremental Learning of Gait Covariate Factors -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_12.html">
      <font color="black">iLGaCo: Incremental Learning of Gait Covariate Factors</font>
    </a>
  </h2>
  <font color="black">この論文では、歩行認識のための共変量因子の最初のインクリメンタル学習アプローチであるiLGaCoを提案します。そこでは、データセット全体を使用して、最初から再トレーニングすることなく新しい情報でディープモデルを更新できます。CASIAでiLGaCoを評価します- 2つの増分方法でのBデータセット：新しい視点の追加と新しい歩行条件の追加..含める必要がある場合は、古いデータサンプルと新しいデータサンプルでモデルを再トレーニングする必要があります。 
[ABSTRACT]ディープラーニングに基づく歩行認識アプローチは、トレーニングデータセット全体を使用してトレーニングされます。含める必要があるため、新旧のデータサンプルで再トレーニングする必要があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Reinforced Axial Refinement Network for Monocular 3D Object Detection -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_13.html">
      <font color="black">Reinforced Axial Refinement Network for Monocular 3D Object Detection</font>
    </a>
  </h2>
  <font color="black">これには、いくつかのステップの後に報酬を得るポリシーを設計する必要があるため、強化学習を採用してそれを最適化します。サンプリングの効率を向上させるために、最初の予測から始め、それを各ステップで1つの3dパラメーターが変更されます。提案されているフレームワークである強化軸細分割ネットワーク（RAR-Net）は、既存の単眼3D検出方法に自由に統合できる後処理段階として機能し、KITTIデータセットのパフォーマンスを向上させます。追加の計算コストが少ない。 
[ABSTRACT] 3D検出は、深さによる情報の損失にある大きな困難を伴う悪質な問題です-診断カメラ。サンプリングの効率を向上させるために、最初の予測から始めて、徐々にそれを地上の真実に向かって洗練することを提案します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Extracting full-field subpixel structural displacements from videos via
  deep learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_14.html">
      <font color="black">Extracting full-field subpixel structural displacements from videos via
  deep learning</font>
    </a>
  </h2>
  <font color="black">変位は十分なテクスチャコントラストのある領域でのみ信頼できるため、テクスチャマスクによって引き起こされるモーションフィールドのスパース性は、ネットワークアーキテクチャの設計と損失関数の定義を介して考慮されます。トレーニング済みネットワークのパフォーマンスは、他の構造のさまざまなビデオでテストされますフルフィールドモーション（変位時間履歴など）を抽出します。これは、訓練されたネットワークが十分なテクスチャコントラストを持つピクセルのフルフィールド微妙な変位を正確に抽出する一般化可能性を持っていることを示します。この論文は、畳み込みニューラルビデオからのフルフィールドサブピクセル構造変位のリアルタイム抽出を可能にするネットワーク（CNN）。 
[ABSTRACT] 2つの新しいcnnアーキテクチャが1つのラボのデータセットでテストされています。ネットワークは、十分なテクスチャとそのサブピクセル履歴を持つモデルを識別できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: A Multisite, Report-Based, Centralized Infrastructure for Feedback and
  Monitoring of Radiology AI/ML Development and Clinical Deployment -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_15.html">
      <font color="black">A Multisite, Report-Based, Centralized Infrastructure for Feedback and
  Monitoring of Radiology AI/ML Development and Clinical Deployment</font>
    </a>
  </h2>
  <font color="black">画像の表示、ディクテーション、自然言語処理（NLP）および画像所見とレポート間のハイパーリンクの作成を統合するインタラクティブな放射線レポートアプローチにより、日常的な解釈中にローカライズされたラベルが提供されます。リソース要件は、専用の回顧的専門家によるラベル付けに比べて大幅に削減されます。この方法は、市販後調査および外部データに対して提案されている規制要件に対応します。 
[ABSTRACT]インタラクティブな放射線レポートアプローチは、画像表示、ディクテーション、自然言語処理、画像所見とレポート間のハイパーリンクの作成を組み合わせたものです。この方法は、アルゴリズムのパフォーマンスを監視するための実用的で効率的なメカニズムを提供します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Robust Image Retrieval-based Visual Localization using Kapture -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_16.html">
      <font color="black">Robust Image Retrieval-based Visual Localization using Kapture</font>
    </a>
  </h2>
  <font color="black">コードとデータセットはhttps://github.com/naver/kaptureにあります。詳細、更新、ニュースはhttps://europe.naverlabs.com/research/3d-vision/kapture。にあります。実験を容易にするために、オープンソースとしてリリースされたモーションおよびビジュアルローカリゼーションからの構造のための柔軟なデータフォーマットおよび処理パイプラインであるkaptureを紹介します。さらに、このペーパーで使用されているすべてのデータセットをkaptureフォーマットで提供し、研究とデータ処理を容易にします。 
[ABSTRACT]この方法は、正確な姿勢改善のための堅牢な画像検索と堅牢なローカル機能に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-07-27">
        <br><font color="black">2020-07-27</font>
      </time>
    </span>
</section>
<!-- paper0: Transfer Learning for Protein Structure Classification at Low Resolution -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_17.html">
      <font color="black">Transfer Learning for Protein Structure Classification at Low Resolution</font>
    </a>
  </h2>
  <font color="black">入力表現が分類パフォーマンスに及ぼす影響を調査し、側鎖情報がきめの細かい構造予測に必要でない可能性があることを示しています。構造決定は、タンパク質機能を分子レベルで理解するための鍵となります。したがって、低解像度での高速、低コストのタンパク質構造分類の概念、および機能予測への拡張の基礎。 
[ABSTRACT]研究者は、タンパク質の立体構造を視覚化できる必要があります。これは、高価で時間のかかる分析手法に依存しているためです。これにより、構造と機能を予測する方法が説明されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-11">
        <br><font color="black">2020-08-11</font>
      </time>
    </span>
</section>
<!-- paper0: Plug-and-Play Image Restoration with Deep Denoiser Prior -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_18.html">
      <font color="black">Plug-and-Play Image Restoration with Deep Denoiser Prior</font>
    </a>
  </h2>
  <font color="black">次に、さまざまな画像復元の問題を解決するために、モジュラーパーツとして事前にディープデノイザーを半二次分割ベースの反復アルゴリズムにプラグインします。ソースコードはhttps://github.com/cszn/DPIR。で入手できます。パラメータ設定、中間結果、経験的収束の徹底的な分析を提供して、動作メカニズムをよりよく理解します。 
[要約]ノイズ除去器は非常に柔軟なモデルモデルモデルであり、効果的なcnn denoiser.itはプラグアンドプレイの限界を押し上げるために使用されます。また、イメージの復元を使用します。新しいモデルの実験モデルを作成するためにも使用できます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Radar+RGB Attentive Fusion for Robust Object Detection in Autonomous
  Vehicles -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_19.html">
      <font color="black">Radar+RGB Attentive Fusion for Robust Object Detection in Autonomous
  Vehicles</font>
    </a>
  </h2>
  <font color="black">最初に、レーダー情報が特徴抽出ネットワークで融合されます。BIRANetは、NuScenesデータセットで72.3 / 75.3％の平均AP / ARを生成します。これは、基本ネットワークFaster-RCNNの機能ピラミッドネットワーク（FFPN）のパフォーマンスよりも優れています。 RANetは、同じデータセットで平均69.6 / 71.9％のAP / ARを提供します。これは、許容できるパフォーマンスです。 
[要約]提案されたアーキテクチャは、rgbカメラ画像とともにレーダーデータを使用して、堅牢な検出ネットワークを形成することを目的としています。レーダーポイントは、ガイド付きアンカーを生成するために使用されます。これは、ベースネットワークのパフォーマンスよりも高速です-機能ピラミッドネットワークを持つrcnn（ ffpn）</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Structured Graph Learning for Clustering and Semi-supervised
  Classification -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_20.html">
      <font color="black">Structured Graph Learning for Clustering and Semi-supervised
  Classification</font>
    </a>
  </h2>
  <font color="black">クラスタリングと半教師付き分類に関する広範な実験は、提案された方法が他の最先端の方法よりも優れていることを示しています。これの副産物として、グラフ学習とラベル推論は原則的方法で共同かつ反復的に実装されています。理論的にはモデルが特定の条件下でカーネルk-meansメソッドとk-meansメソッドの組み合わせと同等であることを示します。 
[ABSTRACT] 10歳の方が過去に印象的なパフォーマンスを示しました。この方法では、サンプルの自己表現力を使用してグローバル構造をキャプチャします</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Deep Probabilistic Feature-metric Tracking -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_21.html">
      <font color="black">Deep Probabilistic Feature-metric Tracking</font>
    </a>
  </h2>
  <font color="black">最適化ステップは区別可能であり、エンドツーエンドの方法でトレーニングするために展開されています。実験結果は、TUM RGB-Dデータセットと3D剛体オブジェクト追跡データセットの最先端のパフォーマンスを示しています。その確率論的本質により、 、私たちのアプローチは他の残差と簡単に組み合わせることができ、ICPとの組み合わせを示します。 
[ABSTRACT]畳み込みニューラルネットワークによって予測されたピクセル-ワイズディープフィーチャマップとディープフィーチャー-メトリック不確実性マップを学習するための新しいフレームワークを提案します。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Online Spatiotemporal Action Detection and Prediction via Causal
  Representations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_22.html">
      <font color="black">Online Spatiotemporal Action Detection and Prediction via Causal
  Representations</font>
    </a>
  </h2>
  <font color="black">アクションチューブは、時間の経過に伴って接続された一連の境界であり、空間と時間でアクションインスタンスの境界を定めます。この論文では、オンラインおよびリアルタイム処理の観点から問題を理解するビデオアクションに焦点を当てます。後で、オンライン/因果表現が、オフラインの3次元（3D）畳み込みニューラルネットワーク（CNN）と同様のパフォーマンスを、アクション認識、時間的アクションセグメンテーション、早期予測などのさまざまなタスクで達成できることを確認します。 
[要旨]まず、オンラインの時空間アクション検出パイプラインの変換から始めます。このような検出方法の将来の予測機能を調査します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: BIAS: Transparent reporting of biomedical image analysis challenges -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_23.html">
      <font color="black">BIAS: Transparent reporting of biomedical image analysis challenges</font>
    </a>
  </h2>
  <font color="black">BIASステートメントは、適用分野、画像モダリティ、または評価されたタスクカテゴリに関係なく、生物医学画像分析の課題の報告の透明性を向上させることを目的としています。この記事では、BIASステートメントの作成方法について説明し、生物医学画像分析の作成者が作成したチェックリストを示します。チャレンジについては、チャレンジに関する論文をレビューに含める際に提出に含めることをお勧めします。チェックリストの目的は、レビュープロセスを標準化および促進し、関連情報を明確にすることで、チャレンジ結果の解釈可能性と再現性を高めることです。 
[要約]ラボの画像分析の課題（バイアス）イニシアチブにより、課題の報告に関する一連の推奨事項が作成されました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-10-09">
        <br><font color="black">2019-10-09</font>
      </time>
    </span>
</section>
<!-- paper0: Ensemble Transfer Learning for Emergency Landing Field Identification on
  Moderate Resource Heterogeneous Kubernetes Cluster -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_24.html">
      <font color="black">Ensemble Transfer Learning for Emergency Landing Field Identification on
  Moderate Resource Heterogeneous Kubernetes Cluster</font>
    </a>
  </h2>
  <font color="black">ハイパーパラメーター調整は、自作のKubernetesクラスターで実行されます。公開された着陸フィールドが手の届かない範囲にある場合は、緊急着陸フィールドを選択する必要があります。これらのデータレイヤーの最適な構成と、最高のパフォーマンスの転移学習モデルが選択されます。 。 
[ABSTRACT]公開された口径着陸フィールドが手の届かない範囲にない場合、緊急着陸フィールドを選択する必要があります。これは、適切な緊急着陸フィールドのデータベースを使用することが不可欠であるためです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-26">
        <br><font color="black">2020-06-26</font>
      </time>
    </span>
</section>
<!-- paper0: Sentence Guided Temporal Modulation for Dynamic Video Thumbnail
  Generation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_25.html">
      <font color="black">Sentence Guided Temporal Modulation for Dynamic Video Thumbnail
  Generation</font>
    </a>
  </h2>
  <font color="black">このホワイトペーパーでは、ビデオサムネイル生成ネットワークの正規化された時間的活性化を変調するために文章の埋め込みを利用する、文章誘導時間変調（SGTM）メカニズムを提案します。単純で、より多くの並列化を可能にする非反復フレームワークを提案します。大規模なデータセットに対する広範な実験と分析により、フレームワークの有効性が実証されます。 
[ABSTRACT]目標は、動画コンテンツのプレビューを提供するだけでなく、意味的には文を参照する動画サムネイルを作成することです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Initial Classifier Weights Replay for Memoryless Class Incremental
  Learning -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_26.html">
      <font color="black">Initial Classifier Weights Replay for Memoryless Class Incremental
  Learning</font>
    </a>
  </h2>
  <font color="black">ただし、さまざまな状態で学習される分類子の大きさは異なり、すべてのクラスを公平に処理するには正規化が必要です。バニラ微調整バックボーンに基づく別のアプローチを提案します。過去の強力な表現を提供する初期分類子の重みを活用しますクラスはすべてのクラスデータでトレーニングされているためです。 
[ABSTRACT]最も困難な設定では、収集された過去のデータのメモリにアクセスせずに、深いモデルの一定の複雑さと増分モデルの更新が必要です</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Continuous Color Transfer -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_27.html">
      <font color="black">Continuous Color Transfer</font>
    </a>
  </h2>
  <font color="black">最適化には期待値最大化（EM）アルゴリズム（EステップとMステップ）を使用します。ソース画像とサンプル画像の入力が与えられると、この方法では、EMの反復回数を増やしながら、連続的なカラー転送結果を生成できます。特に、転送された画像を混合ガウスモデル（GMM）のサンプル画像と関連付け、転送された画像の色をGMMセントロイドと見なします。 
[ABSTRACT]転送プロセスは、さまざまな問題のため、これまでの課題のままです。これは、以前のセグメンテーションの欠如と以前のセグメンテーションの問題が原因です。代わりに、別の用語を使用して別の情報をより適切に保存</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Evaluating Single Image Dehazing Methods Under Realistic Sunlight Haze -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_28.html">
      <font color="black">Evaluating Single Image Dehazing Methods Under Realistic Sunlight Haze</font>
    </a>
  </h2>
  <font color="black">これは、画像のヘイズ除去方法に新たな課題を提示します。これらの方法を実用的にするには、この問題に対処する必要があります。このホワイトペーパーでは、太陽光によって生成されるヘイズが、最も一般的なヘイズの1つであるため、野生。 
[ABSTRACT]シングルヘイズの曇り除去は、広く研究されているにもかかわらず、課題です。機能するようになると、メソッドに対処する必要があります。これらのメソッドを実用的にするには、この問題に対処する必要があります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Extreme Memorization via Scale of Initialization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_29.html">
      <font color="black">Extreme Memorization via Scale of Initialization</font>
    </a>
  </h2>
  <font color="black">同種のReLUアクティベーションの場合、この動作は損失関数に起因する可能性があることを示します。アーキテクチャやデータ分散の変更など、汎化パフォーマンスに影響を与える他のシナリオでも同様の不整合現象が発生することを示します。私たちの経験的調査により、初期化のスケールを大きくすると、同じクラス内の例間で表現と勾配が次第に不整合になる可能性があることが明らかになりました。 
[要約]汎化能力が影響を受ける範囲と方法は、使用する活性化と損失関数に依存することがわかります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Self-supervised Video Representation Learning by Uncovering
  Spatio-temporal Statistics -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_30.html">
      <font color="black">Self-supervised Video Representation Learning by Uncovering
  Spatio-temporal Statistics</font>
    </a>
  </h2>
  <font color="black">私たちのアプローチは、人間の視覚システムが視野内の急速に変化するコンテンツに敏感であり、視覚的なコンテンツを理解するために大まかな空間位置に関する印象のみが必要であるという観察に触発されています。ソースコードは、https：//で公開されています。 github.com/laura-wang/video_repres_sts ..提案されたアプローチの有効性を検証するために、C3D、3D-ResNet、R（2 + 1）Dなどのいくつかの3Dバックボーンネットワークで大規模な実験を行います。結果は、私たちのアプローチが、アクション認識、ビデオ検索、動的シーン認識、アクション類似性ラベリングなどのさまざまなダウンストリームビデオ分析タスクで、3つのバックボーンネットワーク全体の既存のアプローチよりも優れていることを示しています。 
[要約]ビデオクリップの概念は、一連の統計的要約を提案します。これらには、空間的な位置、最大の動きの支配的な方向、および時間軸に沿った最高の色の多様性の支配的な色が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Integrative Object and Pose to Task Detection for an
  Augmented-Reality-based Human Assistance System using Neural Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_31.html">
      <font color="black">Integrative Object and Pose to Task Detection for an
  Augmented-Reality-based Human Assistance System using Neural Networks</font>
    </a>
  </h2>
  <font color="black">この論文では、コンピュータビジョンタスクにディープニューラルネットワークを組み込んだ複雑な手動タスクで作業者を支援する拡張現実ベースの人間支援システムを提案します。ただし、ARの受け入れと産業プロセスへの統合は、確立された方法の欠如と面倒な統合作業..さらに、私たちの支援システムでユーザーの学習曲線を調査しました。 
[ABSTRACT]拡張現実は、複雑なタスクで労働者を支援する上で大きな可能性を示しています。システムは、ユーザーの理解と空間情報の経験で強化されています。ディープニューラルネットワークは、コンピュータービジョンタスクで驚くべき結果を達成しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Pixel-Face: A Large-Scale, High-Resolution Benchmark for 3D Face
  Reconstruction -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_32.html">
      <font color="black">Pixel-Face: A Large-Scale, High-Resolution Benchmark for 3D Face
  Reconstruction</font>
    </a>
  </h2>
  <font color="black">データセットで既存の3D顔再構成法を慎重にベンチマークします。さらに、各データの正確なランドマークアノテーションと3D登録結果を収集します。コードとデータは、https：//github.com/pixel-face/Pixelで入手できます。 -面。 
[ABSTRACT] pixel-faceは、大規模なアノテーションを備えた大規模で高解像度の多様な3d顔データセットです。各被験者には、さまざまな表情の20を超えるサンプルがあります</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-28">
        <br><font color="black">2020-08-28</font>
      </time>
    </span>
</section>
<!-- paper0: Invisible Backdoor Attacks on Deep Neural Networks via Steganography and
  Regularization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_33.html">
      <font color="black">Invisible Backdoor Attacks on Deep Neural Networks via Steganography and
  Regularization</font>
    </a>
  </h2>
  <font color="black">最後に、提案された見えないバックドア攻撃は、Neural CleanseやTABORなどの最新のトロイの木馬バックドア検出アプローチを効果的に阻止できると主張します。人間の知覚の不可視性の2つの新しい定義を紹介します。 1つは知覚的敵対性類似度スコア（PASS）によって概念化され、もう1つは学習された知覚的イメージパッチ類似性（LPIPS）です。トロイの木馬攻撃の2番目のアプローチは、2種類の追加の正則化用語を使用して、不規則な形状とサイズのトリガーを生成します。 
[ABSTRACT]提案された非表示のバックドアは、さまざまなdnnモデル全体でかなり効果的です。これらには、他のdnnモデルの4つのデータセットと4つのデータセットが含まれます。バックドアは、dnnと人間の検査の両方をだますために使用されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-09-06">
        <br><font color="black">2019-09-06</font>
      </time>
    </span>
</section>
<!-- paper0: Galaxy Morphology Classification using EfficientNet Architectures -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_34.html">
      <font color="black">Galaxy Morphology Classification using EfficientNet Architectures</font>
    </a>
  </h2>
  <font color="black">EfficientNetsは、大規模なSynoptic Space Telescopeなどの大量のデータを提供する将来の光学宇宙調査における大規模な銀河の分類に適用できます。銀河を7つのクラスに分類するためにEfficientNetB5を使用して微調整されたアーキテクチャを提案します-完全に滑らかな丸め、滑らかな、葉巻のような滑らかな、レンチキュラー、縞模様の渦巻き、縞模様のない渦巻き、不規則な中間。ネットワークは、他の一般的な畳み込みネットワークとともに、29,941個の銀河画像を分類するために使用されます。 
[ABSTRACT]効率的なネットを使用して銀河動物園2のテスト画像の結果を予測します。ショーの銀河研究のテスト結果を確認します。結果は火曜日の午後8時に表示されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: DeepFacePencil: Creating Face Images from Freehand Sketches -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_35.html">
      <font color="black">DeepFacePencil: Creating Face Images from Freehand Sketches</font>
    </a>
  </h2>
  <font color="black">この問題に対処するため、トレーニング中に新しいデュアルジェネレーターの画像変換ネットワークに基づいて、手描きのスケッチから写実的な顔の画像を生成できる効果的なツールであるDeepFacePencilを提案します。手描きのスケッチに対する画質とモデルの一般化の両方で既存の方法よりも優れているモデルです。新しい空間的注意プーリング（SAP）は、さまざまなストロークスタイルと詳細の異なるレベルをサポートするために空間的に変化するストロークの歪みを適応的に処理するように設計されています。 。 
[ABSTRACT]作業に加えて、これらのマップは対応する顔の画像のエッジに厳密に合わせて配置されているため、ストロークの多様性が大きく、実際の手描きのスケッチに一般化する能力が制限されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Multimodal Image-to-Image Translation via Mutual Information Estimation
  and Maximization -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_36.html">
      <font color="black">Multimodal Image-to-Image Translation via Mutual Information Estimation
  and Maximization</font>
    </a>
  </h2>
  <font color="black">さらに、私たちの方法は、ソースドメインのコンテンツとターゲットドメインスタイルの間の絡み合いを無料で解消します。このホワイトペーパーでは、潜在コード間の統計的依存を促進するだけで、マルチモーダルな画像から画像への変換を実現できる新しいフレームワークを紹介します。条件付き生成敵対的ネットワークの出力画像。最新の方法と比較して、さまざまなベンチマークの画像から画像への翻訳データセットに対して、監視ありおよび監視なしの設定で実験を行い、マルチモーダルで高品質の結果を実現します。 
[要旨]さまざまなベンチマーク画像からスポットへの方法で、監視ありおよび監視なしの設定で実験を行います。この方法では、両方のソース画像ドメインからターゲット画像ドメインへの片側変換モデルを学習するだけで済みます。教師ありまたは教師なしマルチモーダル画像-から-画像への変換</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-08">
        <br><font color="black">2020-08-08</font>
      </time>
    </span>
</section>
<!-- paper0: VarifocalNet: An IoU-aware Dense Object Detector -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_37.html">
      <font color="black">VarifocalNet: An IoU-aware Dense Object Detector</font>
    </a>
  </h2>
  <font color="black">これら2つの新しいコンポーネントと境界ボックス改良ブランチを組み合わせて、FCOSアーキテクチャ上に新しい高密度オブジェクト検出器を構築します。これは、VarifocalNetまたはVFNetと略して呼ばれます。特に、トレーニング用にVarifocal Lossという新しい損失関数を設計します。 IACSを予測する高密度オブジェクト検出器、およびIACSを推定して粗い境界ボックスを調整するための新しい効率的な星型の境界ボックス機能表現。コードはhttps://github.com/hyz-xmaster/VarifocalNetで入手できます。 
[ABSTRACT]密集物体検出器をトレーニングしてiacsを予測するための、可変焦点損失と呼ばれる新しい損失関数と、iacsを推定するための新しい効率的な星型の境界ボックス機能表現を設計します。この新しい方法を使用して、オブジェクトの検出性能</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Unpaired Learning of Deep Image Denoising -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_38.html">
      <font color="black">Unpaired Learning of Deep Image Denoising</font>
    </a>
  </h2>
  <font color="black">D-BSNと推定ノイズレベルマップの出力が与えられると、ベイズのルールに基づいて、改善されたノイズ除去パフォーマンスをさらに得ることができます。知識の抽出については、最初に学習したノイズモデルを画像に適用して、ペアのトレーニングセットを合成します。画像、および最初の段階で実際のノイズの多い画像と対応するノイズ除去結果を使用して、別のペアのセットを形成します。ノイズの空間的独立性のため、1x1たたみ込み層を積み重ねてネットワークを採用し、各画像のノイズレベルマップを推定します。 
[要約]このペーパーでは、自己管理学習と知識抽出を組み合わせた2段階のスキームを示しました。たとえば、1x1のノイズのあるクリーンな画像を積み重ねてネットワークを採用し、各画像のノイズレベルマップを推定しました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: A Unified Weight Learning and Low-Rank Regression Model for Robust Face
  Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_39.html">
      <font color="black">A Unified Weight Learning and Low-Rank Regression Model for Robust Face
  Recognition</font>
    </a>
  </h2>
  <font color="black">3つの公開顔データベースでの広範な実験結果は、提案されたモデルがエラー分布と構造に非常によく適合し、既存の方法と比較して認識精度が向上することを示しています。効果的な反復最適化が開発され、最適な重み学習と低-rank近似..提案されたモデルは、ランダムノイズと隣接オクルージョンに同時に対処することを可能にします。 
[ABSTRACT]モデルの最も重要な問題は、さまざまな破損や環境の変化によって引き起こされる複雑な表現エラーのフィッティングです</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-10">
        <br><font color="black">2020-05-10</font>
      </time>
    </span>
</section>
<!-- paper0: Learning to Localize Actions from Moments -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_40.html">
      <font color="black">Learning to Localize Actions from Moments</font>
    </a>
  </h2>
  <font color="black">さらに注目すべきは、Kinetics-600のアクションモーメントと、ActivityNet v1.3の200クラスの時間アノテーションを活用して、600カテゴリのアクションをローカライズするようにAherNetをトレーニングすることです。技術的には、重量伝達関数が独自に考案され、分類間の変換を構築アクションモーメントまたはフォアグラウンドのビデオセグメントと、合成コンテキストモーメントまたはトリミングされていないビデオでのアクションのローカリゼーション。特に、そのような設計を1段階のアクションローカリゼーションフレームワークに統合するAction Herald Networks（AherNet）を紹介します。 
[ABSTRACT]アクションのeクラッド動画のコンセプトは完全に監視された方法に発展しました。これらには、生成された機能をトリミングされていない動画の背景の機能と区別するために敵対的なメカニズムを通じて学習される「瞬間」が含まれます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Compensation Tracker: Data Association Method for Lost Object -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_41.html">
      <font color="black">Compensation Tracker: Data Association Method for Lost Object</font>
    </a>
  </h2>
  <font color="black">これは、提案された方法がモデルの追跡パフォーマンスを効果的に改善できることを示しています。実験では、このホワイトペーパーで設計された補正トラッカーを使用した後、評価指標がMOTチャレンジデータセットのさまざまな程度で改善されたことを示しています。特に、マルチ高密度シナリオの2020データセットでは、オブジェクト追跡の精度が66％に達しました。 
[要約]新しい論文は、molmanフィルターと予測修正に基づいた補正トラッカーを開発しました。これは、密な方向のデータセットのデータセットに基づいています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-27">
        <br><font color="black">2020-08-27</font>
      </time>
    </span>
</section>
<!-- paper0: Diverse and Admissible Trajectory Forecasting through Multimodal Context
  Understanding -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_42.html">
      <font color="black">Diverse and Admissible Trajectory Forecasting through Multimodal Context
  Understanding</font>
    </a>
  </h2>
  <font color="black">モデルを2つのパブリックデータセット全体の強力なベースラインおよびアブレーションと比較し、以前の最先端のメソッドよりも大幅にパフォーマンスが向上することを示しています。コードはhttps://github.com/kami93/CMU-DATF ..にあります。現実的な具体化された環境では、各エージェントの将来の軌跡は、目的の目標に到達するために複数の妥当な一連のアクションを使用できるため多様であり、物理的な制約に従い、運転可能な領域に留まる必要があるため、許容可能である必要があります。 
[ABSTRACT]このモデルは、マルチモーダルな世界からの複数の入力信号を分析することを目的としています。これは、公衆が将来の軌跡で事後処理を行うことができたという事実に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-03-06">
        <br><font color="black">2020-03-06</font>
      </time>
    </span>
</section>
<!-- paper0: SoftFlow: Probabilistic Framework for Normalizing Flow on Manifolds -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CV/paper_43.html">
      <font color="black">SoftFlow: Probabilistic Framework for Normalizing Flow on Manifolds</font>
    </a>
  </h2>
  <font color="black">フローベースの生成モデルは、同じ次元の2つの確率変数間の可逆変換で構成されます。SoftFlowは、従来のフローベースのモデルとは異なり、多様体データの生来の構造をキャプチャし、高品質のサンプルを生成できることを実験的に示しています。 、フローベースのモデルは、データ分布のディメンションが基になるターゲット分布のディメンションと一致しない場合、適切にトレーニングできません。 
[ABSTRACT] softflowは、データ分布を直接学習するのではなく、条件付きフレームワークを推定します。提案されたフレームワークは、フローベースのモデルの変数を形成する難しさを軽減するように設計されています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-06-08">
        <br><font color="black">2020-06-08</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label3"/>
    <div class="hidden_show">
<!-- paper0: Detecting Generic Music Features with Single Layer Feedforward Network
  using Unsupervised Hebbian Computation -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CL/paper_0.html">
      <font color="black">Detecting Generic Music Features with Single Layer Feedforward Network
  using Unsupervised Hebbian Computation</font>
    </a>
  </h2>
  <font color="black">著者らは、パターンとしての音楽機能学習のトレーニングにおいて、そのようなアルゴリズムが単層フィードフォワードネットワークをどのように支援できるかをシミュレートする詳細な経験的調査結果を示しています。検出..類似のタスクに対する比較分析のために、著者は以前のいくつかのベンチマーク作品と同様の結果を出します。 
[ABSTRACT]著者は、人気のある音楽コーパスからそのような機能に関する情報を抽出します。また、同じデータセットを使用して、単層ニューラルネットワークに教師なしヘブ学習手法を適用することにより、新しい認識手法を探りました</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: SemEval-2020 Task 6: Definition extraction from free text with the DEFT
  corpus -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CL/paper_1.html">
      <font color="black">SemEval-2020 Task 6: Definition extraction from free text with the DEFT
  corpus</font>
    </a>
  </h2>
  <font color="black">フリーテキストの定義と光沢は、多くの場合、明示的なインジケータなしで、文の境界を越えて、またはその他の複雑な言語の方法で表示されます。DeftEvalには、1）文の分類、2）シーケンスのラベル付け、および3）関係抽出が含まれます。定義の抽出は10年以上にわたって行われてきましたが、主に考慮される定義のタイプに大きな制約がありました。 
[ABSTRACT] deftevalは、参加者が用語を使用してフリーテキストから定義を抽出する必要がある、セメバルの共有タスクです。自然言語での定義の複雑な現実を反映する定義ペアコーパス</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Classifier Combination Approach for Question Classification for Bengali
  Question Answering System -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CL/paper_2.html">
      <font color="black">Classifier Combination Approach for Question Classification for Bengali
  Question Answering System</font>
    </a>
  </h2>
  <font color="black">ここで紹介するアプローチは、他のインドアーリア語やインド語の言語で使用して、質問応答システムを開発できます。1層および2層の分類法の両方で実験を行いました。全体的に、スタッキングアプローチは、きめの細かい分類で、87.79 \％の精度を実現します。 
[要約]複数のモデルの組み合わせにより、ベンガル語の質問アンサンブルタスクの既存の個々のモデルで得られるものよりも優れた分類パフォーマンスが得られます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Multi-task Learning for Low-resource Second Language Acquisition
  Modeling -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CL/paper_3.html">
      <font color="black">Multi-task Learning for Low-resource Second Language Acquisition
  Modeling</font>
    </a>
  </h2>
  <font color="black">広範な実験は、提案された方法が低リソースシナリオで最先端のベースラインよりもはるかに優れていることを示しています。一方、非リソースシナリオでもわずかに改善が得られます。このアイデアに触発され、この論文では、マルチタスク学習によって異なる言語学習データセット間の潜在的な共通パターンを学習し、さらに、低リソースシナリオでの予測パフォーマンスの向上に適用される、新しいSLAモデリング手法を提案します。 
[ABSTRACT]言語言語言語-学習システムは新しい方法で使用できます。提案された方法は、低水準のリソースシナリオにおける最新のベースラインよりもはるかに優れたパフォーマンスを発揮します</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2019-08-25">
        <br><font color="black">2019-08-25</font>
      </time>
    </span>
</section>
<!-- paper0: Discovering Bilingual Lexicons in Polyglot Word Embeddings -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CL/paper_4.html">
      <font color="black">Discovering Bilingual Lexicons in Polyglot Word Embeddings</font>
    </a>
  </h2>
  <font color="black">この作業では、多言語コーパスでトレーニングされた単一のスキップグラムモデルを利用して、ポリグロット単語の埋め込みを生成し、この埋め込みスペースで驚くほど単純な制約付き最近傍サンプリング手法を使用して、厳しい社会でもバイリンガル語彙を取得できるという新しい発見を示します。メディアデータセットは主に英語とローマ字ヒンディー語で記述され、多くの場合、コードの切り替えを示します。バイリンガルの語彙集とフレーズテーブルは、現代の機械翻訳システムにとって重要なリソースです。さらに、3つのヨーロッパ言語のペア全体で、多言語の単語の埋め込みが実際に豊富な単語の意味表現と実質的なバイリンガル辞書は、制約付き最近傍サンプリングを使用して取得できます。 
[ABSTRACT]シードレキシコンがなければ、最新のレキシコンは教師なしの方法を使用して学習できます。これらの方法は、大きくてきれいな単一言語の単語の存在に依存しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: A Bidirectional Tree Tagging Scheme for Jointly Extracting Overlapping
  Entities and Relations -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CL/paper_5.html">
      <font color="black">A Bidirectional Tree Tagging Scheme for Jointly Extracting Overlapping
  Entities and Relations</font>
    </a>
  </h2>
  <font color="black">エンコーダーモジュールとして、Bi-LSTMレイヤーと事前トレーニング済みのBERTエンコーダーをそれぞれ採用し、中国語のデータセットだけでなく、英語のパブリックデータセットでも有望な結果を取得します。BiTTスキームに基づいて、 BiTTタグを予測するための分類フレームワークを終了します。文では、同じ関連カテゴリを持つトリプルは特に2つのバイナリツリーとして表され、それぞれが単語レベルのタグシーケンスに変換されて各単語にラベルが付けられます。 
[ABSTRACT]新しい研究では、テキスト内の重複トリプルにラベルを付けるための新しい双方向ツリータギング（bitt）スキームを提案しています。プロジェクトでは、ビットスキームに基づいて、ビットタグを予測するエンドツーエンドの分類フレームワークを開発しています。</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-31">
        <br><font color="black">2020-08-31</font>
      </time>
    </span>
</section>
<!-- paper0: Characterizing COVID-19 Misinformation Communities Using a Novel Twitter
  Dataset -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/cs.CL/paper_6.html">
      <font color="black">Characterizing COVID-19 Misinformation Communities Using a Novel Twitter
  Dataset</font>
    </a>
  </h2>
  <font color="black">最後に、私たちの社会言語学的分析は、COVID-19に通じたユーザーは誤解したユーザーよりも多くのナラティブを使用する傾向があることを示唆しています。この研究の目的は2つあります。有意義な分析を行うための研究コミュニティによる。 （ii）2つのターゲットコミュニティを、ネットワーク構造、言語パターン、および他のコミュニティへのメンバーシップの観点から特徴づけます。陰謀理論から偽の治療法および偽の治療法まで、COVID-19は、感染拡大の温床となっています。オンラインの誤った情報。 
[要約]研究者はオンラインで誤った情報をデバンキングして修正しようとしています。この研究の目的は、誤った情報のデータを拡散することです。これらは、注釈付きcovid-19のTwitterデータセットの分析に基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-03">
        <br><font color="black">2020-08-03</font>
      </time>
    </span>
</section>
</div>
  <div class="hidden_box">
    <input type="checkbox" id="label4"/>
    <div class="hidden_show">
<!-- paper0: Approximal operator with application to audio inpainting -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/eess.AS/paper_0.html">
      <font color="black">Approximal operator with application to audio inpainting</font>
    </a>
  </h2>
  <font color="black">ただし、それらのマッピングは不適切に正当化されています。時間周波数表現の最近の評価とオーディオ修復への構造化スパースアプローチでは、LiebとStark（2018）は特定のマッピングを近接演算子として使用しています。この演算子は基本的な部分として機能します。反復数値ソルバーの。 
[ABSTRACT]現在の記事は、それらのマッピングが実際に近位演算子であることを証明し、適切な対応物も導出しています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-04">
        <br><font color="black">2020-05-04</font>
      </time>
    </span>
</section>
<!-- paper0: Neural Granular Sound Synthesis -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/eess.AS/paper_1.html">
      <font color="black">Neural Granular Sound Synthesis</font>
    </a>
  </h2>
  <font color="black">また、元の粒子が合成用に保存されていないことも意味します。音声記述子の基礎を、変分オートエンコーダーで学習した確率的潜在空間に効率的に置き換えます。このモデルは、ピッチのあるノートやピッチなしのライブラリなど、さまざまなタイプのライブラリに適用できます。ドラムと環境騒音。 
[ABSTRACT]グラニュラーシンセシスは、サウンドを制御するために使用するようにプログラムできます。しかし、このグレインスペースの品質は、ディスクリプターの品質によって制限されます</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-04">
        <br><font color="black">2020-08-04</font>
      </time>
    </span>
</section>
<!-- paper0: AutoSpeech: Neural Architecture Search for Speaker Recognition -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/eess.AS/paper_2.html">
      <font color="black">AutoSpeech: Neural Architecture Search for Speaker Recognition</font>
    </a>
  </h2>
  <font color="black">結果は、提案されたアプローチから派生したCNNアーキテクチャが、VGG-M、ResNet-18、およびResNet-34バックボーンに基づく現在の話者認識システムを大幅に上回る一方で、モデルの複雑さを低減していることを示しています。最終的な話者認識モデルを取得できます。提案されたアプローチを評価するために、VoxCeleb1データセットを使用して話者識別タスクと話者検証タスクの両方について実験を行います。 
[要約]スピーカーの構築とスピーカーの検証の両方で実験を行います。このモデルは、画像分類用に最初に提案されたバックボーンに基づいています</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-05-07">
        <br><font color="black">2020-05-07</font>
      </time>
    </span>
</section>
<!-- paper0: Non-Parallel Voice Conversion with Augmented Classifier Star Generative
  Adversarial Networks -->
<section>
  <h2 class="entry-title" itemprop="headline">
    <a href="../../list/2020-09-01/eess.AS/paper_3.html">
      <font color="black">Non-Parallel Voice Conversion with Augmented Classifier Star Generative
  Adversarial Networks</font>
    </a>
  </h2>
  <font color="black">この論文では、StarGANと呼ばれる生成的敵対的ネットワーク（GAN）のバリアントを使用して、非並列マルチドメイン音声変換（VC）を可能にする方法を提案します。次に、単一のジェネレーターネットワークを使用して複数のドメインにまたがるマッピングを同時に学習できます。異なるドメイン間で共有できる共通の潜在的な特徴をキャプチャすることにより、複数のドメインから収集された利用可能なトレーニングデータを十分に活用できること。3番目に、リアルタイムの実装を可能にするのに十分な速さで変換された音声信号を生成でき、数分しか必要ありません。適度にリアルな音声を生成するためのトレーニング例。 
[要約] stargan-vcメソッドでは、音声ジェネレータのトレーニングに、並列発話、翻訳、または時間調整手順は必要ありません</font>
    <span class="entry-meta">
      <time itemprop="datePublished" datetime="2020-08-27">
        <br><font color="black">2020-08-27</font>
      </time>
    </span>
</section>
</div>
</main>
	<!-- Footer -->
	<footer role="contentinfo" class="footer">
		<div class="container">
			<hr>
				<div align="center">
					<font color="black">Copyright
							&#064;Akari All rights reserved.</font>
					<a href="https://twitter.com/akari39203162">
						<img src="../../images/twitter.png" hight="128px" width="128px">
					</a>
	  			<a href="https://www.miraimatrix.com/">
	  				<img src="../../images/mirai.png" hight="128px" width="128px">
	  			</a>
	  		</div>
		</div>
		</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>
